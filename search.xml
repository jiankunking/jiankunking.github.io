<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SQL union vs or 性能]]></title>
    <url>%2Fsql-performance-union-vs-or.html</url>
    <content type="text"><![CDATA[本文整理自：stackoverflow地址：https://stackoverflow.com/questions/13750475/sql-performance-union-vs-or 翻译自Bill Karwin回答： 要么你读的那篇文章用了一个不好的例子，要么你误解了他们的观点。 1select username from users where company = &apos;bbc&apos; or company = &apos;itv&apos;; 等价于： 1select username from users where company IN (&apos;bbc&apos;, &apos;itv&apos;); 在这个查询中MySQL会使用company上的索引。不需要改成UNION。 更棘手的情况是，OR条件涉及两个不同的列。 1select username from users where company = &apos;bbc&apos; or city = &apos;London&apos;; 假设company列和city列都有一个独立的索引。MySQL通常在一个给定的查询中每个表只使用一个索引，那么应该使用哪个索引呢?如果它使用company上的索引，它仍然必须执行表扫描，以找到伦敦所在的行。如果它使用city上的索引，则必须对company为bbc的行进行表扫描。 UNION 解决方案适用于这种情况。 123select username from users where company = &apos;bbc&apos; unionselect username from users where city = &apos;London&apos;; 这样的话每个子查询都可以使用索引进行搜索,然后再将子查询的结果合并在一起。 union ? union all ? 取决于需求及where过滤后的数据量对于索引列来最好使用union all，因复杂的查询【包含运算等】将使or、in放弃索引而全表扫描，除非你能确定or、in会使用索引。对于只有非索引字段来说你就老老实实的用or或者in，因为 非索引字段本来要全表扫描而union all 只成倍增加表扫描的次数。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>UNION</tag>
        <tag>OR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时间量级]]></title>
    <url>%2Ftime-scales.html</url>
    <content type="text"><![CDATA[本文整理自：《性能之巅》作者：【美】 Brendan Gregg英文版出版时间：2014年 我们可以用数字来作为时间的比较方法,同时可以用时间的长短经验来判断延时的源头。系统各组件的操作所处的时间量级差别巨大,大到了难以体会的地步。表2.2提供的延时示例,从访问3.3GHz的CPU寄存器的延时开始,阐释了我们所打交道的时间量级的差别,表中是发生单次操作的时间均值,等比放大成为想象的系统,一次寄存器访问0.3ns(十亿分之一秒的三分之一)相当于现实生活中的1秒。 表2.2 系统的各种延时 事件 延时 相对时间比例 1个CPU周期 0.3ns 1s L1缓存访问 0.9ns 3s L2缓存访问 2.8ns 9s L3缓存访问 12.9ns 43s 主存访问(从CPU访问DRAM) 120ns 6分钟 固态硬盘I/O(闪存) 50-150us 2-6天 旋转磁盘I/O 1-10 ms 1-12月 互联网:从旧金山到纽约 40 ms 4年 互联网:从旧金山到英国 81 ms 8年 互联网:从旧金山到澳大利亚 183ms 19年 TCP包重传 1-3s 105-317年 OS虚拟化系统重启 4s 423年 SCSI命令超时 30s 3千年 硬件虚拟化系统重启 40s 4千年 物理系统重启 5m 32千年 这个表需要时刻记在心中。]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>Performance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[译]ZGC: 未使用堆内存归还操作系统]]></title>
    <url>%2Fzgc-uncommit-unused-memory.html</url>
    <content type="text"><![CDATA[翻译自：JEP 351地址：https://openjdk.java.net/jeps/351 一、摘要增强ZGC，将未使用的堆内存返回给操作系统。 二、动机目前ZGC不会将未使用的内存归还给操作系统，即使该内存已经很长时间没有使用了。这种行为并不适合所有类型的应用程序和环境，特别是那些需要考虑内存占用的应用程序和环境。例如： 按资源使用付费的容器环境。 应用程序可能长时间处于空闲状态并与许多其他应用程序共享或竞争资源的环境。 应用程序在执行期间可能有非常不同的堆空间需求。例如，启动期间所需的堆可能大于稍后在稳定状态执行期间所需的堆。 HotSpot中的其他垃圾收集器(如G1和Shenandoah)已经提供了这种功能，该功能对于一些用户非常有用。将此功能添加到ZGC将受到这些用户的欢迎。 三、描述ZGC堆由一组称为ZPages的堆区域组成。每个Zpage与数量可变的已提交内存相关联。当ZGC压缩堆时，ZPages被释放并插入到页面缓存ZPageCache中。页面缓存中的ZPages可以重用，以满足新的堆分配，在这种情况下，它们将从缓存中删除。页面缓存对性能至关重要，因为提交和不提交内存都是昂贵的操作。 页面缓存中的ZPages集合表示堆中未使用的部分，这些部分可以归还给操作系统。因此，取消提交内存可以通过简单地从页面缓存中删除一组精心选择的ZPages，并取消与这些页面关联的内存的提交来完成。页面缓存已经将ZPages保持在最近最少使用(LRU)的顺序，并按大小(小、中、大)进行分隔，因此清除ZPages和取消提交内存的机制相对简单。挑战在于设计策略来决定何时从缓存中驱逐ZPage。 一个简单的策略是设置一个timeout或delay值，该值指定ZPage在被清除之前可以在页面缓存中驻留多长时间。这个超时将有一些合理的默认值，可以使用命令行选项覆盖它。Shenandoah GC使用这样的策略，默认值为5分钟，命令行选项-XX:ShenandoahUncommitDelay=&lt;milliseconds&gt;来覆盖默认值。 类似上述策略的效果可能相当不错。然而，人们也可以设想更复杂的策略，不涉及添加新的命令行选项。例如，根据GC频率或其他数据找到合适超时值的启发式方法。我们将首先提供一个简单的超时策略，使用-XX:ZUncommitDelay=&lt;seconds&gt;选项，稍后再提供一个更复杂的策略(如果找到了)。 默认情况下将启用uncommit功能。但是无论策略如何决定，ZGC都不能把堆内存降到低于Xms。这就意味着，如果Xmx和Xms相等的话，这个能力就失效了，-XX:-ZUncommit这个参数也能让这个内存管理能力失效。 最后，Linux/x64上的ZGC使用tmpfs或hugetlbfs文件来支持堆。这些文件使用的未提交内存需要fallocate(2)和FALLOC_FL_PUNCH_HOLE支持，FALLOC_FL_PUNCH_HOLE支持最早出现在Linux 3.5 (tmpfs)和4.3(hugetlbfs)中。在旧的Linux内核上运行时，ZGC应该像以前一样继续工作，但是禁用了uncommit功能。 Java 越来越云原生了，下一个期盼就是fiber了。]]></content>
      <categories>
        <category>GC</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>Java</tag>
        <tag>GC</tag>
        <tag>ZGC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何用Linux命令行管理网络：11个你必须知道的命令]]></title>
    <url>%2Fhow-to-work-with-network-from-linux-terminal.html</url>
    <content type="text"><![CDATA[如何用Linux命令行管理网络：11个你必须知道的命令 无论你是要下载文件、诊断网络问题、管理网络接口，还是查看网络的统计数据，都有终端命令可以来完成。这篇文章收集了久经考验靠谱的命令，也收集了几个比较新的命令。 多数命令都可以在图形桌面执行，即使是没什么终端使用经验的Linux用户也会常常执行命令来使用ping或是其它的网络诊断工具。 curl &amp; wget使用curl或wget命令，不用离开终端就可以下载文件。如你用curl，键入curl -O后面跟一个文件路径。wget则不需要任何选项。下载的文件在当前目录。 12curl -O website.com/filewget website.com/file pingping发送ECHO_REQUEST包到你指定的地址。这样你可以很方便确认你的电脑和Internet或是一个指定的IP地址是不是通的。使用-c开关，可以指定发送ECHO_REQUEST包的个数。 1ping -c 4 google.com tracepath &amp; traceroutetracepath命令和traceroute命令功能类似，但不需要root权限。并且Ubuntu预装了这个命令，traceroute命令没有预装的。tracepath追踪出到指定的目的地址的网络路径，并给出在路径上的每一跳（hop）。如果你的网络有问题或是慢了，tracepath可以查出网络在哪里断了或是慢了。 1tracepath example.com mtrmtr命令把ping命令和tracepath命令合成了一个。mtr会持续发包，并显示每一跳ping所用的时间。也会显示过程中的任何问题，在下面的示例中，可以看到在第6跳丢了超过20%的包。 1mtr howtogeek.com 键入q或是CTRL + C来退出命令。 hosthost命令用来做DNS查询。如果命令参数是域名，命令会输出关联的IP；如果命令参数是IP，命令则输出关联的域名。 12host howtogeek.comhost 208.43.115.82 whoiswhois命令输出指定站点的whois记录，可以查看到更多如谁注册和持有这个站点这样的信息。 1whois example.com ifplugstatusifplugstatus命令可以告诉你是否有网线插到在网络接口上。这个命令Ubuntu没有预装，通过下面的命令来安装： 1sudo apt-get install ifplugd 这个命令可以查看所有网络接口的状态，或是指定网络接口的状态： 12ifplugstatusifplugstatus eth0 命令输出『Link beat detected』（检测到连接心跳）表示有网线插着，如没有则会输出『unplugged』（未插入）。 ifconfigifconfig用于输出网络接口配置、调优和Debug的各种选项。可以快捷地查看IP地址和其它网络接口的信息。键入ifconfig查看所有启用的网络接口的状态，包括它们的名字。可以指定网络接口的名字来只显示这一个接口的信息。 12ifconfigifconfig eth0 ifdown &amp; ifupifdown和ifup命令和运行ifconfig up，ifconfig down的功能一样。给定网络接口的名字可以只禁用或启用这一个接口。需要root权限，所以在Ubuntu上需要使用sudo来运行。 12sudo ifdown eth0sudo ifup eth0 在Linux桌面系统上运行这2个命令，很可能会输出出错信息。Linux桌面通过使用网络管理器（NetworkManager）来管理你的网络接口。不过在没有安装网络管理器的服务器版上，这2个命令仍然可用。 如果确实要在命令行上配置网络管理器，用nmcli命令。 dhclientdhclient命令可以释放你的电脑的IP地址并从DHCP服务器上获得一个新的。需要root权限，所以在Ubuntu上需要sudo。无选项运行命令获取新IP，或指定-r开关来释放当前的IP地址。 12sudo dhclient -rsudo dhclient netstatnetstat命令可以显示网络接口的很多统计信息，包括打开的socket和路由表。无选项运行命令显示打开的socket。 这条命令还有很多功能。比如，netstat -p命令可以显示打开的socket对应的程序。 netstat -s则显示所有端口的详细统计信息。 原文https://github.com/oldratlee/translations/blob/master/how-to-work-with-network-from-linux-terminal/README.md]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾回收算法手册：自动内存管理的艺术 笔记]]></title>
    <url>%2Fthe-garbage-collection-handbook-the-art-of-automatic-memory-management-note.html</url>
    <content type="text"><![CDATA[本文整理自：《垃圾回收算法手册：自动内存管理的艺术》作者：Richard Jones、Antony Hosking、Eliot Moss 引言几乎所有的现代编程语言都使用动态内存分配(allocation),即允许进程在运行时分配或者释放无法在编译期确定大小的对象,且允许对象的存活时间超出创建这些对象的子程序时间。动态分配的对象存在于堆(heap)中而非栈(stack)或者静态区(statically)中。所谓栈,即程序的活动记录(activation record)或者栈帧(stack frame);静态区则是指在编译期或者链接期就可以确定范围的存储区域。堆分配是十分重要的功能,它允许开发者: 在运行时动态地确定新创建对象的大小(从而避免程序在运行时遭遇硬编码数组长度不足产生的失败)。 定义和使用具有递归特征的数据结构,例如链表(list)、树(tree)和映射(map)。 向父过程返回新创建的对象,例如工厂方法。 将一个函数作为另一个函数的返回值,例如函数式语言中的闭包(closure)或者悬挂(suspension)。 在不支持自动化动态内存管理的语言中,众多研究者已经付出了相当大的努力来解决这一难题,其方法主要是管理对象的所有权(ownership)[Belotsky,2003; Cline, Lomo,1995]。Belotsky[2003]等人针对C++提出了几个可行策略: 第一,开发者在任何情况下都应当避免堆分配,例如可以将对象分配在栈上,当创建对象的函数返回之后,栈的弹出(pop)操作会自动将对象释放。 第二,在传递参数与返回值时,应尽量传值而非引用。尽管这些方法避免了分配、释放错误,但其不仅会造成内存方面的压力,而且失去了对象共享的能力。 另外,开发者也可以在一些特殊场景下使用自定义内存分配器,例如对象池(pool of object),在程序的一个阶段完成之后,池中的对象将作为一个整体全部释放。 垃圾算法之间的比较安全性垃圾回收器首先要考虑的因素是安全性(safety),即在任何时候都不能回收存活对象。但安全性是需要付出一定代价的,特别是在并发回收器中。 吞吐量对程序的最终用户而言,程序当然是运行得越快越好,但这是由几方面因素决定的。其中的一方面便是花费在垃圾回收上的时间应当越少越好,文献中通常用标记/构造率(mark/cons ratio)衡量这一指标。这一概念是在早期的Lisp语言中最先提出的,它表示回收器(对存活对象进行标记)与赋值器(mutator)(创建或者构造新的链表单元)活跃度的比值。然而在大多数设计良好的架构中,赋值器会比回收器占用更多的CPU时间,因此在适当牺牲回收器效率的基础上提升赋值器的吞吐量,并进一步提升整个程序(赋值器+回收器)的执行速度,一般来说是值得的。例如,使用标记一清扫回收的系统偶尔会执行存活对象整理以减少内存碎片,虽然这一操作开销较大,但它可以提升赋值器的分配性能。 完整性与及时性理想情况下,垃圾回收过程应当是完整的,即堆中的所有垃圾最终都应当得到回收,但这通常是不现实的,甚至是不可取的,例如纯粹的引用计数回收器便无法回收环状引用垃圾(自引用结构)。从性能方面考虑,在一次回收过程(collection cycle)中只处理堆中部分对象或许更加合理,例如分代回收器会依照堆中对象的年龄将其划分为两代或者更多代,并把回收的主要精力集中在年轻代,这样不仅可以提高回收效率,而且可以减少单次回收的平均停顿时间(pause time)。 在并发垃圾回收器中,赋值器与回收器同时工作,其目的在于避免或者尽量减少用户程序的停顿。此类回收器会遇到浮动垃圾(floating garbage)问题,即如果某个对象在回收过程启动之后才变成垃圾,那么该对象只能在下一个回收周期内得到回收。因此在并发回收器中,衡量完整性更好的方法是统计所有垃圾的最终回收情况,而不是单个回收周期的回收情况。不同的回收算法在回收及时性(promptness)方面存在较大差异,进而需要在时间和空间上进行权衡。 停顿时间许多回收器在进行垃圾回收时需要中断赋值器线程,因此会导致在程序执行过程中出现停顿。回收器应当尽量减少对程序主要执行过程的影响,因此要求停顿时间越短越好,这点对于交互式程序或者事务处理服务器(超时将引发事务的重试,进而导致事务的积压)尤为重要。但正如我们在后面章节中将要看到的,限制停顿时间会带来一些副作用。例如,分代式回收器通过频繁且快速地回收较小的、较为年轻的对象来缩短停顿时间,而对较大的、较为年老对象的回收则只是偶尔进行。显然,在对分代回收器进行调优时,需要平衡不同分代的大小,进而才能平衡不同分代之间的停顿时间与回收频率。但由于分代回收器必须记录些分代间指针的来源,因此赋值器的指针写操作会存在少量的额外开销。 并行回收器(parallel collector)虽然也需要停顿整个程序,但它可以通过多线程回收的策略缩短停顿时间。为进一步减少停顿时间,并发回收器与增量回收器(incremental collector)偶尔会将部分回收工作与赋值器动作交替进行或者同时进行,但这一过程需要确保赋值器与回收器之间的同步,因而增大了赋值器的额外开销。回收机制的选择会影响程序在空间和时间两个方面的开销,也会影响垃圾回收周期的结束。赋值器在时间方面的额外开销取决于需要记录的赋值器操作类型(读或者写)及其如何记录。回收器在空间方面的开销以及回收周期的结束取决于系统可以容忍的浮动垃圾数量。多线程赋值器与回收器会增大设计复杂度。不论如何,缩短停顿时间的措施通常会增大整体处理时间(即降低整体处理速度)。 仅对最大或者平均停顿时间进行度量是不够的,必须要同时考虑赋值器的性能,因此停顿时间的分布也值得关注。 空间开销内存管理的目的是安全且高效地使用内存空间。不论是显式内存管理还是自动内存管理,不同的管理策略均会产生不同程度的空间开销(space overhead)。某些垃圾回收器需要在每个对象内部占用一定的空间(例如保存引用计数),还有一些回收器会复用对象现有布局上已经存在的域(例如将标记位放在对象头部的某个字中,或者将转发指针(forwarding pointer)记录在用户数据上)。回收器也可能会引入堆级别的空间开销,例如复制式回收器需要将堆分为两个半区,任何时候赋值器只能使用一个半区,另一个半区会被回收器保留,并在回收过程中将存活对象复制到其中。回收器也可能需要一些辅助的数据结构,例如追踪式回收器需要通过标记栈来引导堆中指针图表的遍历,回收器在标记对象时也可以使用额外的位图(bitmap)而非对象中的域;对于并发回收器或者其他需要将堆划分为数个独立区域的回收器,其需要额外的记忆集(remembered set)来保存赋值器所修改的指针值或者跨区域指针的位置。 针对特定语言的优化垃圾回收算法可以根据它们所服务的不同语言范式来归类。在函数式语言中,内存管理有着很大的优化空间。某些语言(例如ML)将可变数据与不可变数据进行区分,纯函数式语言(例如Haskell)则更为极端,它不允许用户改变任何数据,即程序是透明引用(referentially transparent)的。然而,在函数式语言内部,数据结构的更新一般不超过一次,即从待计算值(thunk)到一个弱头部范式(weak head normal form,WHNF),分代垃圾回收器可以据此尽快提升已经完成计算的数据结构。研究者们还提出了基于引用计数来处理环状数据结构的完整机制。声明式语言(declarative language)或许还可以使用其他策略来提升堆空间管理的效率,即如果某一对象创建于一个“选择点”(choice point)之后,那么当程序再次回到该选择点时,该对象将不可达,如果对象在堆中的布局是按照其分配时间排布的,那么某个选择点之后分配的内存可以在一个固定的时间内全部回收。不同种类的语言可能对回收器具有不同的要求,最显著的差异是语言中指针功能的不同,以及回收器调用对象终结的需求不同。 所谓透明引用,即以相同的参数调用同一个函数两次,所得到的结果总是相同的,也可理解为函数没有副作用。thunk和WHNF均为函数式语言中的与懒情计算相关的概念。 可扩展性与可移植性可扩展性(scalability)与可移植性(portability)是我们定义的最后两个指标。随着PC,甚至笔记本计算机中(且不说大型服务器)多核硬件的普及,借助硬件的并行优势来提升垃圾回收的性能将变得越来越重要。我们期待并行硬件在规模上(内核与套接字数量上)能有进一步发展,也希望异构处理器(heterogeneous processer)越来越普遍。在服务器方面,堆的大小可以达到数十甚至数百吉字节,事务型负载也越来越多,这些都给垃圾回收带来更多的要求。很多垃圾回收算法需要操作系统或者硬件的支持(例如需要依赖页保护机制,需要对虚拟内存空间进行二次映射,或者要求处理器能够提供特定的原子操作),但这些技术并不需要很强的可移植性。 性能上的劣势与显式内存管理相比,自动内存管理是否存在性能上的劣势?我们将通过对这一问题的分析,来对两者的优劣进行总结。一般来说,自动内存管理的运行开销很大程度上取决于程序的行为,甚至硬件条件,因而很难对其进行简单评估。一个长期以来的观点是,垃圾回收通常会在总内存吞吐量以及垃圾回收停顿时间方面引人一些不可接受的开销,从而导致应用程序的执行速度慢于显式内存管理策略。自动内存管理确实会牺牲程序的部分性能,但是远不如想象中那样严重。诸如ma1loc和free等显式内存操作也会带来一些显著开销。Herts, Feng, Herger[2005测量了多种Java基准测试程序和回收算法花费在垃圾回收上的真正开销。他们构建了一个Java虚拟机并用其精确地观察到对象何时不可达,同时使用可达追踪的方法驱动模拟器来测量回收周期与高速缓存不命中(cache miss)的情况。他们将许多不同种类的垃圾回收器配置与各种不同的ma1lo/free实现进行比较,比较的方法是:如果追踪发现某一对象变成垃圾,则调用free将其释放。 Herts等人发现,尽管用这两种方式的测量结果差异较大,但是如果堆足够大(达到所需最小空间的5倍),那么垃圾回收器的执行时间性能将可以与显式分配相匹敌,但对于一般大小的堆,垃圾回收的开销会平均增大17%。 实验方法内存管理涉及时间和空间两方面的权衡。大多数环境下,降低回收停顿时间的一种方法是增大堆的空间(增大到一定程度后将达到最优,但如果再增大,则由于局部性原理,执行时间将会变长)。 术语和符号首先需要说明的是存储的单位。我们遵循一个字节包含八个位这一惯例。我们简单地使用KB(kilobyte)、MB(megabyte)、GB(gigabyte)、TB(terabyte)来描述对应的2的整数次幂内存单元(分别是20、20、20、20),而不使用SI数字前缀的标准定义。 赋值器与回收器对于使用垃圾回收的程序, Dijkstra等[1976、1978]将其执行过程划分为两个半独立的部分: 赋值器执行应用代码。这一过程会分配新的对象,并且修改对象之间的引用关系,进而改变堆中对象图的拓扑结构,引用域可能是堆中对象,也可能是根,例如静态变量、线程栈等。随着引用关系的不断变更,部分对象会失去与根的联系,即从根出发沿着对象图的任何一条边进行遍历都无法到达该对象。 回收器(collector)执行垃圾回收代码,即找到不可达对象并将其回收。 一个程序可能拥有多个赋值器线程,但是它们共用同一个堆。相应的,也可能存在多个回收器线程。 分配器分配器(allocator)与回收器在功能上是正交关系。分配器支持两种操作:分配(allocate)和释放(free)。分配是为某一对象保留底层的内存存储,释放是将内存归还给分配器以便复用。分配存储空间的大小是由一个可选参数来控制的,如果我们在伪代码中忽略这一参数,意味着分配器将返回一个固定大小的对象,或者对象大小对于算法的理解并非必要。分配操作也可能支持更多参数,例如将数组的分配与单个对象的分配进行区分,或者将指针数组的分配和不包含指针的数组进行区分,或者包含其他一些必要信息以便初始化对象头部。 标记-清扫回收标记一清扫算法是一种间接回收(indirect collection)算法,它并非直接检测垃圾本身而是先确定所有存活对象,然后反过来判定其他对象都是垃圾。需要注意的是,该算法的每次调用都需要重新计算存活对象集合,但并非所有的垃圾回收算法都需要如此。 三色抽象三色抽象(tricolour abstraction)[Dijkstra等,1976,1978]可以简洁地描述回收过程中对象状态的变化(是否已被标记、是否在工作列表中等)。三色抽象是描述追踪式回收器的种十分有用的方法,利用它可以推演回收器的正确性,这正是回收器必须保证的。在三色抽象中,回收器将对象图划分为黑色对象(确定存活)和白色对象(可能死亡)。任意对象在初始状态下均为白色,当回收器初次扫描到某一对象时将其着为灰色,当完成该对象的扫描并找到其所有子节点之后,回收器会将其着为黑色。从概念上讲,黑色意味着已经被回收器处理过,灰色意味着已经被回收器遍历但尚未完成处理(或者需要再次进行处理)。三色抽象也可以推广到对象的域中:灰色表示正在处理的域,黑色表示已经处理过的域。如果把赋值器也当作一个对象,则三色抽象也可用于推演赋值器根集合的状态变化[Pirinen,1998]灰色赋值器表示回收器尚未完成对其根集合的扫描,黑色赋值器表示回收器已经完成对其根集合的扫描(并且不需要再次扫描)。一次堆遍历过程可以形象地看作是回收器以灰色对象作为“波面”(wavefront),将黑色对象和白色对象分离,不断向前推进波面,直到所有可达对象都变成黑色的过程。 上述算法中存在一个重要的不变式:在标记过程完成后,对象图中将不可能存在从黑色对象指向白色对象的引用,因此在标记过程中,所有白色可达对象都只能是从灰色对象可达。如果这一不变式被打破,那么回收器将不会进一步处理黑色对象,从而可能导致某个黑色对象的后代可达但未被标记(进而被错误地释放)。 改进的标记-清扫算法程序的性能通常与其高速缓存的相关行为有很大关系。从内存中加载一个值可能要花费上百个时钟周期,但从L1高速缓存(L1 cache)中加载可能只需要花费三到四个时钟周期。高速缓存之所以能够提升程序的性能,主要是因为程序在运行时表现出了良好的时间局部性」(temporal locality),即一旦程序访问了某个内存地址,则很可能在不久之后再次访问该地址,因此值得将它的值缓存。程序也可能表现出良好的空间局部性(space locality),即一旦程序访问了某个内存地址,则很有可能在不久之后访问该地址附近的数据。现代硬件可以从两个方面利用程序的局部性特征:一方面,高速缓存与更低级别内存之间不会进行单个字节的数据传输,而是以一个固定的字节数为最小传输单元(即高速缓存行或者高速缓存块的大小),通常是32~128字节;另一方面,处理器可能会使用硬件预取(prefetch)技术,例如Intel Core微处理器架构可以探测到有规律的步进内存访问操作,进而提前读取数据流。开发者也可以利用显式预取指令引导预取过程。 位图标记回收器可以将对象的标记位保存在其头部的某个字中,除此之外也可以使用一个独立的位图来维护标记位,即:位图中的每个位关联堆中每个可能分配对象的地址。位图所需的空间取决于虚拟机的字节对齐要求。位图可以只有一个,也可以存在多个,例如在块结构的堆中,回收器可以为每个内存块维护独立的位图,这一方式可以避免由于堆不连续导致的内存浪费。回收器可以将每个内存块的位图置于其自身内部,但如果所有内存块中位图的相对位置全部相同,则可能导致性能的下降,因为不同内存块的位图之间可能会争用相同的组相关高速缓存(set- associative cache)。对位图的访问同时也意味着对位图所在页的访问(即可能导致缺页异常——译者注),因此基于换页和高速缓存相关性的考虑,在访问位图时花费更多的指令以保持程序的局部性通常来说是值得的。为避免高速缓存的相关问题,可以将内存块中位图的位置增加一个简单偏移量,例如内存块地址的简单哈希值。还可以将位图存放在个额外的区域[Boehm and Weiser,1988年]中,并以其所对应内存块的哈希值等作为索引,这样既避免了换页问题,也避免了高速缓存冲突。 位图标记通常仅适用于单线程环境,因为多线程同时修改位图可能存在较大的写冲突风险。设置对象头部中的标记位通常是安全的,因为该操作是幂等的,即最多只会将标记位设置多次。相对于位图,实践中更常用的是字节图(byte-map),虽然它占用的空间是前者的8倍,但却解决了写冲突问题。另外还可以使用同步操作来设置位图中的位。在实际应用中,如果将标记位保存在对象头部通常会带来额外的复杂度,因为头部通常会存放一些赋值器共享数据,例如锁或者哈希值,那么当标记线程与赋值器线程并发执行时可能会产生冲突。因此,为了确保安全,标记位通常会占用头部中一个额外的字,以便与赋值器共享数据区分,当然也可以使用原子操作来设置头部中的标记位。 相对于将标记位放置在对象头部这一策略,位图可以使得标记位更加密集;对于使用位图的标记一清扫回收器,标记过程只需读取存活对象的指针域而不会修改任何对象;对于不包含引用的对象,回收器只需要读取其类型信息描述域;清扫器不会对存活对象进行任何读写操作,它只会在释放垃圾对象的过程中覆盖其某些域(例如将它们链接到空闲链表上)。因此,位图标记不仅可以减少内存中需要修改的字节数,而且减少了对高速缓存行的写入,进而减少需要写回内存的数据量。 位图标记最初应用在保守式回收器(conservative collector)中。保守式回收器的设计初衷是为C和C++等“不合作”的语言提供自动内存管理功能[Boehm and Weiser,1988]型精确(type-accurate)系统可以精确地识别每一个包含指针的槽,不论其位于对象中,是位于线程栈或者其他根集合中,而保守式回收器则无法得到编译器和运行时系统的支持因而其在识别指针时必须采用保守的判定方式,即:如果槽中某个值看起来像是指针引用,那么就必须假定它是一个指针。保守式回收器可能错误地将一个槽当作指针,这带来了两个安全上的要求:第一,回收器不能修改任何赋值器可能访问到的内存地址的值(包括对象和根集合)。这一要求导致保守式回收器不能使F任何可能移动对象的算法,因为对象被移动之后需要更新指向该对象的所有引用。这同时导致在头域中保存标记位的方案不可行,因为错误的指针会指向一个实际并不存在的象”,因此设置或者清理标记位可能会破坏用户数据。第二,应当尽可能减少赋值器破坏回收器数据的可能性。与将标记位等回收器元数据存放在一个单独区域的方案相比,为每个对象增加一个回收器专用头部数据会存在更高的风险。 使用位图标记的另一个重要目的是减少回收过程中的换页次数[Boehm,2000在现代系统中,任何由回收器导致的换页行为通常都是不可接受的,因此位图标记是否可以提升高速缓存性能便成为一个值得关注的问题。许多证据表明,对象往往成簇诞生并成批死亡[Hayes,1991; Jones, Ryder,2008],而许多分配器往往也会将这些对象分配在相邻的空间。使用位图来引导清扫可以带来两个好处:第一,在位图/字节图中,一个字内部的每个位/字节全部都被设置/清空的情况会经常出现,因此回收器可以批量读取/清空一批对象的标记位;第二,通过位图标记可以更简单地判定某一内存块中的所有对象是否都是垃圾,进而可能一次性回收整个内存块。 许多内存管理器都使用块结构堆(如Boehm and Weiser[1988])。最直接的位图标记实现策略可能是在每个内存块的前端保留一块内存以用作位图。但正如我们前面所提到的,这策略可能会导致不必要的高速缓存冲突或者换页,因此回收器通常将位图与用户数据块分开并单独存放。 Garner等[2007]提出了一种混合标记策略,即将分区适应分配器(segregated fitsallocator)所管理的每个数据块与字节图中的一个字节相关联,同时依然保留对象头部的标记位。当且仅当内存块中至少存在一个存活对象时,该内存块所对应的标记字节才会被设置。清扫器可以根据字节图快速地判定某一内存块是否完全为空(即不包含存活对象),进而可以将其整体回收。这一策略有两个优点:第一,在并发情况下,无需使用同步操作来设置字节图中的标记字节以及对象头部的标记位;第二,写操作没有数据依赖(这可能导致高速缓存延迟),且对字节图中标记字节的写操作也是无条件的。 标记过程中的高速缓存不命中问题Bohm[2000]发现标记过程的开销决定着回收时间:在 Intel PentiumⅢ系统中,预取对象第一个指针域的开销通常会占到标记该对象总开销的三分之一。为此 Boehm提出一种灰色预取( prefetching on gray)技术,即当对象为灰色时,预取其第一个高速缓存行中的数据,如果被扫描的对象很大,则预取适当数量的高速缓存行。然而,这种技术依赖于预取时间,如果过早地进行高速缓存行预取,则数据很可能在得到使用之前就被换出,而过晚地预取则会导致高速缓存不命中。 需要考虑的问题空间利用率将标记位放在对象头部基本不会产生额外的空间开销,而如果使用位图来保存标记位,则额外空间开销的大小取决于对象的字节对齐要求,但其总大小不会超过堆的字节对齐要求的倒数(即堆空间的1/64或1/32,具体的值取决于堆的组织架构)。 引用计数环状引用计数对于环状数据结构而言,其内部对象的引用计数至少为1,因此仅靠引用计数本身无法回收环状垃圾。不论是在应用程序还是在运行时系统中,环状数据结构都十分普遍,如双向链表或者环状缓冲区。对象一关系映射(object-relations mapping)系统可能要求数据库和其中的表互相引用对方的一些信息。真实世界中的某些结构天然就是环状的,例如地理信息系统中的道路。懒惰函数式语言(lazy functional language)通常使用环来表示递归[urner 1979,Y组合子(Combinator)]。研究者们提出了多种解决环状引用计数问题的策略,我们介绍其中的几种。 最简单的策略是在引用计数之外偶尔使用追踪式回收作为补充。该方法假定大多数对象不会被环状数据结构所引用,因此可以通过引用计数方法实现快速回收,而追踪式回收则负责处理剩余的环状数据结构。这一方案简单地减少了追踪式回收的发起频率。在语言层面上, Friedman和wise[1979]发现,纯函数式语言中只有递归定义才会产生环,因此只要遵从一定的规则便可对这种情况下的环状引用计数进行特殊处理。在 Bobrow1980]的方法中,开发者可以把一组对象作为整体进行引用计数操作,当整体引用计数为零时便可将其集体回收。 许多学者建议将导致闭环出现的指针与其他指针进行区分[Friedman and Wise,1979;Brownbridge,1985; Salkeld,1987; Repels等,1988; Axford,19901。他们将普通引用称为强引用(strong reference),将导致闭环出现的引用称为弱引用(weak reference)。如果不允许强引用组成环,则强引用图可以使用标准引用计数算法处理。Brownbridge的算法得到了广泛应用,简而言之,每个对象需要包含一个强引用计数以及一个弱引用计数,在进行写操作时,写屏障会检测指针以及目标对象的强弱,并将所有可能产生环的引用设置为弱引用。为维护“所有可达对象均为强可达,且强引用不产生环”这一不变式,赋值器在删除引用时可能需要改变指针的强弱属性。但是,这一算法并不安全,且可能导致对象提前被回收,具体可以参见Salkeld的引用计数示例[Jones,1996,6.5节]。Salkeld[1987]对该算法进行修正并提升了它的安全性,但代价是在某些情况下算法将无法结束。Repels等[1988]提出了一种非常复杂的解决方案,但该算法在空间以及性能方面的开销却更加明显:与普通的引用计数相比,其所需的空间开销翻倍,在大多数情况下,其性能开销是标准引用计数的两倍,在极端情况下,甚至会呈现指数级增长。 在所有能够处理环状数据结构的引用计数算法中,得到了最广泛认可的是试验删除(trial deletion)算法。该算法无须使用后备的追踪式回收器来进行整个存活对象图的扫描,相反,它将注意力集中在可能会因删除引用而产生环状垃圾的局部对象图上。在引用计数算法中: 在环状垃圾指针结构内部,所有对象的引用计数均由其内部对象之间的指针产生。 只有在删除某一对象的某个引用后该对象的引用计数仍大于零时,才有可能出现环状垃圾。 部分追踪(partial tracing)算法充分利用上述两个结论,该算法从一个可能是垃圾的对象开始进行子图追踪。对于遍历到的每个引用,算法将对其目标对象进行试验删除,即临时性地减少目标对象的引用计数,从而移除由内部指针产生的引用计数。追踪完成后,如果某个对象的引用计数仍然不是零,则必然是因为子图之外的其他对象引用了该对象,进而可以判定该对象及其传递闭包都不是垃圾。 Recycler算法[Bacon等,2001; Bacon and Rajan,2001;Paz等,2007]支持环状引用计数的并发回收。环状数据结构的回收分为3个阶段: 首先,回收器从某个可能是环状垃圾成员的对象出发进行子图追踪,同时减少由内部指针产生的引用计数。算法将遍历到的对象着为灰色。 其次,对子图中的所有对象进行检测,如果某一对象的引用计数不是零,则该对象必然被子图外的其他对象引用。此时需要对第一阶段的试验删除操作进行修正,算法将存活的灰色对象重新着为黑色,同时将其他灰色对象着为白色。 最后,子图中所有依然为白色的对象必然是垃圾,算法可以将其回收。 内存分配对于首次适应或循环首次适应分配而言,将空闲内存单元依照地址进行排序的另一种有效策略是位图适应分配(bitmapped- fits allocation)。该算法使用额外的位图来记录堆中每个可分配内存颗粒的状态,因此在进行内存分配时,分配器可以基于位图而非堆本身进行搜索。借助一张预先计算好的映射表,分配器仅需要对位图中一个字节进行计算,便可得知其所对应的8个连续内存颗粒所能组成的最长连续可用空间。也可以使用额外的长度信息记录较大的空闲内存单元或者已分配内存单元,从而快速将其跳过以提升分配性能。位图适应分配具有如下一些优点: 位图本身与对象相互隔离,因此不容易遭到破坏。这一特性不仅对于诸如C和C++等安全性稍低的语言十分重要,而且对于安全性更高的语言也十分有用,因为这可以提升回收器的可靠性以及可调试性。 引入位图之后,无论是对于空闲内存单元还是已分配内存单元,回收器都不需要占据其中的任何空间来记录回收相关信息,从而最大限度地降低了对内存单元大小的要求。如果以一个32位的字作为最小内存分配单元,该策略会引入大约3%的空间开销,但其所带来收益却远大于这一开销。不过,基于其他一些方面的考虑,对象可能依然需要一个头部,因而这一优点并非始终能够得到体现。 相对于堆中的内存单元,位图更加紧凑,因此基于位图进行扫描可以提升高速缓存命中率,从而提升分配器的局部性。 分区适应分配空间大小分级的填充伙伴系统(buddy system)其空间大小分级均为2的整数次幂[Knowlton,1965; Peterson and Norman,1977]。我们可以将一个大小为2^(i+1)的空闲内存单元分裂为两个大小为2^i的空闲内存单元,同时也可以将两个相邻的大小为2^i的空闲内存单元合并成大小为2^(i+1)的一个,但进行合并的前提是两个相邻空闲内存单元原本就是由同一个较大的空闲内存单元分裂得到的。在该算法中,大小为2^i的空闲内存单元两两成对,因而称之为伙伴。由于伙伴系统的内部碎片通常较为严重(对于任意的内存分配需求,其平均空间浪费率会达到25%),因此该算法基本已经成为历史,在实践中较少使用。 斐波那契伙伴系统(Fibonacci buddy system)[Hirschberg,1973; Burton,1976; Peterson andNorman,1977是伙伴系统的一个变种,其空间大小分级符合斐波那契序列,即si+2 = si+1 + si,同时需要选定合适的s0和s1。与传统的伙伴系统相比,该算法相邻空闲内存单元的大小比值更小,因而在一定程度上缓解了内部碎片问题。但该算法的问题在于,在回收完成后将相邻空闲内存单元合并的操作会更加复杂,因为回收器需要判定某一空闲内存单元究竟应当与相邻两个空闲内存单元中的哪一个进行合并。 其它需要考虑的问题字节对齐将对象按照特定的边界要求进行对齐,一方面是底层硬件或者机器指令集的要求,另方面这样做有助于提升各层次存储器的性能(包括高速缓存、转译后备缓冲区、内存页以Java语言的 double数组为例,某些机器可能要求double这一双字浮点数必须以双字为边界进行对齐,即其地址必须是8的整数倍(地址的后三位为零)。一种简单但稍显浪费的解决方案是将双字作为内存分配的颗粒,即所有已分配或未分配内存单元的大小均为8的整数倍,且均按照8字节边界对齐。但即便如此,当分配一个doub1e类型的数组时,分配器仍需要进行一些额外工作。假设Java语言中纯对象(即非数组对象)头部都必须保留两个字,一个指向对象的类型信息(用于虚函数调用、类型判定等),另一个用于记录对象的哈希值以及同步操作所需的锁(这也是一种典型的设计方式)。数组对象则需要第三个字来记录其中元素的个数。如果将这三个头部字保存在已分配内存单元的起始位置,则数组元素就不得不以奇数字为单位进行对齐。如果使用双字作为内存颗粒,则可以简单地用四个字(即两个双字)来保存这三个头部字,然后浪费掉一个。 但如果内存颗粒是一个字,我们则希望尽量减少上述的内存浪费。此时,如果某个空闲内存单元按照奇数字对齐(即其地址模8余4),则我们可以简单地将三个头部字放在内存单字对齐,则我们必须浪费一个字以满足对齐要求。这一方案增加了分配过程的复杂度,因为某一空闲内存单元是否满足分配需求不仅取决于所需空间的大小,还取决于字节对齐要求。 字：16个位为一个字，它代表计算机处理指令或数据的二进制数位数，是计算机进行数据存储和数据处理的运算的单位。通常称16位是一个字，而32位呢，则是一个双字，64位是两个双字。 空间大小限制某些回收器要求对象(内存单元)的大小必须大于某一下界。例如,基本的整理式回收要求对象内部至少可以容纳一个指针,还有一些回收器可能需要用两个字来保存锁或状态以及转发指针,这就意味着即使开发者仅需要分配一个字,分配器也必须多分配两个字。如果开发者需要分配不包含任何数据、仅用作唯一标识的对象,原则上编译器无需分配任何空间,但在实际情况下这通常不可行:对象必须要有唯一的地址,因此对象的大小至少应为一个字节。 边界标签为了确保在释放内存时可以将相邻空闲内存单元合并,许多内存分配系统为每个内存单元增加了额外的头部或者边界标签,它们通常不属于可用内存的范畴[Knuth,1973]。边界标签保存了内存单元的大小及其状态(即空闲或已分配),还可以在其中记录上一个内存单元的大小,从而可以快速读取上一个内存单元的状态并判断其是否为空。当内存单元空闲时,边界标签也可用于保存构建空闲链表的指针。基于这些原因,边界标签可能达到两个字或者更大,但如果使用一些额外的方法,并允许在分配和释放的过程中引入一定的额外开销,则仍有可能将边界标签压缩到一个字。 如果使用额外的位图来标记堆中每个内存颗粒的状态,则不仅无需使用边界标签,而且可以增加程序的鲁棒性。这一方法是否会减少空间开销,取决于对象的平均大小以及内存颗粒的大小。 我们进一步注意到,垃圾回收通常会一次性释放大量对象,因此某些特定的算法可能不再需要边界标签,或者其边界标签中需要包含的信息较少。另外,托管语言中对象的大小通常可以通过其类型得出,因而无需使用额外的边界标签来单独记录相关信息。 局部性事实证明,同一时刻分配的对象通常也会在同一时刻成为垃圾,因此非移动式回收器所面临的内存碎片问题比人们预想的要小[Hayes,1991; Dimpsey等,2000; Blackburn and McKinley,2008],这同时也说明,将连续两次分配的对象连续排列或者尽可能靠近排列的启发式方法是有价值的。 并发系统中的内存分配在多线程环境下,分配过程的许多操作都需要原子化以确保分配数据结构的完整性,这些操作都必须使用原子操作或者锁,但这样一来,内存分配就可能成为性能瓶颈。最基本的解决方案是为每个线程开辟独立的内存分配空间,如果某个线程的可分配空间耗尽,则从全局内存池中为其分配一个新的空闲块,此时只有与全局内存池的交互才需要原子化。不同线程的内存分配频度可能不同,因此如果在为线程分配内存块时使用自适应算法(即:为分配速度较慢的线程分配较小的内存块,而为分配速度较快的线程分配较大的内存块),则程序的时间和空间性能均可获得提升。Dimpsey等[20001声称,在多处理器Java系统中,为每个线程配备一个合适的本地分配缓冲区(local allocation buffer,LAB)可以大幅提升性能。他们进一步指出,由于几乎所有的小对象都是从本地分配缓冲区分配的,因而我们有理由对全局(基于空闲链表的)分配器进行调整,以使其能够更加高效地分配用于线程本地分配缓冲区的内存块。 Garthwaite等[2005]讨论了如何对本地分配缓冲区的大小进行自适应调整,他们同时发现,将本地分配缓冲区与处理器而非线程相关联效果更佳。该算法通过如下方式对本地分配缓冲区的大小进行调整:线程初次申请本地分配缓冲区时将获得24个字的内存块,之后每次新申请的内存块均为上一次的1.5倍,同时每经历一次垃圾回收过程,回收器都会将线程的本地分配缓冲区的大小折半。该算法同时也会根据不同线程的分配次数调整年轻代的空间大小。每处理器(per-processor)本地分配缓冲区的实现依赖于多处理器的可重启临界区(restartable critical section),Garthwaite等人对此做了介绍。其基本原理是,线程可以判断自身是否被抢占(preempt)或者被重新调度(reschedule),然后可以据此判断自身是否被切换到其他处理器上运行。当线程抢占发生时,处理器会对某个本地寄存器进行修改,该操作会为抢占完成后的写入操作设置一个陷阱,而陷阱处理函数则会重启被中断的分配过程。尽管每处理器本地分配缓冲区需要更多的指令支持,但与每线程本地分配缓冲区相比,其分配时延相同,且不需要复杂的缓冲区调整机制。 Garthwaite同时发现,当线程数量较少时(特别是当线程数量小于处理器数量时),每线程(per-thread)本地分配缓冲区的性能较好,而在线程数量较多的情况下,每处理器本地分配缓冲区的表现更佳,因此他们将系统设计成可在两种方案之间进行动态切换。 本地分配缓冲区通常使用顺序分配策略。每个线程(或处理器)也可以独立维护自身对应的分区适应空闲链表,同时使用增量清扫策略。线程在内存分配过程中会执行增量清扫,并将清扫所得的空闲内存单元添加到自身空闲链表中,但Berger等[2000]指出,如果将该算法用于显式内存管理会存在一些问题。例如,在某一使用生产者一消费者模型的程序中消息对象通常由生产者创建并由消费者释放,因此两个线程之间将会产生单方向的内存转移。在垃圾回收环境下通常不会存在这一问题,因为回收器可以将空闲内存释放到全局内存池中。如果使用增量清扫,空闲内存单元将被执行清扫的线程所获取,从而自然地将回收所得的内存返还给分配最频繁的线程。 需要考虑的问题使用额外的位图表来标记内存颗粒的状态(空闲/已分配)以及内存单元/对象的起始地址,不仅能提升程序的鲁棒性,而且可以简化对象头部的设计。该策略同时可以加速回收器的操作,并可以在存储器层次结构方面提升回收器的性能。基于位图的分配策略空间开销不大,但其分配过程通常会存在额外的时间开销。 小注读到了9.7 对程序行为的适应]]></content>
      <categories>
        <category>GC</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA 个人常用插件]]></title>
    <url>%2Fidea-plugin.html</url>
    <content type="text"><![CDATA[IDEA 个人常用插件整理，持续更新中 mybatis-lite根据数据库表自动生成代码git地址：https://github.com/mustfun/mybatis-lite MyBatis Log Plugin把mybatis输出的日志还原成完整的sql语句git地址：https://github.com/kookob/mybatis-log-plugin]]></content>
      <categories>
        <category>IDE</category>
      </categories>
      <tags>
        <tag>IDE</tag>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[码出高效：Java 开发手册]]></title>
    <url>%2Falibaba-p3c.html</url>
    <content type="text"><![CDATA[开发规范 在线PDF版本：阿里巴巴Java开发手册（华山版）.pdf https://github.com/alibaba/p3c]]></content>
      <categories>
        <category>编码规范</category>
      </categories>
      <tags>
        <tag>编码规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP/IP详解 卷1：协议 笔记]]></title>
    <url>%2Ftcp-ip-illustrated-volume-1-the-protocols-note.html</url>
    <content type="text"><![CDATA[本文整理自：《TCP/IP详解 卷1:协议》作者：W. Richard Stevens 一个bit是二进制中的最小单位，代表一个0或1的位置。 bit是位。 Byte是字节。 1Byte=8bit。 概述分层网络协议通常分不同层次进行开发，每一层分别负责不同的通信功能。一个协议族，比如TCP/IP，是一组不同层次上的多个协议的组合。TCP/IP通常被认为是一个四层协议系统，如图1-1所示。 每一层负责不同的功能：1)链路层，有时也称作数据链路层或网络接口层，通常包括操作系统中的设备驱动程序和计算机中对应的网络接口卡。它们一起处理与电缆（或其他任何传输媒介）的物理接口细节。2)网络层，有时也称作互联网层，处理分组在网络中的活动，例如分组的选路。在TCP/IP协议族中，网络层协议包括IP协议（网际协议），ICMP协议（Internet互联网控制报文协议），以及IGMP协议（Internet组管理协议）。3)运输层主要为两台主机上的应用程序提供端到端的通信。在TCP/IP协议族中，有两个互不相同的传输协议：TCP（传输控制协议）和UDP（用户数据报协议）。TCP为两台主机提供高可靠性的数据通信。它所做的工作包括把应用程序交给它的数据分成合适的小块交给下面的网络层，确认接收到的分组，设置发送最后确认分组的超时时钟等。由于运输层提供了高可靠性的端到端的通信，因此应用层可以忽略所有这些细节。而另一方面，UDP则为应用层提供一种非常简单的服务。它只是把称作数据报的分组从一台主机发送到另一台主机，但并不保证该数据报能到达另一端。任何必需的可靠性必须由应用层来提供。这两种运输层协议分别在不同的应用程序中有不同的用途，这一点将在后面看到。4)应用层负责处理特定的应用程序细节。几乎各种不同的TCP/IP实现都会提供下面这些通用的应用程序： Telnet 远程登录。 FTP 文件传输协议。 SMTP 简单邮件传送协议。 SNMP 简单网络管理协议。 在TCP/IP协议族中，网络层IP提供的是一种不可靠的服务。也就是说，它只是尽可能快地把分组从源结点送到目的结点，但是并不提供任何可靠性保证。而另一方面，TCP在不可靠的IP层上提供了一个可靠的运输层。为了提供这种可靠的服务，TCP采用了超时重传、发送和接收端到端的确认分组等机制。由此可见，运输层和网络层分别负责不同的功能。 连接网络的另一个途径是使用网桥。网桥是在链路层上对网络进行互连，而路由器则是在网络层上对网络进行互连。网桥使得多个局域网（LAN）组合在一起，这样对上层来说就好像是一个局域网。 TCP/IP的分层在TCP/IP协议族中，有很多种协议。图1-4给出了本书将要讨论的其他协议。 IP是网络层上的主要协议，同时被TCP和UDP使用。TCP和UDP的每组数据都通过端系统和每个中间路由器中的IP层在互联网中进行传输。 ICMP是IP协议的附属协议。IP层用它来与其他主机或路由器交换错误报文和其他重要信息。 IGMP是Internet组管理协议。它用来把一个UDP数据报多播到多个主机。 ARP（地址解析协议）和RARP（逆地址解析协议）是某些网络接口（如以太网和令牌环网）使用的特殊协议，用来转换IP层和网络接口层使用的地址。 互联网地址互联网上的每个接口必须有一个唯一的Internet地址（也称作IP地址）。IP地址长32bit。 这些32位的地址通常写成四个十进制的数，其中每个整数对应一个字节。这种表示方法称作“点分十进制表示法（Dotted decimal notation）”。 区分各类地址的最简单方法是看它的第一个十进制整数。图1-6列出了各类地址的起止范围，其中第一个十进制整数用加黑字体表示。 需要再次指出的是，多接口主机具有多个IP地址，其中每个接口都对应一个IP地址。由于互联网上的每个接口必须有一个唯一的IP地址，因此必须要有一个管理机构为接入互联网的网络分配IP地址。这个管理机构就是互联网络信息中心（Internet Network Information Centre），称作InterNIC。InterNIC只分配网络号。主机号的分配由系统管理员来负责。 有三类IP地址：单播地址（目的为单个主机）、广播地址（目的端为给定网络上的所有主机）以及多播地址（目的端为同一组内的所有主机）。 封装当应用程序用TCP传送数据时，数据被送入协议栈中，然后逐个通过每一层直到被当作一串比特流送入网络。其中每一层对收到的数据都要增加一些首部信息（有时还要增加尾部信息），该过程如图1-7所示。TCP传给IP的数据单元称作TCP报文段或简称为TCP段（TCP segment）。IP传给网络接口层的数据单元称作IP数据报(IP datagram)。通过以太网传输的比特流称作帧(Frame)。 图1-7中帧头和帧尾下面所标注的数字是典型以太网帧首部的字节长度。 以太网数据帧的物理特性是其长度必须在46～1500字节之间。 更准确地说，图1-7中IP和网络接口层之间传送的数据单元应该是分组（packet）。分组既可以是一个IP数据报，也可以是IP数据报的一个片（fragment）。 UDP数据与TCP数据基本一致。唯一的不同是UDP传给IP的信息单元称作UDP数据报（UDP datagram），而且UDP的首部长为8字节。 由于TCP、UDP、ICMP和IGMP都要向IP传送数据，因此IP必须在生成的IP首部中加入某种标识，以表明数据属于哪一层。为此，IP在首部中存入一个长度为8 bit的数值，称作协议域。1表示为ICMP协议，2表示为IGMP协议，6表示为TCP协议，17表示为UDP协议。 类似地，许多应用程序都可以使用TCP或UDP来传送数据。运输层协议在生成报文首部时要存入一个应用程序的标识符。TCP和UDP都用一个16 bit的端口号来表示不同的应用程序。TCP和UDP把源端口号和目的端口号分别存入报文首部中。 网络接口分别要发送和接收IP、ARP和RARP数据，因此也必须在以太网的帧首部中加入某种形式的标识，以指明生成数据的网络层协议。为此，以太网的帧首部也有一个16 bit的帧类型域。 分用当目的主机收到一个以太网数据帧时，数据就开始从协议栈中由底向上升，同时去掉各层协议加上的报文首部。每层协议盒都要去检查报文首部中的协议标识，以确定接收数据的上层协议。这个过程称作分用（Demultiplexing），图1-8显示了该过程是如何发生的。 小结TCP/IP协议族分为四层：链路层、网络层、运输层和应用层，每一层各有不同的责任。在TCP/IP中，网络层和运输层之间的区别是最为关键的：网络层（IP）提供点到点的服务，而运输层（TCP和UDP）提供端到端的服务。 链路层从图1-4中可以看出，在TCP/IP协议族中，链路层主要有三个目的：（1）为IP模块发送和接收IP数据报；（2）为ARP模块发送ARP请求和接收ARP应答；（3）为RARP发送RARP请求和接收RARP应答。 环回接口大多数的产品都支持环回接口（Loopback Interface），以允许运行在同一台主机上的客户程序和服务器程序通过TCP/IP进行通信。A类网络号127就是为环回接口预留的。根据惯例，大多数系统把IP地址127.0.0.1分配给这个接口，并命名为localhost。一个传给环回接口的IP数据报不能在任何网络上出现。 IP：网际协议引言IP是TCP/IP协议族中最为核心的协议。所有的TCP、UDP、ICMP及IGMP数据都以IP数据报格式传输（见图1-4）。 不可靠（unreliable）的意思是它不能保证IP数据报能成功地到达目的地。IP仅提供最好的传输服务。如果发生某种错误时，如某个路由器暂时用完了缓冲区，IP有一个简单的错误处理算法：丢弃该数据报，然后发送ICMP消息报给信源端。任何要求的可靠性必须由上层来提供（如TCP）。 无连接（connectionless）这个术语的意思是IP并不维护任何关于后续数据报的状态信息。每个数据报的处理是相互独立的。这也说明，IP数据报可以不按发送顺序接收。如果一信源向相同的信宿发送两个连续的数据报（先是A，然后是B），每个数据报都是独立地进行路由选择，可能选择不同的路线，因此B可能在A到达之前先到达。 IP首部IP数据报的格式如图3-1所示。普通的IP首部长为20个字节，除非含有选项字段。 分析图3-1中的首部。最高位在左边，记为0 bit；最低位在右边，记为31 bit。 4个字节的32 bit值以下面的次序传输：首先是0～7 bit，其次8～15 bit，然后16～23 bit，最后是24~31 bit。这种传输次序称作bigendian字节序。由于TCP/IP首部中所有的二进制整数在网络中传输时都要求以这种次序，因此它又称作网络字节序。以其他形式存储二进制整数的机器，如little endian格式，则必须在传输数据之前把首部转换成网络字节序。 目前的协议版本号是4，因此IP有时也称作IPv4。 首部长度指的是首部占32 bit字的数目，包括任何选项。由于它是一个4比特字段，因此首部最长为60个字节。普通IP数据报（没有任何选择项）字段的值是5。 总长度字段是指整个IP数据报的长度，以字节为单位。利用首部长度字段和总长度字段，就可以知道IP数据报中数据内容的起始位置和长度。 总长度字段是IP首部中必要的内容，因为一些数据链路（如以太网）需要填充一些数据以达到最小长度。尽管以太网的最小帧长为46字节，但是I P数据可能会更短。如果没有总长度字段，那么IP层就不知道46字节中有多少是IP数据报的内容。 标识字段唯一地标识主机发送的每一份数据报。通常每发送一份报文它的值就会加 1。 TTL（time-to-live）生存时间字段设置了数据报可以经过的最多路由器数。它指定了数据报的生存时间。TTL的初始值由源主机设置（通常为32或64），一旦经过一个处理它的路由器，它的值就减去1。当该字段的值为0时，数据报就被丢弃，并发送ICMP报文通知源主机。 首部检验和字段是根据IP首部计算的检验和码。它不对首部后面的数据进行计算。ICMP、IGMP、UDP和TCP在它们各自的首部中均含有同时覆盖首部和数据检验和码。 为了计算一份数据报的IP检验和，首先把检验和字段置为 0。然后，对首部中每个16 bit进行二进制反码求和（整个首部看成是由一串 16 bit的字组成），结果存在检验和字段中。当收到一份IP数据报后，同样对首部中每个 16 bit进行二进制反码的求和。由于接收方在计算过程中包含了发送方存在首部中的检验和，因此，如果首部在传输过程中没有发生任何差错，那么接收方计算的结果应该为全 1。如果结果不是全1（即检验和错误），那么IP就丢弃收到的数据报。但是不生成差错报文，由上层去发现丢失的数据报并进行重传。 选项字段一直都是以 32 bit作为界限，在必要的时候插入值为 0的填充字节。这样就保证IP首部始终是32 bit的整数倍（这是首部长度字段所要求的）。 IP路由选择从概念上说，IP路由选择是简单的，特别对于主机来说。如果目的主机与源主机直接相连（如点对点链路）或都在一个共享网络上（以太网或令牌环网），那么IP数据报就直接送到目的主机上。否则，主机把数据报发往一默认的路由器上，由路由器来转发该数据报。大多数的主机都是采用这种简单机制。 在一般的体制中，IP可以从TCP、UDP、ICMP和IGMP接收数据报（即在本地生成的数据报）并进行发送，或者从一个网络接口接收数据报（待转发的数据报）并进行发送。IP层在内存中有一个路由表。当收到一份数据报并进行发送时，它都要对该表搜索一次。当数据报来自某个网络接口时，IP首先检查目的IP地址是否为本机的IP地址之一或者IP广播地址。如果确实是这样，数据报就被送到由IP首部协议字段所指定的协议模块进行处理。如果数据报的目的不是这些地址，那么（1）如果IP层被设置为路由器的功能，那么就对数据报进行转发（也就是说，像下面对待发出的数据报一样处理）；否则（2）数据报被丢弃。 路由表中的每一项都包含下面这些信息： 目的IP地址。它既可以是一个完整的主机地址，也可以是一个网络地址，由该表目中的标志字段来指定（如下所述）。主机地址有一个非0的主机号（见图1-5），以指定某一特定的主机，而网络地址中的主机号为0，以指定网络中的所有主机（如以太网，令牌环网）。 下一站（或下一跳）路由器（next-hoprouter）的IP地址，或者有直接连接的网络IP地址。下一站路由器是指一个在直接相连网络上的路由器，通过它可以转发数据报。下一站路由器不是最终的目的，但是它可以把传送给它的数据报转发到最终目的。 标志。其中一个标志指明目的IP地址是网络地址还是主机地址，另一个标志指明下一站路由器是否为真正的下一站路由器，还是一个直接相连的接口。 为数据报的传输指定一个网络接口。 IP路由选择是逐跳地（hop-by-hop）进行的。从这个路由表信息可以看出，IP并不知道到达任何目的的完整路径（当然，除了那些与主机直接相连的目的）。所有的IP路由选择只为数据报传输提供下一站路由器的IP地址。它假定下一站路由器比发送数据报的主机更接近目的，而且下一站路由器与该主机是直接相连的。 IP路由选择主要完成以下这些功能： 搜索路由表，寻找能与目的IP地址完全匹配的表目（网络号和主机号都要匹配）。如果找到，则把报文发送给该表目指定的下一站路由器或直接连接的网络接口（取决于标志字段的值）。 搜索路由表，寻找能与目的网络号相匹配的表目。如果找到，则把报文发送给该表目指定的下一站路由器或直接连接的网络接口（取决于标志字段的值）。目的网络上的所有主机都可以通过这个表目来处置。例如，一个以太网上的所有主机都是通过这种表目进行寻径的。这种搜索网络的匹配方法必须考虑可能的子网掩码。 搜索路由表，寻找标为“默认（default）”的表目。如果找到，则把报文发送给该表目指定的下一站路由器。 如果上面这些步骤都没有成功，那么该数据报就不能被传送。如果不能传送的数据报来自本机，那么一般会向生成数据报的应用程序返回一个“主机不可达”或“网络不可达”的错误。完整主机地址匹配在网络号匹配之前执行。只有当它们都失败后才选择默认路由。默认路由，以及下一站路由器发送的ICMP间接报文（如果我们为数据报选择了错误的默认路由），是IP路由选择机制中功能强大的特性。 为一个网络指定一个路由器，而不必为每个主机指定一个路由器，这是IP路由选择机制的另一个基本特性。这样做可以极大地缩小路由表的规模，比如Internet上的路由器有只有几千个表目，而不会是超过100万个表目。 子网寻址现在所有的主机都要求支持子网编址（RFC 950 [Mogul and Postel 1985]）。不是把IP地址看成由单纯的一个网络号和一个主机号组成，而是把主机号再分成一个子网号和一个主机号。 这样做的原因是因为A类和B类地址为主机号分配了太多的空间，可分别容纳的主机数为2^24 -2和2^16 -2。事实上，在一个网络中人们并不安排这么多的主机（各类IP地址的格式如图1-5所示）。由于全0或全1的主机号都是无效的，因此我们把总数减去 2。 子网掩码任何主机在引导时进行的部分配置是指定主机IP地址。大多数系统把IP地址存在一个磁盘文件里供引导时读用。 除了IP地址以外，主机还需要知道有多少比特用于子网号及多少比特用于主机号。这是在引导过程中通过子网掩码来确定的。这个掩码是一个32 bit的值，其中值为1的比特留给网络号和子网号，为0的比特留给主机号。图3-7是一个B类地址的两种不同的子网掩码格式。第一个例子是noao.edu网络采用的子网划分方法，子网号和主机号都是8 bit宽。第二个例子是一个B类地址划分成10 bit的子网号和6 bit的主机号。 给定IP地址和子网掩码以后，主机就可以确定IP数据报的目的是：(1)本子网上的主机；(2)本网络中其他子网中的主机；(3)其他网络上的主机。如果知道本机的IP地址，那么就知道它是否为A类、B类或C类地址(从IP地址的高位可以得知)，也就知道网络号和子网号之间的分界线。而根据子网掩码就可知道子网号与主机号之间的分界线。 将来或许会有更多的主机和网络，但是为了不让主机跨越不同的网络就得使用不同的子网号。我们的解决方法是把子网号从8 bit扩充到11 bit，把主机号从8 bit减为5 bit。这就叫作变长子网，因为140.252网络中的大多数子网都采用8 bit子网掩码，而我们的子网却采用11 bit的子网掩码。 作者子网中的IP地址结构如图3-11所示，11位子网号中的前8bit始终是13。在剩下的3 bit中，我们用二进制001表示以太网，010表示点对点SLIP链路。这个变长子网掩码在140.252网络中不会给其他主机和路由器带来问题—只要目的是子网140.252.13的所有数据报都传给路由器sun（IP地址是140.252.1.29），如图3-11所示。如果sun知道子网13中的主机有11bit子网号，那么一切都好办了。 140.252.13子网中的所有接口的子网掩码是255.255.255.224，或0xffffffe0。这表明最右边的5bit留给主机号，左边的27bit留给网络号和子网号。 ifconfig命令12345678910111213141516171819202122232425262728293031[jiankunking@VM_0_3_centos ~]# ifconfig -adocker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 ether 02:42:a5:fc:a8:b0 txqueuelen 0 (Ethernet) RX packets 1803610 bytes 751152220 (716.3 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1715508 bytes 1981302829 (1.8 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.21.0.3 netmask 255.255.240.0 broadcast 172.21.15.255 ether 52:54:00:9f:9a:52 txqueuelen 1000 (Ethernet) RX packets 25626090 bytes 6434619435 (5.9 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 27765770 bytes 6026731597 (5.6 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 2 bytes 272 (272.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 2 bytes 272 (272.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0veth7423cb0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 ether 0a:19:63:74:aa:32 txqueuelen 0 (Ethernet) RX packets 1939 bytes 158399 (154.6 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1810 bytes 2114402 (2.0 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 环回接口被认为是一个网络接口。它是一个 A类地址，没有进行子网划分。 小结在进行路由选择决策时，主机和路由器都使用路由表。在表中有三种类型的路由：特定主机型、特定网络型和默认路由型。路由表中的表目具有一定的优先级。在选择路由时，主机路由优先于网络路由，最后在没有其他可选路由存在时才选择默认路由。 IP路由选择是通过逐跳来实现的。数据报在各站的传输过程中目的 IP地址始终不变，但是封装和目的链路层地址在每一站都可以改变。大多数的主机和许多路由器对于非本地网络的数据报都使用默认的下一站路由器。 ARP：地址解析协议引言当一台主机把以太网数据帧发送到位于同一局域网上的另一台主机时，是根据 48 bit的以太网地址来确定目的接口的。设备驱动程序从不检查 IP数据报中的目的IP地址。 地址解析为这两种不同的地址形式提供映射： 32 bit的IP地址和数据链路层使用的任何类型的地址。 ARP为IP地址到对应的硬件地址之间提供动态映射。我们之所以用动态这个词是因为这个过程是自动完成的，一般应用程序用户或系统管理员不必关心。 RARP是被那些没有磁盘驱动器的系统使用（一般是无盘工作站或X终端），它需要系统管理员进行手工设置。 一个例子任何时候我们敲入下面这个形式的命令： 1%ftp bsdi 都会进行以下这些步骤。这些步骤的序号如图4-2所示。 应用程序FTP客户端调用函数gethostbyname(3)把主机名（bsdi）转换成32bit的IP地址。这个函数在DNS（域名系统）中称作解析器，我们将在第14章对它进行介绍。这个转换过程或者使用DNS，或者在较小网络中使用一个静态的主机文件（/etc/hosts）。 FTP客户端请求TCP用得到的IP地址建立连接。 TCP发送一个连接请求分段到远端的主机，即用上述IP地址发送一份IP数据报。 如果目的主机在本地网络上（如以太网、令牌环网或点对点链接的另一端），那么IP数据报可以直接送到目的主机上。如果目的主机在一个远程网络上，那么就通过IP选路函数来确定位于本地网络上的下一站路由器地址，并让它转发IP数据报。在这两种情况下，IP数据报都是被送到位于本地网络上的一台主机或路由器。 假定是一个以太网，那么发送端主机必须把32bit的IP地址变换成48bit的以太网地址。从逻辑Internet地址到对应的物理硬件地址需要进行翻译。这就是ARP的功能。ARP本来是用于广播网络的，有许多主机或路由器连在同一个网络上。 ARP发送一份称作ARP请求的以太网数据帧给以太网上的每个主机。这个过程称作广播，如图4-2中的虚线所示。ARP请求数据帧中包含目的主机的IP地址（主机名为bsdi），其意思是“如果你是这个IP地址的拥有者，请回答你的硬件地址。” 目的主机的ARP层收到这份广播报文后，识别出这是发送端在寻问它的IP地址，于是发送一个ARP应答。这个ARP应答包含IP地址及对应的硬件地址。 收到ARP应答后，使ARP进行请求—应答交换的IP数据报现在就可以传送了。 发送IP数据报到目的主机。 在ARP背后有一个基本概念，那就是网络接口有一个硬件地址（一个48bit的值，标识不同的以太网或令牌环网络接口）。在硬件层次上进行的数据帧交换必须有正确的接口地址。但是，TCP/IP有自己的地址：32bit的IP地址。知道主机的IP地址并不能让内核发送一帧数据给主机。内核（如以太网驱动程序）必须知道目的端的硬件地址才能发送数据。ARP的功能是在32bit的IP地址和采用不同网络技术的硬件地址之间提供动态映射。 ARP高速缓存ARP高效运行的关键是由于每个主机上都有一个ARP高速缓存。这个高速缓存存放了最近Internet地址到硬件地址之间的映射记录。高速缓存中每一项的生存时间一般为20分钟，起始时间从被创建时开始算起。 我们可以用arp命令来检查ARP高速缓存。参数-a的意思是显示高速缓存中所有的内容。 12345678910111213[jiankunking@VM_0_3_centos ~]# arp -a? (169.254.0.15) at fe:ee:0b:ca:e5:69 [ether] on eth0? (169.254.0.3) at fe:ee:0b:ca:e5:69 [ether] on eth0? (172.17.0.4) at 02:42:ac:11:00:04 [ether] on docker0? (169.254.0.2) at fe:ee:0b:ca:e5:69 [ether] on eth0? (169.254.0.4) at fe:ee:0b:ca:e5:69 [ether] on eth0? (172.17.0.3) at 02:42:ac:11:00:03 [ether] on docker0? (172.17.0.2) at 02:42:ac:11:00:02 [ether] on docker0? (169.254.0.23) at fe:ee:0b:ca:e5:69 [ether] on eth0? (169.254.128.3) at fe:ee:0b:ca:e5:69 [ether] on eth0? (169.254.128.5) at fe:ee:0b:ca:e5:69 [ether] on eth0gateway (172.21.0.1) at fe:ee:0b:ca:e5:69 [ether] on eth0[jiankunking@VM_0_3_centos ~]# 48 bit的以太网地址用6个十六进制的数来表示，中间以冒号隔开。 ARP的分组格式在以太网上解析IP地址时，ARP请求和应答分组的格式如图4-3所示（ARP可以用于其他类型的网络，可以解析IP地址以外的地址。紧跟着帧类型字段的前四个字段指定了最后四个字段的类型和长度）。 以太网报头中的前两个字段是以太网的源地址和目的地址。目的地址为全 1的特殊地址是广播地址。电缆上的所有以太网接口都要接收广播的数据帧。 两个字节长的以太网帧类型表示后面数据的类型。对于ARP请求或应答来说，该字段的值为0x0806。 形容词hardware(硬件)和protocol(协议)用来描述ARP分组中的各个字段。例如，一个ARP请求分组询问协议地址（这里是IP地址）对应的硬件地址（这里是以太网地址）。 硬件类型字段表示硬件地址的类型。它的值为1即表示以太网地址。协议类型字段表示要映射的协议地址类型。它的值为0x0800即表示IP地址。它的值与包含IP数据报的以太网数据帧中的类型字段的值相同，这是有意设计的。 接下来的两个1字节的字段，硬件地址长度和协议地址长度分别指出硬件地址和协议地址的长度，以字节为单位。对于以太网上IP地址的ARP请求或应答来说，它们的值分别为6和4。 操作字段指出四种操作类型，它们是ARP请求（值为1）、ARP应答（值为2）、RARP请求（值为3）和RARP应答（值为4）（我们在第5章讨论RARP）。这个字段必需的，因为ARP请求和ARP应答的帧类型字段值是相同的。 接下来的四个字段是发送端的硬件地址（在本例中是以太网地址）、发送端的协议地址（IP地址）、目的端的硬件地址和目的端的协议地址。注意，这里有一些重复信息：在以太网的数据帧报头中和ARP请求数据帧中都有发送端的硬件地址。 对于一个ARP请求来说，除目的端硬件地址外的所有其他的字段都有填充值。当系统收到一份目的端为本机的ARP请求报文后，它就把硬件地址填进去，然后用两个目的端地址分别替换两个发送端地址，并把操作字段置为2，最后把它发送回去。 ARP代理如果ARP请求是从一个网络的主机发往另一个网络上的主机，那么连接这两个网络的路由器就可以回答该请求，这个过程称作委托ARP或ARP代理(Proxy ARP)。这样可以欺骗发起ARP请求的发送端，使它误以为路由器就是目的主机，而事实上目的主机是在路由器的“另一边”。路由器的功能相当于目的主机的代理，把分组从其他主机转发给它。 ARP代理也称作混合ARP（promiscuousARP）或ARP出租(ARPhack)。这些名字来自于ARP代理的其他用途：通过两个物理网络之间的路由器可以互相隐藏物理网络。在这种情况下，两个物理网络可以使用相同的网络号，只要把中间的路由器设置成一个ARP代理，以响应一个网络到另一个网络主机的ARP请求。这种技术在过去用来隐藏一组在不同物理电缆上运行旧版TCP/IP的主机。分开这些旧主机有两个共同的理由，其一是它们不能处理子网划分，其二是它们使用旧的广播地址（所有比特值为0的主机号，而不是目前使用的所有比特值为1的主机号）。 免费ARP我们可以看到的另一个ARP特性称作免费ARP(gratuitous ARP)。它是指主机发送ARP查找自己的IP地址。通常，它发生在系统引导期间进行接口配置的时候。 免费ARP可以有两个方面的作用： 一个主机可以通过它来确定另一个主机是否设置了相同的IP地址。主机bsdi并不希望对此请求有一个回答。但是，如果收到一个回答，那么就会在终端日志上产生一个错误消息“以太网地址：a:b:c:d:e:f发送来重复的IP地址”。这样就可以警告系统管理员，某个系统有不正确的设置。 如果发送免费ARP的主机正好改变了硬件地址（很可能是主机关机了，并换了一块接口卡，然后重新启动），那么这个分组就可以使其他主机高速缓存中旧的硬件地址进行相应的更新。一个比较著名的ARP协议事实[Plummer1982]是，如果主机收到某个IP地址的ARP请求，而且它已经在接收者的高速缓存中，那么就要用ARP请求中的发送端硬件地址（如以太网地址）对高速缓存中相应的内容进行更新。主机接收到任何ARP请求都要完成这个操作（ARP请求是在网上广播的，因此每次发送ARP请求时网络上的所有主机都要这样做）。 arp命令 参数-a来显示ARP高速缓存中的所有内容 超级用户可以用选项-d来删除ARP高速缓存中的某一项内容 可以通过选项-s来增加高速缓存中的内容。这个参数需要主机名和以太网地址：对应于主机名的IP地址和以太网地址被增加到高速缓存中。新增加的内容是永久性的（比如，它没有超时值），除非在命令行的末尾附上关键字temp。 位于命令行末尾的关键字pub和-s选项一起，可以使系统起着主机ARP代理的作用。系统将回答与主机名对应的IP地址的ARP请求，并以指定的以太网地址作为应答。如果广播的地址是系统本身，那么系统就为指定的主机名起着委托ARP代理的作用。 RARP：逆地址解析协议引言具有本地磁盘的系统引导时，一般是从磁盘上的配置文件中读取IP地址。但是无盘机，如X终端或无盘工作站，则需要采用其他方法来获得IP地址。 网络上的每个系统都具有唯一的硬件地址，它是由网络接口生产厂家配置的。无盘系统的RARP实现过程是从接口卡上读取唯一的硬件地址，然后发送一份RARP请求（一帧在网络上广播的数据），请求某个主机响应该无盘系统的IP地址（在RARP应答中）。 RARP的分组格式RARP分组的格式与ARP分组基本一致（见图4-3）。它们之间主要的差别是RARP请求或应答的帧类型代码为0x8035，而且RARP请求的操作代码为3，应答操作代码为4。 对应于ARP，RARP请求以广播方式传送，而RARP应答一般是单播(unicast)传送的。 RARP服务器的设计虽然RARP在概念上很简单，但是一个RARP服务器的设计与系统相关而且比较复杂。相反，提供一个ARP服务器很简单，通常是TCP/IP在内核中实现的一部分。由于内核知道IP地址和硬件地址，因此当它收到一个询问IP地址的ARP请求时，只需用相应的硬件地址来提供应答就可以了。 作为用户进程的RARP服务器RARP服务器的复杂性在于，服务器一般要为多个主机（网络上所有的无盘系统）提供硬件地址到IP地址的映射。该映射包含在一个磁盘文件中（在Unix系统中一般位于/etc/ethers目录中）。由于内核一般不读取和分析磁盘文件，因此RARP服务器的功能就由用户进程来提供，而不是作为内核的TCP/IP实现的一部分。 更为复杂的是，RARP请求是作为一个特殊类型的以太网数据帧来传送的（帧类型字段值为0x8035）。这说明RARP服务器必须能够发送和接收这种类型的以太网数据帧。在附录A中，我们描述了BSD分组过滤器、Sun的网络接口栓以及SVR4数据链路提供者接口都可用来接收这些数据帧。由于发送和接收这些数据帧与系统有关，因此RARP服务器的实现是与系统捆绑在一起的。 每个网络有多个RARP服务器RARP服务器实现的一个复杂因素是RARP请求是在硬件层上进行广播的。这意味着它们不经过路由器进行转发。为了让无盘系统在RARP服务器关机的状态下也能引导，通常在一个网络上（例如一根电缆）要提供多个RARP服务器。 当服务器的数目增加时（以提供冗余备份），网络流量也随之增加，因为每个服务器对每个RARP请求都要发送RARP应答。发送RARP请求的无盘系统一般采用最先收到的RARP应答（对于ARP，我们从来没有遇到这种情况，因为只有一台主机发送ARP应答）。另外，还有一种可能发生的情况是每个RARP服务器同时应答，这样会增加以太网发生冲突的概率。 小结RARP协议是许多无盘系统在引导时用来获取IP地址的。RARP分组格式基本上与ARP分组一致。一个RARP请求在网络上进行广播，它在分组中标明发送端的硬件地址，以请求相应IP地址的响应。应答通常是单播传送的。 RARP带来的问题包括使用链路层广播，这样就阻止大多数路由器转发RARP请求，只返回很少信息：只是系统的IP地址。 虽然RARP在概念上很简单，但是RARP服务器的实现却与系统相关。因此，并不是所有的TCP/IP实现都提供RARP服务器。 ICMP：Internet控制报文协议引言 ICMP（Internet Control Message Protocol） ICMP经常被认为是IP层的一个组成部分。它传递差错报文以及其他需要注意的信息。ICMP报文通常被IP层或更高层协议（TCP或UDP）使用。一些ICMP报文把差错报文返回给用户进程。 ICMP报文是在IP数据报内部被传输的，如图6-1所示。 ICMP报文的格式如图6-2所示。所有报文的前4个字节都是一样的，但是剩下的其他字节则互不相同。下面我们将逐个介绍各种报文格式。 类型字段可以有15个不同的值，以描述特定类型的ICMP报文。某些ICMP报文还使用代码字段的值来进一步描述不同的条件。 检验和字段覆盖整个ICMP报文。ICMP的检验和是必需的。 ICMP报文的类型各种类型的ICMP报文如图6-3所示，不同类型由报文中的类型字段和代码字段来共同决定。 图中的最后两列表明ICMP报文是一份查询报文还是一份差错报文。因为对ICMP差错报文有时需要作特殊处理，因此我们需要对它们进行区分。例如，在对ICMP差错报文进行响应时，永远不会生成另一份ICMP差错报文（如果没有这个限制规则，可能会遇到一个差错产生另一个差错的情况，而差错再产生差错，这样会无休止地循环下去）。 当发送一份ICMP差错报文时，报文始终包含IP的首部和产生ICMP差错报文的IP数据报的前8个字节。这样，接收ICMP差错报文的模块就会把它与某个特定的协议（根据IP数据报首部中的协议字段来判断）和用户进程（根据包含在IP数据报前8个字节中的TCP或UDP报文首部中的TCP或UDP端口号来判断）联系起来。 下面各种情况都不会导致产生ICMP差错报文： ICMP差错报文（但是，ICMP查询报文可能会产生ICMP差错报文）。 目的地址是广播地址或多播地址（D类地址）的IP数据报。 作为链路层广播的数据报。 不是IP分片的第一片。 源地址不是单个主机的数据报。这就是说，源地址不能为零地址、环回地址、广播地址或多播地址。 这些规则是为了防止过去允许ICMP差错报文对广播分组响应所带来的广播风暴。 ICMP地址掩码请求与应答ICMP地址掩码请求用于无盘系统在引导过程中获取自己的子网掩码。系统广播它的ICMP请求报文（这一过程与无盘系统在引导过程中用RARP获取IP地址是类似的）。无盘系统获取子网掩码的另一个方法是BOOTP协议。 ICMP地址掩码请求和应答报文的格式如图6-4所示。 ICMP报文中的标识符和序列号字段由发送端任意选择设定，这些值在应答中将被返回。这样，发送端就可以把应答与请求进行匹配。 Ping程序ping测试主机之间网络的连通性 补充说明ping命令 用来测试主机之间网络的连通性。执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。 语法 1ping(选项)(参数) 选项 1234567891011121314-d：使用Socket的SO_DEBUG功能；-c&lt;完成次数&gt;：设置完成要求回应的次数；-f：极限检测；-i&lt;间隔秒数&gt;：指定收发信息的间隔时间；-I&lt;网络界面&gt;：使用指定的网络界面送出数据包；-l&lt;前置载入&gt;：设置在送出要求信息之前，先行发出的数据包；-n：只输出数值；-p&lt;范本样式&gt;：设置填满数据包的范本样式；-q：不显示指令执行过程，开头和结尾的相关信息除外；-r：忽略普通的Routing Table，直接将数据包送到远端主机上；-R：记录路由过程；-s&lt;数据包大小&gt;：设置数据包的大小；-t&lt;存活数值&gt;：设置存活数值TTL的大小；-v：详细显示指令的执行过程。 参数目的主机：指定发送ICMP报文的目的主机。 引言“ping”这个名字源于声纳定位操作。Ping程序由MikeMuuss编写，目的是为了测试另一台主机是否可达。该程序发送一份ICMP回显请求报文给主机，并等待返回ICMP回显应答（图6-3列出了所有的ICMP报文类型）。 一般来说，如果不能Ping到某台主机，那么就不能Telnet或者FTP到那台主机。反过来，如果不能Telnet到某台主机，那么通常可以用Ping程序来确定问题出在哪里。Ping程序还能测出到这台主机的往返时间，以表明该主机离我们有“多远”。 Ping程序我们称发送回显请求的ping程序为客户，而称被ping的主机为服务器。大多数的TCP/IP实现都在内核中直接支持Ping服务器—这种服务器不是一个用户进程。 ICMP回显请求和回显应答报文如图7-1所示。 对于其他类型的ICMP查询报文，服务器必须响应标识符和序列号字段。另外，客户发送的选项数据必须回显，假设客户对这些信息都会感兴趣。 Unix系统在实现ping程序时是把ICMP报文中的标识符字段置成发送进程的ID号。这样即使在同一台主机上同时运行了多个ping程序实例，ping程序也可以识别出返回的信息。 序列号从0开始，每发送一次新的回显请求就加1。ping程序打印出返回的每个分组的序列号，允许我们查看是否有分组丢失、失序或重复。IP是一种最好的数据报传递服务，因此这三个条件都有可能发生。 旧版本的ping程序曾经以这种模式运行，即每秒发送一个回显请求，并打印出返回的每个回显应答。但是，新版本的实现需要加上-s选项才能以这种模式运行。默认情况下，新版本的ping程序只发送一个回显请求。如果收到回显应答，则输出“host is alive”；否则，在20秒内没有收到应答就输出“no answer（没有回答）”。 LAN/WAN 输出12345678910111213[jiankunking@VM_0_3_centos ~]# ping www.jiankunking.comPING www.jiankunking.com (139.199.31.69) 56(84) bytes of data.64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=1 ttl=63 time=0.336 ms64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=2 ttl=63 time=0.288 ms64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=3 ttl=63 time=0.295 ms64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=4 ttl=63 time=0.295 ms64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=5 ttl=63 time=0.323 ms64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=6 ttl=63 time=0.324 ms^C 键入中断来停止显示--- www.jiankunking.com ping statistics ---6 packets transmitted, 6 received, 0% packet loss, time 6510msrtt min/avg/max/mdev = 0.288/0.310/0.336/0.020 ms[jiankunking@VM_0_3_centos ~]# 默认情况下发送的ICMP报文有56个字节。再加上20个字节的IP首部和8个字节的ICMP首部，IP数据报的总长度为84字节。 当返回ICMP回显应答时，要打印出序列号和TTL，并计算往返时间（TTL位于IP首部中的生存时间字段。当前的BSD系统中的ping程序每次收到回显应答时都打印出收到的TTL—有些系统并不这样做。）。 从上面的输出中可以看出，回显应答是以发送的次序返回的（0，1，2等）。 ping程序通过在ICMP报文数据中存放发送请求的时间值来计算往返时间。当应答返回时，用当前时间减去存放在ICMP报文中的时间值，即是往返时间。 通过广域网还有可能看到重复的分组（即相同序列号的分组被打印两次或更多次），失序的分组（序列号为N + 1的分组在序列号为N的分组之前被打印）。 IP记录路由选项ping程序为我们提供了查看IP记录路由（RR）选项的机会。大多数不同版本的ping程序都提供-R选项，以提供记录路由的功能。它使得ping程序在发送出去的IP数据报中设置IP RR选项（该IP数据报包含ICMP回显请求报文）。这样，每个处理该数据报的路由器都把它的IP地址放入选项字段中。当数据报到达目的端时，IP地址清单应该复制到ICMP回显应答中，这样返回途中所经过的路由器地址也被加入清单中。当ping程序收到回显应答时，它就打印出这份IP地址清单。 123456789101112131415161718192021222324[jiankunking@VM_0_3_centos ~]# ping -R www.jiankunking.comPING www.jiankunking.com (139.199.31.69) 56(124) bytes of data.64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=1 ttl=63 time=13.3 msRR: VM_0_3_centos (172.21.0.3) 10.53.209.129 (10.53.209.129) VM_0_3_centos (172.21.0.3) VM_0_3_centos (172.21.0.3) 10.53.209.130 (10.53.209.130) VM_0_3_centos (172.21.0.3)64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=2 ttl=63 time=4.91 ms (same route)64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=3 ttl=63 time=6.81 ms (same route)64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=4 ttl=63 time=5.03 ms (same route)64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=5 ttl=63 time=4.59 ms (same route)64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=6 ttl=63 time=4.77 ms (same route)64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=7 ttl=63 time=5.18 ms (same route)64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=8 ttl=63 time=3.69 ms (same route)64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=9 ttl=63 time=4.24 ms (same route)64 bytes from 139.199.31.69 (139.199.31.69): icmp_seq=10 ttl=63 time=5.43 ms (same route)^C--- www.jiankunking.com ping statistics ---11 packets transmitted, 11 received, 0% packet loss, time 10015msrtt min/avg/max/mdev = 3.697/5.696/13.385/2.545 ms[jiankunking@VM_0_3_centos ~]# 但是，最大的问题是IP首部中只有有限的空间来存放IP地址。我们从图3-1可以看到，IP首部中的首部长度字段只有4bit，因此整个IP首部最长只能包括15个32bit长的字（即60个字节）。由于IP首部固定长度为20字节，RR选项用去3个字节（下面我们再讨论），这样只剩下37个字节（60-20-3）来存放IP地址清单，也就是说只能存放9个IP地址。对于早期的ARPANET来说，9个IP地址似乎是很多了，但是现在看来是非常有限的。除了这些缺点，记录路由选项工作得很好，为详细查看如何处理IP选项提供了一个机会。 IP数据报中的RR选项的一般格式如图7-3所示。 code是一个字节，指明IP选项的类型。对于RR选项来说，它的值为7。len是RR选项总字节长度，在这种情况下为39（尽管可以为RR选项设置比最大长度小的长度，但是ping程序是提供39字节的选项字段，最多可以记录9个IP地址。由于IP首部中留给选项的空间有限，它一般情况都设置成最大长度）。 ptr称作指针字段。它是一个基于1的指针，指向存放下一个IP地址的位置。它的最小值为4，指向存放第一个IP地址的位置。随着每个IP地址存入清单，ptr的值分别为8，12，16，最大到36。当记录下9个IP地址后，ptr的值为40，表示清单已满。 当路由器（根据定义应该是多穴的）在清单中记录IP地址时，它应该记录哪个地址呢？是入口地址还是出口地址？为此，RFC791[Postel1981a]指定路由器记录出口IP地址。我们在后面将看到，当原始主机（运行ping程序的主机）收到带有RR选项的ICMP回显应答时，它也要把它的入口IP地址放入清单中。 小结ping程序是对两个TCP/IP系统连通性进行测试的基本工具。它只利用ICMP回显请求和回显应答报文，而不用经过传输层（TCP/UDP）。Ping服务器一般在内核中实现ICMP的功能。 Traceroute程序traceroute显示数据包到主机间的路径 补充说明traceroute命令 用于追踪数据包在网络上的传输时的全部路径，它默认发送的数据包大小是40字节。 通过traceroute我们可以知道信息从你的计算机到互联网另一端的主机是走的什么路径。当然每次数据包由某一同样的出发点（source）到达某一同样的目的地(destination)走的路径可能会不一样，但基本上来说大部分时候所走的路由是相同的。 traceroute通过发送小的数据包到目的设备直到其返回，来测量其需要多长时间。一条路径上的每个设备traceroute要测3次。输出结果中包括每次测试的时间(ms)和设备的名称（如有的话）及其ip地址。语法 1traceroute(选项)(参数) 选项 123456789101112131415-d：使用Socket层级的排错功能；-f&lt;存活数值&gt;：设置第一个检测数据包的存活数值TTL的大小；-F：设置勿离断位；-g&lt;网关&gt;：设置来源路由网关，最多可设置8个；-i&lt;网络界面&gt;：使用指定的网络界面送出数据包；-I：使用ICMP回应取代UDP资料信息；-m&lt;存活数值&gt;：设置检测数据包的最大存活数值TTL的大小；-n：直接使用IP地址而非主机名称；-p&lt;通信端口&gt;：设置UDP传输协议的通信端口；-r：忽略普通的Routing Table，直接将数据包送到远端主机上。-s&lt;来源地址&gt;：设置本地主机送出数据包的IP地址；-t&lt;服务类型&gt;：设置检测数据包的TOS数值；-v：详细显示指令的执行过程；-w&lt;超时秒数&gt;：设置等待远端主机回报的时间；-x：开启或关闭数据包的正确性检验。 参数主机：指定目的主机IP地址或主机名。 引言Traceroute程序可以让我们看到IP数据报从一台主机传到另一台主机所经过的路由。 Traceroute程序的操作在上节中，我们描述了IP记录路由选项（RR）。为什么不使用这个选项而另外开发一个新的应用程序？有三个方面的原因。首先，原先并不是所有的路由器都支持记录路由选项，因此该选项在某些路径上不能使用（Traceroute程序不需要中间路由器具备任何特殊的或可选的功能）。 其次，记录路由一般是单向的选项。发送端设置了该选项，那么接收端不得不从收到的IP首部中提取出所有的信息，然后全部返回给发送端。在上节中，我们看到大多数Ping服务器的实现（内核中的ICMP回显应答功能）把接收到的RR清单返回，但是这样使得记录下来的IP地址翻了一番（一来一回）。这样做会受到一些限制，这一点我们在下一段讨论（Traceroute程序只需要目的端运行一个UDP模块—其他不需要任何特殊的服务器应用程序）。 最后一个原因也是最主要的原因是，IP首部中留给选项的空间有限，不能存放当前大多数的路径。在IP首部选项字段中最多只能存放9个IP地址。在原先的ARPANET中这是足够的，但是对现在来说是远远不够的。 Traceroute程序使用ICMP报文和IP首部中的TTL字段（生存周期）。TTL字段是由发送端初始设置一个8bit字段。推荐的初始值由分配数字RFC指定，当前值为64。较老版本的系统经常初始化为15或32。我们从第7章中的一些ping程序例子中可以看出，发送ICMP回显应答时经常把TTL设为最大值255。 每个处理数据报的路由器都需要把TTL的值减1或减去数据报在路由器中停留的秒数。由于大多数的路由器转发数据报的时延都小于1秒钟，因此TTL最终成为一个跳站的计数器，所经过的每个路由器都将其值减1。 RFC 1009 [Braden and Postel 1987]指出，如果路由器转发数据报的时延超过1秒，那么它将把TTL值减去所消耗的时间（秒数）。但很少有路由器这么实现。新的路由器需求文档RFC [Almquist 1993]为此指定它为可选择功能，允许把TTL看成一个跳站计数器。 TTL字段的目的是防止数据报在选路时无休止地在网络中流动。例如，当路由器瘫痪或者两个路由器之间的连接丢失时，选路协议有时会去检测丢失的路由并一直进行下去。在这段时间内，数据报可能在循环回路被终止。TTL字段就是在这些循环传递的数据报上加上一个生存上限。 当路由器收到一份IP数据报，如果其TTL字段是0或1，则路由器不转发该数据报（接收到这种数据报的目的主机可以将它交给应用程序，这是因为不需要转发该数据报。但是在通常情况下，系统不应该接收TTL字段为0的数据报）。相反，路由器将该数据报丢弃，并给信源机发一份ICMP“超时”信息。Traceroute程序的关键在于包含这份ICMP信息的IP报文的信源地址是该路由器的IP地址。 我们现在可以猜想一下Traceroute程序的操作过程。它发送一份TTL字段为1的IP数据报给目的主机。处理这份数据报的第一个路由器将TTL值减1，丢弃该数据报，并发回一份超时ICMP报文。这样就得到了该路径中的第一个路由器的地址。然后Traceroute程序发送一份TTL值为2的数据报，这样我们就可以得到第二个路由器的地址。继续这个过程直至该数据报到达目的主机。但是目的主机哪怕接收到TTL值为1的IP数据报，也不会丢弃该数据报并产生一份超时ICMP报文，这是因为数据报已经到达其最终目的地。那么我们该如何判断是否已经到达目的主机了呢？ Traceroute程序发送一份UDP数据报给目的主机，但它选择一个不可能的值作为UDP端口号（大于30000），使目的主机的任何一个应用程序都不可能使用该端口。因为，当该数据报到达时，将使目的主机的UDP模块产生一份“端口不可达”错误的ICMP报文。这样，Traceroute程序所要做的就是区分接收到的ICMP报文是超时还是端口不可达，以判断什么时候结束。 Traceroute程序必须可以为发送的数据报设置TTL字段。并非所有与TCP/IP接口的程序都支持这项功能，同时并非所有的实现都支持这项能力，但目前大部分系统都支持这项功能，并可以运行Traceroute程序。这个程序界面通常要求用户具有超级用户权限，这意味着它可能需要特殊的权限以在你的主机上运行该程序。 IP源站选路选项通常IP路由是动态的，即每个路由器都要判断数据报下面该转发到哪个路由器。应用程序对此不进行控制，而且通常也并不关心路由。它采用类似Traceroute程序的工具来发现实际的路由。 源站选路(source routing)的思想是由发送者指定路由。它可以采用以下两种形式： 严格的源路由选择。发送端指明IP数据报所必须采用的确切路由。如果一个路由器发现源路由所指定的下一个路由器不在其直接连接的网络上，那么它就返回一个“源站路由失败”的ICMP差错报文。 宽松的源站选路。发送端指明了一个数据报经过的IP地址清单，但是数据报在清单上指明的任意两个地址之间可以通过其他路由器。 IP选路引言选路是IP最重要的功能之一。需要进行选路的数据报可以由本地主机产生，也可以由其他主机产生。在后一种情况下，主机必须配置成一个路由器，否则通过网络接口接收到的数据报，如果目的地址不是本机就要被丢弃（例如，悄无声息地被丢弃）。 netstat查看Linux中网络系统状态信息。 补充说明netstat命令 用来打印Linux中网络系统的状态信息，可让你得知整个Linux系统的网络情况。 语法 1netstat(选项) 选项 123456789101112131415161718192021222324-a或--all：显示所有连线中的Socket；-A&lt;网络类型&gt;或--&lt;网络类型&gt;：列出该网络类型连线中的相关地址；-c或--continuous：持续列出网络状态；-C或--cache：显示路由器配置的快取信息；-e或--extend：显示网络其他相关信息；-F或--fib：显示FIB；-g或--groups：显示多重广播功能群组组员名单；-h或--help：在线帮助；-i或--interfaces：显示网络界面信息表单；-l或--listening：显示监控中的服务器的Socket；-M或--masquerade：显示伪装的网络连线；-n或--numeric：直接使用ip地址，而不通过域名服务器；-N或--netlink或--symbolic：显示网络硬件外围设备的符号连接名称；-o或--timers：显示计时器；-p或--programs：显示正在使用Socket的程序识别码和程序名称；-r或--route：显示Routing Table；-s或--statistice：显示网络工作信息统计表；-t或--tcp：显示TCP传输协议的连线状况；-u或--udp：显示UDP传输协议的连线状况；-v或--verbose：显示指令执行过程；-V或--version：显示版本信息；-w或--raw：显示RAW传输协议的连线状况；-x或--unix：此参数的效果和指定&quot;-A unix&quot;参数相同；--ip或--inet：此参数的效果和指定&quot;-A inet&quot;参数相同。 选路的原理开始讨论IP选路之前，首先要理解内核是如何维护路由表的。路由表中包含的信息决定了IP层所做的所有决策。 IP搜索路由表的几个步骤： 搜索匹配的主机地址； 搜索匹配的网络地址； 搜索默认表项（默认表项一般在路由表中被指定为一个网络表项，其网络号为0）。 匹配主机地址步骤始终发生在匹配网络地址步骤之前。 IP层进行的选路实际上是一种选路机制，它搜索路由表并决定向哪个网络接口发送分组。这区别于选路策略，它只是一组决定把哪些路由放入路由表的规则。IP执行选路机制，而路由守护程序则一般提供选路策略。 简单路由表首先来看一看一些典型的主机路由表。在主机上，我们先执行带-r选项的netstat命令列出路由表，然后以-n选项再次执行该命令，以数字格式打印出IP地址。 12345678[jiankunking@VM_0_3_centos ~]# netstat -rnKernel IP routing tableDestination Gateway Genmask Flags MSS Window irtt Iface0.0.0.0 172.21.0.1 0.0.0.0 UG 0 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0172.21.0.0 0.0.0.0 255.255.240.0 U 0 0 0 eth0[jiankunking@VM_0_3_centos ~]# 对于一个给定的路由器，可以打印出五种不同的标志（flag）： 标志 解释 U 该路由可以使用。 G 该路由是到一个网关（路由器）。如果没有设置该标志，说明目的地是直接相连的。 H 该路由是到一个主机，也就是说，目的地址是一个完整的主机地址。如果没有设置该标志，说明该路由是到一个网络，而目的地址是一个网络地址：一个网络号，或者网络号与子网号的组合。 D 该路由是由重定向报文创建的。 M 该路由已被重定向报文修改。 标志G是非常重要的，因为由它区分了间接路由和直接路由（对于直接路由来说是不设置标志G的）。其区别在于，发往直接路由的分组中不但具有指明目的端的IP地址，还具有其链路层地址。当分组被发往一个间接路由时，IP地址指明的是最终的目的地，但是链路层地址指明的是网关（即下一站路由器）。 理解G和H标志之间的区别是很重要的。G标志区分了直接路由和间接路由，如上所述。但是H标志表明，目的地址（netstat命令输出第一行）是一个完整的主机地址。没有设置H标志说明目的地址是一个网络地址（主机号部分为0）。当为某个目的IP地址搜索路由表时，主机地址项必须与目的地址完全匹配，而网络地址项只需要匹配目的地址的网络号和子网号就可以了。另外，大多数版本的netstat命令首先打印出所有的主机路由表项，然后才是网络路由表项。 ICMP主机与网络不可达差错当路由器收到一份IP数据报但又不能转发时，就要发送一份ICMP“主机不可达”差错报文。 ICMP重定向差错当IP数据报应该被发送到另一个路由器时，收到数据报的路由器就要发送ICMP重定向差错报文给IP数据报的发送端。这在概念上是很简单的，正如图9-3所示的那样。只有当主机可以选择路由器发送分组的情况下，我们才可能看到ICMP重定向报文。 我们假定主机发送一份IP数据报给R1。这种选路决策经常发生，因为R1是该主机的默认路由。 R1收到数据报并且检查它的路由表，发现R2是发送该数据报的下一站。当它把数据报发送给R2时，R1检测到它正在发送的接口与数据报到达接口是相同的（即主机和两个路由器所在的LAN）。这样就给路由器发送重定向报文给原始发送端提供了线索。 R1发送一份ICMP重定向报文给主机，告诉它以后把数据报发送给R2而不是R1。 重定向一般用来让具有很少选路信息的主机逐渐建立更完善的路由表。主机启动时路由表中可以只有一个默认表项（在图9-3所示的例子中，为R1或R2）。一旦默认路由发生差错，默认路由器将通知它进行重定向，并允许主机对路由表作相应的改动。ICMP重定向允许TCP/IP主机在进行选路时不需要具备智能特性，而把所有的智能特性放在路由器端。 小结系统产生的或转发的每份IP数据报都要搜索路由表，它可以被路由守护程序或ICMP重定向报文修改。系统在默认情况下不转发数据报，除非进行特殊的配置。用route命令可以进入静态路由，可以利用新ICMP路由器发现报文来初始化默认表项，并进行动态修改。主机在启动时只有一个简单的路由表，它可以被来自默认路由器的ICMP重定向报文动态修改。 动态选路协议在前面各章中，我们讨论了静态选路。在配置接口时，以默认方式生成路由表项（对于直接连接的接口），并通过route命令增加表项（通常从系统自引导程序文件），或是通过ICMP重定向生成表项（通常是在默认方式出错的情况下）。 在网络很小，且与其他网络只有单个连接点且没有多余路由时（若主路由失败，可以使用备用路由），采用这种方法是可行的。如果上述三种情况不能全部满足，通常使用动态选路。 动态选路当相邻路由器之间进行通信，以告知对方每个路由器当前所连接的网络，这时就出现了动态选路。路由器之间必须采用选路协议进行通信，这样的选路协议有很多种。路由器上有一个进程称为路由守护程序（ routing daemon），它运行选路协议，并与其相邻的一些路由器进行通信。 路由守护程序将选路策略（routing policy）加入到系统中，选择路由并加入到内核的路由表中。如果守护程序发现前往同一信宿存在多条路由，那么它（以某种方法）将选择最佳路由并加入内核路由表中。如果路由守护程序发现一条链路已经断开（可能是路由器崩溃或电话线路不好），它可以删除受影响的路由或增加另一条路由以绕过该问题。 在像Internet这样的系统中，目前采用了许多不同的选路协议。Internet是以一组自治系统(AS，Autonomous System)的方式组织的，每个自治系统通常由单个实体管理。常常将一个公司或大学校园定义为一个自治系统。NSFNET的Internet骨干网形成一个自治系统，这是因为骨干网中的所有路由器都在单个的管理控制之下。 每个自治系统可以选择该自治系统中各个路由器之间的选路协议。这种协议我们称之为内部网关协议IGP（Interior Gateway Protocol）或域内选路协议（intradomain routing protocol）。最常用的IGP是选路信息协议RIP。一种新的IGP是开放最短路径优先OSPF（Open Shortest PathFirst）协议。它意在取代RIP。 新的RFC[Almquist 1993]规定，实现任何动态选路协议的路由器必须同时支持OSPF和RIP，还可以支持其他IGP协议。 RIP：选路信息协议正常运行让我们来看一下采用RIP协议的routed程序正常运行的结果。RIP常用的UDP端口号是520。 初始化：在启动一个路由守护程序时，它先判断启动了哪些接口，并在每个接口上发送一个请求报文，要求其他路由器发送完整路由表。在点对点链路中，该请求是发送给其他终点的。如果网络支持广播的话，这种请求是以广播形式发送的。目的UDP端口号是520（这是其他路由器的路由守护程序端口号）。这种请求报文的命令字段为1，但地址系列字段设置为0，而度量字段设置为16。这是一种要求另一端完整路由表的特殊请求报文。 接收到请求。如果这个请求是刚才提到的特殊请求，那么路由器就将完整的路由表发送给请求者。否则，就处理请求中的每一个表项：如果有连接到指明地址的路由，则将度量设置成我们的值，否则将度量置为16（度量为16是一种称为“无穷大”的特殊值，它意味着没有到达目的的路由）。然后发回响应。 接收到响应。使响应生效，可能会更新路由表。可能会增加新表项，对已有的表项进行修改，或是将已有表项删除。 定期选路更新。每过30秒，所有或部分路由器会将其完整路由表发送给相邻路由器。发送路由表可以是广播形式的（如在以太网上），或是发送给点对点链路的其他终点的。 触发更新。每当一条路由的度量发生变化时，就对它进行更新。不需要发送完整路由表，而只需要发送那些发生变化的表项。 每条路由都有与之相关的定时器。如果运行RIP的系统发现一条路由在3分钟内未更新，就将该路由的度量设置成无穷大（16），并标注为删除。这意味着已经在6个30秒更新时间里没收到通告该路由的路由器的更新了。再过60秒，将从本地路由表中删除该路由，以保证该路由的失效已被传播开。 度量RIP所使用的度量是以跳(hop)计算的。所有直接连接接口的跳数为1。考虑图10-4所示的路由器和网络。画出的4条虚线是广播RIP报文。 路由器R1通过发送广播到N1通告它与N2之间的跳数是1（发送给N1的广播中通告它与N1之间的路由是无用的）。同时也通过发送广播给N2通告它与N1之间的跳数为1。同样，R2通告它与N2的度量为1，与N3的度量为1。 如果相邻路由器通告它与其他网络路由的跳数为1，那么我们与那个网络的度量就是2，这是因为为了发送报文到该网络，我们必须经过那个路由器。在我们的例子中，R2到N1的度量是2，与R1到N3的度量一样。 由于每个路由器都发送其路由表给邻站，因此，可以判断在同一个自治系统AS内到每个网络的路由。如果在该AS内从一个路由器到一个网络有多条路由，那么路由器将选择跳数最小的路由，而忽略其他路由。 跳数的最大值是15，这意味着RIP只能用在主机间最大跳数值为15的AS内。度量为16表示到无路由到达该IP地址。 OSPF：开放最短路径优先OSPF是除RIP外的另一个内部网关协议。它克服了RIP的所有限制。RFC1247[Moy 1991]中对第2版OSPF进行了描述。 与采用距离向量的RIP协议不同的是，OSPF是一个链路状态协议。距离向量的意思是，RIP发送的报文包含一个距离向量（跳数）。每个路由器都根据它所接收到邻站的这些距离向量来更新自己的路由表。 在一个链路状态协议中，路由器并不与其邻站交换距离信息。它采用的是每个路由器主动地测试与其邻站相连链路的状态，将这些信息发送给它的其他邻站，而邻站将这些信息在自治系统中传播出去。每个路由器接收这些链路状态信息，并建立起完整的路由表。 从实际角度来看，二者的不同点是链路状态协议总是比距离向量协议收敛更快。收敛的意思是在路由发生变化后，例如在路由器关闭或链路出故障后，可以稳定下来。 OSPF与RIP（以及其他选路协议）的不同点在于，OSPF直接使用IP。也就是说，它并不使用UDP或TCP。对于IP首部的protocol字段，OSPF有其自己的值。 另外，作为一种链路状态协议而不是距离向量协议，OSPF还有着一些优于RIP的特点。 OSPF可以对每个IP服务类型计算各自的路由集。这意味着对于任何目的，可以有多个路由表表项，每个表项对应着一个IP服务类型。 给每个接口指派一个无维数的费用。可以通过吞吐率、往返时间、可靠性或其他性能来进行指派。可以给每个IP服务类型指派一个单独的费用。 当对同一个目的地址存在着多个相同费用的路由时，OSPF在这些路由上平均分配流量。我们称之为流量平衡。 OSPF支持子网：子网掩码与每个通告路由相连。这样就允许将一个任何类型的IP地址分割成多个不同大小的子网。到一个主机的路由是通过全1子网掩码进行通告的。默认路由是以IP地址为0.0.0.0、网络掩码为全0进行通告的。 路由器之间的点对点链路不需要每端都有一个IP地址，我们称之为无编号网络。这样可以节省IP地址—现在非常紧缺的一种资源。 采用了一种简单鉴别机制。可以采用类似于RIP-2机制的方法指定一个明文口令。 OSPF采用多播，而不是广播形式，以减少不参与OSPF的系统负载。 随着大部分厂商支持OSPF，在很多网络中OSPF将逐步取代RIP。 BGP：边界网关协议BGP系统与其他BGP系统之间交换网络可到达信息。这些信息包括数据到达这些网络所必须经过的自治系统AS中的所有路径。这些信息足以构造一幅自治系统连接图。然后，可以根据连接图删除选路环，制订选路策略。 首先，我们将一个自治系统中的IP数据报分成本地流量和通过流量。在自治系统中，本地流量是起始或终止于该自治系统的流量。也就是说，其信源IP地址或信宿IP地址所指定的主机位于该自治系统中。其他的流量则称为通过流量。在Internet中使用BGP的一个目的就是减少通过流量。 可以将自治系统分为以下几种类型：1) 残桩自治系统(stub AS)，它与其他自治系统只有单个连接。 stub AS只有本地流量。2) 多接口自治系统(multihomed AS)，它与其他自治系统有多个连接，但拒绝传送通过流量。3) 转送自治系统(transit AS)，它与其他自治系统有多个连接，在一些策略准则之下，它可以传送本地流量和通过流量。 这样，可以将Internet的总拓扑结构看成是由一些残桩自治系统、多接口自治系统以及转送自治系统的任意互连。残桩自治系统和多接口自治系统不需要使用BGP——它们通过运行EGP在自治系统之间交换可到达信息。 BGP允许使用基于策略的选路。由自治系统管理员制订策略，并通过配置文件将策略指定给BGP。制订策略并不是协议的一部分，但指定策略允许BGP实现在存在多个可选路径时选择路径，并控制信息的重发送。选路策略与政治、安全或经济因素有关。 BGP与RIP和OSPF的不同之处在于BGP使用TCP作为其传输层协议。两个运行BGP的系统之间建立一条TCP连接，然后交换整个BGP路由表。从这个时候开始，在路由表发生变化时，再发送更新信号。 BGP是一个距离向量协议，但是与（通告到目的地址跳数的）RIP不同的是，BGP列举了到每个目的地址的路由（自治系统到达目的地址的序列号）。这样就排除了一些距离向量协议的问题。采用16bit数字表示自治系统标识。 BGP通过定期发送keepalive报文给其邻站来检测TCP连接对端的链路或主机失败。两个报文之间的时间间隔建议值为30秒。应用层的keepalive报文与TCP的keepalive选项是独立的。 CIDR：无类型域间选路CIDR的基本观点是采用一种分配多个IP地址的方式，使其能够将路由表中的许多表项总和(summarization)成更少的数目。例如，如果给单个站点分配16个C类地址，以一种可以用总和的方式来分配这16个地址，这样，所有这16个地址可以参照Internet上的单个路由表表项。同时，如果有8个不同的站点是通过同一个Internet服务提供商的同一个连接点接入Internet的，且这8个站点分配的8个不同IP地址可以进行总和，那么，对于这8个站点，在Internet上，只需要单个路由表表项。 要使用这种总和，必须满足以下三种特性：1) 为进行选路要对多个IP地址进行总和时，这些IP地址必须具有相同的高位地址比特。2) 路由表和选路算法必须扩展成根据 32 bit IP地址和32 bit掩码做出选路决策。3) 必须扩展选路协议使其除了32 bit地址外，还要有32 bit掩码。OSPF和RIP-2都能够携带第4版BGP所提出的32 bit掩码。 CIDR同时还使用一种技术，使最佳匹配总是最长的匹配：即在32bit掩码中，它具有最大值。 UDP：用户数据报协议引言UDP是一个简单的面向数据报的运输层协议：进程的每个输出操作都正好产生一个UDP数据报，并组装成一份待发送的IP数据报。这与面向流字符的协议不同，如TCP，应用程序产生的全体数据与真正发送的单个IP数据报可能没有什么联系。 UDP数据报封装成一份IP数据报的格式如图11-1所示。 应用程序必须关心IP数据报的长度。如果它超过网络的MTU，那么就要对IP数据报进行分片。如果需要，源端到目的端之间的每个网络都要进行分片，并不只是发送端主机连接第一个网络才这样做。 UDP首部UDP首部的各字段如图11-2所示。 UDP长度字段指的是UDP首部和UDP数据的字节长度。该字段的最小值为8字节（发送一份0字节的UDP数据报是OK）。这个UDP长度是有冗余的。IP数据报长度指的是数据报全长，因此UDP数据报长度是全长减去IP首部的长度。 UDP检验和UDP和TCP在首部中都有覆盖它们首部和数据的检验和。UDP的检验和是可选的，而TCP的检验和是必需的。 如果发送端没有计算检验和而接收端检测到检验和有差错，那么UDP数据报就要被悄悄地丢弃。不产生任何差错报文（当IP层检测到IP首部检验和有差错时也这样做）。UDP检验和是一个端到端的检验和。它由发送端计算，然后由接收端验证。其目的是为了发现UDP首部和数据在发送端到接收端之间发生的任何改动。 注意，TCP发生检验和差错的比例与UDP相比要高得多。这很可能是因为在该系统中的TCP连接经常是“远程”连接（经过许多路由器和网桥等中间设备），而UDP一般为本地通信。 IP分片物理网络层一般要限制每次发送数据帧的最大长度。任何时候IP层接收到一份要发送的IP数据报时，它要判断向本地哪个接口发送数据（选路），并查询该接口获得其MTU。IP把MTU与数据报长度进行比较，如果需要则进行分片。分片可以发生在原始发送端主机上，也可以发生在中间路由器上。 把一份IP数据报分片以后，只有到达目的地才进行重新组装（这里的重新组装与其他网络协议不同，它们要求在下一站就进行进行重新组装，而不是在最终的目的地）。重新组装由目的端的IP层来完成，其目的是使分片和重新组装过程对运输层（TCP和UDP）是透明的，除了某些可能的越级操作外。已经分片过的数据报有可能会再次进行分片（可能不止一次）。IP首部中包含的数据为分片和重新组装提供了足够的信息。 回忆IP首部（图3-1），下面这些字段用于分片过程。对于发送端发送的每份IP数据报来说，其标识字段都包含一个唯一值。该值在数据报分片时被复制到每个片中（我们现在已经看到这个字段的用途）。标志字段用其中一个比特来表示“更多的片”。除了最后一片外，其他每个组成数据报的片都要把该比特置1。片偏移字段指的是该片偏移原始数据报开始处的位置。另外，当数据报被分片后，每个片的总长度值要改为该片的长度值。 最后，标志字段中有一个比特称作“不分片”位。如果将这一比特置1，IP将不对数据报进行分片。相反把数据报丢弃并发送一个ICMP差错报文（“需要进行分片但设置了不分片比特”，见图6-3）给起始端。在下一节我们将看到出现这个差错的例子。 当IP数据报被分片后，每一片都成为一个分组，具有自己的IP首部，并在选择路由时与其他分组独立。这样，当数据报的这些片到达目的端时有可能会失序，但是在IP首部中有足够的信息让接收端能正确组装这些数据报片。 尽管IP分片过程看起来是透明的，但有一点让人不想使用它：即使只丢失一片数据也要重传整个数据报。为什么会发生这种情况呢？因为IP层本身没有超时重传的机制——由更高层来负责超时和重传（TCP有超时和重传机制，但UDP没有。一些UDP应用程序本身也执行超时和重传）。当来自TCP报文段的某一片丢失后，TCP在超时后会重发整个TCP报文段，该报文段对应于一份IP数据报。没有办法只重传数据报中的一个数据报片。事实上，如果对数据报分片的是中间路由器，而不是起始端系统，那么起始端系统就无法知道数据报是如何被分片的。就这个原因，经常要避免分片。文献[KentandMogul1987]对避免分片进行了论述。 在一个以太网上，数据帧的最大长度是1500字节，其中1472字节留给数据，假定IP首部为20字节，UDP首部为8字节。 IP数据报是指IP层端到端的传输单元（在分片之前和重新组装之后），分组是指在IP层和链路层之间传送的数据单元。一个分组可以是一个完整的IP数据报，也可以是IP数据报的一个分片。 广播和多播引言为了弄清广播和多播，需要了解主机对由信道传送过来帧的过滤过程。图12-1说明了这一过程。 首先，网卡查看由信道传送过来的帧，确定是否接收该帧，若接收后就将它传往设备驱动程序。通常网卡仅接收那些目的地址为网卡物理地址或广播地址的帧。另外，多数接口均被设置为混合模式，这种模式能接收每个帧的一个复制。作为一个例子，tcpdump使用这种模式。 目前，大多数的网卡经过配置都能接收目的地址为多播地址或某些子网多播地址的帧。对于以太网，当地址中最高字节的最低位设置为1时表示该地址是一个多播地址，用十六进制可表示为01:00:00:00:00:00（以太网广播地址ff:ff:ff:ff:ff:ff可看作是以太网多播地址的特例）。 如果网卡收到一个帧，这个帧将被传送给设备驱动程序（如果帧检验和错，网卡将丢弃该帧）。设备驱动程序将进行另外的帧过滤。首先，帧类型中必须指定要使用的协议（IP、ARP等等）。其次，进行多播过滤来检测该主机是否属于多播地址说明的多播组。设备驱动程序随后将数据帧传送给下一层，比如，当帧类型指定为IP数据报时，就传往IP层。IP根据IP地址中的源地址和目的地址进行更多的过滤检测。如果正常，就将数据报传送给下一层（如TCP或UDP）。 每次UDP收到由IP传送来的数据报，就根据目的端口号，有时还有源端口号进行数据报过滤。如果当前没有进程使用该目的端口号，就丢弃该数据报并产生一个ICMP不可达报文（TCP根据它的端口号作相似的过滤）。如果UDP数据报存在检验和错，将被丢弃。 使用广播的问题在于它增加了对广播数据不感兴趣主机的处理负荷。拿一个使用UDP广播应用作为例子。如果网内有50个主机，但仅有20个参与该应用，每次这20个主机中的一个发送UDP广播数据时，其余30个主机不得不处理这些广播数据报。一直到UDP层，收到的UDP广播数据报才会被丢弃。这30个主机丢弃UDP广播数据报是因为这些主机没有使用这个目的端口。 多播的出现减少了对应用不感兴趣主机的处理负荷。使用多播，主机可加入一个或多个多播组。这样，网卡将获悉该主机属于哪个多播组，然后仅接收主机所在多播组的那些多播帧。 广播受限的广播受限的广播地址是255.255.255.255。该地址用于主机配置过程中IP数据报的目的地址，此时，主机可能还不知道它所在网络的网络掩码，甚至连它的IP地址也不知道。 在任何情况下，路由器都不转发目的地址为受限的广播地址的数据报，这样的数据报仅出现在本地网络中。 指向网络的广播指向网络的广播地址是主机号为全1的地址。A类网络广播地址为netid.255.255.255，其中netid为A类网络的网络号。 一个路由器必须转发指向网络的广播，但它也必须有一个不进行转发的选择。 指向子网的广播指向子网的广播地址为主机号为全1且有特定子网号的地址。作为子网直接广播地址的IP地址需要了解子网的掩码。例如，如果路由器收到发往128.1.2.255的数据报，当B类网络128.1的子网掩码为255.255.255.0时，该地址就是指向子网的广播地址；但如果该子网的掩码为255.255.254.0，该地址就不是指向子网的广播地址。 指向所有子网的广播指向所有子网的广播也需要了解目的网络的子网掩码，以便与指向网络的广播地址区分开。指向所有子网的广播地址的子网号及主机号为全1。例如，如果目的子网掩码为255.255.255.0，那么IP地址128.1.255.255是一个指向所有子网的广播地址。然而，如果网络没有划分子网，这就是一个指向网络的广播。 多播IP多播提供两类服务： 向多个目的地址传送数据。 客户对服务器的请求。例如，无盘工作站需要确定启动引导服务器。目前，这项服务是通过广播来提供的，但是使用多播可降低不提供这项服务主机的负担。 多播组地址图12-2显示了D类IP地址的格式。 多播组地址包括为1110的最高4bit和多播组号。它们通常可表示为点分十进制数，范围从224.0.0.0到239.255.255.255。 能够接收发往一个特定多播组地址数据的主机集合称为主机组(hostgroup)。一个主机组可跨越多个网络。主机组中成员可随时加入或离开主机组。主机组中对主机的数量没有限制，同时不属于某一主机组的主机可以向该组发送信息。 小结广播是将数据报发送到网络中的所有主机（通常是本地相连的网络），而多播是将数据报发送到网络的一个主机组。这两个概念的基本点在于当收到送往上一个协议栈的数据帧时采用不同类型的过滤。每个协议层均可以因为不同的理由丢弃数据报。 目前有四种类型的广播地址：受限的广播、指向网络的广播、指向子网的广播和指向所有子网的广播。最常用的是指向子网的广播。受限的广播通常只在系统初始启动时才会用到。 试图通过路由器进行广播而发生的问题，常常是因为路由器不了解目的网络的子网掩码。结果与多种因素有关：广播地址类型、配置参数等等。 D类IP地址被称为多播组地址。通过将其低位23bit映射到相应以太网地址中便可实现多播组地址到以太网地址的转换。由于地址映射是不唯一的，因此需要其他的协议实现额外的数据报过滤。 IGMP：Internet组管理协议引言本章将介绍用于支持主机和路由器进行多播的Internet组管理协议（IGMP）。它让一个物理网络上的所有系统知道主机当前所在的多播组。多播路由器需要这些信息以便知道多播数据报应该向哪些接口转发。 正如ICMP一样，IGMP也被当作IP层的一部分。IGMP报文通过IP数据报进行传输。不像我们已经见到的其他协议，IGMP有固定的报文长度，没有可选数据。图13-1显示了IGMP报文如何封装在IP数据报中。 IGMP报文通过IP首部中协议字段值为2来指明。 IGMP报文图13-2显示了长度为8字节的IGMP报文格式。 这是版本为1的IGMP。IGMP类型为1说明是由多播路由器发出的查询报文，为2说明是主机发出的报告报文。检验和的计算和ICMP协议相同。组地址为D类IP地址。在查询报文中组地址设置为0，在报告报文中组地址为要参加的组地址。 DNS：域名系统引言域名系统（DNS）是一种用于TCP/IP应用程序的分布式数据库，它提供主机名字和IP地址之间的转换及有关电子邮件的选路信息。这里提到的分布式是指在Internet上的单个站点不能拥有所有的信息。 DNS 基础DNS的层次组织: 每个问题有一个查询类型，而每个响应（也称一个资源记录，我们下面将谈到）也有一个类型。大约有20个不同的类型值，其中的一些目前已经过时。图14-7显示了其中的一些值。查询类型是类型的一个超集(superset)：图中显示的类型值中只有两个能用于查询类型。 最常用的查询类型是A类型，表示期望获得查询名的IP地址。一个PTR查询则请求获得一个IP地址对应的域名。 查询类通常是1，指互联网地址（某些站点也支持其他非IP地址）。 指针查询DNS中一直难于理解的部分就是指针查询方式，即给定一个IP地址，返回与该地址对应的域名。 小结应用程序通过名字解析器将一个主机名转换为一个IP地址，也可将一个IP地址转换为与之对应的主机名。名字解析器将向一个本地名字服务器发出查询请求，这个名字服务器可能通过某个根名字服务器或其他名字服务器来完成这个查询。 TCP：传输控制协议TCP的服务TCP通过下列方式来提供可靠性： 应用数据被分割成TCP认为最适合发送的数据块。这和UDP完全不同，应用程序产生的数据报长度将保持不变。由TCP传递给IP的信息单位称为报文段或段（segment）。 当TCP发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。 当TCP收到发自TCP连接另一端的数据，它将发送一个确认。这个确认不是立即发送，通常将推迟几分之一秒。 TCP将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP将丢弃这个报文段和不确认收到此报文段（希望发端超时并重发）。 既然TCP报文段作为IP数据报来传输，而IP数据报的到达可能会失序，因此TCP报文段的到达也可能会失序。如果必要，TCP将对收到的数据进行重新排序，将收到的数据以正确的顺序交给应用层。 既然IP数据报会发生重复，TCP的接收端必须丢弃重复的数据。 TCP还能提供流量控制。TCP连接的每一方都有固定大小的缓冲空间。TCP的接收端只允许另一端发送接收端缓冲区所能接纳的数据。这将防止较快主机致使较慢主机的缓冲区溢出。 TCP对字节流的内容不作任何解释。TCP不知道传输的数据字节流是二进制数据，还是ASCII字符、EBCDIC字符或者其他类型数据。对字节流的解释由TCP连接双方的应用层解释。 这种对字节流的处理方式与Unix操作系统对文件的处理方式很相似。Unix的内核对一个应用读或写的内容不作任何解释，而是交给应用程序处理。对Unix的内核来说，它无法区分一个二进制文件与一个文本文件。 未读章节17章之后未读]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>读书笔记</tag>
        <tag>TCP</tag>
        <tag>IP</tag>
        <tag>Protocol</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go 设计模式]]></title>
    <url>%2Fgo-solid-design.html</url>
    <content type="text"><![CDATA[Golang 代码设计及规范 代码评审为什么要代码评审？ 如果代码评审是要捕捉糟糕的代码，那么你如何知道你审查的代码是好的还是糟糕的？ 我在找一些客观的方式来谈论代码的好坏属性。 糟糕的代码你可能会在代码审查中遇到以下这些糟糕的代码： Rigid - 代码是否死板？它是否有强类型或参数以至于修改起来很困难？ Fragile - 代码是否脆弱？对代码做轻微的改变是否就会引起程序极大的破坏？ Immobile - 代码是否很难重构？ Complex - 代码是否过于复杂，是否过度设计？ Verbose - 代码是否过于冗长而使用起来很费劲？当查阅代码是否很难看出来代码在做什么？ 当你做代码审查的时候是否会很高兴看到这些词语？ 当然不会。 好的设计如果有一些描述优秀的设计属性的方式就更好了，不仅仅是糟糕的设计，是否能在客观条件下做？ SOLID在2002年，Robert Martin的《Agile Software Development, Principles, Patterns, and Practices》书中提到了五个可重用软件设计的原则 - “SOLID”（英文首字母缩略字）: Single Responsibility Principle - 单一功能原则 Open/Closed Principle - 开闭原则 Liskov Substitution Principle - 里氏替换原则 Interface Segregation Principle - 接口隔离原则 Dependency Inversion Principle - 依赖反转原则 这本书有点点过时，使用的语言也是十多年前的。但是，或许SOLID原则的某些方面可以给我们一个有关如何谈论一个精心设计的Go语言程序的线索。 Single Responsibility Principle A class should have one, and only one, reason to change. –Robert C Martin 现在Go语言显然没有classses - 相反，我们有更为强大的组合的概念 - 但是如果你可以看到过去class的使用，我认为这里有其价值。 为什么一段代码应该只有一个原因改变如此重要？当然，和你自己的代码要修改比较起来，发现自己代码所依赖的代码要修改会更令人头疼。而且，当你的代码不得不要修改的时候，它应该对直接的刺激有反应，而不应该是一个间接伤害的受害者。 所以，代码有单一功能原则从而有最少的原因来改变。 Coupling&amp;Cohesion - 耦合与内聚这两个词语描绘了修改一段软件代码是何等的简单或困难。 Coupling - 耦合是两个东西一起改变 - 一个移动会引发另一个移动。Cohesion - 内聚是相关联但又隔离，一种相互吸引的力量。 在软件方面，内聚是形容代码段自然吸引到另一个的属性。 要描述Go语言的耦合与内聚，我们可以要谈论一下functions和methods，当讨论单一功能原则时它们很常见，但是我相信它始于Go语言的package模型。 Pakcage命名在Go语言中，所有的代码都在某个package中。好的package设计始于他的命名。package名字不仅描述了它的目的而且还是一个命名空间的前缀。Go语言标准库里有一些好的例子： net/http - 提供了http客户端和服务 os/exec - 执行外部的命令 encoding/json - 实现了JSON的编码与解码 在你自己的项目中使用其他pakcage时要用import声明，它会在两个package之间建立一个源码级的耦合。 糟糕的pakcage命名关注于命名并不是在卖弄。糟糕的命名会失去罗列其目的的机会。 比如说server、private、common、utils 这些糟糕的命名都很常见。这些package就像是一个混杂的场所，因为他们好多都是没有原因地经常改变。 Go语言的UNIX哲学以我的观点，涉及到解耦设计必须要提及Doug Mcllroy的Unix哲学：小巧而锋利的工具的结合解决更大的任务或者通常原创作者并没有预想到的任务。 我认为Go语言的Package体现了UNIX哲学精神。实际上每个package自身就是一个具有单一原则的变化单元的小型Go语言项目。 Open/Closed PrincipleBertrand Meyer曾经写道： Software entities should be open for extension, but closed for modification. –Bertrand Meyer, Object-Oriented Software Construction 该建议如何应用到现在的编程语言上： 12345678910111213141516171819202122package maintype A struct &#123; year int&#125;func (a A) Greet() &#123; fmt.Println(&quot;Hello GolangUK&quot;, a.year) &#125;type B struct &#123; A&#125;func (b B) Greet() &#123; fmt.Println(&quot;Welcome to GolangUK&quot;, b.year) &#125;func main() &#123; var a A a.year = 2016 var b B b.year = 2016 a.Greet() // Hello GolangUK 2016 b.Greet() // Welcome to GolangUK 2016&#125; typeA有一个year字段以及Greet方法。 typeB嵌入了A做为字段，从而，使B提供的Greet方法遮蔽了A的，调用时可以看到B的方法覆盖了A。 但是嵌入不仅仅是对于方法，它还能提供嵌入type的字段访问。如你所见，由于A和B都在同一个package内，B可以访问A的私有year字段就像B已经声明过。 因此嵌入是一个强大的工具，它允许Go语言type对扩展是开放的。 1234567891011121314151617181920212223package maintype Cat struct &#123; Name string&#125;func (c Cat) Legs() int &#123; return 4 &#125;func (c Cat) PrintLegs() &#123; fmt.Printf(&quot;I have %d legs\n&quot;, c.Legs())&#125;type OctoCat struct &#123; Cat&#125;func (o OctoCat) Legs() int &#123; return 5 &#125;func main() &#123; var octo OctoCat fmt.Println(octo.Legs()) // 5 octo.PrintLegs() // I have 4 legs&#125; 在上边这个例子中，typeCat有Legs方法来计算它有几条腿。我们嵌入Cat到一个新的typeOctoCat中，并声明Octocats有五条腿。然而，尽管OctoCat定义了自己有五条腿，但是PrintLegs方法被调用时会返回4。 这是因为PrintLegs在typeCat中定义。它会将Cat做为它的接收者，因此它会使用Cat的Legs方法。Cat并不了解已嵌入的type，因此它的嵌入方法不能被修改。 由此，我们可以说Go语言的types对扩展开放，但是对修改是关闭的。 事实上，Go语言接收者的方法仅仅是带有预先声明形式的参数的function的语法糖而已： 1234567func (c Cat) PrintLegs() &#123; fmt.Printf(&quot;I have %d legs\n&quot;, c.Legs())&#125;func PrintLegs(c Cat) &#123; fmt.Printf(&quot;I have %d legs\n&quot;, c.Legs())&#125; 第一个function的接收者就是你传进去的参数，而且由于Go语言不知道重载，所以说OctoCats并不能替换普通的Cats，这就引出了接下来一个原则： Liskov Substitution Principle该原则由Barbara Liskov提出，大致上,它规定了两种类型如果调用者不能区分出他们行为的不同，那么他们是可替代的。 基于class的编程语言，里氏替换原则通常被解释为一个抽象基类的各种具体子类的规范。但是Go语言没有class或者inheritance（继承），因此就不能以抽象类的层次结构实现替换。 Interfaces相反，Go语言的interface才有权替换。在Go语言中，type不需要声明他们具体要实现的某个interface，相反的，任何想要实现interface的type仅需提供与interface声明所匹配的方法。 就Go语言而言，隐式的interface要比显式的更令人满意，这也深刻地影响着他们使用的方式。 精心设计的interface更可能是小巧的，流行的做法是一个interface只包含一个方法。逻辑上来讲小巧的interface使实现变得简单，反之就很难做到。这就导致了由常见行为连接的简单实现而组成的package。 io.Reader1234type Reader interface &#123; // Read reads up to len(buf) bytes into buf. Read(buf []byte) (n int, err error)&#125; 我最喜爱的Go语言interface - io.Reader interface io.Reader非常简单，Read读取数据到提供的buffer，并返回调用者读取数据的bytes的数量以及读取期间的任何错误。它看起来简单但是很强大。 因为io.Reader可以处理任何能转换为bytes流的数据，我们可以在任何事情上构建readers：string常量、byte数组、标准输入、网络数据流、gzip后的tar文件以及通过ssh远程执行的命令的标准输出。 所有这些实现对于另外一个都是可替换的，因为他们都履行了相同的简单合同。 因此，里氏替换原则在Go语言的应用，可以用 Jim Weirich 的格言来总结： Require no more, promise no less. –Jim Weirich 接下来就到了”SOLID”第四个原则。 Interface Segregation Principle Clients should not be forced to depend on methods they do not use. –Robert C. Martin 在Go语言中，接口隔离原则的应用是指一个方法来完成其工作的孤立行为的过程。举个“栗子”，编写方法来保存一个文档结构到磁盘的任务。 12// Save writes the contents of doc to the file f.func Save(f *os.File, doc *Document) error 我可以这样定义这个Save方法，使用*os.File做为保存Document的文件。但是这样做会有一些问题。 Save方法排除了保存数据到网络位置的选项。假如过后要加入网络储存的需求，那么该方法就需要修改也就意味着要影响到所有使用该方法的调用者。 因为Save直接地操作磁盘上的文件，测试起来很不方便。要验证其操作，测试不得不在文件被写入后读取其内容。另外测试必须确保f被写入一个临时的位置而且过后还要删除。 *os.File还包含了许多跟Save无关的方法，像读取路径以及检查路径是否是软连接。如果Save方法只使用*os.File相关的部分将会非常有用。 我们如何做呢？ 12// Save writes the contents of doc to the supplied ReadWriterCloser.func Save(rwc io.ReadWriteCloser, doc *Document) error 使用io.ReadWriteCloser来应用接口隔离原则，这样就重新定义了Save方法使用一个interface来描述更为通用的类型。 随着修改，任何实现了io.ReadWriteCloser接口的type都可以代替之前的*os.File。这使得Save不仅扩展了它的应用范围同时也给Save的调用者说明了type *os.File哪些方法是操作相关的。 做为Save的作者，我没有了在*os.File上调用无关的方法选项了，因为他们都被隐藏于io.ReadWriteCloser接口。我们可以进一步地应用接口隔离原则。 首先，Save方法不太可能会保持单一功能原则，因为它要读取的文件内容应该是另外一段代码的责任。因此我们可以缩小接口范围，只传入writing和closing。 12// Save writes the contents of doc to the supplied WriteCloser.func Save(wc io.WriteCloser, doc *Document) error 其次，通过向Save提供一种机制来关闭它的数据流，会导致另外一个问题：wc会在什么情况下关闭。Save可能会无条件的调用Close或在成功的情况下调用Close。 如果它想要在写入document之后再写入额外的数据时会引起Save的调用者一个问题。 123456type NopCloser struct &#123; io.Writer&#125;// Close has no effect on the underlying writer.func (c *NopCloser) Close() error &#123; return nil &#125; 一个原始解决方案回事定义一个新的type，在其内嵌入io.Writer以及重写Close方法来阻止Save方法关闭底层数据流。 但是这样可能会违反里氏替换原则，如果NopCloser并没有关闭任何东西。 12// Save writes the contents of doc to the supplied Writer.func Save(w io.Writer, doc *Document) error 一个更好的解决办法是重新定义Save只传入io.Writer，剥离它的所有责任除了写入数据到数据流。 通过对Save方法应用接口隔离原则，同时得到了最具体以及最通用的需求函数。我们现在可以使用Save方法来保存数据到任何实现了io.Writer的地方。 A great rule of thumb for Go is accept interfaces, return structs. –Jack Lindamood Dependency Inversion Principle High-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details. Details should depend on abstractions. –Robert C. Martin 对于Go语言来讲，依赖反转意味着什么呢? 如果你应用以上所有的原则，代码已经被分解成离散的有明确责任和目的的package，你的代码应该描述了它的依赖interface以及这些interface应该只描述他们需要的功能行为。换句话说就是他们不会再过多的改变。 因此，我认为Martin所讲的在Go语言的应用是context，即你import graph（译注：后文用“导入图”代替）的结构。 在Go语言中，你的导入图必须是非循环。不遵守此非循环的需求会导致编译错误，但是更为严重的是它代表了一系列的设计错误。 所有条件都相同的情况下精心设计的导入图应该是广泛的以及相对平坦的，而不是又高又窄。如果你有一个package的函数在没有其他package的情况下就无法操作，也许这就表明了代码没有考虑pakcage的边界。 依赖反转原则鼓励你尽可能地像导入图一样在main package或者最高层级的处理程序内对具体细节负责，让低层级代码来处理抽象的接口。 “SOLID” Go语言设计回顾一下，当应用到Go语言设计中，每个“SOLID”原则都是强有力的声明，但是加在一起他们有一个中心主题。 单一功能原则鼓励你在package中构建functions、types以及方法表现出自然的凝聚力。types属于彼此，functions为单一目的服务。 开闭原则鼓励你使用嵌入将简单的type组合成更为复杂的。 里氏替换原则鼓励你在package之间表达依赖关系时用interface，而非具体类型。通过定义小巧的interface，我们可以更有信心地切实满足其合约。 接口隔离原则鼓励你仅取决于所需行为来定义函数和方法。如果你的函数仅仅需要有一个方法的interface做为参数，那么它很有可能只有一个责任。 依赖反转原则鼓励你在编译时将package所依赖的东西移除 - 在Go语言中我们可以看到这样做使得运行时用到的某个特定的package的import声明的数量减少。 如果总结这个演讲（译注：该篇文章取自Dave大神在Golang UK Conference 2016的演讲文字内容，文章结尾处有YouTube链接）它可能会是： &gt; interfaces let you apply the SOLID principles to Go programs 因为interface描绘了他们的pakcage的规定，而不是如何规定的。换个说法就是“解耦”，这确实是我们的目标，因为解耦的软件修改起来更容易。 就像Sandi Metz提到的： Design is the art of arranging code that needs to work today, and to be easy to change forever. –Sandi Metz 因为如果Go语言想要成为公司长期投资的编程语言，Go程序的维护，更容易的变更将是他们决定的关键因素。 结尾Go语言程序员应当讨论更多的是设计而非框架。我们应当不惜一切代价地关注重用而非性能。 我想要看到是今天的人们谈论关于如何使用编程语言，无论是设计解决方案还是解决实际问题的选择和局限性。 我想要听到的是人们谈论如何通过精心设计、解耦、重用以及适应变化的方式来设计Go语言程序。 …还有一点我们需要告诉世界优秀的软件该如何编写。告诉他们使用Go语言如何编写优秀的、可组合的及易于变化的软件。 原文链接：http://dave.cheney.net/2016/08/20/solid-go-design 本文转载自：https://blog.gokit.info/post/go-solid-design/]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Design</tag>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang 初始化顺序]]></title>
    <url>%2Fgolang-package-init-order.html</url>
    <content type="text"><![CDATA[golang package init order Go程序的初始化和执行总是从 main.main 函数开始的。但是如果 main 包里导入了其它的包，则会按照顺序将它们包含进 main 包里（这里的导入顺序依赖具体实现，一般可能是以文件名或包路径名的字符串顺序导入）。如果某个包被多次导入的话，在执行的时候只会导入一次。当一个包被导入时，如果它还导入了其它的包，则先将其它的包包含进来，然后创建和初始化这个包的常量和变量。然后就是调用包里的 init 函数，如果一个包有多个 init 函数的话，实现可能是以文件名的顺序调用，同一个文件内的多个 init 则是以出现的顺序依次调用（ init 不是普通函数，可以定义有多个，所以不能被其它函数调用）。最终，在 main 包的所有包常量、包变量被创建和初始化，并且 init 函数被执行后，才会进入 main.main 函数，程序开始正常执行。下图是Go程序函数启动顺序的示意图： 要注意的是，在 main.main 函数执行之前所有代码都运行在同一个Goroutine中，也是运行在程序的主系统线程中。如果某个 init 函数内部用go关键字启动了新的Goroutine的话，新的Goroutine和 main.main 函数是并发执行的。因为所有的 init 函数和 main 函数都是在主线程完成，它们也是满足顺序一致性模型的。 整理自：Go语言高级编程 作者：柴树杉、曹春晖图片来自：beego官网router部分]]></content>
      <tags>
        <tag>Go</tag>
        <tag>Init</tag>
        <tag>Order</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Goroutine和系统线程]]></title>
    <url>%2Fgoroutine-and-system-threads.html</url>
    <content type="text"><![CDATA[goroutine and system threads Goroutine是Go语言特有的并发体，是一种轻量级的线程，由go关键字启动。在真实的Go语言的实现中，goroutine和系统线程也不是等价的。尽管两者的区别实际上只是一个量的区别，但正是这个量变引发了Go语言并发编程质的飞跃。 首先，每个系统级线程都会有一个固定大小的栈（一般默认可能是2MB），这个栈主要用来保存函数递归调用时参数和局部变量。固定了栈的大小导致了两个问题：一是对于很多只需要很小的栈空间的线程来说是一个巨大的浪费，二是对于少数需要巨大栈空间的线程来说又面临栈溢出的风险。针对这两个问题的解决方案是：要么降低固定的栈大小，提升空间的利用率；要么增大栈的大小以允许更深的函数递归调用，但这两者是没法同时兼得的。相反，一个Goroutine会以一个很小的栈启动（可能是2KB或4KB），当遇到深度递归导致当前栈空间不足时，Goroutine会根据需要动态地伸缩栈的大小（主流实现中栈的最大值可达到1GB）。因为启动的代价很小，所以我们可以轻易地启动成千上万个Goroutine。 Go的运行时还包含了其自己的调度器，这个调度器使用了一些技术手段，可以在n个操作系统线程上多工调度m个Goroutine。Go调度器的工作和内核的调度是相似的，但是这个调度器只关注单独的Go程序中的Goroutine。Goroutine采用的是半抢占式的协作调度，只有在当前Goroutine发生阻塞时才会导致调度；同时发生在用户态，调度器会根据具体函数只保存必要的寄存器，切换的代价要比系统线程低得多。运行时有一个 runtime.GOMAXPROCS 变量，用于控制当前运行正常非阻塞Goroutine的系统线程数目。 在Go语言中启动一个Goroutine不仅和调用函数一样简单，而且Goroutine之间调度代价也很低，这些因素极大地促进了并发编程的流行和发展。 整理自：Go语言高级编程 作者：柴树杉、曹春晖]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>Goroutine</tag>
        <tag>Thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go 密码学应用]]></title>
    <url>%2Fgolang-for-crypto-developers.html</url>
    <content type="text"><![CDATA[Go Crypto 视频信息Go for Crypto Developersby George Tankersleyat GopherCon 2016 https://www.youtube.com/watch?v=2r_KMzXB74w 幻灯地址：https://speakerdeck.com/gtank/crypto-for-go-developers代码：https://github.com/gtank/cryptopasta Don’t write your own crypto很多人把这句话误解为不要使用加密、不要使用任何密码学的技术，因为你不够聪明。No。完全不是这个意思。 这句话是说不要试图去发明创造那些加密类的算法。因为你不大可能会创造一个超过 AES 的加密算法、也不大可能会创造一个比 SHA 更好的 hash 算法。所以自己闭门造的算法一般意味着安全性的大大降低。 全世界能干这件事情的人不超过5个，而且他们今天都不在这里。另外一半都是 Daniel J. Bernstein 干的（开个玩笑，不过这人很牛，今天我们在用的很多加密的东西都是他设计、实现的）。 表面上好像这是个限制，其实这是个优势。因为我们不需要编写自己的加密算法，一切痛苦的工作都已经由别人做好了。我们只需要和搭积木一样去使用这些密码学工具就好了。 经常听到这样的建议 对传输中的数据用 TLS 对静止的数据用 GPG TLSGo 可以很容易使用 TLS，因为必须的东西都内置了，从客户端到服务端。 客户端 1234567891011121314151617var minimalTLSConfig = &amp;tls.Config&#123; MinVersion: tls.VersionTLS12,&#125;var tlsTransport = &amp;http.Transport&#123; TLSClientConfig: minimalTLSConfig,&#125;var httpClient = &amp;http.Client&#123; Transport: tlsTransport, Timeout: 10 * time.Second,&#125;func MakeRequest() error &#123; resp, err := httpClient.Get(&quot;https://www.google.com&quot;) if err != nil &#123; return err &#125; // have fun&#125; 这里最重要的是 tls.VersionTLS12，因为低于 1.2 的话，会导致 Go 使用一些不安全的低版本的实现，所以这里限定 1.2 比较安全。而且大部分网站也都支持。 服务端 123456789101112131415161718var minimalTLSConfig = &amp;tls.Config&#123; MinVersion: tls.VersionTLS12, PreferServerCipherSuites: true,&#125;var srv = &amp;http.Server&#123; Addr: &quot;localhost:8080&quot;, TLSConfig: minimalTLSConfig,&#125;func handleReq(w http.ResponseWriter, r *http.Request) &#123; fmt.Fprintf(w, &quot;Hello, world&quot;)&#125;func main() &#123; http.HandleFunc(&quot;/&quot;, handleReq) err := srv.ListenAndServeTLS(&quot;cert.pem&quot;, &quot;key.pem&quot;) if err != nil &#123; log.Fatal(err) &#125;&#125; 这里和客户端一样，限定最小版本是 1.2，并且多了一个额外的参数，要求以服务器的 Cipher 优先，因此不按照客户端给的选择，而是按照Go服务端给的选择。因为 Go 的 TLS 包中有大量针对各种安全问题的调整，因此选择加密包的话，遵循 Go 内部的决定是最好的。 GPGGPG 是为了人和人之间的交流信息，而不是机器和机器之间交流信息的。 那么如何安全的使用 GPG 呢？答案就是不用GPG。因为作者觉得 GPG 太过阴谋论了，陷入了很多本不需要过多注意的区域，即使那么做了，也不见得更安全。 这个 Talk 不讲 TLS 和 GPG因为当你实现安全的信息系统的时候，通常不会用到这两个东西。TLS 和 GPG 有他们应用的场合，但是对于每天的密码学工作来说，基本都不用这两个工具： 对文件计算散列 生成随机 ID API 验证 网站密码存储 签名、加密 cookies JWT 签名更新 在 Go 的 crypto 包里的算法可不都是好的算法加密下列划掉的算法都不应该再使用了： DES 3DES RC4 TEA XTEA Blowfish ✔️ Twofish CAST5 ✔️ Salsa20 ✔️ AES 只有 Twofish、Salsa20 和 AES 还算是安全的加密算法，但是 AES 在大部分的计算机上都有硬件加速。因此只剩下一个 AES 是最佳加密算法。 怎么使用 AES要注意，aesCipher.Encrypt() 只会加密前16个字节。不少人掉到这个坑里了，结果不知道为啥就前几个字符是密文，后面全都是明文。解决办法就是不直接用AES，通过 Mode 来使用。 Block cipher mode 一样有很多种选择： CBC CFB CTR OFB ✔️ GCM 这里只有 GCM 是验证的加密算法，因此别的都可以不选。 加密12345678910111213141516171819202122232425import ( &quot;crypto/aes&quot; &quot;crypto/cipher&quot; &quot;crypto/rand&quot;)func Encrypt(data []byte, key [32]byte) ([]byte, error) &#123; // 初始化 block cipher block, err := aes.NewCipher(key[:]) if err != nil &#123; return nil, err &#125; // 设置 block cipher mode gcm, err := cipher.NewGCM(block) if err != nil &#123; return nil, err &#125; // 生成随机 nonce nonce := make([]byte, gcm.NonceSize()) _, err = rand.Read(nonce) if err != nil &#123; return nil, err &#125; // 封装、返回 return gcm.Seal(nonce, nonce, data, nil), nil&#125; 解密1234567891011121314151617181920212223import ( &quot;crypto/aes&quot; &quot;crypto/cipher&quot; &quot;crypto/rand&quot;)func Decrypt(ciphertext []byte, key [32]byte) (plaintext []byte, err error) &#123; // 初始化 block cipher block, err := aes.NewCipher(key[:]) if err != nil &#123; return nil, err &#125; // 设置 block cipher mode gcm, err := cipher.NewGCM(block) if err != nil &#123; return nil, err &#125; // 返回解开的包，注意这里的 nonce 是直接取的。 return gcm.Open(nil, ciphertext[:gcm.NonceSize()], ciphertext[gcm.NonceSize():], nil, )&#125; 哈希散列将一大段数据使用 Hash 算法，希望得到一串数值，这串数值可以反映你的这段数据，任何数据变化，这串数值都不同。而且希望无法从这串数值反推数据。这就是密码学 Hash 函数要保证的。（注意：哈希不等于加密，很多人这点容易搞混） 同样 Hash 函数一样有很多选择，一样是大部分都不用： MD4 MD5 RIPEMD160 SHA1 ✔️ SHA2 ✔️ SHA3选择 SHA3 并不是因为它比 SHA2 更新更好，而是因为它不同于 SHA1 和 SHA2。几年前密码学家已经开始担心，因为MD以及SHA的哈希算法本质太相似了，那么一旦这个依赖出现问题，就意味着这个体系的不再安全。因此开始寻找一种不同的算法。经过竞赛、挑选，最终 SHA3 脱颖而出。他本身是个很出色的 Hash 算法，同时其设计和之前的这几个算法完全不一样。 但是由于 SHA3 并不被广泛支持，所以如果你明确知道你可以用 SHA3，那么就用 SHA3。其它情况用 SHA2。 但是和加密一样，我们不应该直接使用 Hash 算法。因为可能会面临一系列的攻击： Length extension Rainbow tables Small number of possibilities (phone numbers) Salt? Peper? 我们应该使用 HMAC，而不要直接用 Hash 实现 Hash1234567891011121314151617181920import ( &quot;crypto/hmac&quot; &quot;crypto/sha512&quot;)func Hash(tag string, data []byte) []byte &#123; h := hmac.New(sha512.New512_256, []byte(tag)) h.Write(data) return h.Sum(nil)&#125;func ExampleHash() error &#123; tag := &quot;hashing file for storage key&quot; contents, err := ioutil.ReadFile(&quot;testfile&quot;) if err != nil &#123; return error &#125; digest := Hash(tag, contents) fmt.Println(hex.EncodeToString(digest))&#125;// Output:// 9f4c795d8ae5e207f19184ccebee6a606c1fdfe509c793614006d613580f03e1 Hash 密码一般东西的 Hash 用刚才的就行了，但是除了密码Hash。密码 Hash 和数据 Hash 的特征完全不同。 数据 Hash 希望的是 Hash 算法越快越好 而密码 Hash 则希望 Hash 算法越慢越好 过快的密码哈希会导致暴力破解的成本降低。因此密码哈希需要特殊算法。 使用 bcrypt1234567891011121314151617import ( &quot;golang.org/x/crypto/bcrypt&quot;)func HashPassword(password []byte) ([]byte, error) &#123; return bcrypt.GenerateFromPassword(password, 14)&#125;func CheckPasswordHash(hash, password []byte) error &#123; return bcrypt.CompareHashAndPassword(hash, password)&#125;func Example() &#123; myPassword := []byte(&quot;password&quot;) hashed, err := HashPassword(myPassword) if err != nil &#123; return &#125; fmt.Println(string(hashed))&#125; 14 是计算量的复杂度，14 是个比较好的值，如果觉得性能无法接受，可以降到 12，但是不要再低了。 签名首先是有一对密钥，一个是公钥、一个是私钥。任何拥有私钥的人可以对一段信息签名，而所有拥有公钥的人都可以来验证这个消息确实是由那个人签名的。 通过签名可以确保两件事情： 消息未曾被篡改 是谁发出的这个消息 和前面一样，Go 有很多签名算法可以选择： RSA PKCS1v15 PSS ECDSA ✔️ P256 P385 P521 Ed25519 这次和前面不同，签名算法的安全更多的不是取决于算法选择，而是取决于你是怎么使用的。 比如这里比较推荐使用 ECDSA/P256，但是要注意，当初 PS3 被黑，被解出私钥就是用的这个算法，当时是由于那个算法实现是非常烂的。幸运的是 Go 没这个问题。所以相对于其他语言，Go 可以使用这个比较安全的签名算法。 实现生成密钥 12345678import ( &quot;crypto/ecdsa&quot; &quot;crypto/elliptic&quot; &quot;crypto/rand&quot;)func NewSigningKey() (*ecdsa.PrivateKey, error) &#123; return ecdsa.GenerateKey(elliptic.P256(), rand.Reader)&#125; 签名数据 123456789101112131415func Sign(data []byte, priv *ecdsa.PrivateKey) ([]byte, error) &#123; digest := sha256.Sum256(data) r, s, err := ecdsa.Sign(rand.Reader, priv, digest[:]) if err != nil &#123; return nil, err &#125; // encode the signature &#123;R, S&#125; params := priv.Curve.Params() curveByteSize := params.P.BitLen() / 8 rBytes, sBytes := r.Bytes(), s.Bytes() signature := make([]byte, curveByteSize * 2) copy(signature[curveByteSize - len(rBytes):], rBytes) copy(signature[curveByteSize*2 - len(sBytes):], sBytes) return signature, nil&#125; 验证签名 1234567891011121314import ( &quot;crypto/ecdsa&quot; &quot;crypto/sha256&quot; &quot;math/big&quot;)// 验证成功返回 true，否则 falsefunc Verify(data, sig []byte, pub *ecdsa.PublicKey) bool &#123; digest := sha256.Sum256(data) curveByteSize := pub.Curve.Params().P.BitLen() / 8 r, s := new(big.Int), new(big.Int) r.SetBytes(signature[:curveByteSize]) s.SetBytes(signature[curveByteSize:]) return ecdsa.Verify(pub, digest[:], r, s)&#125;]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>Crypto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gRPC 从学习到生产]]></title>
    <url>%2Fgolang-grpc-from-tutorial-to-production.html</url>
    <content type="text"><![CDATA[gRPC Practice了解gRPC，更知REST 视频信息grpc: From Tutorial to Productionby Alan Shreveat GopherCon 2017 https://www.youtube.com/watch?v=7FZ6ZyzGex0 博文：https://about.sourcegraph.com/go/grpc-in-production-alan-shreve/ 微服务之间应该如何通讯？答案就是：SOAP……好吧，开个玩笑，当然不可能是 SOAP 了。 现在流行的做法是 HTTP + JSON (REST API) Alan 说“如果这辈子再也不写另一个 REST 客户端库的话，那就可以很幸福的死去了……😂”，因为这是最无聊的事情，一遍一遍的在做同样的事情。 为什么 REST API 不好用？ 实现 Stream 太难了 而双向的流就根本不可能 很难对操作建立模型 效率很差，文本表示对于网络来说并不是最好的选择 而且，其实服务内部根本不是 RESTful 的方式，这只是 HTTP endpoint 很难在一个请求中取得多个资源数据 （反例看 GraphQL） 没有正式的（机器可读的）API约束 因此写客户端需要人类 而且因为👷很贵，而且不喜欢写客户端 什么是 gRPC gPRC 是高性能、开源、通用的 RPC 框架。 与其讲解定义，不如来实际做个东西更清楚。 建一个缓存服务使用 gRPC 这类东西，我们并非开始于写 Go 代码，我们是从撰写 gRPC 的 IDL 开始的。 app.proto123456789101112131415161718syntax = &quot;proto3&quot;package rpc;service Cache &#123; rpc Store(StoreReq) returns (StoreResp) &#123;&#125; rpc Get(GetReq) returns (GetResp) &#123;&#125;&#125;message StoreReq &#123; string key = 1; bytes val = 2;&#125;message StoreResp &#123;&#125;message GetReq &#123; string key = 1;&#125;message GetResp &#123; bytes val = 1;&#125; 当写了这个文件后，我们立刻拥有了 9 种语言的客户端的库。 C++ Java(and Android) Python Go Ruby C# Javascript(node.js) Objective-C (iOS!) PHP 同时，我们也拥有了 7 种语言的服务端的 API Stub： C++ Java Python Go Ruby C# Javascript(node.js) server.go12345678910111213141516func serverMain() &#123; if err := runServer(); err != nil &#123; fmt.Fprintf(os.Stderr, &quot;Failed to run cache server: %s\n&quot;, err) os.Exit(1) &#125;&#125;func runServer() error &#123; srv := grpc.NewServer() rpc.RegisterCacheServer(srv, &amp;CacheService&#123;&#125;) l, err := net.Listen(&quot;tcp&quot;, &quot;localhost:5051&quot;) if err != nil &#123; return err &#125; // block return srv.Serve(l)&#125; 暂时先不实现 CacheService，先放个空的，稍后再实现。 12345678type CacheService struct &#123;&#125;func (s *CacheService) Get(ctx context.Context, req *rpc.GetReq) (*rpc.GetResp, error) &#123; return nil, fmt.Errorf(&quot;unimplemented&quot;)&#125;func (s *CacheService) Store(ctx context.Context, req *rpc.StoreReq) (*rpc.StoreResp, error) &#123; return nil, fmt.Errorf(&quot;unimplemented&quot;)&#125; client.go123456789101112131415161718192021222324252627func clientMain() &#123; if err != runClient(); err != nil &#123; fmt.Fprintf(os.Stderr, &quot;failed: %v\n&quot;, err) os.Exit(1) &#125;&#125;func runClient() error &#123; // 建立连接 conn, err := grpc.Dial(&quot;localhost:5053&quot;, grpc.WithInsecure()) if err != nil &#123; return fmt.Errorf(&quot;failed to dial server: %v&quot;, err) &#125; cache := rpc.NewCacheClient(conn) // 调用 grpc 的 store() 方法存储键值对 &#123; &quot;gopher&quot;: &quot;con&quot; &#125; _, err = cache.Store(context.Background(), &amp;rpc.StoreReq&#123;Key: &quot;gopher&quot;, Val: []byte(&quot;con&quot;)&#125;) if err != nil &#123; return fmt.Errorf(&quot;failed to store: %v&quot;, err) &#125; // 调用 grpc 的 get() 方法取回键为 `gopher` 的值 resp, err := cache.Get(context.Background(), &amp;rpc.GetReq&#123;Key: &quot;gopher&quot;&#125;) if err != nil &#123; return fmt.Errorf(&quot;failed to get: %v&quot;, err) &#125; // 输出 fmt.Printf(&quot;Got cached value %s\n&quot;, resp.Val) return nil&#125; 这不就是 WSDL 么？或许有些人会认为这和 WSDL 也太像了，这么想没有错，因为 gRPC 在借鉴之前的 SOAP/WSDL 的错误基础上，也吸取了他们优秀的地方。 和 XML 关系没那么紧(grpc 是可插拔式的，可以换成各种底层表述) 写过 XML/XSD 的人都知道这些服务定义太繁重了，gRPC 没有这个问题 WSDL这类有完全不必要的复杂度、和基本不需要的功能（两步 commit） WSDL 不灵活、而且无法前向兼容（不像 protobuf） SOAP/WSDL 性能太差，以及无法使用流 但是WSDL中的机器可以理解的API定义确实是个好东西 实现具体的 CacheServiceserver.go 1234567891011type CacheService struct &#123; store map[string][]byte&#125;func (s *CacheService) Get(ctx context.Context, req *rpc.GetReq) (*rpc.GetResp, error) &#123; val := s.store[req.Key] return &amp;rpc.GetResp&#123;Val: val&#125;, nil&#125;func (s *CacheService) Store(ctx context.Context, req *rpc.StoreReq) (*rpc.StoreResp, error) &#123; s.store[req.Key] = req.Val return &amp;rpc.StoreResp&#123;&#125;, nil&#125; 注意这里没有锁，你可以想想他们中有，因为将来他们会被并发的调用的。 错误处理当然，gRPC 支持错误处理。假设改写上面的 Get()，对不存在的键进行报错： 1234567func (s *CacheService) Get(ctx context.Context, req *rpc.GetReq) (*rpc.GetResp, error) &#123; val, ok := s.store[req.Key] if !ok &#123; return nil, status.Errorf(code.NotFound, &quot;Key not found %s&quot;, req.Key) &#125; return &amp;rpc.GetResp&#123;Val: val&#125;, nil&#125; 加密传输如果这样的代码打算去部署的话，一定会被 SRE 拦截下来，因为所有通讯必须加密传输。 在 gRPC 中添加 TLS 加密传输很容易。比如我们修改 runServer() 添加 TLS 加密传输。 12345678func runServer() error &#123; tlsCreds, err := credentials.NewServerTLSFromFile(&quot;tls.crt&quot;, &quot;tls.key&quot;) if err != nil &#123; return err &#125; srv := grpc.NewServer(grpc.Creds(tlsCreds)) ...&#125; 同样，我们也需要修改一下 runClient()。 12345func runClient() error &#123; tlsCreds := credentials.NewTLS(&amp;tls.Config(InsecureSkipVerify: true)) conn, err := grpc.Dial(&quot;localhost:5051&quot;, grpc.WithTransportCredentials(tlsCreds)) ...&#125; 生产环境如何使用 gRPC HTTP/2 protobuf serialization (pluggable) 客户端会和 grpc 服务器打开一个长连接 对于每一个 RPC 调用都将是一个新的 HTTP/2 stream 允许模拟飞行模式的 RPC 调用 允许客户端 和 服务端 Streaming gRPC 的实现现在有3个高性能的、事件驱动的实现 C Ruby, Python, Node.js, PHP, C#, Objective-C, C++ 都是对这个 C core 实现的绑定 PHP 则是通过 PECL 和这个实现的绑定 Java Netty + BoringSSL 通过 JNI Go 纯 Go 实现，使用了 Go 标准库的 crypto/tls gRPC 从哪来的 最初是 Google 的一个团队创建的 更早期的是 Google 一个内部项目叫做 stubby 这个 gRPC 是其下一代开源项目，并且现在不仅仅是 Google 在使用，很多公司都在贡献代码 当然，Google 还是主要代码贡献者 生产环境案例：多租户上线生产后，发现有一部分客户产生了大量的键值，询问得知，有的客户希望对所有东西都缓存，这显然不是对我们这个缓存服务很好的事情。 我们希望限制这种行为，但对于当前系统而言，无法满足这种需求，因此我们需要修改实现，对每个客户发放客户 token，那么我们就可以约束特定客户最多可以建立多少键值，避免系统滥用。这就成为了多租户的缓存服务。 和之前一样，我们还是从 IDL 开始，我们需要修改接口，增加 account_token 项。 12345message StoreReq &#123; string key = 1; bytes val = 2; string account_token = 3;&#125; 同样，我们需要有独立的服务针对账户服务，来获取账户所允许的缓存键数： 123456789101112service Accounts &#123; rpc GetByToken(GetByTokenReq) return (GetByTokenResp) &#123;&#125;&#125;message GetByTokenReq &#123; string token = 1;&#125;message GetByTokenResp &#123; Account account = 1;&#125;message Account &#123; int64 max_cache_keys = 1;&#125; 这里建立了一个新的 Accounts 服务，并且有一个 GetByToken() 方法，给入 token，返回一个 Account 类型的结果，而 Account 内有 max_cache_keys 键对应最大可缓存的键值数。 现在我们进一步修改 client.go 12345678910111213func runClient() error &#123; ... cache := rpc.NewCacheClient(conn) _, err = cache.Store(context.Background(), &amp;rpc.StoreReq&#123; AccountToken: &quot;inconshreveable&quot;, Key: &quot;gopher&quot;, Val: []byte(&quot;con&quot;), &#125;) if err != nil &#123; return fmt.Errorf(&quot;failed to store: %v&quot;, err) &#125; ...&#125; 服务端的改变要稍微大一些，但不过分。 12345type CacheService struct &#123; accounts rpc.AccountsClient store map[string][]byte keysByAccount map[string]int64&#125; 注意这里的 accounts 是一个 grpc 的客户端，因为我们这个服务，同时也是另一个 grpc 服务的客户端。所以在接下来的 Store() 实现中，我们需要先通过 accounts 调用另一个服务取得账户信息。 1234567891011121314151617181920func (s *CacheService) Store(ctx context.Context, req *rpc.StoreReq) (*rpc.StoreResp, error) &#123; // 调用另一个服务取得账户信息，包含其键值限制 resp, err := s.accounts.GetByToken(context.Background(), &amp;rpc.GetByTokenReq&#123; Token: req.AccountToken, &#125;) if err != nil &#123; return nil, err &#125; // 检查是否超量使用 if s.keysByAccount[req.AccountToken] &gt;= resp.Account.MaxCacheKeys &#123; return nil, status.Errorf(codes.FailedPrecondition, &quot;Account %s exceeds max key limit %d&quot;, req.AccountToken, resp.Account.MaxCacheKeys) &#125; // 如果键不存在，需要新加键值，那么我们就对计数器加一 if _, ok := s.store[req.Key]; !ok &#123; s.keysByAccount[req.AccountToken] += 1 &#125; // 保存键值 s.store[req.Key] = req.Val return &amp;rpc.StoreResp&#123;&#125;, nil&#125; 生产环境案例：性能上面的问题解决了，我们服务又恢复了正常，不会有用户建立过多的键值了。但是很快，我们就又收到了其他用户发来的新的 issue，很多人反应说新系统变慢了，没有达到 SLA 的要求。 可是我们根本不知道到底发生了什么，于是意识到了，我们的程序没有任何可观察性（Observability），换句话说，我们的程序没有任何计量系统来统计性能相关的数据。 我们先从最简单的做起，添加日志。 我们先从 client.go 开始，增加一些测量和计数以及日志输出。 12345678910111213141516171819202122...// 开始计时start := time.Now()_, err = cache.Store(context.Background(), &amp;rpc.StoreReq&#123; AccountToken: &quot;inconshreveable&quot;, Key: &quot;gopher&quot;, Val: []byte(&quot;con&quot;),&#125;)// 计算 cache.Store() 调用时间log.Printf(&quot;cache.Store duration %s&quot;, time.Since(start))if err != nil &#123; return fmt.Errorf(&quot;failed to store: %v&quot;, err)&#125;// 再次开始计时start = time.Now()// 调用 grpc 的 get() 方法取回键为 `gopher` 的值resp, err := cache.Get(context.Background(), &amp;rpc.GetReq&#123;Key: &quot;gopher&quot;&#125;)// 计算 cache.Get() 调用时间log.Printf(&quot;cache.Get duration %s&quot;, time.Since(start))if err != nil &#123; return fmt.Errorf(&quot;failed to get: %v&quot;, err)&#125; 同样，在服务端也这么处理。 1234567891011func (s *CacheService) Store(ctx context.Context, req *rpc.StoreReq) (*rpc.StoreResp, error) &#123; // 开始计时 start := time.Now() // 调用另一个服务取得账户信息，包含其键值限制 resp, err := s.accounts.GetByToken(context.Background(), &amp;rpc.GetByTokenReq&#123; Token: req.AccountToken, &#125;) // 输出 account.GetByToken() 的调用时间 log.Printf(&quot;accounts.GetByToken duration %s&quot;, time.Since(start)) ...&#125; 经过这些修改后，我们发现一样的事情在反反复复的做，那么有什么办法可以改变这种无聊的做法么？查阅 grpc 文档后，看到有一个叫做 Client Interceptor 的东西。 这相当于是一个中间件，但是是在客户端。当客户端进行 rpc 调用的时候，这个中间件先会被调用，因此这个中间件可以对调用进行一层包装，然后再进行调用。 为了实现这个功能，我们创建一个新的文件，叫做 interceptor.go： 1234567891011121314151617func WithClientInterceptor() grpc.DialOption &#123; return grpc.WithUnaryInterceptor(clientInterceptor)&#125;func clientInterceptor( ctx context.Context, method string, req interface&#123;&#125;, reply interface&#123;&#125;, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption,) error &#123; start := time.Now() err := invoker(ctx, method, req, reply, cc, opts...) log.Printf(&quot;invoke remote method=%s duration=%s error=%v&quot;, method, time.Since(start), err) return err&#125; 我们有了这个 WithClientInterceptor() 之后，可以在 grpc.Dial() 的时候注册进去。client.go 1234567func runClient() error &#123; ... conn, err := grpc.Dial(&quot;localhost:5051&quot;, grpc.WithTransportCredentials(tlsCreds), WithClientInterceptor()) ...&#125; 注册之后，所有的 grpc 调用都会经过我们注册的 clientInterceptor()，因此所有的时间就都有统计了，而不用每个函数内部反反复复的添加时间、计量、输出。 添加了客户端的这个计量后，自然而然就联想到服务端是不是也可以做同样的事情？经过查看文档，可以，有个叫做 Server Interceptor 的东西。 同样的做法，我们在服务端添加 interceptor.go，并且添加 ServerInterceptor() 函数。 1234567891011121314151617func ServerInterceptor() grpc.ServerOption &#123; return grpc.UnaryInterceptor(serverInterceptor)&#125;func serverInterceptor( ctx context.Context, req interface&#123;&#125;, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler,) (interface&#123;&#125;, error) &#123; start := time.Now() resp, err := handler(ctx, req) log.Printf(&quot;invoke server method=%s duration=%s error=%v&quot;, info.FullMethod, time.Since(start), err) return resp, err&#125; 和客户端一样，需要在 runServer() 的时候注册我们定义的这个中间件。 12345func runServer() error &#123; ... srv := grpc.NewServer(grpc.Creds(tlsCreds), ServerInterceptor()) ...&#125; 生产环境案例：超时添加了日志后，我们终于在日志中发现，/rpc.Accounts/GetByToken/ 花了好长的时间。我们需要对这个操作设置超时。server.go 1234567func (s *CacheService) Store(ctx context.Context, req *rpc.StoreReq) (*rpc.StoreResp, error) &#123; accountsCtx, _ := context.WithTimeout(context.Background(), 2 * time.Second) resp, err := s.accounts.GetByToken(accountsCtx, &amp;rpc.GetByTokenReq&#123; Token: req.AccountToken, &#125;) ...&#125; 这里操作很简单，直接使用标准库中 context.WithTimeout() 就可以了。 生产环境案例：上下文传递经过上面修改后，客户依旧抱怨说没有满足 SLA，仔细一想也对。就算这里约束了 2 秒钟，客户端调用还需要时间，别的代码在中间也有时间开销。而且有的客户说，我们这里需要1秒钟，而不是2秒钟。 好吧，让我们把这个时间设定推向调用方。 首先我们要求在客户端进行调用时间约束的设定：client.go 123456789func runClient() error &#123; ... ctx, _ := context.WithTimeout(context.Background(), time.Second) _, err = cache.Store(ctx, &amp;rpc.StoreReq&#123;Key: &quot;gopher&quot;, Val: []byte(&quot;con&quot;)&#125;) ... ctx, _ = context.WithTimeout(context.Background(), 50*time.Millisecond) resp, err := cache.Get(ctx, &amp;rpc.GetReq&#123;Key: &quot;gopher&quot;&#125;) ...&#125; 然后在服务端，我们将上下文传递。直接取调用方的 ctx。 123456func (s *CacheService) Store(ctx context.Context, req *rpc.StoreReq) (*rpc.StoreResp, error) &#123; resp, err := s.accounts.GetByToken(ctx, &amp;rpc.GetByTokenReq&#123; Token: req.AccountToken, &#125;) ...&#125; 生产环境案例：GRPC Metadata上面的问题都解决了，终于可以松一口气了。可是客户又提新的需求了……😅，说我们能不能增加一个 Dry Run 的标志，就是说我希望你做所有需要做的事情，除了真的修改键值库。 GRPC metadata，也称为 GRPC 的 Header。就像 HTTP 头一样，可以有一些 Metadata 信息传递过来。使用 metadata，可以让我们的 Dry Run 的实现变得更简洁，不必每个 RPC 方法内都实现一遍检查 Dry Run 标志的逻辑，我们可以独立出来。 1234567891011121314151617181920212223242526func (s *CacheService) Store(ctx context.Context, req *rpc.StoreReq) (*rpc.StoreResp, error) &#123; resp, err := s.accounts.GetByToken(ctx, &amp;rpc.GetByTokenReq&#123; Token: req.AccountToken, &#125;) if !dryRun(ctx) &#123; if _, ok := s.store[req.Key]; !ok &#123; s.keysByAccount[req.AccountToke] += 1 &#125; s.store[req.Key] = req.Val &#125; return &amp;rpc.StoreResp&#123;&#125;, nil&#125;func dryRun(ctx context.Context) bool &#123; md, ok := metadata.FromContext(ctx) if !ok &#123; return false &#125; val, ok := md[&quot;dry-run&quot;] if !ok &#123; return false &#125; if len(val) &lt; 1 &#123; return false &#125; return val[0] == &quot;1&quot;&#125; 当然，这么做是有妥协的，因为通用化后就失去了类型检查的能力。 在客户端调用的时候，则需要根据情况添加 dry-run 参数给 metadata。 1234567func runClient() error &#123; ... ctx, _ := context.WithTimeout(context.Background(), time.Second) ctx = metadata.NewContext(ctx, metadata.Pairs(&quot;dry-run&quot;, &quot;1&quot;)) _, err = cache.Store(ctx, &amp;rpc.StoreReq&#123;Key: &quot;gopher&quot;, Val: []byte(&quot;con&quot;)&#125;) ...&#125; 生产环境案例：Retry实现了 Dry Run 以为可以休息了，之前抱怨慢的客户又来抱怨了，虽然有超时控制，满足 SLA，但是服务那边还是慢，总超时不成功。检查了一下，发现是网络上的事情，我们没有太多可以做的事情。为了解决客户的问题，我们来添加一个重试的机制。 我们可以对每一个 gRPC 调用添加一个 Retry 机制，我们也可以像之前计时统计那样，使用 Interceptor 吧？ 12345678910111213141516171819202122232425func clientInterceptor(...) error &#123; var ( start = time.Now() attempts = 0 err error backoff retryBackOff ) for &#123; attempts += 1 select &#123; case &lt;-ctx.Done(): err = status.Errorf(codes.DeadlineExceeded, &quot;timeout reached before next retry attempt&quot;) case &lt;-backoff.Next(): startAttempt := time.Now() err = invoker(ctx, method, req, reply, cc, opts...) if err != nil &#123; log.Printf(...) continue &#125; &#125; break &#125; log.Printf(...) return err&#125; 看起来还不错，然后就打算发布这个代码了。结果提交审核的时候被打回来了，说这个代码不合理，因为如果是非幂等（non-idempotent） 的操作，这样就会导致多次执行，改变期望结果了。 看来我们得针对幂等和非幂等操作区别对待了。 1silo.FireZeMissiles(NotIdempotent(ctx), req) 嗯，当然，没这个东西。所以我们需要自己来创造一个标记，通过 context，来标明操作是否幂等。 12345678910func NotIdempotent(ctx context.Context) context.Context &#123; return context.WithValue(ctx, &quot;idempotent&quot;, false)&#125;func isIdempotent(ctx context.Context) bool &#123; val, ok := ctx.Value(&quot;idempotent&quot;).(bool) if !ok &#123; return true &#125; return val&#125; 然后在我们的 clientInterceptor() 实现中加入 isIdempotent() 判断： 12345678910111213141516171819202122232425func clientInterceptor(...) error &#123; var ( start = time.Now() attempts = 0 err error backoff retryBackOff ) for &#123; attempts += 1 select &#123; case &lt;-ctx.Done(): err = status.Errorf(codes.DeadlineExceeded, &quot;timeout reached before next retry attempt&quot;) case &lt;-backoff.Next(): startAttempt := time.Now() err = invoker(ctx, method, req, reply, cc, opts...) if err != nil &amp;&amp; isIdempotent(ctx) &#123; log.Printf(...) continue &#125; &#125; break &#125; log.Printf(...) return err&#125; 这样当调用失败后，客户端检查发现是幂等的情况，才重试，否则不重试。避免了非幂等操作的反复操作。 生产环境案例：结构化错误感觉没啥问题了，于是部署上线了。可是运行一段时间后，发现有些不对劲。所有成功的RPC调用，也就是说这个操作本身是正确的，都没有问题，超时重试也正常。但是所有失败的 RPC 调用都不对了，所有失败的 RPC 调用，都返回超时，而不是错误本身。这里说的失败，不是说网络问题导致超时啥的，而是说请求本身的失败，比如之前提到的，Get() 不存在的键，应该返回错误；或者 Store() 超过了配额，应该返回错误，这类错误在日志中都没看到，反而都对应了超时。 经过分析发现，服务端该报错都报错，没啥问题，但是客户端不对，本应该返回错误给调用方的地方，客户端代码反而又开始重试这个操作了。看来之前重试的代码还有问题。 12345err = invoker(ctx, method, req, reply, cc, opts...)if err != nil &amp;&amp; isIdempotent(ctx) &#123; log.Printf(...) continue&#125; 如果仔细观察这部分代码，会发现，无论 err 是什么，只要非 nil，我们就重试。其实这是不对的，我们只有针对某些错误重试，比如网络问题之类的，而不应该对我们希望返回给调用方的错误重试，那没有意义。 那么问题就变成了，我们到底应该怎么对 err 判断来决定是否重试？ 可以使用不同的 Error Code，特定的 Code 需要 Retry，其它的不需要，那就需要自定义 gRPC 错误码； 我们也可以定义一个 Error 类型的数据，里面包含了某种标志位，来告知是否值得 retry 或者干脆把错误码放到 Response 的消息里，确保每个消息都有一个我们定义的错误码，来标明是否需要 retry。 所以，我们需要的是一个完整的结构化的错误信息，而不是简单的一个 Error Code 和字符串。当然这条路不好走，但是我们已经做了这么多了，坚持一下还是可以克服的。 这里我们还是从 IDL 开始： 123456message Error &#123; int64 code = 1; string messsage = 2; bool temporary = 3; int64 userErrorCode = 4;&#125; 然后我们实现这个 Error 类型。rpc/error.go 12345678910func (e *Error) Error() string &#123; return e.Message&#125;func Errorf(code codes.Code, temporary bool, msg string, args ..interface&#123;&#125;) error &#123; return &amp;Error&#123; Code: int64(code), Message: fmt.Sprintf(msg, args...), Temporary: temporary, &#125;&#125; 有这两个函数，我们可以显示和构造这个 Error 类型的变量了，但是我们该怎么把错误消息传回客户端呢？然后问题就开始变的繁琐起来了：rpc/error.go 123456789101112131415161718192021222324252627func MarshalError (err error, ctx context.Context) error &#123; rerr, ok := err.(*Error) if !ok &#123; return err &#125; pberr, marshalerr := pb.Marshal(rerr) if marshalerr == nil &#123; md := metadata.Pairs(&quot;rpc-error&quot;, base64.StdEncoding.EncodeToString(pberr)) _ = grpc.SetTrailer(ctx, md) &#125; return status.Errorf(codes.Code(rerr.Code), rerr.Message)&#125;func UnmarshalError(err error, md metadata.MD) *Error &#123; vals, ok := md[&quot;rpc-error&quot;] if !ok &#123; return nil &#125; buf, err := base64.StdEncoding.DecodeString(vals[0]) if err != nil &#123; return nil &#125; var rerr Error if err := pb.Unmarshal(buf, &amp;rerr); err != nil &#123; return nil &#125; return &amp;rerr&#125; interceptor.go 123456789101112func serverInterceptor ( ctx context.Context, req interface&#123;&#125;, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler,) (interface&#123;&#125;, error) &#123; start := time.Now() resp, err := handler(ctx, req) err = rpc.MarshalError(err, ctx) log.Print(...) return resp, err&#125; it’s ugly，but works. 这是在 gRPC 不支持高级 Error 的情况下，怎么去 work around 这个问题，并且凑合用起来。现在这么做，错误就可以跨主机边界传递了。 生产环境案例：Dump又有客户前来提需求了，有的客户说我们可以存、也可以取，但是如何才能把里面所有的数据都获取下来？于是有了需求，希望实现 Dump() 操作，可以取回所有数据。 现在已经轻车熟路了，我们先改 IDL，添加一个 Dump() 函数。 1234567891011121314service Cache &#123; rpc Store(StoreReq) returns (StoreResp) &#123;&#125; rpc Get(GetReq) returns (GetResp) &#123;&#125; rpc Dump(DumpReq) returns (DumpResp) &#123;&#125;&#125;message DumpReq&#123;&#125;message DumpResp &#123; repeated DumpItem items = 1;&#125;message DumpItem &#123; string key = 1; bytes val = 2;&#125; 这里 DumpResp 里面用的是 repeated，因为 protobuf 里面不知道为啥不叫 array。 生产环境案例：流量控制新功能 Dump 上线了，结果发现大家都很喜欢 Dump，有很多人在 Dump，结果服务器的内存开始不够了。于是我们需要一些限制手段，可以控制流量。 查阅了文档后，发现我们可以控制同时最大有多少并发可以访问，以及可以多频繁的来访问服务。server.go 1234567891011121314func runServer() error &#123; ... srv := grpc.NewServer(grpc.Creds(tlsCreds), ServerInterceptor(), grpc.MaxConcurrentStreams(64), grpc.InTapHandle(NewTap().Handler)) rpc.RegisterCacheServer(srv, NewCacheService(accounts)) l, err := net.Listen(&quot;tcp&quot;, &quot;localhost:5051&quot;) if err != nil &#123; return err &#125; l = netutil.LimitListener(l, 1024) return srv.Serve(l)&#125; 这里使用了 netutil.LimitListener(l, 1024) 控制了总共可以有多少个连接，然后用 grpc.MaxConcurrentStreams(64) 指定了每个 grpc 的连接可以有多少个并发流(stream)。这两个结合起来基本控制了并发的总数。 但是 gRPC 里没有地方限定可以多频繁的访问。因此这里用了 grpc.InTapHandle(NewTap().Handler)) 来进行定制，这是在更靠前的位置执行的。 tap.go 123456789101112type Tap struct &#123; lim *rate.Limiter&#125;func NewTap() *Tap &#123; return &amp;Tap(rate.NewLimiter(150, 5))&#125;func (t *Tap) Handler(ctx context.Context, info *tap.Info) (context.Context, error) &#123; if !t.lim.Allow() &#123; return nil, status.Errorf(codes.ResourceExhausted, &quot;service is over rate limit&quot;) &#125; return ctx, nil&#125; 生产环境案例：Streaming之前的方案部署后，内存终于降下来了，但是还没休息，就发现大家越来越喜欢用这个缓存服务，内存又不够用了。这个时候我们就开始思考，是不是可以调整一下设计，不是每次 Dump 就立即在内存生成完整的返回数组，而是以流的形式，按需发回。app.proto 12345678910111213syntax = &quot;proto3&quot;;package rpc;service Cache &#123; rpc Store(StoreReq) returns (StoreResp) &#123;&#125; rpc Get(GetReq) returns (GetResp) &#123;&#125; rpc Dump(DumpReq) returns (stream DumpItem) &#123;&#125;&#125;message DumpReq&#123;&#125;message DumpItem &#123; string key = 1; bytes val = 2;&#125; 这里不再使用数组性质的 repeated，而是用 stream，客户端请求 Dump() 后，将结果以流的形式发回去。server.go 123456789func (s *CacheService) Dump(req *rpc.DumpReq, stream rpc.Cache_DumpServer) error &#123; for k, v := range s.store &#123; stream.Send(&amp;rpc.DumpItem&#123; Key: k, Val: v, &#125;) &#125; return nil&#125; 我们修改 Dump() 的实现，对于每个记录，利用 stream.Send() 发送到流。 注意这里我们没有 context，只有个 stream。client.go 1234567891011121314151617func runClient() error &#123; ... stream, err := cache.Dump(context.Background(), &amp;rpc.DumpReq&#123;&#125;) if err != nil &#123; return fmt.Errorf(&quot;failed to dump: %v&quot;, err) &#125; for &#123; item, err := stream.Recv() if err == io.EOF &#123; break &#125; if err != nil &#123; return fmt.Errorf(&quot;failed to stream item: %v&quot;, err) &#125; &#125; return nil&#125; 生产环境案例：横向扩展、负载均衡使用流后，服务器性能提高了很多，但是，我们的服务太吸引人了，用户越来越多，结果又内存不够了。这时候我们审查代码，感觉能做的事情都做了，或许是时候从单一服务器，扩展为多个服务器，然后之间使用负载均衡。 gRPC 是长连接性质的通讯，因此如果一个客户端连接了一个 gRPC Endpoint，那么他就会一直连接到一个固定的服务器，因此多服务器的负载均衡对同一个客户端来说是没有意义的，不会因为这个客户端有大量的请求而导致分散请求到不同的服务器上去。 如果我们希望客户端可以利用多服务器的机制，我们就需要更智能的客户端，让客户端意识到服务器存在多个副本，因此客户端建立多条连接到不同的服务器，这样就可以让单一客户端利用负载均衡的横向扩展能力。 生产环境案例：多语言协作在复杂的环境中，我们 gRPC 的客户端（甚至服务端）可能是不同语言平台的。这其实是 gRPC 的优势，可以比较容易的实现跨语言平台的通讯。 比如我们可以做一个 Python 客户端： 12345678import grpcimport rpc_pb2 as rpcchannel = grpc.insecure_channel(&apos;localhost:5051&apos;)cache_svc = rpc.CacheStub(channel)resp = cache_svc.Get(rpc.GetReq( key=&quot;gopher&quot;,))print resp.val 一个不是很爽的地方是虽然 gRPC 的跨语言通讯很方便，但是各个语言的实现都比较随意，比如 Go 中叫做 CacheClient()，而 Python 中则叫做 CacheStub()。这里没有什么特别的原因非不一样的名字，就是由于不同的作者实现的时候按照自己的想法命名的。 gRPC 尚不完美的地方 负载均衡 结构化的错误信息 还不支持浏览器的 JS （某种角度上讲，这是最常用的客户端） 还经常发生 API 改变（即使都1.0了） 某些语言实现的文档非常差 没有跨语言的标准化的做法 gRPC 在生产环境中的用例 ngrok，所有内部20多个通讯都走的是 gRPC Square，将内部的通讯都换成了 gRPC，是最早使用 gRPC 的用户和贡献者 CoreOS，etcd v3 完全走的是 gRPC Google，Google Cloud Service（PubSub, Speech Rec）走的是 gRPC Netflix, Yik Yak, VSCO, Cockroach, … gRPC 未来的变化 想了解未来的变化可以查看： grpc/proposal grpc-io 邮件列表 新的语言支持（Swift 和 Haskell正在试验阶段） 稳定性、可靠性、性能的提高 增加更多细化的 API 来支持自定义的行为（连接管理、频道跟踪） 浏览器的 JS 本文转载自：https://blog.lab99.org/post/golang-2017-10-04-video-understanding-channels.html#fa-song-jie-shou]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>gRPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给 Go 库作者的建议]]></title>
    <url>%2Fgolang-practice-advice-for-go-library-authors.html</url>
    <content type="text"><![CDATA[Golang Practice Advice 视频信息Practical Advice for Go Library Authorsby Jack Lindamoodat GopherCon 2016 https://www.youtube.com/watch?v=5v2fqm_8jYI 幻灯地址：http://go-talks.appspot.com/github.com/cep21/go-talks/practical-advice-for-go-library-authors.slide#1 命名包名是将来使用过程中的一部分，所以避免重复包名和结构与函数。比如 1var h client.Client → var h http.Client 1context.NewContext() =&gt; context.Background() Object Creationgolang 没有构造函数，因此创建对象一般有两种办法: 默认的0值 单独的构造函数，NewSomething() 推荐使用默认 0 值的构造方法 在默认0值的情况下，各个方法要处理好0值，比如有些东西发现是0值后，给入一个默认值。 New() 构造函数很灵活，可以做任何事情，因此对于代码阅读上不利，意味着隐藏了很多东西。 有些库使用私有 struct，公开接口的方法，authImpl struct and Auth interface，这是反模式，不推荐使用。 不推荐使用 Singleton，虽然标准库中大量使用了 Singleton 模式，但是 Jack 个人不喜欢这种模式。 使用高阶函数作为选项这种形式不推荐：NewSomething(WithThingA(), WithThingB()) 日志一些日志是直接打印到标准输出去，这是非常不好的设计，因为用户如果想关根本关不了。 建议 确定一下作为库是不是真的需要打印日志，是不是应该把输出日志的工作交给调用方决定？ 如果一定需要日志，那么使用回调函数方式 输出日志到一个 interface 不要假定传进来的就是标准库的 log ，有很多选择。 尊重 stdout 和 stderr 不要使用 singleton interface vs struct接受 interface ，但返回的是 struct 这点和 Java 不同，Java 更倾向于所有东西都是通过 interface 操作。而 golang 不需要，golang 使用的是隐性interface。 什么时候 panic最好都不 panic。如果非要 panic，可能最合适的地方是 init 的时候，因为刚一运行就能看到挂了，比较容易处理。但即使如此，也尽量不要 panic。 检查 error问：我们是需要检查所有的 error 么？比如有些似乎不大容易出错。答：需要，特别是你说的这些不大容易出错的！！ 我们用 error 代替了 exception，所以不要忽略这个东西。 处理的办法 最好的办法是 Bubble up，也就是传回调用方 但有的时候（比如 goroutine) 不适合，那就： 做日志 或者增加某个计数器 什么时候应该返回错误比较合适？ 当不满足约定 当需要的答案无法得到 允许启用库的调试能力为测试而设计 为了方便自己测试 为了方便库用户测试 并发channels虽然 channel 是 golang 一个处理并发很好地东西，但是并非所有场合都需要。比如标准库中就很少有在 API 中使用 channel 的。 将使用 channel 的位置向上层移动。 可以使用回调函数。 不要混合使用 mutex 和 channel 什么时候发起 goroutine 有一些库的 New() 会发起他们的 goroutine，这是不好的。 标准库使用的是 Serve() 函数。以及对应的 Close() 函数 将 goroutine 向上层推 什么时候使用 context.Context 所有的阻塞、长时间的操作，都应该可以被 cancel 由于 context.Context 很容易存储东西，所以很容易被滥用。要尽力去避免使用 Context Singleton 和 context.Value() 是同样性质的东西，像全局变量一样，对于程序状态来说是个黑箱。 其它注意事项 如果什么东西很难做，嗯，那就让别人去做吧 为了效率而升级 但是，正确性要比效率重要，在正确性的前提下，注意效率 不要在库中使用 /vendor （在 main 包中可以） 注意 build tag 保持干净 尽量使用所有的静态分析工具来检查代码。 原文https://blog.lab99.org/post/golang-2017-09-21-video-practice-advice-for-go-library-authors.html]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>Practice</tag>
        <tag>Advice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang 如何正确使用 Context]]></title>
    <url>%2Fhow-to-correctly-use-package-context.html</url>
    <content type="text"><![CDATA[Golang Context 视频信息How to correctly use package contextby Jack Lindamoodat Golang UK Conf. 2017 视频：https://www.youtube.com/watch?v=-_B5uQ4UGi0博文：https://medium.com/@cep21/how-to-correctly-use-context-context-in-go-1-7-8f2c0fafdf39 为什么需要 Context 每一个长请求都应该有个超时限制 需要在调用中传递这个超时 比如开始处理请求的时候我们说是 3 秒钟超时 那么在函数调用中间，这个超时还剩多少时间了？ 需要在什么地方存储这个信息，这样请求处理中间可以停止 如果进一步考虑。如上图这样的 RPC 调用，开始调用 RPC 1 后，里面分别调用了 RPC 2, RPC 3, RPC 4，等所有 RPC 用成功后，返回结果。 这是正常的方式，但是如果 RPC 2 调用失败了会发生什么？ RPC 2 失败后，如果没有 Context 的存在，那么我们可能依旧会等所有的 RPC 执行完毕，但是由于 RPC 2 败了，所以其实其它的 RPC 结果意义不大了，我们依旧需要给用户返回错误。因此我们白白的浪费了 10ms，完全没必要去等待其它 RPC 执行完毕。 那如果我们在 RPC 2 失败后，就直接给用户返回失败呢？用户是在 30ms 的位置收到了错误消息，可是 RPC 3 和 RPC 4 依然在没意义的运行，还在浪费计算和IO资源。 所以理想状态应该是如上图，当 RPC 2 出错后，除了返回用户错误信息外，我们也应该有某种方式可以通知 RPC 3 和 RPC 4，让他们也停止运行，不再浪费资源。 所以解决方案就是： 用信号的方式来通知请求该停了 包含一些关于什么时间请求可能会结束的提示（超时） 用 channel 来通知请求结束了 那干脆让我们把变量也扔那吧。😈 在 Go 中没有线程/go routine 变量 其实挺合理的，因为这样就会让 goroutine 互相产生依赖 非常容易被滥用 Context 实现细节context.Context： 是不可变的(immutable)树节点 Cancel 一个节点，会连带 Cancel 其所有子节点 （从上到下） Context values 是一个节点 Value 查找是回溯树的方式 （从下到上） 示例 Context 链完整代码：https://play.golang.org/p/ddpofBV1QS 123456789package mainfunc tree() &#123; ctx1 := context.Background() ctx2, _ := context.WithCancel(ctx1) ctx3, _ := context.WithTimeout(ctx2, time.Second * 5) ctx4, _ := context.WithTimeout(ctx3, time.Second * 3) ctx5, _ := context.WithTimeout(ctx3, time.Second * 6) ctx6 := context.WithValue(ctx5, &quot;userID&quot;, 12)&#125; 如果这样构成的 Context 链，其形如下图：那么当 3 秒超时到了时候：可以看到 ctx4 超时退出了。 当 5秒钟 超时到达时：可以看到，不仅仅 ctx3 退出了，其所有子节点，比如 ctx5 和 ctx6 也都退出了。 context.Context API基本上是两类操作： 3个函数用于限定什么时候你的子节点退出； 1个函数用于设置请求范畴的变量12345678type Context interface &#123; // 啥时候退出 Deadline() (deadline time.Time, ok bool) Done() &lt;-chan struct&#123;&#125; Err() error // 设置变量 Value(key interface&#123;&#125;) interface&#123;&#125;&#125; 什么时候应该使用 Context？ 每一个 RPC 调用都应该有超时退出的能力，这是比较合理的 API 设计 不仅仅 是超时，你还需要有能力去结束那些不再需要操作的行为 context.Context 是 Go 标准的解决方案 任何函数可能被阻塞，或者需要很长时间来完成的，都应该有个 context.Context 如何创建 Context？ 在 RPC 开始的时候，使用 context.Background() 有些人把在 main() 里记录一个 context.Background()，然后把这个放到服务器的某个变量里，然后请求来了后从这个变量里继承 context。这么做是不对的。直接每个请求，源自自己的 context.Background() 即可。 如果你没有 context，却需要调用一个 context 的函数的话，用 context.TODO() 如果某步操作需要自己的超时设置的话，给它一个独立的 sub-context（如前面的例子） 如何集成到 API 里？ 如果有 Context，将其作为第一个变量。 如 func (d* Dialer) DialContext(ctx context.Context, network, address string) (Conn, error) 有些人把 context 放到中间的某个变量里去，这很不合习惯，不要那么做，放到第一个去。 将其作为可选的方式，用 request 结构体方式。 如：func (r *Request) WithContext(ctx context.Context) *Request Context 的变量名请用 ctx（不要起一些诡异的名字😓） Context 放哪？ 把 Context 想象为一条河流流过你的程序（另一个意思就是说不要喝河里的水……🙊） 理想情况下，Context 存在于调用栈（Call Stack） 中 不要把 Context 存储到一个 struct 里 除非你使用的是像 http.Request 中的 request 结构体的方式 request 结构体应该以 Request 结束为生命终止 当 RPC 请求处理结束后，应该去掉对 Context 变量的引用（Unreference） Request 结束，Context 就应该结束。（这俩是一对儿，不求同年同月同日生，但求同年同月同日死……💕） Context 包的注意事项 要养成关闭 Context 的习惯 特别是 超时的 Contexts 如果一个 context 被 GC 而不是 cancel 了，那一般是你做错了 12ctx, cancel := context.WithTimeout(parentCtx, time.Second * 2)defer cancel() 使用 Timeout 会导致内部使用 time.AfterFunc，从而会导致 context 在计时器到时之前都不会被垃圾回收。 在建立之后，立即 defer cancel() 是一个好习惯。 终止请求 (Request Cancellation)当你不再关心接下来获取的结果的时候，有可能会 Cancel 一个 Context？ 以 golang.org/x/sync/errgroup 为例，errgroup 使用 Context 来提供 RPC 的终止行为。 123456type Group struct &#123; cancel func() wg sync.WaitGroup errOnce sync.Once err error&#125; 创建一个 group 和 context： 1234func WithContext(ctx context.Context) (*Group, context.Context) &#123; ctx, cancel := context.WithCancel(ctx) return &amp;Group&#123;cancel: cancel&#125;, ctx&#125; 这样就返回了一个可以被提前 cancel 的 group。 而调用的时候，并不是直接调用 go func()，而是调用 Go()，将函数作为参数传进去，用高阶函数的形式来调用，其内部才是 go func() 开启 goroutine。 1234567891011121314func (g *Group) Go(f func() error) &#123; g.wg.Add(1) go func() &#123; defer g.wg.Done() if err := f(); err != nil &#123; g.errOnce.Do(func() &#123; g.err = err if g.cancel != nil &#123; g.cancel() &#125; &#125;) &#125; &#125;()&#125; 当给入函数 f 返回错误，则使用 sync.Once 来 cancel context，而错误被保存于 g.err 之中，在随后的 Wait() 函数中返回。 1234567func (g *Group) Wait() error &#123; g.wg.Wait() if g.cancel != nil &#123; g.cancel() &#125; return g.err&#125; 注意：这里在 Wait() 结束后，调用了一次 cancel()。 123456789101112131415161718192021package mainfunc DoTwoRequestsAtOnce(ctx context.Context) error &#123; eg, egCtx := errgroup.WithContext(ctx) var resp1, resp2 *http.Response f := func(loc string, respIn **http.Response) func() error &#123; return func() error &#123; reqCtx, cancel := context.WithTimeout(egCtx, time.Second) defer cancel() req, _ := http.NewRequest(&quot;GET&quot;, loc, nil) var err error *respIn, err = http.DefaultClient.Do(req.WithContext(reqCtx)) if err == nil &amp;&amp; (*respIn).StatusCode &gt;= 500 &#123; return errors.New(&quot;unexpected!&quot;) &#125; return err &#125; &#125; eg.Go(f(&quot;http://localhost:8080/fast_request&quot;, &amp;resp1)) eg.Go(f(&quot;http://localhost:8080/slow_request&quot;, &amp;resp2)) return eg.Wait()&#125; 在这个例子中，同时发起了两个 RPC 调用，当任何一个调用超时或者出错后，会终止另一个 RPC 调用。这里就是利用前面讲到的 errgroup 来实现的，应对有很多并非请求，并需要集中处理超时、出错终止其它并发任务的时候，这个 pattern 使用起来很方便。 Context.Value - Request 范畴的值context.Value API 的万金油（duct tape)胶带（duct tape) 几乎可以修任何东西，从破箱子，到人的伤口，到汽车引擎，甚至到NASA登月任务中的阿波罗13号飞船（Yeah! True Story)。所以在西方文化里，胶带是个“万能”的东西。在中文里，恐怕万金油是更合适的对应词汇，从头疼、脑热，感冒发烧，到跌打损伤几乎无所不治。 当然，治标不治本，这点东西方文化中的潜台词都是一样的。这里提及的 context.Value 对于 API 而言，就是这类性质的东西，啥都可以干，但是治标不治本。 value 节点是 Context 链中的一个节点123456789101112131415package contexttype valueCtx struct &#123; Context key, val interface&#123;&#125;&#125;func WithValue(parent Context, key, val interface&#123;&#125;) Context &#123; // ... return &amp;valueCtx&#123;parent, key, val&#125;&#125;func (c *valueCtx) Value(key interface&#123;&#125;) interface&#123;&#125; &#123; if c.key == key &#123; return c.val &#125; return c.Context.Value(key)&#125; 可以看到，WithValue() 实际上就是在 Context 树形结构中，增加一个节点罢了。 Context 是 immutable 的。 约束 key 的空间为了防止树形结构中出现重复的键，建议约束键的空间。比如使用私有类型，然后用 GetXxx() 和 WithXxxx() 来操作私有实体。 1234567891011type privateCtxType stringvar ( reqID = privateCtxType(&quot;req-id&quot;))func GetRequestID(ctx context.Context) (int, bool) &#123; id, exists := ctx.Value(reqID).(int) return id, exists&#125;func WithRequestID(ctx context.Context, reqid int) context.Context &#123; return context.WithValue(ctx, reqID, reqid)&#125; 这里使用 WithXxx 而不是 SetXxx 也是因为 Context 实际上是 immutable 的，所以不是修改 Context 里某个值，而是产生新的 Context 带某个值。 Context.Value 是 immutable 的再多次的强调 Context.Value 是 immutable 的也不过分。 context.Context 从设计上就是按照 immutable （不可变的）模式设计的 同样，Context.Value 也是 immutable 的 不要试图在 Context.Value 里存某个可变更的值，然后改变，期望别的 Context 可以看到这个改变 更别指望着在 Context.Value 里存可变的值，最后多个 goroutine 并发访问没竞争冒险啥的，因为自始至终，就是按照不可变来设计的 比如设置了超时，就别以为可以改变这个设置的超时值 在使用 Context.Value 的时候，一定要记住这一点 应该把什么放到 Context.Value 里？ 应该保存 Request 范畴的值 任何关于 Context 自身的都是 Request 范畴的（这俩同生共死） 从 Request 数据衍生出来，并且随着 Request 的结束而终结 什么东西不属于 Request 范畴？ 在 Request 以外建立的，并且不随着 Request 改变而变化 比如你 func main() 里建立的东西显然不属于 Request 范畴 数据库连接 如果 User ID 在连接里呢？(稍后会提及) 全局 logger 如果 logger 里需要有 User ID 呢？（稍后会提及） 那么用 Context.Value 有什么问题？ 不幸的是，好像所有东西都是由请求衍生出来的 那么我们为什么还需要函数参数？然后干脆只来一个 Context 就完了？123func Add(ctx context.Context) int &#123; return ctx.Value(&quot;first&quot;).(int) + ctx.Value(&quot;second&quot;).(int)&#125; 曾经看到过一个 API，就是这种形式： 1234func IsAdminUser(ctx context.Context) bool &#123; userID := GetUser(ctx) return authSingleton.IsAdmin(userID)&#125; 这里API实现内部从 context 中取得 UserID，然后再进行权限判断。但是从函数签名看，则完全无法理解这个函数具体需要什么、以及做什么。 代码要以可读性为优先设计考虑。 别人拿到一个代码，一般不是掉进函数实现细节里去一行行的读代码，而是会先浏览一下函数接口。所以清晰的函数接口设计，会更加利于别人（或者是几个月后的你自己）理解这段代码。 一个良好的 API 设计，应该从函数签名就清晰的理解函数的逻辑。如果我们将上面的接口改为： 1func IsAdminUser(ctx context.Context, userID string, authenticator auth.Service) bool 我们从这个函数签名就可以清楚的知道： 这个函数很可能可以提前被 cancel 这个函数需要 User ID 这个函数需要一个authenticator来 而且由于 authenticator 是传入参数，而不是依赖于隐式的某个东西，我们知道，测试的时候就很容易传入一个模拟认证函数来做测试 userID 是传入值，因此我们可以修改它，不用担心影响别的东西 所有这些信息，都是从函数签名得到的，而无需打开函数实现一行行去看。 那什么可以放到 Context.Value 里去？现在知道 Context.Value 会让接口定义更加模糊，似乎不应该使用。那么又回到了原来的问题，到底什么可以放到 Context.Value 里去？换个角度去想，什么不是衍生于 Request？ Context.Value 应该是告知性质的东西，而不是控制性质的东西 应该永远都不需要写进文档作为必须存在的输入数据 如果你发现你的函数在某些 Context.Value 下无法正确工作，那就说明这个 Context.Value 里的信息不应该放在里面，而应该放在接口上。因为已经让接口太模糊了。 什么东西不是控制性质的东西？ Request ID 只是给每个 RPC 调用一个 ID，而没有实际意义 这就是个数字/字符串，反正你也不会用其作为逻辑判断 一般也就是日志的时候需要记录一下 而 logger 本身不是 Request 范畴，所以 logger 不应该在 Context 里 非 Request 范畴的 logger 应该只是利用 Context 信息来修饰日志 User ID （如果仅仅是作为日志用） Incoming Request ID 什么显然是控制性质的东西？ 数据库连接 显然会非常严重的影响逻辑 因此这应该在函数参数里，明确表示出来 认证服务(Authentication) 显然不同的认证服务导致的逻辑不同 也应该放到函数参数里，明确表示出来 例子调试性质的 Context.Value - net/http/httptracehttps://medium.com/@cep21/go-1-7-httptrace-and-context-debug-patterns-608ae887224a 12345678910111213141516package mainfunc trace(req *http.Request, c *http.Client) &#123; trace := &amp;httptrace.ClientTrace&#123; GotConn: func(connInfo httptrace.GotConnInfo) &#123; fmt.Println(&quot;Got Conn&quot;) &#125;, ConnectStart: func(network, addr string) &#123; fmt.Println(&quot;Dial Start&quot;) &#125;, ConnectDone: func(network, addr string, err error) &#123; fmt.Println(&quot;Dial done&quot;) &#125;, &#125; req = req.WithContext(httptrace.WithClientTrace(req.Context(), trace)) c.Do(req)&#125; net/http 是怎么使用 httptrace 的？ 如果有 trace 存在的话，就执行 trace 回调函数 这只是告知性质，而不是控制性质 http 不会因为存在 trace 与否就有不同的执行逻辑 这里只是告知 API 的用户，帮助用户记录日志或者调试 因此这里的 trace 是存在于 Context 里的123456789package httpfunc (req *Request) write(w io.Writer, usingProxy bool, extraHeaders Header, waitForContinue func() bool) (err error) &#123; // ... trace := httptrace.ContextClientTrace(req.Context()) // ... if trace != nil &amp;&amp; trace.WroteHeaders != nil &#123; trace.WroteHeaders() &#125;&#125; 回避依赖注入 - github.com/golang/oauth2 这里比较诡异，使用 ctx.Value 来定位依赖 不推荐这样做 这里这样做基本上只是为了满足测试需求12345678package mainimport &quot;github.com/golang/oauth2&quot;func oauth() &#123; c := &amp;http.Client&#123;Transport: &amp;mockTransport&#123;&#125;&#125; ctx := context.WithValue(context.Background(), oauth2.HTTPClient, c) conf := &amp;oauth2.Config&#123; /* ... */ &#125; conf.Exchange(ctx, &quot;code&quot;)&#125; 人们滥用 Context.Value 的原因 中间件的抽象 很深的函数调用栈 混乱的设计 context.Value 并没有让你的 API 更简洁，那是假象，相反，它让你的 API 定义更加模糊。 总结 Context.Value 对于调试非常方便 将必须的信息放入 Context.Value 中，会让接口定义更加不透明 如果可以尽量明确定义在接口 尽量不要用 Context.Value 总结 Context 所有的长的、阻塞的操作都需要 Context errgroup 是构架于 Context 之上很好的抽象 当 Request 的结束的时候，Cancel Context Context.Value 应该被用于告知性质的事物，而不是控制性质的事物 约束 Context.Value 的键空间 Context 以及 Context.Value 应该是不可变的（immutable），并且应该是线程安全 Context 应该随 Request 消亡而消亡 Q&amp;A数据库的访问也用 Context 么？之前说过长时间、可阻塞的操作都用 Context，数据库操作也是如此。不过对于超时 Cancel 操作来说，一般不会对写操作进行 cancel；但是对于读操作，一般会有 Cancel 操作。 原文https://blog.lab99.org/post/golang-2017-10-27-video-how-to-correctly-use-package-context.html]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>Context</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[encrypted communication elasticsearch java rest client]]></title>
    <url>%2Fencrypted-communication-elasticsearch-java-rest-client.html</url>
    <content type="text"><![CDATA[ElasticSearch 7.3.1Java Rest Client HTTPS连接操作 ElasticSearch版本7.3.1，elasticsearch.yml配置如下： 12345678910xpack.security.enabled: truexpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.verification_mode: certificatexpack.security.transport.ssl.key: /home/jiankunking/elasticsearch-7.3.1/config/certs/_.jiankunking.com.keyxpack.security.transport.ssl.certificate: /home/jiankunking/elasticsearch-7.3.1/config/certs/_.jiankunking.com.cerxpack.security.transport.ssl.certificate_authorities: [ &quot;/home/jiankunking/elasticsearch-7.3.1/config/certs/_.jiankunking.com_ca.crt&quot; ]xpack.security.http.ssl.enabled: truexpack.security.http.ssl.key: /home/jiankunking/elasticsearch-7.3.1/config/certs/_.jiankunking.com.keyxpack.security.http.ssl.certificate: /home/jiankunking/elasticsearch-7.3.1/config/certs/_.jiankunking.com.cerxpack.security.http.ssl.certificate_authorities: [ &quot;/home/jiankunking/elasticsearch-7.3.1/config/certs/_.jiankunking.com_ca.crt&quot; ] 由于ElasticSearch Java client中的KeyStore Types只支持以下几种： Type Description jceks The proprietary keystore implementation provided by the SunJCE provider. jks The proprietary keystore implementation provided by the SUN provider. dks A domain keystore is a collection of keystores presented as a single logical keystore. It is specified by configuration data whose syntax is described in DomainLoadStoreParameter. pkcs11 A keystore backed by a PKCS #11 token. pkcs12 The transfer syntax for personal identity information as defined in PKCS #12. 而我这边证书格式为cer，所以通过keytool进行转换： 1keytool -import -v -trustcacerts -file _.jiankunking.com.cer -keystore my_keystore.jks -keypass password -storepass password 证书转换完成后，操作代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package ssl;import org.apache.http.HttpHost;import org.apache.http.auth.AuthScope;import org.apache.http.auth.UsernamePasswordCredentials;import org.apache.http.client.CredentialsProvider;import org.apache.http.impl.client.BasicCredentialsProvider;import org.apache.http.ssl.SSLContexts;import org.elasticsearch.client.RequestOptions;import org.elasticsearch.client.RestClient;import org.elasticsearch.client.RestClientBuilder;import org.elasticsearch.client.RestHighLevelClient;import org.elasticsearch.client.indices.CreateIndexRequest;import org.elasticsearch.client.indices.CreateIndexResponse;import org.elasticsearch.common.settings.Settings;import org.elasticsearch.common.xcontent.XContentType;import javax.net.ssl.SSLContext;import java.io.File;import java.io.IOException;import java.security.KeyManagementException;import java.security.KeyStoreException;import java.security.NoSuchAlgorithmException;import java.security.cert.CertificateException;import java.util.HashMap;import java.util.Map;/** * @Author: jiankunking * @Date: 2019/8/27 15:32 * @Description: */public class es &#123; public static void main(String[] args) throws KeyStoreException, IOException, NoSuchAlgorithmException, KeyManagementException, CertificateException &#123; CredentialsProvider credentialsProvider = new BasicCredentialsProvider(); credentialsProvider.setCredentials(AuthScope.ANY, new UsernamePasswordCredentials(&quot;elastic&quot;, &quot;jiankunking&quot;)); SSLContext sslContext = SSLContexts.custom() .loadTrustMaterial(new File(&quot;I:\\certs\\my_keystore.jks&quot;)) .build(); String host = &quot;es.jiankunking.com&quot;; int port = 9200; String scheme = &quot;https&quot;; String indexName = &quot;twitter2&quot;; RestClientBuilder restClientBuilder = RestClient.builder(new HttpHost(host, port, scheme)).setHttpClientConfigCallback(httpClientBuilder -&gt; httpClientBuilder .setDefaultCredentialsProvider(credentialsProvider) .setSSLContext(sslContext) ); // 到这里RestHighLevelClient已经初始化完成，下面的创建索引是测试 RestHighLevelClient restHighLevelClient = new RestHighLevelClient(restClientBuilder); // 创建索引请求 CreateIndexRequest request = new CreateIndexRequest(indexName); request.settings(Settings.builder() .put(&quot;index.number_of_shards&quot;, 3) .put(&quot;index.number_of_replicas&quot;, 2) ); request.mapping( &quot;&#123;\n&quot; + &quot; \&quot;properties\&quot;: &#123;\n&quot; + &quot; \&quot;message\&quot;: &#123;\n&quot; + &quot; \&quot;type\&quot;: \&quot;text\&quot;\n&quot; + &quot; &#125;\n&quot; + &quot; &#125;\n&quot; + &quot;&#125;&quot;, XContentType.JSON); Map&lt;String, Object&gt; message = new HashMap&lt;&gt;(); message.put(&quot;type&quot;, &quot;text&quot;); Map&lt;String, Object&gt; properties = new HashMap&lt;&gt;(); properties.put(&quot;message&quot;, message); Map&lt;String, Object&gt; mapping = new HashMap&lt;&gt;(); mapping.put(&quot;properties&quot;, properties); request.mapping(mapping); CreateIndexResponse createIndexResponse; try &#123; createIndexResponse = restHighLevelClient.indices().create(request, RequestOptions.DEFAULT); System.out.println(createIndexResponse); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>Java</tag>
        <tag>ElasticSearch</tag>
        <tag>Rest</tag>
        <tag>Client</tag>
        <tag>Encrypted</tag>
        <tag>Communication</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Web性能权威指南 笔记]]></title>
    <url>%2Fhigh-performance-browser-networking-note.html</url>
    <content type="text"><![CDATA[本文整理自：《Web性能权威指南》 作者：Ilya Grigorik 出版时间：2014-04 网络技术概览带宽与延迟延迟的最后一公里traceroute (Windows 系统下是tracert) 命令利用ICMP协议定位您的计算机和目标计算机之间的所有路由器。 TCP的构成因特网有两个核心协议：IP和TCP。IP，即 Internet Protocol（因特网协议），负责联网主机之间的路由选择和寻址；TCP，即 Transmission Control Protocol（传输控制协议），负责在不可靠的传输信道之上提供可靠的抽象层。 三次握手所有TCP连接一开始都要经过三次握手（见图 2-1）。客户端与服务器在交换应用数据之前，必须就起始分组序列号，以及其他一些连接相关的细节达成一致。出于安全考虑，序列号由两端随机生成。 SYN客户端选择一个随机序列号x，并发送一个SYN分组，其中可能还包括其他TCP标志和选项。 SYN ACK服务器给x加1，并选择自己的一个随机序列号y，追加自己的标志和选项，然后返回响应。 ACK客户端给x和y加1并发送握手期间的最后一个ACK分组。 SYN：同步序列编号（Synchronize Sequence Numbers）ACK (Acknowledge character）即是确认字符 三次握手完成后，客户端与服务器之间就可以通信了。客户端可以在发送ACK分组之后立即发送数据，而服务器必须等接收到ACK分组之后才能发送数据。这个启动通信的过程适用于所有TCP连接，因此对所有使用TCP的应用具有非常大的性能影响，因为每次传输应用数据之前，都必须经历一次完整的往返。 队首阻塞 丢包就丢包事实上，丢包是让TCP达到最佳性能的关键。被删除的包恰恰是一种反馈机制，能够让接收端和发送端各自调整速度，以避免网络拥堵，同时保持延迟最短。另外，有些应用程序可以容忍丢失一定数量的包，比如语音和游戏状态通信，就不需要可靠传输或按序交付。就算有个包丢了，音频编解码器只要在音频中插入一个小小的间歇，就可以继续处理后来的包。只要间歇够小，用户就注意不到，而等待丢失的包则可能导致音频输出产生无法预料的暂停。相对来说，后者的用户体验更糟糕。类似地，更新3D游戏中角色的状态也一样：收到T时刻的包而等待T-1时刻的包通常毫无必要。理想情况下，应该可以接收所有状态更新，但为避免游戏延迟，间歇性的丢包也是可以接受的。 针对TCP的优化建议TCP是一个自适应的、对所有网络节点一视同仁的、最大限制利用底层网络的协议。因此，优化TCP的最佳途径就是调整它感知当前网络状况的方式，根据它之上或之下的抽象层的类型和需求来改变它的行为。 服务器配置调优在着手调整TCP的缓冲区、超时等数十个变量之前，最好先把主机操作系统升级到最新版本。TCP 的最佳实践以及影响其性能的底层算法一直在与时俱进，而且大多数变化都只在最新内核中才有实现。一句话，让你的服务器跟上时代是优化发送端和接收端TCP栈的首要措施。 有了最新的内核，我们推荐你遵循如下最佳实践来配置自己的服务器。 增大TCP的初始拥塞窗口加大起始拥塞窗口可以让TCP在第一次往返就传输较多数据，而随后的速度提升也会很明显。对于突发性的短暂连接，这也是特别关键的一个优化。 慢启动重启在连接空闲时禁用慢启动可以改善瞬时发送数据的长TCP连接的性能。 窗口缩放启用窗口缩放可以增大最大接收窗口大小，可以让高延迟的连接达到更好吞吐量。 TCP快速打开在某些条件下，允许在第一个SYN分组中发送应用程序数据。TFO（TCP Fast Open，TCP 快速打开）是一种新的优化选项，需要客户端和服务器共同支持。为此，首先要搞清楚你的应用程序是否可以利用这个特性。 Linux用户可以使用ss来查看当前打开的套接字的各种统计信息。在命令行里运行ss –options –extended –memory –processes –info ，可以看到当前通信节点以及它们相应的连接设置。 UDP的构成关于UDP的应用，最广为人知同时也是所有浏览器和因特网应用都赖以运作的，就是DNS（Domain Name System，域名系统）。 IETF和W3C工作组共同制定了一套新API WebRTC（Web Real-Time Communication，Web 实时通信）。WebRTC着眼于在浏览器中通过UDP实现原生的语音和视频实时通信，以及其他形式的 P2P（Peer-to-Peer，端到端）通信。 无协议服务要理解为什么UDP被人称作“无协议”，必须从作为TCP和UDP下一层的IP协议说起。IP层的主要任务就是按照地址从源主机向目标主机发送数据报。为此，消息会被封装在一个IP分组内（图3-1），其中载明了源地址和目标地址，以及其他一些路由参数。注意，数据报这个词暗示了一个重要的信息：IP层不保证消息可靠的交付，也不发送失败通知，实际上是把底层网络的不可靠性直接暴露给了上一层。如果某个路由节点因为网络拥塞、负载过高或其他原因而删除了IP分组，那么在必要的情况下，IP 的上一层协议要负责检测、恢复和重发数据。 UDP协议会用自己的分组结构（图3-2）封装用户消息，它只增加 4个字段：源端口、目标端口、分组长度和校验和。这样，当IP把分组送达目标主机时，该主机能够拆开UDP分组，根据目标端口找到目标应用程序，然后再把消息发送过去。仅此而已。 事实上，UDP数据报中的源端口和校验和字段都是可选的。IP分组的首部也有校验和，应用程序可以忽略UDP校验和。也就是说，所有错误检测和错误纠正工作都可以委托给上层的应用程序。说到底，UDP仅仅是在IP层之上通过嵌入应用程序的源端口和目标端口，提供了一个“应用程序多路复用”机制。明白了这一点，就可以总结一下UDP的无服务是怎么回事了。 不保证消息交付不确认，不重传，无超时。 不保证交付顺序不设置包序号，不重排，不会发生队首阻塞。 不跟踪连接状态不必建立连接或重启状态机。 不需要拥塞控制不内置客户端或网络反馈机制。 TCP是一个面向字节流的协议，能够以多个分组形式发送应用程序消息，且对分组中的消息范围没有任何明确限制。因此，连接的两端存在一个连接状态，每个分组都有序号，丢失还要重发，并且要按顺序交付。相对来说，UDP数据报有明确的限制：数据报必须封装在IP分组中，应用程序必须读取完整的消息。换句话说，数据报不能分片。 UDP与网络地址转换器作为监管全球IP地址分配的机构，IANA（Internet Assigned Numbers Authority，因特网号码分配机构）为私有网络保留了三段IP地址，这些IP地址经常可以在NAT设备后面的内网中看到。 保留的IP地址范围： IP地址范围 地址数量 10.0.0.0~10.255.255.255 16 777 216 172.16.0.0~172.31.255.255 1 048 576 192.168.0.0~192.168.255.255 65 536 为防止路由错误和引起不必要的麻烦，不允许给外网计算机分配这些保留的私有IP地址。 传输层安全（TLS）SSL（Secure Sockets Layer，安全套接字层）协议最初是网景公司为了保障网上交易安全而开发的，该协议通过加密来保护客户个人资料，通过认证和完整性检查来确保交易安全。为达到这个目标，SSL协议在直接位于TCP上一层的应用层被实现（图 4-1）。SSL不会影响上层协议（如HTTP、电子邮件、即时通讯），但能够保证上层协议的网络通信安全。 在正确使用SSL的情况下，第三方监听者只能推断出连接的端点、加密类型，以及发送数据的频率和大致数量，不能实际读取或修改任何数据。 加密、身份验证与完整性 Web 代理、中间设备、TLS 与新协议HTTP 良好的扩展能力和获得的巨大成功，使得 Web 上出现了大量代理和中间设备：缓存服务器、安全网关、Web 加速器、内容过滤器，等等。有时候，我们知道这些设备的存在（显式代理），而有时候，这些设备对终端用户则完全不可见。 然而，这些服务器的存在及成功也给那些试图脱离 HTTP 协议的人带了一些不便。比如，有的代理服务器只会简单地转发自己无法解释的 HTTP 扩展或其他在线格式（wire format），而有的则不管是否必要都会对所有数据执行自己设定的逻辑，还有一些安全设备可能会把本来正常的数据误判成恶意通信。 换句话说，现实当中如果想脱离 HTTP 和 80 端口的语义行事，经常会遭遇各种部署上的麻烦。比如，某些客户端表现正常，另一些可能就会异常，甚至在某个网段表现正常的客户端到了另一个网段又会变得异常。 为解决这些问题，出现了一些新协议和对 HTTP 的扩展，比如 WebSocket、SPDY等。这些新协议一般要依赖于建立 HTTPS 信道，以绕过中间代理，从而实现可靠的部署，因为加密的传输信道会对所有中间设备都混淆数据。这样虽然解决了中间设备的问题，但却导致通信两端不能再利用这些中间设备，从而与这些设备提供的身份验证、缓存、安全扫描等功能失之交臂。 信任链与证书颁发机构Web以及浏览器中的身份验证需要回答以下几个问题：我的浏览器信任谁？我在使用浏览器的时候信任谁？这个问题至少有三个答案。 手工指定证书所有浏览器和操作系统都提供了一种手工导入信任证书的机制。至于如何获得证书和验证完整性则完全由你自己来定。 证书颁发机构CA（Certificate Authority，证书颁发机构）是被证书接受者（拥有者）和依赖证书的一方共同信任的第三方。 浏览器和操作系统每个操作系统和大多数浏览器都会内置一个知名证书颁发机构的名单。因此，你也会信任操作系统及浏览器提供商提供和维护的可信任机构。 实践中，保存并手工验证每个网站的密钥是不可行的（当然，如果你愿意，也可以）。现实中最常见的方案就是让证书颁发机构替我们做这件事（图 4-5）：浏览器指定可信任的证书颁发机构（根CA），然后验证他们签署的每个站点的责任就转移到了他们头上，他们会审计和验证这些站点的证书没有被滥用或冒充。持有CA证书的站点的安全性如果遭到破坏，那撤销该证书也是证书颁发机构的责任。 所有浏览器都允许用户检视自己安全连接的信任链，常见的访问入口就是地址栏头儿上的锁图标，点击即可查看。 证书撤销证书撤销名单（CRL）CRL（Certificate Revocation List，证书撤销名单）是RFC 5280规定的一种检查所有证书状态的简单机制：每个证书颁发机构维护并定期发布已撤销证书的序列号名单。这样，任何想验证证书的人都可以下载撤销名单，检查相应证书是否榜上有名。如果有，说明证书已经被撤销了。 CRL文件本身可以定期发布、每次更新时发布，或通过HTTP或其他文件传输协议来提供访问。这个名单同样由证书颁发机构签名，通常允许被缓存一定时间。实践中，这种机制效果很好，但也存在一些问题： CRL名单会随着要撤销的证书增多而变长，每个客户端都必须取得包含所有序列号的完整名单； 没有办法立即更新刚刚被撤销的证书序列号，比如客户端先缓存了CRL，之后某证书被撤销，那到缓存过期之前，该证书将一直被视为有效。 在线证书状态协议（OCSP）为解决CRL机制的上述问题，RFC 2560定义了OCSP（Online Certificate Status Protocol，在线证书状态协议），提供了一种实时检查证书状态的机制。与CRL包含被撤销证书的序列号不同，OCSP 支持验证端直接查询证书数据库中的序列号，从而验证证书链是否有效。总之，OCSP 占用带宽更少，支持实时验证。 然而，没有什么机制是完美无缺的！实时OCSP查询也带了一些问题： 证书颁发机构必须处理实时查询； 证书颁发机构必须确保随时随地可以访问； 客户端在进一步协商之前阻塞OCSP请求； 由于证书颁发机构知道客户端要访问哪个站点，因此实时OCSP请求可能会泄露客户端的隐私。 实践中，CRL和OCSP机制是互补存在的，大多数证书既提供指令也支持查询。更重要的倒是客户端的支持和行为。有的浏览器会分发自己的CRL名单，有的浏览器从证书颁发机构取得并缓存CRL文件。类似地，有的浏览器会进行实时OCSP检查，但在OCSP请求失败的情况下行为又会有所不同。要了解具体的情况，可以检查浏览器和操作系统的证书撤销网络设置。 针对TLS的优化建议TLS记录大小不过对于在浏览器中运行的Web应用来说，倒是有一个值得推荐的做法：每个TCP分组恰好封装一个TLS记录，而TLS记录大小恰好占满TCP分配的MSS（Maximum Segment Size，最大段大小）。换句话说，一方面不要让TLS记录分成多个TCP分组，另一方面又要尽量在一条记录中多发送数据。以下数据可作为确定最优TLS记录大小的参考： IPv4 帧需要 20 字节，IPv6 需要 40 字节； TCP 帧需要 20 字节； TCP 选项需要 40 字节（时间戳、SACK 等）。 假设常见的MTU为1500字节，则TLS记录大小在IPv4下是1420字节，在IPv6下是1400字节。为确保向前兼容，建议使用IPv6下的大小：1400字节。当然，如果MTU更小，这个值也要相应调小。 可惜的是，我们不能在应用层控制TLS记录大小。TLS记录大小通常是一个设置，甚至是TLS服务器上的编译时常量或标志。要了解具体如何设置这个值，请参考服务器文档。 如果服务器要处理大量TLS连接，那么关键的优化是把每个连接占用的内存量控制在最小。默认情况下，OpenSSL等常用的库会给每个连接分配50KB空间，但正像设置记录大小一样，有必要查一查文档或者源代码，然后再决定如何调整这个值。谷歌的服务器把OpenSSL缓冲区的大小减少到了大约5KB。 TLS压缩TLS还有一个内置的小功能，就是支持对记录协议传输的数据进行无损压缩。压缩算法在TLS握手期间商定，压缩操作在对记录加密之前执行。然而，出于如下原因，实践中往往需要禁用服务器上的TLS压缩功能： 2012 年公布的“CRIME”攻击会利用TLS压缩恢复加密认证cookie，让攻击者实施会话劫持； 传输级的TLS压缩不关心内容，可能会再次压缩已经压缩过的数据（图像、视频等等）。 双重压缩会浪费服务器和客户端的CPU时间，而且暴露的安全漏洞也很严重，因此请禁用TLS压缩。实践中，大多数浏览器会禁用TLS压缩，但即便如此你也应该在服务器的配置中明确禁用它，以保护用户的利益。 虽然不能使用TLS压缩，但应该使用服务器的Gzip设置压缩所有文本资源，同时对图像、视频、音频等媒体采用最合适的压缩格式。 无线网络性能移动网络的优化建议消除周期性及无效的数据传输对推送而言，原生应用可以访问平台专有的推送服务，因此应该尽可能使用。对 Web 应用来说，可以使用SSE（Server Sent Events，服务器发送事件）和WebSocket以降低延迟时间和协议消耗，尽可能不使用轮询和更耗资源的XHR技术。 消除不必要的长连接TCP或UDP连接的连接状态及生命期与设备的无线状态是相互独立的。换句话说，即便与运营商网络仍维持着（两端间）连接不中断，无线模块也可以处于低耗电状态。外部网络的分组到来时，运营商无线网络会通知设备，使其无线模块切换到连接状态，从而恢复数据传输。 明白了吗，应用不必让无线模块“活动”也可以保持连接不被断开。但不必要的长连接也有可能极大地消耗电量，而且由于人们对移动网络无线通信的误解，这种情况经常发生。 预测网络延迟上限在移动网络中，一个HTTP请求很可能会导致一连串长达几百甚至上几千ms的网络延迟。这一方面是因为有往返延迟，另一方面也不能忘记DNS、TCP、TLS及控制面的延迟（图8-2）。 HTTPHTTP 简史HTTP 1.0：迅速发展及参考性RFC今天，几乎所有Web服务器都支持，而且以后还会继续支持HTTP 1.0。除此之外，剩下的你都知道了。但HTTP 1.0对每个请求都打开一个新TCP连接严重影响性能。 HTTP 1.1：互联网标准HTTP 1.1 标准厘清了之前版本中很多有歧义的地方，而且还加入了很多重要的性能优化：持久连接、分块编码传输、字节范围请求、增强的缓存机制、传输编码及请求管道。 HTTP 1.1 改变了HTTP协议的语义，默认使用持久连接。换句话说，除非明确告知（通过Connection: close 首部），否则服务器默认会保持连接打开。 不过，这个功能也反向移植到了HTTP 1.0，可以通过Connection: Keep-Alive 首部来启用。实际上，如果你使用的是HTTP 1.1，从技术上说不需要Connection: Keep-Alive首部，但很多客户端还是选择加上它。 此外，HTTP 1.1 协议添加了内容、编码、字符集，甚至语言的协商机制，还添加了传输编码、缓存指令、客户端cookie 等十几个可以每次请求都协商的字段。 HTTP 2.0：改进传输性能 HTTP（Hypertext Transfer Protocol）是一个应用层协议，可用于分布协作式的超媒体系统。它是一个通用、无状态的协议。除了超文本，通过扩展它的请求方式、错误编码及首部，还可以将它用于很多其他领域，比如域名服务器和分布式对象管理系统。HTTP的一个功能就是允许数据的类型变化和协商，从而允许系统独立于被传输的数据构建。——RFC 2616：HTTP/1.1（1999 年 6 月） 当前，出现了一种保持HTTP语义，但脱离HTTP/1.x消息分帧及语法的协议用法。这种用法被证明有碍于性能，并且是在鼓励滥用底层传输协议。本工作组将制定一个新规范，从有序、半双工流的角度重新表达当前HTTP的语义。与HTTP/1.x一样，主要将使用TCP作为传输层，不过也应该支持其他传输协议。——HTTP 2.0 纲领 （2012 年 1 月） HTTP 2.0 的主要目标是改进传输性能，实现低延迟和高吞吐量。主版本号的增加听起来像是要做大的改进，从性能角度说的确如此。但从另一方面看，HTTP的高层协议语义并不会因为这次版本升级而受影响。所有 HTTP 首部、值，以及它们的使用场景都不会变。 Web性能要点剖析现代Web应用速度、性能与用户期望时间和用户感觉 时间 感觉 0 ~100 ms 很快 100~300 ms 有一点点慢 300~1000 ms 机器在工作呢 &gt; 1000 ms 先干点别的吧 &gt; 10000 ms 不能用了 这个表格解释了Web性能社区总结的经验法则：必须250 ms内渲染页面，或者至少提供视觉反馈，才能保证用户不走开！ HTTP 1.xHTTP 1.0的优化策略非常简单，就一句话：升级到HTTP 1.1。完了！ 改进 HTTP 的性能是 HTTP 1.1 工作组的一个重要目标，后来这个版本也引入了大量增强性能的重要特性，其中一些大家比较熟知的有： 持久化连接以支持连接重用； 分块传输编码以支持流式响应； 请求管道以支持并行请求处理； 字节服务以支持基于范围的资源请求； 改进的更好的缓存机制。 HTTP管道HTTP 1.x 只能严格串行地返回响应。特别是，HTTP 1.x 不允许一个连接上的多个响应数据交错到达（多路复用），因而一个响应必须完全返回后，下一个响应才会开始传输。为说明这一点，我们可以看看服务器并行处理请求的情况（图 11-4）。 图 11-4 演示了如下几个方面： HTML 和 CSS 请求同时到达，但先处理的是 HTML 请求； 服务器并行处理两个请求，其中处理 HTML 用时 40 ms，处理 CSS 用时 20 ms； CSS 请求先处理完成，但被缓冲起来以等候发送 HTML 响应； 发送完 HTML 响应后，再发送服务器缓冲中的 CSS 响应。 HTTP 管道会导致 HTTP 服务器、代理和客户端出现很多微妙的，不见文档记载的问题： 一个慢响应就会阻塞所有后续请求； 并行处理请求时，服务器必须缓冲管道中的响应，从而占用服务器资源，如果有个响应非常大，则很容易形成服务器的受攻击面； 响应失败可能终止 TCP 连接，从页强迫客户端重新发送对所有后续资源的请求，导致重复处理； 由于可能存在中间代理，因此检测管道兼容性，确保可靠性很重要； 如果中间代理不支持管道，那它可能会中断连接，也可能会把所有请求串联起来。 今天，一些支持管道的浏览器，通常都将其作为一个高级配置选项，但大多数浏览器都会禁用它。换句话说，如果浏览器是 Web 应用的主要交付工具，那还是很难指望通过 HTTP 管道来提升性能。 要在你自己的应用中启用管道，要注意如下事项： 确保 HTTP 客户端支持管道； 确保 HTTP 服务器支持管道； 应用必须处理中断的连接并恢复； 应用必须处理中断请求的幂等问题； 应用必须保护自身不受出问题的代理的影响。 实践中部署 HTTP 管道的最佳途径，就是在客户端和服务器间使用安全通道（HTTPS）。这样，就能可靠地避免那些不理解或不支持管道的中间代理的干扰。 使用多个TCP连接由于 HTTP 1.x 不支持多路复用，浏览器可以不假思索地在客户端排队所有 HTTP请求，然后通过一个持久连接，一个接一个地发送这些请求。然而，这种方式在实践中太慢。实际上，浏览器开发商没有别的办法，只能允许我们并行打开多个 TCP会话。多少个？现实中，大多数现代浏览器，包括桌面和移动浏览器，都支持每个主机打开 6 个连接。 消耗客户端和服务器资源限制每个主机最多 6 个连接，可以让浏览器检测出无意（或有意）的 DoS（Denial of Service）攻击。如果没有这个限制，客户端有可能消耗掉服务器的所有资源。讽刺的是，同样的安全检测在某些浏览器上却会招致反向攻击：如果客户端超过了最大连接数，那么所有后来的客户端请求都将被阻塞。大家可以做个试验，在一个主机上同时打开 6 个并行下载，然后再打开第 7 个下载请求，这个请求会挂起，直到前面的请求完成才会执行。 用足客户端连接的限制似乎是一个可以接受的安全问题，但对于需要实时交付数据的应用而言，这样做越来越容易造成部署上的问题。比如 WebSocket、ServerSent Event 和挂起 XHR，这些会话都会占用整整一个 TCP 流，而不管有无数据传输——记住，没有多路复用一说！实际上，如果你不注意，那很可能自己对自己的应用施加 DoS 攻击。 域名分区根据 HTTP Archive 的统计，目前平均每个页面都包含 90 多个独立的资源，如果这些资源都来自同一个主机，那么仍然会导致明显的排队等待。实际上，何必把自己只限制在一个主机上呢？我们不必只通过一个主机（例如 www.example.com）提供所有资源，而是可以手工将所有资源分散到多个子域名：{shard1,shardn}.example.com。由于主机名称不一样了，就可以突破浏览器的连接限制，实现更高的并行能力。域名分区使用得越多，并行能力就越强！ 当然，天下没有免费的午餐，域名分区也不例外：每个新主机名都要求有一次额外的 DNS 查询，每多一个套接字都会多消耗两端的一些资源，而更糟糕的是，站点作者必须手工分离这些资源，并分别把它们托管到多个主机上。 实践中，把多个域名（如 shard1.example.com、shard2.example.com）解析到同一个 IP 地址是很常见的做法。所有分区都通过 CNAME DNS 记录指向同一个服务器，而浏览器连接限制针对的是主机名，不是 IP 地址。另外，每个分区也可以指向一个 CDN 或其他可以访问到的服务器。 DNS 查询和 TCP 慢启动导致的额外消耗对高延迟客户端的影响最大。换句话说，移动（3G、4G）客户端经常是受过度域名分区影响最大的！ Cookie 在很多应用中都是常见的性能瓶颈，很多开发者都会忽略它给每次请求增加的额外负担。 计算图片对内存的需求所有编码的图片经浏览器解析后都会以 RGBA 位图的形式保存于内存当中。每个RGBA 图片的像素需要占用 4 字节：红、绿、蓝通道各占 1 字节，Alpha（透明）通道占 1 字节。这样算下来，一张图片占用的内存量就是图片像素宽度 × 像素高度 ×4 字节。举个例子，800×600 像素的位图会占多大内存呢？800 × 600 × 4 B = 1 920 000 B ≈ 1.83 MB在资源受限的设备，比如手机上，内存占用很快就会成为瓶颈。对于游戏等严重依赖图片的应用来说，这个问题就会更明显。 打包文件到底多大合适呢？可惜的是，没有理想的大小。然而，谷歌 PageSpeed团队的测试表明，30~50 KB（压缩后）是每个 JavaScript 文件大小的合适范围：既大到了能够减少小文件带来的网络延迟，还能确保递增及分层式的执行。具体的结果可能会由于应用类型和脚本数量而有所不同。 嵌入资源嵌入资源是另一种非常流行的优化方法，把资源嵌入文档可以减少请求的次数。比如，JavaScript 和 CSS 代码，通过适当的 script 和 style 块可以直接放在页面中，而图片甚至音频或 PDF 文件，都可以通过数据 URI（data:[mediatype][;base64],data ）的方式嵌入到页面中： 123&lt;img src=&quot;data:image/gif;base64,R0lGODlhAQABAIAAAAAAAAAAACH5BAAAAAAALAAAAAABAAEAAAICTAEAOw==&quot;alt=&quot;1x1 transparent (GIF) pixel&quot; /&gt; 前面的例子是在文档中嵌入了一个 1×1 的透明 GIF 像素。而任何 MIME类型，只要浏览器能理解，都可以通过类似方式嵌入到页面中，包括PDF、音频、视频。不过，有些浏览器会限制数据 URI 的大小，比如 IE8最大只允许 32 KB。 数据 URI 适合特别小的，理想情况下，最好是只用一次的资源。以嵌入方式放到页面中的资源，应该算是页面的一部分，不能被浏览器、CDN 或其他缓存代理作为单独的资源缓存。换句话说，如果在多个页面中都嵌入同样的资源，那么这个资源将会随着每个页面的加载而被加载，从而增大每个页面的总体大小。另外，如果嵌入资源被更新，那么所有以前出现过它的页面都将被宣告无效，而由客户端重新从服务器获取。 最后，虽然 CSS 和 JavaScript 等基于文本的资源很容易直接嵌入页面，也不会带来多余的开销，但非文本性资源则必须通过 base64 编码，而这会导致开销明显增大：编码后的资源大小比原大小增大 33% ！ base64 编码使用 64 个 ASCII 符号和空白符将任意字节流编码为 ASCII字符串。编码过程中，base64 会导致被编码的流变成原来的 4/3，即增大33% 的字节开销。 实践中，常见的一个经验规则是只考虑嵌入 1~2 KB 以下的资源，因为小于这个标准的资源经常会导致比它自身更高的 HTTP 开销。然而，如果嵌入的资源频繁变更，又会导致宿主文档的无效缓存率升高。嵌入资源也不是完美的方法。如果你的应用要使用很小的、个别的文件，在考虑是否嵌入时，可以参照如下建议： 如果文件很小，而且只有个别页面使用，可以考虑嵌入； 如果文件很小，但需要在多个页面中重用，应该考虑集中打包； 如果小文件经常需要更新，就不要嵌入了； 通过减少 HTTP cookie 的大小将协议开销最小化。 HTTP 2.0HTTP 2.0 的目的就是通过支持请求与响应的多路复用来减少延迟，通过压缩 HTTP首部字段将协议开销降至最低，同时增加对请求优先级和服务器端推送的支持。 HTTP 2.0 不会改动 HTTP 的语义。HTTP 方法、状态码、URI 及首部字段，等等这些核心概念一如往常。但是，HTTP 2.0 修改了格式化数据（分帧）的方式，以及客户端与服务器间传输这些数据的方式。这两点统帅全局，通过新的组帧机制向我们的应用隐藏了所有复杂性。换句话说，所有原来的应用都可以不必修改而在新协议运行。这当然是好事。 走向HTTP 2.0在此，有必要回顾一下 HTTP 2.0 宣言草稿，因为这份宣言明确了该协议的范围和关键设计要求： HTTP/2.0 应该满足如下条件： 相对于使用 TCP 的 HTTP 1.1，用户在大多数情况下的感知延迟要有实质上、可度量的改进； 解决 HTTP 中的“队首阻塞”问题； 并行操作无需与服务器建立多个连接，从而改进 TCP 的利用率，特别是拥塞控制方面； 保持 HTTP 1.1 的语义，利用现有文档，包括（但不限于）HTTP 方法、状态码、URI，以及首部字段； 明确规定 HTTP 2.0 如何与 HTTP 1.x 互操作，特别是在中间介质上； 明确指出所有新的可扩展机制以及适当的扩展策略。 之所以要递增一个大版本到 2.0，主要是因为它改变了客户端与服务器之间交换数据的方式。为实现宏伟的性能改进目标，HTTP 2.0增加了新的二进制分帧数据层，而这一层并不兼容之前的 HTTP 1.x 服务器及客户端——是谓 2.0。 除非你在实现 Web 服务器或者定制客户端，需要使用原始的 TCP 套接字，否则你很可能注意不到 HTTP 2.0 技术面的实际变化：所有新的、低级分帧机制都是浏览器和服务器为你处理的。或许唯一的区别就是可选的 API多了一些，比如服务器推送！ 设计和技术目标 HTTP/2.0 通过支持首部字段压缩和在同一连接上发送多个并发消息，让应用更有效地利用网络资源，减少感知的延迟时间。而且，它还支持服务器到客户端的主动推送机制。——HTTP/2.0，Draft 4 二进制分帧层HTTP 2.0 性能增强的核心，全在于新增的二进制分帧层（图 12-1），它定义了如何封装 HTTP 消息并在客户端与服务器之间传输。 这里所谓的“层”，指的是位于套接字接口与应用可见的高层 HTTP API 之间的一个新机制：HTTP 的语义，包括各种动词、方法、首部，都不受影响，不同的是传输期间对它们的编码方式变了。HTTP 1.x 以换行符作为纯文本的分隔符，而 HTTP2.0 将所有传输的信息分割为更小的消息和帧，并对它们采用二进制格式的编码。 流、消息和帧 流已建立的连接上的双向字节流。 消息与逻辑消息对应的完整的一系列数据帧。 帧HTTP 2.0 通信的最小单位，每个帧包含帧首部，至少也会标识出当前帧所属的流。 所有 HTTP 2.0 通信都在一个连接上完成，这个连接可以承载任意数量的双向数据流。相应地，每个数据流以消息的形式发送，而消息由一或多个帧组成，这些帧可以乱序发送，然后再根据每个帧首部的流标识符重新组装。 HTTP 2.0 的所有帧都采用二进制编码，所有首部数据都会被压缩。因此，图 12-2 只是说明了数据流、消息和帧之间的关系，而非它们实际传输时的编码结果。 要理解 HTTP 2.0，就必须理解流、消息和帧这几个基本概念。 所有通信都在一个 TCP 连接上完成。 流是连接中的一个虚拟信道，可以承载双向的消息；每个流都有一个唯一的整数标识符（1、2…N）。 消息是指逻辑上的 HTTP 消息，比如请求、响应等，由一或多个帧组成。 帧是最小的通信单位，承载着特定类型的数据，如 HTTP 首部、负荷，等等。 简言之，HTTP 2.0 把 HTTP 协议通信的基本单位缩小为一个一个的帧，这些帧对应着逻辑流中的消息。相应地，很多流可以并行地在同一个 TCP 连接上交换消息。 多向请求与响应在 HTTP 1.x 中，如果客户端想发送多个并行的请求以及改进性能，那么必须使用多个 TCP 连接。这是 HTTP 1.x 交付模型的直接结果，该模型会保证每个连接每次只交付一个响应（多个响应必须排队）。更糟糕的是，这种模型也会导致队首阻塞，从而造成底层 TCP 连接的效率低下。 HTTP 2.0 中新的二进制分帧层突破了这些限制，实现了多向请求和响应：客户端和服务器可以把 HTTP 消息分解为互不依赖的帧（图 12-3），然后乱序发送，最后再在另一端把它们重新组合起来。 图 12-3 中包含了同一个连接上多个传输中的数据流：客户端正在向服务器传输一个DATA 帧（stream 5），与此同时，服务器正向客户端乱序发送 stream 1 和 stream 3的一系列帧。此时，一个连接上有 3 个请求/响应并行交换！ 总之，HTTP 2.0 的二进制分帧机制解决了 HTTP 1.x 中存在的队首阻塞问题，也消除了并行处理和发送请求及响应时对多个连接的依赖。 支持多向请求与响应，可以省掉针对 HTTP 1.x 限制所费的那些脑筋和工作，比如拼接文件、图片精灵、域名分区。类似地，通过减少 TCP 连接的数量，HTTP 2.0 也会减少客户端和服务器的 CPU 及内存占用。 请求优先级把 HTTP 消息分解为很多独立的帧之后，就可以通过优化这些帧的交错和传输顺序，进一步提升性能。为了做到这一点，每个流都可以带有一个 31 比特的优先值： 0 表示最高优先级； 2^31 -1 表示最低优先级。 有了这个优先值，客户端和服务器就可以在处理不同的流时采取不同的策略，以最优的方式发送流、消息和帧。具体来讲，服务器可以根据流的优先级，控制资源分配（CPU、内存、带宽），而在响应数据准备好之后，优先将最高优先级的帧发送给客户端。 浏览器请求优先级与 HTTP 2.0浏览器在渲染页面时，并非所有资源都具有相同的优先级：HTML 文档本身对构建 DOM 不可或缺，CSS 对构建 CSSOM 不可或缺，而 DOM 和 CSSOM 的构建都可能受到 JavaScript 资源的阻塞，其他资源（如图片）的优先级都可以降低。为加快页面加载速度，所有现代浏览器都会基于资源的类型以及它在页面中的位置排定请求的优先次序，甚至通过之前的访问来学习优先级模式——比如，之前的渲染如果被某些资源阻塞了，那么同样的资源在下一次访问时可能就会被赋予更高的优先级。在 HTTP 1.x 中，浏览器极少能利用上述优先级信息，因为协议本身并不支持多路复用，也没有办法向服务器通告请求的优先级。此时，浏览器只能依赖并行连接，且最多只能同时向一个域名发送 6 个请求。于是，在等连接可用期间，请求只能在客户端排队，从而增加了不必要的网络延迟。理论上，HTTP 管道可以解决这个问题，只是由于缺乏支持而无法付诸实践。HTTP 2.0 一举解决了所有这些低效的问题：浏览器可以在发现资源时立即分派请求，指定每个流的优先级，让服务器决定最优的响应次序。这样请求就不必排队了，既节省了时间，也最大限度地利用了每个连接。 HTTP 2.0 没有规定处理优先级的具体算法，只是提供了一种赋予数据优先级的机制，而且要求客户端与服务器必须能够交换这些数据。这样一来，优先值作为提示信息，对应的次序排定策略可能因客户端或服务器的实现而不同：客户端应该明确指定优先值，服务器应该根据该值处理和交付数据。 在这个规定之下，尽管你可能无法控制客户端发送的优先值，但或许你可以控制服务器。因此，在选择 HTTP 2.0 服务器时，可以多留点心！为说明这一点，考虑下面几个问题。 如果服务器对所有优先值视而不见怎么办？ 高优先值的流一定优先处理吗？ 是否存在不同优先级的流应该交错的情况？如果服务器不理睬所有优先值，那么可能会导致应用响应变慢：浏览器明明在等关键的 CSS 和 JavaScript，服务器却在发送图片，从而造成渲染阻塞。不过，规定严格的优先级次序也可能带来次优的结果，因为这可能又会引入队首阻塞问题，即某个高优先级的慢请求会不必要地阻塞其他资源的交付。 服务器可以而且应该交错发送不同优先级别的帧。只要可能，高优先级流都应该优先，包括分配处理资源和客户端与服务器间的带宽。不过，为了最高效地利用底层连接，不同优先级的混合也是必需的。 每个来源一个连接有了新的分帧机制后，HTTP 2.0 不再依赖多个 TCP 连接去实现多流并行了。现在，每个数据流都拆分成很多帧，而这些帧可以交错，还可以分别优先级。于是，所有HTTP 2.0 连接都是持久化的，而且客户端与服务器之间也只需要一个连接即可。 实验表明，客户端使用更少的连接肯定可以降低延迟时间。HTTP 2.0 发送的总分组数量比 HTTP 差不多要少 40%。而服务器处理大量并发连接的情况也变成了可伸缩性问题，因为 HTTP 2.0 减轻了这个负担。——HTTP/2.0 Draft 2 每个来源一个连接显著减少了相关的资源占用：连接路径上的套接字管理工作量少了，内存占用少了，连接吞吐量大了。此外，从上到下所有层面上也都获得了相应的好处： 所有数据流的优先次序始终如一； 压缩上下文单一使得压缩效果更好； 由于 TCP 连接减少而使网络拥塞状况得以改观； 慢启动时间减少，拥塞和丢包恢复速度更快。 大多数 HTTP 连接的时间都很短，而且是突发性的，但 TCP 只在长时间连接传输大块数据时效率才最高。HTTP 2.0 通过让所有数据流共用同一个连接，可以更有效地使用 TCP 连接。 丢包、高 RTT 连接和 HTTP 2.0 性能等一等，我听你说了一大堆每个来源一个 TCP 连接的好处，难道它就一点坏处都没有吗？有，当然有。； 虽然消除了 HTTP 队首阻塞现象，但 TCP 层次上仍然存在队首阻塞; 如果 TCP 窗口缩放被禁用，那带宽延迟积效应可能会限制连接的吞吐量； 丢包时，TCP 拥塞窗口会缩小。 上述每一点都可能对 HTTP 2.0 连接的吞吐量和延迟性能造成不利影响。然而，除了这些局限性之外，实验表明一个 TCP 连接仍然是 HTTP 2.0 基础上的最佳部署策略。 总之，一定要知道 HTTP 2.0 与之前的版本一样，并不强制使用 TCP。UDP 等其他传输协议也并非不可以。 流量控制在同一个 TCP 连接上传输多个数据流，就意味着要共享带宽。标定数据流的优先级有助于按序交付，但只有优先级还不足以确定多个数据流或多个连接间的资源分配。为解决这个问题，HTTP 2.0 为数据流和连接的流量控制提供了一个简单的机制： 流量控制基于每一跳进行，而非端到端的控制； 流量控制基于窗口更新帧进行，即接收方广播自己准备接收某个数据流的多少字节，以及对整个连接要接收多少字节； 流量控制窗口大小通过WINDOW_UPDATE 帧更新，这个字段指定了流 ID 和窗口大小递增值； 流量控制有方向性，即接收方可能根据自己的情况为每个流乃至整个连接设置任意窗口大小； 流量控制可以由接收方禁用，包括针对个别的流和针对整个连接。 HTTP 2.0 连接建立之后，客户端与服务器交换 SETTINGS 帧，目的是设置双向的流量控制窗口大小。除此之外，任何一端都可以选择禁用个别流或整个连接的流量控制。 优先级可以决定交付次序，而流量控制则可以控制 HTTP 2.0 连接中每个流占用的资源：接收方可以针对特定的流广播较低的窗口大小，以限制它的传输速度。 服务器推送HTTP 2.0 新增的一个强大的新功能，就是服务器可以对一个客户端请求发送多个响应。换句话说，除了对最初请求的响应外，服务器还可以额外向客户端推送资源，而无需客户端明确地请求。 建立 HTTP 2.0 连接后，客户端与服务器交换 SETTINGS 帧，借此可以限定双向并发的流的最大数量。因此，客户端可以限定推送流的数量，或者通过把这个值设置为 0 而完全禁用服务器推送。 为什么需要这样一个机制呢？通常的 Web 应用都由几十个资源组成，客户端需要分析服务器提供的文档才能逐个找到它们。那为什么不让服务器提前就把这些资源推送给客户端，从而减少额外的时间延迟呢？服务器已经知道客户端下一步要请求什么资源了，这时候服务器推送即可派上用场。事实上，如果你在网页里嵌入过 CSS、JavaScript，或者通过数据 URI 嵌入过其他资源，那你就已经亲身体验过服务器推送了。 所有推送的资源都遵守同源策略。换句话说，服务器不能随便将第三方资源推送给客户端，而必须是经过双方确认才行。 首部压缩HTTP 的每一次通信都会携带一组首部，用于描述传输的资源及其属性。在 HTTP 1.x 中，这些元数据都是以纯文本形式发送的，通常会给每个请求增加 500~800 字节的负荷。如果算上 HTTP cookie，增加的负荷通常会达到上千字节。为减少这些开销并提升性能，HTTP 2.0 会压缩首部元数据： HTTP 2.0 在客户端和服务器端使用“首部表”来跟踪和存储之前发送的键－值对，对于相同的数据，不再通过每次请求和响应发送； 首部表在HTTP 2.0的连接存续期内始终存在，由客户端和服务器共同渐进地更新; 每个新的首部键－值对要么被追加到当前表的末尾，要么替换表中之前的值。 于是，HTTP 2.0 连接的两端都知道已经发送了哪些首部，这些首部的值是什么，从而可以针对之前的数据只编码发送差异数据（图 12-5）。 请求与响应首部的定义在 HTTP 2.0 中基本没有改变，只是所有首部键必须全部小写，而且请求行要独立为 :method 、 :scheme 、 :host 和 :path 这些键－值对。 在前面的例子中，第二个请求只需要发送变化了的路径首部（:path），其他首部没有变化，不用再发送了。这样就可以避免传输冗余的首部，从而显著减少每个请求的开销。通信期间几乎不会改变的通用键－值对（用户代理、可接受的媒体类型，等等）只需发送一次。事实上，如果请求中不包含首部（例如对同一资源的轮询请求），那么首部开销就是零字节。此时所有首部都自动使用之前请求发送的首部！ 有效的HTTP 2.0升级与发现通过常规非加密信道建立 HTTP 2.0 连接需要多做一点工作。因为 HTTP 1.0 和HTTP 2.0 都使用同一个端口（80），又没有服务器是否支持 HTTP 2.0 的其他任何信息，此时客户端只能使用 HTTP Upgrade 机制通过协调确定适当的协议： 1234567891011121314GET /page HTTP/1.1Host: server.example.comConnection: Upgrade, HTTP2-SettingsUpgrade: HTTP/2.0 ➊HTTP2-Settings: (SETTINGS payload) ➋HTTP/1.1 200 OK ➌Content-length: 243Content-type: text/html(... HTTP 1.1 response ...)(or)HTTP/1.1 101 Switching Protocols ➍Connection: UpgradeUpgrade: HTTP/2.0(... HTTP 2.0 response ...) ➊ 发起带有 HTTP 2.0 Upgrade 首部的 HTTP 1.1 请求➋ HTTP/2.0 SETTINGS 净荷的 Base64 URL 编码➌ 服务器拒绝升级，通过 HTTP 1.1 返回响应➍ 服务器接受 HTTP 2.0 升级，切换到新分帧 使用这种 Upgrade 流，如果服务器不支持 HTTP 2.0，就立即返回 HTTP 1.1 响应。否则，服务器就会以 HTTP 1.1 格式返回 101 Switching Protocols 响应，然后立即切换到 HTTP 2.0 并使用新的二进制分帧协议返回响应。无论哪种情况，都不需要额外往返。 为确定服务器和客户端都有意使用 HTTP 2.0 对话，双方还必须发送“连接首部”，也就是一串标准的字节。这种信息交换本质上是一种“尽早失败”（fail-fast）的机制，可以避免客户端、服务器，以及中间设备偶尔接受请求的升级却不理解新协议。而且，这种信息交换也不会带来额外的往返，只是在连接开始时要多传一些字节。 最后，如果客户端因为自己保存有或通过其他手段（如 DNS 记录、手工配置等）获得了关于 HTTP 2.0 的支持信息，它也可以直接发送 HTTP 2.0 分帧，而不必依赖Upgrade 机制。有了这些信息，客户端可以一上来就通过非加密信道发送 HTTP 2.0 分帧，其他就不管了。最坏的情况，就是无法建立连接，客户端再回退一步，重新使用 Upgrade 首部，或者切换到带 ALPN 协商的 TLS 信道。 服务器之前的 HTTP 2.0 支持信息并不能保证下一次就能可靠地建立连接。以这种方式通信的前提，就是各端都必须支持 HTTP 2.0。如果任何中间设备不支持，连接都不会成功。 二进制分帧简介HTTP 2.0 的根本改进还是新增的长度前置的二进制分帧层。 建立了 HTTP 2.0 连接后，客户端与服务器会通过交换帧来通信，帧是基于这个新协议通信的最小单位。所有帧都共享一个 8 字节的首部（图 12-6），其中包含帧的长度、类型、标志，还有一个保留位和一个 31 位的流标识符。 16 位的长度前缀意味着一帧大约可以携带 64 KB 数据，不包括 8 字节首部。 8 位的类型字段决定如何解释帧其余部分的内容。 8 位的标志字段允许不同的帧类型定义特定于帧的消息标志。 1 位的保留字段始终置为 0。 31 位的流标识符唯一标识 HTTP 2.0 的流。 在调试 HTTP 2.0 通信时，有人会使用自己喜欢的十六进制查看器。其实，Wireshark 及其他类似的工具也有相应的插件，使用很简单，也很人性化。比如，谷歌 Chrome 就支持 chrome://internals#spdy ，通过它可以查看通信细节。 知道了 HTTP 2.0 规定的这个共享的帧首部，就可以自己编写一个简单的解析器，通过分析 HTTP 2.0 字节流，根据每个帧的前 8 字节找到帧的类型、标志和长度。而且，由于每个帧的长度都是预先定义好的，解析器可以迅速而准确地跳到下一帧的开始，这也是相对于 HTTP 1.x 的一个很大的性能提升。 知道了帧类型，解析器就知道该如何解释帧的其余内容了。HTTP 2.0 规定了如下帧类型。 DATA：用于传输 HTTP 消息体。 HEADERS：用于传输关于流的额外的首部字段。 PRIORITY：用于指定或重新指定引用资源的优先级。 RST_STREAM：用于通知流的非正常终止。 SETTINGS：用于通知两端通信方式的配置数据。 PUSH_PROMISE：用于发出创建流和服务器引用资源的要约。 PING：用于计算往返时间，执行“活性”检查。 GOAWAY：用于通知对端停止在当前连接中创建流。 WINDOW_UPDATE：用于针对个别流或个别连接实现流量控制。 CONTINUATION：用于继续一系列首部块片段。 服务器可以利用 GOAWAY 类型的帧告诉客户端要处理的最后一个流的 ID，从而消除一些请求竞争，而且浏览器也可以据此智能地重试或取消“悬着的”请求。这也是保证复用连接安全的一个重要和必要的功能！ 既然有了这个分帧层，即使它对我们的应用不可见，我们也应该更进一步，分析一下两种最常见的工作流：发起新流和交换应用数据。只有明白了一个请求或响应如何转换成一个一个的帧，才能理解 HTTP 2.0 对性能的提升来自哪里。 固定长度与可变长度字段HTTP 2.0 只使用固定长度字段，HTTP 2.0 帧占用带宽很少（帧首部是 8 字节）。采用可变长度编码的确可以节省一点带宽和时延，但却无法抵偿由此带来的分析复杂性。即使可变长度编码能减少 50% 的带宽占用，那么在 1 Mbit/s 的连接上传输 1400 字节的分组，也只能节省 4 字节（0.3%）和每帧不到 100 纳秒的延迟时间。 发起新流在发送应用数据之前，必须创建一个新流并随之发送相应的元数据，比如流优先级、HTTP 首部等。HTTP 2.0 协议规定客户端和服务器都可以发起新流，因此有两种可能： 客户端通过发送HEADERS 帧来发起新流（图 12-7），这个帧里包含带有新流 ID 的公用首部、可选的 31 位优先值，以及一组 HTTP 键－值对首部； 服务器通过发送PUSH_PROMISE 帧来发起推送流，这个帧与 HEADERS 帧等效，但它包含“要约流 ID”，没有优先值。 这两种帧的类型字段都只用于沟通新流的元数据，净荷会在 DATA 帧中单独发送。同样，由于两端都可以发起新流，流计数器偏置：客户端发起的流具有偶数 ID，服务器发起的流具有奇数 ID。这样，两端的流 ID 不会冲突，而且各自持有一个简单的计数器，每次发起新流时递增 ID 即可。 由于流的元数据与应用数据是单独发送的，因此客户端和服务器可以分别给它们设定不同的优先级。比如，“控制流量”的流优先级可以高一些，但只将其应用给 DATA 帧。 发送应用数据创建新流并发送 HTTP 首部之后，接下来就是利用 DATA 帧（图 12-8）发送应用数据。应用数据可以分为多个 DATA 帧，最后一帧要翻转帧首部的 END_STREAM 字段。 数据净荷不会被另行编码或压缩。编码方式取决于应用或服务器，纯文本、gzip 压缩、图片或视频压缩格式都可以。既然如此，关于 DATA 帧再也没有什么新东西好说了！整个帧由公用的 8 字节首部，后跟 HTTP 净荷组成。 从技术上说， DATA 帧的长度字段决定了每帧的数据净荷最多可达 2^16-1（65 535）字节。可是，为减少队首阻塞，HTTP 2.0 标准要求 DATA 帧不能超过 2^14 -1（16383）字节。长度超过这个阀值的数据，就得分帧发送。 HTTP 2.0帧数据流分析 有 3 个活动的流：stream 1、stream 3 和 stream 5。 3 个流的 ID 都是奇数，说明都是客户端发起的。 这里没有服务器发起的流。 服务器发送的 stream 1 包含多个DATA 帧，这是对客户端之前请求的响应数据。 这也说明在此之前已经发送过 HEADERS 帧了。 服务器在交错发送 stream 1 的DATA 帧和 stream 3 的 HEADERS 帧，这就是响应的多路复用！ 客户端正在发送 stream 5 的DATA 帧，表明 HEADERS 帧之前已经发送过了。 简言之，图 12-3 中连接正在并行传送 3 个数据流，每个流都处于各自处理周期的不同阶段。服务器决定帧的顺序，而我们不用关心每个流的类型或内容。stream 1 携带的数据量可能比较大，也许是视频，但它不会阻塞共享连接中的其他流！ 优化应用的交付事实上，影响绝大多数 Web 应用性能的并非带宽，而是延迟。网速虽然越来越快，但不幸的是，延迟似乎并没有缩短。 说到底，成功的、可持续的 Web 性能优化策略其实很简单：先度量，然后拿业务目标与性能指标进行比较，采取优化措施，紧了松点，松了紧点，如此反复。开发和购买合用的度量工具及选择恰当的度量手段具有最高优先级； 经典的性能优化最佳实践无论什么网络，也不管所用网络协议是什么版本，所有应用都应该致力于消除或减少不必要的网络延迟，将需要传输的数据压缩至最少。这两条标准是经典的性能优化最佳实践，是其他数十条性能准则的出发点。 减少DNS查找每一次主机名解析都需要一次网络往返，从而增加请求的延迟时间，同时还会阻塞后续请求。 重用TCP连接尽可能使用持久连接，以消除 TCP 握手和慢启动延迟。 减少HTTP重定向HTTP 重定向极费时间，特别是不同域名之间的重定向，更加费时；这里面既有额外的 DNS 查询、TCP 握手，还有其他延迟。最佳的重定向次数为零。 使用CDN（内容分发网络）把数据放到离用户地理位置更近的地方，可以显著减少每次 TCP 连接的网络延迟，增大吞吐量。这一条既适用于静态内容，也适用于动态内容(比如：不缓存的原始获取)。 去掉不必要的资源任何请求都不如没有请求快。 不缓存的原始获取使用 CDN 或代理服务器取得资源的技术，如果要根据用户定制或者涉及隐私数据，则不能做到全球缓存，这种情况被称为“不缓存的原始获取”（uncached origin fetch）。虽然只有把数据缓存到全球各地的服务器上 CDN 才能发挥最大的效用，但“不缓存的原始获取”仍然具有性能优势：客户端连接终止于附近的服务器，从而显著减少握手延迟。相应地，CDN 或你的代理服务器可以维护一个“热连接池”（warm connection pool），通过它将数据转发给原始服务器，同时做到对客户端快速响应。事实上，作为附加的一个优化层，CDN 提供商在连接两端都会使用邻近服务器！客户端连接终止于邻近 CDN 节点，该节点将请求转发到与对端服务器邻近的 CDN节点，之后请求才会被路由到原始服务器。CDN 网络中多出来这一跳，可以让数据在优化的 CDN 骨干网中寻路，从而进一步减少客户端与服务器之间的延迟。 在客户端缓存资源应该缓存应用资源，从而避免每次请求都发送相同的内容。 传输压缩过的内容传输前应该压缩应用资源，把要传输的字节减至最少：确保对每种要传输的资源采用最好的压缩手段。 消除不必要的请求开销减少请求的 HTTP 首部数据（比如 HTTP cookie），节省的时间相当于几次往返的延迟时间。 并行处理请求和响应请求和响应的排队都会导致延迟，无论是客户端还是服务器端。这一点经常被忽视，但却会无谓地导致很长延迟。 针对协议版本采取优化措施HTTP 1.x 支持有限的并行机制，要求打包资源、跨域分散资源，等等。相对而言，HTTP 2.0 只要建立一个连接就能实现最优性能，同时无需针对 HTTP 1.x 的那些优化方法。 在客户端缓存资源要说最快的网络请求，那就是不用发送请求就能获取资源。将之前下载过的数据缓存并维护好，就可以做到这一点。对于通过 HTTP 传输的资源，要保证首部包含适当的缓存字段： Cache-Control 首部用于指定缓存时间； Last-Modified 和 ETag 首部提供验证机制。 只要可能，就给每种资源都指定一个明确的缓存时间。这样客户端就可以直接使用本地副本，而不必每次都请求相同的内容。类似地，指定验证机制可以让客户端检查过期的资源是否有更新。没有更新，就没必要重新发送。 最后，还要注意应同时指定缓存时间和验证方法！只指定其中之一是最常见的错误，于是要么导致每次都在没有更新的情况下重发相同内容（这是没有指定验证），要么导致每次使用资源时都多余地执行验证检查（这是没有指定缓存时间）。 压缩传输的数据利用本地缓存可以让客户端避免每次请求都重复取得数据。不过，还是有一些资源是必须取得的，比如原来的资源过期了，或者有新资源，再或者资源不能缓存。对于这些资源，应该保证传输的字节数最少。因此要保证对它们进行最有效的压缩。 HTML、CSS 和 JavaScript 等文本资源的大小经过 gzip 压缩平均可以减少 60%~80%。而图片则需要仔细考量： 图片一般会占到一个网页需要传输的总字节数的一半； 通过去掉不必要的元数据可以把图片文件变小； 要调整大小就在服务器上调整，避免传输不必要的字节； 应该根据图像选择最优的图片格式； 尽可能使用有损压缩。 不同图片格式的压缩率迥然不同，因为不同的格式是分别为不同使用场景设计的。事实上，如果选错了图片格式（比如，使用了 PNG 而非 JPG 或 WebP），多产生几百甚至上千 KB 数据是轻而易举的事。建议大家多找一些工具和自动化手段，以确定最佳图片格式。 选定图片格式后，其次就是不要让图片超过它需要的大小。如果在客户端对超出需要大小的图片做调整，那么除了额外传输不必要的字节之外，还会浪费 CPU、GPU和内存资源。 最后，选择了正确的格式，确定了必需的大小，接下来就要研究使用哪一种有损图片格式，比如 JPEG 还是 WebP，以及压缩到哪个级别：较高压缩率可以明显减少字节数，同时图片品质不会有太大或太明显的损失，尤其是在较小（手机）的屏幕上看，不容易发现。 WebP：Web 上的新图片格式WebP 是谷歌开发的一种新图片格式，得到了 Chrome 和 Opera 浏览器支持。这种格式的无损压缩和有损压缩效能都有所提升： WebP 的无损压缩图片比 PNG 的小 26%； WebP 的有损压缩图片比 JPG 的小 25%~34%； WebP 支持无损透明压缩，但因此仅增加 22% 的字节。 在现有网页平均 1 MB 大小，其中图片占一半的情况下，WebP 节省的 20%~30%，对每个页面而言就是几百 KB。这种格式需要客户端 CPU 多花点时间解码（大约相当于处理 JPG 的 1.4 倍），但字节的节省完全可以补偿处理时间的增长。此外，由于数据流量的限制和高速网络的存在，对很多用户而言，节省字节才是当务之急。事实上，Chrome Data Compression Proxy 和 Opera Turbo 等工具为用户降低带宽占用的主要手段，就是重新把每张图片编码为 WebP 格式。正常情况下，Chrome Data Compression Proxy 的数据压缩率可以达到 50%，这说明我们自己的应用也有很多可以通过压缩提升性能的空间。 消除不必要的请求字节HTTP 是一种无状态协议，也就是说服务器不必保存每次请求的客户端的信息。然而，很多应用又依赖于状态信息以实现会话管理、个性化、分析等功能。为了实现这些功能，HTTP State Management Mechanism（RFC 2965）作为扩展，允许任何网站针对自身来源关联和更新 cookie 元数据：浏览器保存数据，而在随后发送给来源的每一个请求的 Cookie 首部中自动附加这些信息。 上述标准并未规定 cookie 最大不能超过多大，但实践中大多数浏览器都将其限制为4 KB。与此同时，该标准还规定每个站点针对其来源可以有多个关联的 cookie。于是，一个来源的 cookie 就有可能多达几十 KB ！不用说，这么多元数据随请求传递，必然会给应用带来明显的性能损失： 浏览器会在每个请求中自动附加关联的 cookie 数据； 在 HTTP 1.x 中，包括 cookie 在内的所有 HTTP 首部都会在不压缩的状态下传输； 在 HTTP 2.0 中，这些元数据经过压缩了，但开销依然不小； 最坏的情况下，过大的 HTTP cookie 会超过初始的 TCP 拥塞窗口，从而导致多余的网络往返。 应该认真对待和监控 cookie 的大小，确保只传输最低数量的元数据，比如安全会话令牌。同时，还应该利用服务器上共享的会话缓存，从中查询缓存的元数据。更好的结果，则是完全不用cookie。比如，在请求图片、脚本和样式表等静态资源时，浏览器绝大多数情况下不必传输特定于客户端的元数据。 在使用 HTTP 1.x 的情况下，可以指定一个专门的“无需 cookie”的来源服务器。这个服务器可以用于交付那些不区分客户端的共用资源。 针对HTTP 1.x的优化建议针对 HTTP 1.x 的优化次序很重要：首先要配置服务器以最大限度地保证 TCP 和TLS 的性能最优，然后再谨慎地选择和采用移动及经典的应用最佳实践，之后再度量，迭代。 采用了经典的应用优化措施和适当的性能度量手段，还要进一步评估是否有必要为应用采取特定于 HTTP 1.x 的优化措施（其实是权宜之计）。 利用HTTP管道如果你的应用可以控制客户端和服务器这两端，那么使用管道可以显著减少网络延迟。 采用域名分区如果你的应用性能受限于默认的每来源 6 个连接，可以考虑将资源分散到多个来源。 打包资源以减少HTTP请求拼接和精灵图等技巧有助于降低协议开销，又能达成类似管道的性能提升。 嵌入小资源考虑直接在父文档中嵌入小资源，从而减少请求数量。 管道缺乏支持，而其他优化手段又各有各的利弊。事实上，这些优化措施如果过于激进或使用不当，反倒会伤害性能。总之，要有务实的态度，通过度量来评估各种措施对性能的影响，在此基础上再迭代改进。天底下就没有包治百病的灵丹妙药。 对了，还有最后一招儿 —— 升级到 HTTP 2.0。仅此一招儿抵得上前面提到的大多数针对 HTTP 1.x 的优化手段！ HTTP 2.0 不光能让应用加载更快，还能让开发更简单。 针对HTTP 2.0的优化建议HTTP 2.0 的主要目标就是提升传输性能，实现客户端与服务器间较低的延迟和较高的吞吐量。显然，在 TCP 和 TLS 之上实现最佳性能，同时消除不必要的网络延迟，从来没有如此重要过。最低限度： 服务器的初始cwnd 应该是 10 个分组； 服务器应该通过 ALPN（针对 SPDY 则为 NPN）协商支持 TLS； 服务器应该支持 TLS 恢复以最小化握手延迟。 接下来，或许有点意外，那就是采用移动及其他经典的最佳做法：少发数据、削减请求，根据无线网络情况调整资源供给。不管使用什么版本的协议，减少传输的数据量和消除不必要的网络延迟，对任何应用都是最有效的优化手段。 最后，杜绝和忘记域名分区、文件拼接、图片精灵等不良的习惯，这些做法在HTTP 2.0 之上完全没有必要。事实上，继续使用这些手段反而有害！可以利用HTTP 2.0 内置的多路分发以及服务器推送等新功能。 去掉对1.x的优化针对 HTTP 2.0 和 HTTP 1.x 的优化策略没有什么重叠。因此，不仅不必担心 HTTP 1.x 协议的种种限制，而且要撤销原先那些必要的做法。 每个来源使用一个连接HTTP 2.0 通过将一个 TCP 连接的吞吐量最大化来提升性能。事实上，在 HTTP 2.0 之下再使用多个连接（比如域名分区）反倒成了一种反模式，因为多个连接会抵消新协议中首部压缩和请求优先级的效用。 去掉不必要的文件合并和图片拼接打包资源的缺点很多，比如缓存失效、占用内存、延缓执行，以及增加应用复杂性。有了 HTTP 2.0，很多小资源都可以并行发送，导致打包资源的效率反而更低。 利用服务器推送之前针对 HTTP 1.x 而嵌入的大多数资源，都可以而且应该通过服务器推送来交付。这样一来，客户端就可以分别缓存每个资源，并在页面间实现重用，而不必把它们放到每个页面里了。 要获得最佳性能，应该尽可能把所有资源都集中在一个域名之下。域名分区在 HTTP 2.0 之下属于反模式，对发挥协议的性能有害：分区是开始，之后影响会逐渐扩散。打包资源不会影响 HTTP 2.0 协议本身，但对缓存性能和执行速度有负面影响。 类似地，把嵌入资源改为服务器推送能提升客户端的缓存性能，又不会导致额外网络延迟。事实上，由于 3G 和 4G 网络的往返时间更长，因而服务器推送对移动应用来说效果更明显。 HTTP 2.0 中的打包与协议开销由于 HTTP 1.x 做不到多路复用，而且每次请求的协议开销很高，这才有了连接和拼合等打包技术。在 HTTP 2.0 之下，多路复用已经不成问题，首部压缩也可以降低每次 HTTP 请求要传输的元数据量，打包技术在多数情况下都不再需要了。不过，请求开销只是减少了，并没有等于零。少数情况下，某些资源必须一块使用，而且更新也不频繁，此时使用打包技术仍然可以提升性能。但这些情况很少见，可以算作例外。具体措施可以通过性能度量确定。 双协议应用策略遗憾的是，升级到 HTTP 2.0 不会在一夜之间完成。因此，很多应用都需要认真考虑双协议并存的部署策略，即同一个应用既能通过 HTTP 1.x 交付，也能通过 HTTP2.0 交付，无需任何改动。然而，过于激进的 HTTP 1.x 优化可能伤害 HTTP 2.0 性能，反之亦然。 如果应用可以同时控制服务器和客户端，那倒简单了，因为它可以决定使用什么协议。但大多数应用不能也无法控制客户端，只有采用一种混合或自动策略，以适应两种协议并存的现实。下面我们就分析几种可能的情况。 相同的应用代码，双协议部署相同的应用代码可能通过 HTTP 1.x 也可能通过 HTTP 2.0 交付。可能任何一种协议之下都达不到最佳性能，但可以追求性能足够好。所谓足够好，需要通过针对每一种应用单独度量来保证。这种情况下，第一步可以先撤销域名分区以实现HTTP 2.0 交付。然后，随着更多用户迁移到 HTTP 2.0，可以继续撤销资源打包并尽可能利用服务器推送。 分离应用代码，双协议部署根据协议不同分别交付不同版本的应用。这样会增加运维的复杂性，但实践中对很多应用倒是十分可行。比如，一台负责完成连接的边界服务器可以根据协商后的协议版本，把客户端请求引导至适当的服务器。 动态HTTP 1.x和HTTP 2.0优化某些自动化的 Web 优化框架，以及开源及商业产品，都可以在响应请求时动态重写交付的应用代码（包括连接、拼合、分区，等等）。此时，服务器也可以考虑协商的协议版本，并动态采用适当的优化策略。 HTTP 2.0，单协议部署如果应用可以控制服务器和客户端，那没理由不只使用 HTTP 2.0。事实上，如果真有这种可能，那就应该专一使用 HTTP 2.0。 选择路线时，要看当前的基础设施、应用的复杂程度，以及用户的构成。让人哭笑不得的是，那些在 HTTP 1.x 优化上投资很大的应用，反倒在这种情况下最难办。如果你能控制客户端，有自动的应用优化策略，或者没有使用任何特定于 1.x 的优化，那么就可以专注于 HTTP 2.0，而没有后顾之忧了。 使用 PageSpeed 实现动态优化谷歌的 PageSpeed Optimization Libraries（PSOL）提供了 40 多种“Web 优化过滤器”的开源实现，可以集成到任何服务器运行时，动态应用各种优化策略。 在使用 PSOL 库的情况下， mod_pagespeed （Apache）和 ngx_pagespeed （Nginx）模块都可以基于指定的优化过滤器（如嵌入、压缩、拼接、分片等）实现动态重写，并优化资源交付方式。每次优化都在请求时动态应用（并被缓存），整个优化过程完全自动化了。在动态优化下，服务器还可以根据所用协议，甚至用户代理的类型和版本调整优化策略。比如，可以配置 mod_pagespeed 模块，在客户端使用 HTTP 2.0 时跳过某些优化： 12345678# 对 SPDY/HTTP 2.0 客户端禁用拼接&lt;ModPagespeedIf spdy&gt;ModPagespeedDisableFilters combine_css,combine_javascript&lt;/ModPagespeedIf&gt;# 只对 HTTP 1.x 客户端使用域名分区&lt;ModPagespeedIf !spdy&gt;ModPagespeedShardDomain www.site.com s1.site.com,s2.site.com&lt;/ModPagespeedIf&gt; 使用 PageSpeed 这样的自动 Web 优化库，可以让我们省去不少麻烦，值得考虑。 1.x与2.0的相互转换除了双协议优化策略，很多已部署的应用都需要在自己的应用服务器上采取一种折中方案：两端都是 HTTP 2.0 是追求最佳性能的目标，但（新增）一个转换层（图13-2）也可以让 1.x 服务器利用 HTTP 2.0。 一台居间服务器可以接受 HTTP 2.0 会话，处理之后再向既有基础设施分派 1.x 格式的请求。接到响应后，再将其转换成 HTTP 2.0 的流并返回客户端。通常，这是应用 HTTP 2.0 更新的最简单方式，因为这样可以重用已有的 1.x 基础设施，而且基本不用修改。 大多数支持 HTTP 2.0 的 Web 服务器默认都提供 2.0 到 1.x 的转换机制：2.0 会话终止于服务器（Apache 或 Nginx），如果服务器被配置为反向代理，那么分派给具体应用服务器的就是 1.x 请求。 然而，2.0 到 1.x 的这种简单策略并非长久之计。从很多方面来说，这种工作流实际是一种倒退。真正正确的做法，不是把优化的、可复用的会话转换成一系列 1.x请求，因基础设施而废优化，而是相反：把接收到的 1.x 客户端请求转换成 2.0 流，并把我们的基础设施标准化，使其在任何时候都处理 2.0 会话。 为获得最佳性能，同时实现低延迟和实时的 Web 应用，应该要求我们的内部基础设施达到如下标准： 负载均衡器和代理与应用的连接应该持久化； 请求和响应流及多路复用应该是默认配置； 与应用服务器的通信应该基于消息； 客户端与应用服务器的通信应该是双向的。 端到端的 HTTP 2.0 会话符合上述所有条件，能实现对客户端以及数据中心内部的低延迟交付：无需定制的 RPC 层及相应机制，就能实现内部服务之间的通信，并获得理想的性能。简言之，不要把 2.0 降级到 1.x，这不是长久之计。长久之计是把1.x 升级到 2.0，这样才能求得最佳性能。 评估服务器质量与性能HTTP 2.0 服务器实现的质量对客户端性能影响很大。HTTP 服务器的配置当然是一个重要因素，但服务器实现逻辑的质量同样与优先级、服务器推送、多路复用等性能机制的发挥紧密相关。 HTTP 2.0 服务器必须理解流优先级； HTTP 2.0 服务器必须根据优先级处理响应和交付资源； HTTP 2.0 服务器必须支持服务器推送； HTTP 2.0 服务器应该提供不同推送策略的实现。 HTTP 2.0 服务器的初级实现也能支持某些功能，但不能明确支持请求的优先级和服务器推送，可能导致次优性能。比如，发送大型、静态图片导致带宽饱和，而客户端又因为其他重要资源（如 CSS 或 JavaScript）被阻塞。 为尽可能获得最佳性能，HTTP 2.0 客户端必须是个“乐观主义者”：尽可能早地发送所有请求，然后完全听凭服务器的优化。事实上，HTTP 2.0 客户端对服务器的依赖程度较之以前更甚。 2.0与TLS实践中，由于存在很多不兼容的中间代理，早期的 HTTP 2.0 部署必然依赖加密信道。这样一来，我们就面临两种可能出现 ALPN 协商和 TLS 终止的情况： TLS 连接可能会在 HTTP 2.0 服务器上终止； TLS 连接可能会在上游（如负载均衡器）上终止。 第一种情况要求 HTTP 2.0 服务器能够处理 TLS，除此之外就没有什么了。第二种情况复杂一些：TLS+ALPN 握手可能会在上游代理处终止（图 13-3），然后再从那里建立一条加密信道，或者直接将非加密的 HTTP 2.0 流发送到服务器。 代理和应用服务器之间使用安全信道还是非加密信道，取决于应用：只要能控制中间设备，就可以保证未加密的帧不会被修改或丢弃。那么，虽然大多数 HTTP 2.0 服务器都应该支持 TLS+ALPN 协商，但它们同时也应该在不加密的情况下实现HTTP 2.0 通信。 另外，智能负载均衡器也可以使用 TLS+ALPN 协商机制，根据协商后的协议，选择性地将不同的客户端路由到不同的服务器。 负载均衡器、代理及应用服务器根据现有基础设施以及应用的复杂程度和规模，你的基础设施中可能需要一台或多台负载均衡器（图 13-4）或者 HTTP 2.0 代理。 最简单的情况下，HTTP 2.0 服务器与客户端直接对话，并负责完成 TLS 连接，进行 ALPN 协商，以及处理所有请求。 然而，一台服务器对于大型应用是不够的。大型应用必须要添加一台负载均衡器，以分流大量请求。此时，负载均衡器可以终止 TLS 连接（参见 上节 “2.0 与TLS”），也可以经过配置作为 TCP 代理并直接将加密数据发送给应用服务器。 很多云提供商也会提供负载均衡器服务。然而，这些负载均衡器大多支持TLS 终止，却不支持 ALPN 协商，而这对于通过 TLS 实现 HTTP 2.0 通信是必需的。在这种情况下，应该将负载均衡器配置为 TCP 代理，即通过它们将加密数据发送给应用服务器，让应用服务器完成 TLS+ALPN 协商。 实践中，要回答的最重要的一个问题，就是你的基础设施中的哪个组件负责终止TLS 连接，以及它是否能够执行必要的 ALPN 协商？ 要在 TLS 之上实现 HTTP 2.0 通信，终端服务器必须支持 ALPN； 尽可能在接近用户的地方终止 TLS； 如果无法支持 ALPN，那么选择 TCP 负载均衡模式； 如果无法支持 ALPN 且 TCP 负载均衡也做不到，那么就退而求其次，在非加密 信道上使用 HTTP 的 Upgrade 流 浏览器API与协议浏览器网络概述 连接管理与优化运行在浏览器中的 Web 应用并不负责管理个别网络套接字的生命周期，这是好事。通过把这个任务委托给浏览器，可以自动化很多重要的性能优化任务，包括套接字重用、请求优先级排定、晚绑定、协议协商、施加连接数限制，等等。事实上，浏览器是有意把请求管理生命周期与套接字管理分开的。这一点很微妙，但却至关重要。 套接字是以池的形式进行管理的（图 14-2），即按照来源，每个池都有自己的连接限制和安全约束。挂起的请求是排好队的、有优先次序的，然后再适时把它们绑定到池中个别的套接字上。除非服务器有意关闭连接，否则同一个套接字可以自动用于多个请求！ 来源由应用协议、域名和端口三个要件构成，比如 (http, www.example.com, 80) 与(https, www.example.com, 443) 就是两个不同的来源。 套接字池属于同一个来源的一组套接字。实践中，所有主流浏览器的最大池规模都是 6 个套接字。 自动化的套接字池管理会自动重用 TCP 连接，从而有效保障性能。除此之外，这种架构设计还提供了其他优化的机会： 浏览器可以按照优先次序发送排队的请求； 浏览器可以重用套接字以最小化延迟并提升吞吐量； 浏览器可以预测请求提前打开套接字； 浏览器可以优化何时关闭空闲套接字； 浏览器可以优化分配给所有套接字的带宽。 谷歌 Chrome 的推测性网络优化我们已经知道了，现代浏览器的网络组件并非一个套接字管理器那么简单。但是，即使如此有时候也足以客观地评价现代浏览器中的某些优化技术。比如，你使用谷歌 Chrome 浏览器的次数越多，它的速度就会越快。Chrome 会学习访问过的站点的拓扑，以及常见的浏览模式，然后利用这些信息进行各种“推测性优化”，以预测用户下一步的操作，从而消除不必要的网络延迟：DNS 预解析、TCP 预连接、页面预渲染，等等。像鼠标悬停在链接上这么个简单的动作，就可以触发浏览器向其网络组件的“预测器”发送信号，后者则会依据过往的性能数据选择最佳的优化措施。如果你对Chrome 浏览器的网络优化技术感兴趣，可以看看这篇文章“High Performance Networking in Google Chrome”：http://hpbn.co/chrome-networking。 网络安全与沙箱将个别套接字的管理任务委托给浏览器还有另一个重要的用意：可以让浏览器运用沙箱机制，对不受信任的应用代码采取一致的安全与策略限制。比如，浏览器不允许直接访问原始网络套接字 API，因为这样给恶意应用向任意主机发起任意请求（端口扫描、连接邮件服务器或发送未知消息）提供可乘之机。 连接限制浏览器管理所有打开的套接字池并强制施加连接数限制，保护客户端和服务器的资源不会被耗尽。 请求格式化与响应处理浏览器格式化所有外发请求以保证格式一致和符合协议的语义，从而保护服务器。类似地，响应解码也会自动完成，以保护用户。 TLS 协商浏览器执行 TLS 握手和必要的证书检查。任何证书有问题（比如服务器正在使用自已签发的证书），用户都会收到通知。 同源策略浏览器会限制应用只能向哪个来源发送请求。 以上列出的安全限制机制只是一部分，但已经可以体现“最低特权”（least privilege）原则了。浏览器只向应用代码公开那些必要的 API 和资源：应用提供数据和 URL，浏览器执行请求并负责管理每个连接的整个生命周期。 有必要提一句，并没有单独一条原则叫“同源策略”。实际上，这是一组相关的机制，涉及对 DOM 访问、cookie 和会话状态管理、网络及其他浏览器组件的限制。 资源与客户端状态缓存最好最快的请求是没有请求。在分派请求之前，浏览器会自动检查其资源缓存，执行必要的验证，然后在满足限制条件的情况下返回资源的本地副本。类似地，如果某本地资源不在缓存中，那么浏览器就会发送网络请求，将响应自动填充到缓存中，以备后续访问使用。 浏览器针对每个资源自动执行缓存指令。 浏览器会尽可能恢复失效资源的有效性。 浏览器会自动管理缓存大小及资源回收。 浏览器还有一个经常被人忽视的重要功能，那就是提供会话认证和 cookie 管理。浏览器为每个来源维护着独立的 cookie 容器，为读写新 cookie、会话和认证数据提供必要的应用及服务器 API，还会为我们自动追加和处理 HTTP 首部，让一切都自动化。 举一个简单但直观的例子，它能说明把会话状态管理委托给浏览器的好处：认证的会话可以在多个标签页或浏览器口间共享，反之亦然；如果用户在某个标签页中退出，那么其他所有打开窗口中的会话都将失效。 应用API与协议 我们在这个表中有意忽略了 WebRTC，因为那是一种端到端的交付模型，与 XHR、SSE 和 WebSocket 协议有着根本的不同。 XMLHttpRequestXHR简史尽管名字里有 XML 的 X，XHR 也不是专门针对 XML 开发的。这只是因为 Internet Explorer 5 当初发布它的时候，把它放到 MSXML 库里，这才“继承”了这个 X。 跨源资源共享（CORS）XHR 是一个浏览器层面的 API，向我们隐藏了大量底层处理，包括缓存、重定向、内容协商、认证，等等。这样做有两个目的。第一，XHR 的 API 因此非常简单，开发人员可以专注业务逻辑。其次，浏览器可以采用沙箱机制，对应用代码强制施加一套安全限制。 XHR 接口强制要求每个请求都严格具备 HTTP 语义：应用提供数据和 URL，浏览器格式化请求并管理每个连接的完整生命周期。类似地，虽然 XHR API 允许应用添加自定义的 HTTP 首部（通过 setRequestHeader() 方法），同时也有一些首部是应用代码不能设定的： Accept-Charset、Accept-Encoding、Access-Control-* Host、Upgrade、Connection、Referer、Origin Cookie、Sec-*、Proxy-* 以及很多其他首部 浏览器会拒绝对不安全首部的重写，以此保证应用不能假扮用户代理、用户或请求来源。事实上，保护来源（Origin）首部特别重要，因为这是对所有 XHR 请求应用“同源策略”的关键。 一个“源”由应用协议、域名和端口这三个要件共同定义。比如，(http,example.com, 80) 和 (https, example.com, 443) 就是不同的源。 同源策略的出发点很简单：浏览器存储着用户数据，比如认证令牌、cookie 及其他私有元数据，这些数据不能泄露给其他应用。如果没有同源沙箱，那么 example.com 中的脚本就可以访问并操纵 thirdparty.com 的用户数据！ 为解决这个问题，XHR 的早期版本都限制应用只能执行同源请求，即新请求的来源必须与旧请求的来源一致：来自 example.com 的 XHR 请求，只能从 example.com 请求其他资源。如果后续请求不同源，浏览器就拒绝该 XHR 请求并报错。 可是，在某些必要的情况下，同源策略也会给更好地利用 XHR 带来麻烦：如果服务器想要给另一个网站中的脚本提供资源怎么办？这就是 Cross-Origin Resource Sharing（跨源资源共享，CORS）的来由！ CORS 针对客户端的跨源请求提供了安全的选择同意机制： 123456789// 脚本来源： (http, example.com, 80)var xhr = new XMLHttpRequest();xhr.open(&apos;GET&apos;, &apos;/resource.js&apos;); ➊xhr.onload = function() &#123; ... &#125;;xhr.send();var cors_xhr = new XMLHttpRequest();cors_xhr.open(&apos;GET&apos;, &apos;http://thirdparty.com/resource.js&apos;); ➋cors_xhr.onload = function() &#123; ... &#125;;cors_xhr.send(); ➊ 同源 XHR 请求➋ 跨源 XHR 请求 CORS 请求也使用相同的 XHR API，区别仅在于请求资源用的 URL 与当前脚本并不同源。在前面的例子中，当前执行的脚本来自 (http, example.com, 80)，而第二个XHR 请求访问的 resource.js 则来自 (http, thirdparty.com, 80)。 针对 CORS 请求的选择同意认证机制由底层处理：请求发出后，浏览器自动追加受保护的 Origin HTTP 首部，包含着发出请求的来源。相应地，远程服务器可以检查 Origin 首部，决定是否接受该请求，如果接受就返回 Access-Control-Allow-Origin 响应首部： 123456789=&gt; 请求GET /resource.js HTTP/1.1Host: thirdparty.comOrigin: http://example.com ➊...&lt;= 响应HTTP/1.1 200 OKAccess-Control-Allow-Origin: http://example.com ➋... ➊ Origin 首部由浏览器自动设置➋ 选择同意首部由服务器设置 在前面的例子中，thirdparty.com 决定同意与 example.com 跨源共享资源，因此就在响应中返回了适当的访问控制首部。假如它选择不同意接受这个请求，那么只要不在响应中包含 Access-Control-Allow-Origin 首部即可。这样，客户端的浏览器就会自动将发出的请求作废。 如果第三方服务器不支持 CORS，那么客户端请求同样会作废，因为客户端会验证响应中是否包含选择同意的首部。作为一个特例，CORS 还允许服务器返回一个通配值 ( Access-Control-Allow-Origin: * )，表示它允许来自任何源的请求。不过，在启用这个选项前，请大家务必三思！ 这就是全部了吧？准确地讲，不是。因为 CORS 还会提前采取一系列安全措施，以确保服务器支持 CORS： CORS 请求会省略 cookie 和 HTTP 认证等用户凭据； 客户端被限制只能发送“简单的跨源请求”，包括只能使用特定的方法（GET、POST 和 HEAD），以及只能访问可以通过 XHR 发送并读取的 HTTP 首部。 要启用 cookie 和 HTTP 认证，客户端必须在发送请求时通过 XHR 对象发送额外的属性（ withCredentials ），而服务器也必须以适当的首部（Access-Control-Allow-Credentials）响应，表示它允许应用发送用户的隐私数据。类似地，如果客户端需要写或者读自定义的 HTTP 首部，或者想要使用“不简单的方法”发送请求，那么它必须首先要获得第三方服务器的许可，即向第三方服务器发送一个预备（preflight）请求： 1234567891011121314=&gt; 预备请求OPTIONS /resource.js HTTP/1.1 ➊Host: thirdparty.comOrigin: http://example.comAccess-Control-Request-Method: POSTAccess-Control-Request-Headers: My-Custom-Header...&lt;= 预备响应HTTP/1.1 200 OK ➋Access-Control-Allow-Origin: http://example.comAccess-Control-Allow-Methods: GET, POST, PUTAccess-Control-Allow-Headers: My-Custom-Header...（正式的 HTTP 请求） ➌ ➊ 验证许可的预备 OPTIONS 请求➋ 第三方源的成功预备响应➌ 实际的 CORS 请求 W3C 官方的 CORS 规范规定了何时何地必须使用预备请求：“简单的”请求可以跳过它，但很多条件下这个请求都是必需的，因此也会为验证许可而增加仅有一次往返的网络延迟。好在，只要完成预备请求，客户端就会将结果缓存起来，后续请求就不必重复验证了。 CORS 得到了所有现代浏览器支持，参见：caniuse.com/cors。要全面了解CORS 的各种策略及实现，请参考 W3C 官方标准（http://www.w3.org/TR/cors/）。 通过XHR下载数据浏览器可以自动解码的数据类型如下: ArrayBuffer固定长度的二进制数据缓冲区。 Blob二进制大对象或不可变数据。 Document解析后得到的 HTML 或 XML 文档。 JSON表示简单数据结构的 JavaScript 对象。 Text简单的文本字符串。 浏览器可以依靠 HTTP 的 content-type 首部来推断适当的数据类型（比如把application/json 响应解析为 JSON 对象），应用也可以在发起 XHR 请求时显式重写数据类型。 这里的二进制大对象接口（ Blob ）属于 HTML5 的 File API，就像一个不透明的引用，可以指向任何数据块（二进制或文本）。这个对象本身没有太多功能，只能查询其大小、MIME 类型，或将它切分成更小的块。这个对象存在的真正目的，是作为各种 JavaScript API 之间的一种高效的互操作机制。 要估算传输完成的数据量，服务器必须在其响应中提供内容长度（Content-Length）首部。而对于分块数据，由于响应的总长度未知，因此就无法估计进度了。另外，XHR 请求默认没有超时限制，这意味着一个请求的“进度”可以无限长。作为最佳实践，一定要为应用设置合理的超时时间，并适当处理错误。 服务器发送事件Server-Sent Events（SSE）让服务器可以向客户端流式发送文本消息，比如服务器上生成的实时通知或更新。为达到这个目标，SSE 设计了两个组件：浏览器中的EventSource 和新的“事件流”数据格式。其中， EventSource 可以让客户端以 DOM 事件的形式接收到服务器推送的通知，而新数据格式则用于交付每一次更新。 EventSource API 和定义完善的事件流数据格式，使得 SSE 成为了在浏览器中处理实时数据的高效而不可或缺的工具： 通过一个长连接低延迟交付； 高效的浏览器消息解析，不会出现无限缓冲； 自动跟踪最后看到的消息及自动重新连接； 消息通知在客户端以 DOM 事件形式呈现。 实际上，SSE 提供的是一个高效、跨浏览器的 XHR 流实现，消息交付只使用一个长 HTTP 连接。然而，与我们自己实现 XHR 流不同，浏览器会帮我们管理连接、解析消息，从而让我们只关注业务逻辑。 EventSource APIEventSource 接口通过一个简单的浏览器 API 隐藏了所有的底层细节，包括建立连接和解析消息。 SSE 实现了节省内存的 XHR 流。与原始的 XHR 流在连接关闭前会缓冲接收到的所有响应不同，SSE 连接会丢弃已经处理过的消息，而不会在内存中累积。 值得一提的是， EventSource 接口还能自动重新连接并跟踪最近接收的消息：如果连接断开了， EventSource 会自动重新连接到服务器，还可以向服务器发送上一次接收到的消息 ID，以便服务器重传丢失的消息并恢复流。 Event Stream协议SSE 事件流是以流式 HTTP 响应形式交付的：客户端发起常规 HTTP 请求，服务器以自定义的“text/event-stream”内容类型响应，然后交付 UTF-8 编码的事件数据。这么简单几句话似乎都有点说复杂了，看一个例子： 1234567891011121314151617181920=&gt; 请求GET /stream HTTP/1.1 ➊Host: example.comAccept: text/event-stream&lt;= 响应HTTP/1.1 200 OK ➋Connection: keep-aliveContent-Type: text/event-streamTransfer-Encoding: chunkedretry: 15000 ➌data: First message is a simple string. ➍data: &#123;&quot;message&quot;: &quot;JSON payload&quot;&#125; ➎event: foo ➏data: Message of type &quot;foo&quot;id: 42 ➐event: bardata: Multi-line message ofdata: type &quot;bar&quot; and id &quot;42&quot;id: 43 ➑data: Last message, id &quot;43&quot; ➊ 客户端通过 EventSource 接口发起连接➋ 服务器以 “text/event-stream” 内容类型响应➌ 服务器设置连接中断后重新连接的间隔时间（15 s）➍ 不带消息类型的简单文本事件➎ 不带消息类型的 JSON 数据载荷➏ 类型为 “foo” 的简单文本事件➐ 带消息 ID 和类型的多行事件➑ 带可选 ID 的简单文本事件 在接收端， EventSource 接口通过检查换行分隔符来解析到来的数据流。 SSE 中的 UTF-8 编码与二进制传输 EventSource 不会对实际载荷进行任何额外处理：从一或多个 data 字段中提取出来的消息，会被拼接起来直接交给应用。因此，服务器可以推送任何文本格式（例如，简单字符串、JSON，等等），应用必须自己解码。 话虽如此，但所有事件源数据都是 UTF-8 编码的：SSE 不是为传输二进制载荷而设计的！如果有必要，可以把二进制对象编码为 base64 形式，然后再使用 SSE。但这样会导致很高（33%）的字节开销。 担心 UTF-8 编码也会造成高开销？ SSE 连接本质上是 HTTP 流式响应，因此响应是可以压缩的（如 gzip 压缩），就跟压缩其他 HTTP 响应一样，而且是动态压缩！虽然 SSE 不是为传输二进制数据而设计的，但它却是一个高效的机制——只要让你的服务器对 SSE 流应用 gzip 压缩。 不支持二进制传输是有意为之的。SSE 的设计目标是简单、高效，作为一种服务器向客户端传送文本数据的机制。如果你想传输二进制数据，WebSocket 才是更合适的选择。 最后，除了自动解析事件数据，SSE 还内置支持断线重连，以及恢复客户端因断线而丢失的消息。默认情况下，如果连接中断，浏览器会自动重新连接。SSE 规范建议的间隔时间是 2~3 s，这也是大多数浏览器采用的默认值。不过，服务器也可以设置一个自定义的间隔时间，只要在推送任何消息时向客户端发送一个 retry 命令即可。 类似地，服务器还可以给每条消息关联任意 ID 字符串。浏览器会自动记录最后一次收到的消息 ID，并在发送重连请求时自动在 HTTP 首部追加“Last-Event-ID”值。下面看一个例子： 123456789101112131415161718（既有 SSE 连接）retry: 4500 ➊id: 43 ➋data: Lorem ipsum（连接断开）（4500 ms 后）=&gt; 请求GET /stream HTTP/1.1 ➌Host: example.comAccept: text/event-streamLast-Event-ID: 43&lt;= 响应HTTP/1.1 200 OK ➍Content-Type: text/event-streamConnection: keep-aliveTransfer-Encoding: chunkedid: 44 ➎data: dolor sit amet ➊ 服务器将客户端的重连间隔设置为 4.5 s➋ 简单文本事件，ID:43➌ 带最后一次事件 ID 的客户端重连请求➍ 服务器以 ‘text/event-stream’ 内容类型响应➎ 简单文本事件，ID:44 客户端应用不必为重新连接和记录上一次事件 ID 编写任何代码。这些都由浏览器自动完成，然后就是服务器负责恢复了。值得注意的是，根据应用的要求和数据流，服务器可以采取不同的实现策略。 如果丢失消息可以接受，就不需要事件 ID 或特殊逻辑，只要让客户端重连并恢复数据流即可。 如果必须恢复消息，那服务器就需要指定相关事件的 ID，以便客户端在重连时报告最后接收到的 ID。同样，服务器也需要实现某种形式的本地缓存，以便恢复并向客户端重传错过的消息。 SSE使用场景及性能 通过 TLS 实现 SSE 流SSE 通过常规 HTTP 连接实现了简单便捷的实时传输机制，服务器端容易部署，客户端也容易打补丁。可是，现有网络中间设备，比如代理服务器和防火墙，都不支持 SSE，而这有可能带来问题：中间设备可能会缓冲事件流数据，导致额外延迟，甚至彻底毁掉 SSE 连接。 如果你碰到了这样或类似的问题，那么可以考虑通过 TLS 发送 SSE 事件流。 WebSocketWebSocket 可以实现客户端与服务器间双向、基于消息的文本或二进制数据传输。 接收文本和二进制数据WebSocket 协议不作格式假设，对应用的净荷也没有限制：文本或者二进制数据都没问题。从内部看，协议只关注消息的两个信息：净荷长度和类型（前者是一个可变长度字段），据以区别 UTF-8 数据和二进制数据。 浏览器接收到新消息后，如果是文本数据，会自动将其转换成 DOMString 对象，如果是二进制数据或 Blob 对象，会直接将其转交给应用。唯一可以（作为性能暗示和优化措施）多余设置的，就是告诉浏览器把接收到的二进制数据转换成 ArrayBuffer而非 Blob。 用户代理可以将这个选项看作一个暗示，以决定如何处理接收到的二进制数据：如果这里设置为“blob”，那就可以放心地将其转存到磁盘上；而如果设置为“arraybuffer”，那很可能在内存里处理它更有效。自然地，我们鼓励用户代理使用更细微的线索，以决定是否将到来的数据放到内存里…… ——The WebSocket API W3C Candidate Recommendation Blob 对象一般代表一个不可变的文件对象或原始数据。如果你不需要修改它或者不需要把它切分成更小的块，那这种格式是理想的（比如，可以把一个完整的 Blob 对象传给 img 标签）。而如果你还需要再处理接收到的二进制数据，那么选择 ArrayBuffer 应该更合适。 子协议协商WebSocket 协议对每条消息的格式事先不作任何假设：仅用一位标记消息是文本还是二进制，以便客户端和服务器有效地解码数据，而除此之外的消息内容就是未知的。 此外，与 HTTP 或 XHR 请求不同——它们是通过每次请求和响应的 HTTP 首部来沟通元数据，WebSocket 并没有等价的机制。因此，如果需要沟通关于消息的元数据，客户端和服务器必须达成沟通这一数据的子协议。 客户端和服务器可以提前确定一种固定的消息格式，比如所有通信都通过 JSON编码的消息或者某种自定义的二进制格式进行，而必要的元数据作为这种数据结构的一个部分。 如果客户端和服务器要发送不同的数据类型，那它们可以确定一个双方都知道的消息首部，利用它来沟通说明信息或有关净荷的其他解码信息。 混合使用文本和二进制消息可以沟通净荷和元数据，比如用文本消息实现 HTTP首部的功能，后跟包含应用净荷的二进制消息。 子协议名由应用自己定义，且在初次 HTTP 握手期间发送给服务器。除此之外，指定的子协议对核心 WebSocket API 不会有任何影响。 WebSocket协议 WebSocket 协议尝试在既有 HTTP 基础设施中实现双向 HTTP 通信，因此也使用 HTTP 的 80 和 443 端口……不过，这个设计不限于通过 HTTP 实现WebSocket 通信，未来的实现可以在某个专用端口上使用更简单的握手，而不必重新定义么一个协议。 ——WebSocket Protocol RFC 6455 二进制分帧层客户端和服务器 WebSocket 应用通过基于消息的 API 通信：发送端提供任意 UTF-8或二进制的净荷，接收端在整个消息可用时收到通知。为此，WebSocket 使用了自定义的二进制分帧格式（图 17-1），把每个应用消息切分成一或多个帧，发送到目的地之后再组装起来，等到接收到完整的消息后再通知接收端。 帧最小的通信单位，包含可变长度的帧首部和净荷部分，净荷可能包含完整或部分应用消息。 消息一系列帧，与应用消息对等。 每一帧的第一位（FIN）表示当前帧是不是消息的最后一帧。一条消息有可能只对应一帧。 操作码（4 位）表示被传输帧的类型：传输应用数据时，是文本（1）还是二进制（2）；连接有效性检查时，是关闭（8）、呼叫（ping，9）还是回应（pong，10）。 掩码位表示净荷是否有掩码（只适用于客户端发送给服务器的消息）。 净荷长度由可变长度字段表示： 如果是 0~125，就是净荷长度； 如果是 126，则接下来 2 字节表示的 16 位无符号整数才是这一帧的长度； 如果是 127，则接下来 8 字节表示的 64 位无符号整数才是这一帧的长度。 掩码键包含 32 位值，用于给净荷加掩护。 净荷包含应用数据，如果客户端和服务器在建立连接时协商过，也可以包含自定义的扩展数据。 所有客户端发送帧的净荷都要使用帧首部中指定的值加掩码，这样可以防止客户端中运行的恶意脚本对不支持 WebSocket 的中间设备进行缓存投毒攻击（cache poisoning attack）。要了解这种攻击的细节，请参考 W2SP2011 的论文“Talking to Yourself for Fun and Profit”（http://w2spconf.com/2011/papers/websocket.pdf）。 算下来，服务器发送的每个 WebSocket 帧会产生 2~10 字节的分帧开销。而客户端必须发送掩码键，这又会增加 4 字节，结果就是 6~14 字节的开销。除此之外，没有其他元数据（比如首部字段或其他关于净荷的信息）：所有 WebSocket 通信都是通过交换帧实现的，而帧将净荷视为不透明的应用数据块。 WebSocket 的多路复用及队首阻塞WebSocket 很容易发生队首阻塞的情况：消息可能会被分成一或多个帧，但不同消息的帧不能交错发送，因为没有与 HTTP 2.0 分帧机制中“流 ID”对等的字段。显然，如果一个大消息被分成多个 WebSocket 帧，就会阻塞其他消息的帧。如果你的应用不容许有交付延迟，那可以小心控制每条消息的净荷大小，甚至可以考虑把大消息拆分成多个小消息！WebSocket 不支持多路复用，还意味着每个 WebSocket 连接都需要一个专门的TCP 连接。对于 HTTP 1.x 而言，由于浏览器针对每个来源有连接数量限制，因此可能会导致问题。好 在，HyBi Working Group 正 着 手 制 定 的 新 的“Multiplexing Extension for WebSockets”（WebSockets 多路复用扩展）会解决这个问题：这个扩展通过封装帧并加上信道 ID，可以让一个 TCP 连接支持多个虚拟 WebSocket 连接……这个多路复用扩展维护独立的逻辑信道，每个逻辑信道与独立的 WebSocket 连接没有差别，包括独立的握手首部。——WebSocket Multiplexing（Draft 10）有了这个扩展后，多个 WebSocket 连接（信道）就可能在同一个 TCP 连接上得到复用。可是，每个信道依旧容易产生队首阻塞问题！可能的解决方案是使用不同的信道，或者专用 TCP 连接，多路并行发送消息。最后，注意前面的扩展仅对 HTTP 1.x 连接是必要的。虽然通过 HTTP 2.0 传输WebSocket 帧的官方规范尚未发布，但相对来说就容易多了。因为 HTTP 2.0 内置了流的多路复用，只要通过 HTTP 2.0 的分帧机制来封装 WebSocket 帧，多个WebSocket 连接就可以在一个会话中传输。 协议扩展WebSocket 规范允许对协议进行扩展：数据格式和 WebSocket 协议的语义可以通过新的操作码和数据字段扩展。虽然有些不同寻常，但这却是一个非常强大的特性，因为它允许客户端和服务器在基本的 WebSocket 分帧层之上实现更多功能，又不需要应用代码介入或协作。 要使用扩展，客户端必须在第一次的 Upgrade 握手中通知服务器，服务器必须选择并确认要在商定连接中使用的扩展。 HTTP升级协商WebSocket 协议提供了很多强大的特性：基于消息的通信、自定义的二进制分帧层、子协议协商、可选的协议扩展，等等。换句话说，在交换数据之前，客户端必须与服务器协商适当的参数以建立连接。 利用 HTTP 完成握手有几个好处。首先，让 WebSockets 与现有 HTTP 基础设施兼容：WebSocket 服务器可以运行在 80 和 443 端口上，这通常是对客户端唯一开放的端口。其次，让我们可以重用并扩展 HTTP 的 Upgrade 流，为其添加自定义的WebSocket 首部，以完成协商。 Sec-WebSocket-Version客户端发送，表示它想使用的 WebSocket 协议版本（“13”表示 RFC 6455）。如果服务器不支持这个版本，必须回应自己支持的版本。 Sec-WebSocket-Key客户端发送，自动生成的一个键，作为一个对服务器的“挑战”，以验证服务器支持请求的协议版本。Sec-WebSocket-Accept服务器响应，包含 Sec-WebSocket-Key 的签名值，证明它支持请求的协议版本。 Sec-WebSocket-Protocol用于协商应用子协议：客户端发送支持的协议列表，服务器必须只回应一个协议名。 Sec-WebSocket-Extensions用于协商本次连接要使用的 WebSocket 扩展：客户端发送支持的扩展，服务器通过返回相同的首部确认自己支持一或多个扩展。 有了这些协商字段，就可以在客户端和服务器之间进行 HTTP Upgrade 并协商新的WebSocket 连接了： 123456789GET /socket HTTP/1.1Host: thirdparty.comOrigin: http://example.comConnection: UpgradeUpgrade: websocket ➊Sec-WebSocket-Version: 13 ➋Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ== ➌Sec-WebSocket-Protocol: appProtocol, appProtocol-v2 ➍Sec-WebSocket-Extensions: x-webkit-deflate-message, x-custom-extension ➎ ➊ 请求升级到 WebSocket 协议➋ 客户端使用的 WebSocket 协议版本➌ 自动生成的键，以验证服务器对协议的支持➍ 可选的应用指定的子协议列表➎ 可选的客户端支持的协议扩展列表与浏览器中客户端发起的任何连接一样，WebSocket 请求也必须遵守同源策略：浏览器会自动在升级握手请求中追加 Origin 首部，远程服务器可能使用 CORS 判断接受或拒绝跨源请求。要完成握手，服务器必须返回一个成功的“Switching Protocols”（切换协议）响应，并确认选择了客户端发送的哪个选项： 1234567HTTP/1.1 101 Switching Protocols ➊Upgrade: websocketConnection: UpgradeAccess-Control-Allow-Origin: http://example.com ➋Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo= ➌Sec-WebSocket-Protocol: appProtocol-v2 ➍Sec-WebSocket-Extensions: x-custom-extension ➎ ➊ 101 响应码确认升级到 WebSocket 协议➋ CORS 首部表示选择同意跨源连接➌ 签名的键值验证协议支持➍ 服务器选择的应用子协议➎ 服务器选择的 WebSocket 扩展 所有兼容 RFC 6455 的 WebSocket 服务器都使用相同的算法计算客户端挑战的答案：将 Sec-WebSocket-Key 的内容与标准定义的唯一 GUID 字符串拼接起来，计算出 SHA1 散列值，结果是一个 base-64 编码的字符串，把这个字符串发给客户端即可。 最低限度，成功的 WebSocket 握手必须是客户端发送协议版本和自动生成的挑战值，服务器返回 101 HTTP 响应码（Switching Protocols）和散列形式的挑战答案，确认选择的协议版本： 客户端必须发送Sec-WebSocket-Version 和 Sec-WebSocket-Key ； 服务器必须返回Sec-WebSocket-Accept 确认协议； 客户端可以通过Sec-WebSocket-Protocol 发送应用子协议列表； 服务器必须选择一个子协议并通过Sec-WebSocket-Protocol 返回协议名；如果服务器不支持任何一个协议，连接断开； 客户端可以通过Sec-WebSocket-Extensions 发送协议扩展； 服务器可以通过Sec-WebSocket-Extensions 确认一或多个扩展；如果服务器没有返回扩展，则连接不支持扩展。 最后，前述握手完成后，如果握手成功，该连接就可以用作双向通信信道交换WebSocket 消息。从此以后，客户端与服务器之间不会再发生 HTTP 通信，一切由WebSocket 协议接管。 代理、中间设备与 WebSocket实践中，考虑到安全和保密，很多用户都只开放有限的端口，通常只有 80（HTTP）和 443（HTTPS）。正因为如此，WebSocket 协商是通过 HTTP Upgrade流进行的，这样可以确保与现有网络策略及基础设施兼容。不过，正如 “Web 代理、中间设备、TLS 与新协议”所说，很多现有的HTTP 中间设备可能不理解新的 WebSocket 协议，而这可能导致各种问题：盲目的连接升级、意外缓冲 WebSocket 帧、不明就里地修改内容、把 WebSocket 流量误当作不完整的 HTTP 通信，等等。WebSocket 的 Key 和 Accept 握手可以解决其中一些问题：这是服务器的一个安全策略，而盲目“升级”连接的中间设备可能并不理解 WebSocket 协议。虽然这个预防措施对某些代理可以解决问题，但对于那些“透明代理”还是不行，它们可能会分析并意外地修改数据。解决之道？建立一条端到端的安全通道。比如，使用 WSS ！在执行 HTTP Upgrade 握手之前，先协商一次 TLS 会话，在客户端与服务器之间建立一条加密通道，就可以解决前述所有问题。这个方案尤其适合移动客户端，因为它们的流量经常要穿越各种代理服务，这些代理服务很可能不认识 WebSocket。 WebSocket使用场景及性能请求和响应流 把传输机制从 XHR 切换为 SSE 或 WebSocket 并不会减少客户端与服务器间的往返次数！不管什么传输机制，数据包的传播延迟都一样。不过，除了传播延迟，还有一个排队延迟——消息在被发送给另一端之前必须在客户端或服务器上等待的时间。 WebRTC略 PDF书籍下载地址：https://github.com/jiankunking/books-recommendation/tree/master/HTTP]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>读书笔记</tag>
        <tag>TCP</tag>
        <tag>UDP</tag>
        <tag>HTTP</tag>
        <tag>SSL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java ForkJoin 解析]]></title>
    <url>%2Fjava-forkjoin.html</url>
    <content type="text"><![CDATA[本文主要想了解两个地方：如何窃取任务、task如何等待（join）代码基于 OpenJDK 12 窃取算法（work-stealing）从ForkJoin-Paper-DougLea中可以看出: 每个队列创建一个单独的线程来执行队列里的任务，线程和队列一一对应。 队列使用的是双端队列，支持LIFO、FIFO。 子任务会被放到线程（不一定是当前线程）的队列中。 工作线程按照LIFO的顺序处理自己队列中数据。 当一个工作线程处理完自己队列中数据的时候，会随机挑选一个工作线程，并“窃取”的该工作线程队列队尾的task。 到了这里就可以知道，窃取任务从其他线程队列的尾部窃取的了。 窃取算法优缺点工作窃取算法的优点：充分利用线程进行并行计算，减少了线程间的竞争。工作窃取算法的缺点：在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且该算法会消耗了更多的系统资源，比如创建多个线程和多个双端队列。 Task 等待（join）Join方法的主要作用是阻塞当前线程并等待获取结果。具体代码如下： 123456public final V join() &#123; int s; if (((s = doJoin()) &amp; ABNORMAL) != 0) reportException(s); return getRawResult();&#125; 首先，它调用了doJoin()方法，通过doJoin()方法得到当前任务的状态来判断返回什么结果，任务状态有4种：已完成（NORMAL）、被取消（CANCELLED）、信号（SIGNAL）和出现异常（EXCEPTIONAL）。 如果任务状态是已完成，则直接返回任务结果。 如果任务状态是被取消，则直接抛出CancellationException。 如果任务状态是抛出异常，则直接抛出对应的异常。 让我们再来分析一下doJoin()方法的实现代码: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * Implementation for join, get, quietlyJoin. Directly handles * only cases of already-completed, external wait, and * unfork+exec. Others are relayed to ForkJoinPool.awaitJoin. * * @return status upon completion */private int doJoin() &#123; int s; Thread t; ForkJoinWorkerThread wt; ForkJoinPool.WorkQueue w; return //已完成,返回status (s = status) &lt; 0 ? s : //未完成,如果当前线程是ForkJoinWorkerThread,从该线程中取出workQueue,并尝试将 //当前task出队然后执行,执行的结果是完成则返回状态,否则使用当线程池所在的ForkJoinPool的awaitJoin方法等待 ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ? (w = (wt = (ForkJoinWorkerThread)t).workQueue).tryUnpush(this) &amp;&amp; (s = doExec()) &lt; 0 ? s : wt.pool.awaitJoin(w, this, 0L) : //当前线程不是ForkJoinWorkerThread,调用externalAwaitDone方法 //externalAwaitDone: Blocks a non-worker-thread until completion. externalAwaitDone();&#125;/** * Pops the given task only if it is at the current top. */final boolean tryUnpush(ForkJoinTask&lt;?&gt; task) &#123; boolean popped = false; int s, cap; ForkJoinTask&lt;?&gt;[] a; if ((a = array) != null &amp;&amp; (cap = a.length) &gt; 0 &amp;&amp; (s = top) != base &amp;&amp; (popped = QA.compareAndSet(a, (cap - 1) &amp; --s, task, null))) TOP.setOpaque(this, s); return popped;&#125;/** * Primary execution method for stolen tasks. Unless done, calls * exec and records status if completed, but doesn&apos;t wait for * completion otherwise. * * @return status on exit from this method*/final int doExec() &#123; int s; boolean completed; // 仅未完成的任务会运行,其他情况会忽略. if ((s = status) &gt;= 0) &#123; try &#123; //exec是abstract方法 //调用ForkJoinTask子类中exec completed = exec(); &#125; catch (Throwable rex) &#123; completed = false; s = setExceptionalCompletion(rex); &#125; if (completed) s = setDone(); &#125; return s;&#125; 在doJoin()方法里，首先通过查看任务的状态，看任务是否已经执行完成，如果执行完成，则直接返回任务状态；如果没有执行完，则从任务队列中取出任务并执行。如果任务顺利执行完成，则设置任务状态为NORMAL，如果出现异常，则记录异常，并将任务状态设置为EXCEPTIONAL。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>JUC</tag>
        <tag>Java</tag>
        <tag>ForkJoin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java ThreadLocal]]></title>
    <url>%2Fjava-threadlocal.html</url>
    <content type="text"><![CDATA[基于OpenJDK 12 引本文主要想了解两个地方： ThreadLocal实例看起来是在多个线程共享，但实际上是彼此独立的，这个是怎么实现的？ ThreadLocal使用不当真的会OOM吗？如果会，那么原因是啥？ 先看一下ThreadLocal的官方API解释为： 该类提供了线程局部 (thread-local) 变量。这些变量不同于它们的普通对应物，因为访问某个变量（通过其 get 或 set 方法）的每个线程都有自己的局部变量，它独立于变量的初始化副本[原文：These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable.]。ThreadLocal 实例通常是类中的 private static 字段，它们希望将状态与某一个线程（例如，用户 ID 或事务 ID）相关联。 大概的意思有两点： ThreadLocal提供了一种访问某个变量的特殊方式：访问到的变量属于当前线程，即保证每个线程的变量不一样，而同一个线程在任何地方拿到的变量都是一致的，这就是所谓的线程隔离。 如果要使用ThreadLocal，通常定义为private static类型，在我看来最好是定义为private static final类型。 看一段代码： 12345678910111213141516171819202122232425262728293031323334353637// 代码来自：// http://tutorials.jenkov.com/java-concurrency/threadlocal.htmlpublic class ThreadLocalExample &#123; public static class MyRunnable implements Runnable &#123; private ThreadLocal&lt;Integer&gt; threadLocal = new ThreadLocal&lt;Integer&gt;(); @Override public void run() &#123; //注意这里 set的值是run函数的内部变量，如果是MyRunnable的全局变量 //则无法起到线程隔离的作用 threadLocal.set((int) (Math.random() * 100D)); try &#123; //sleep两秒的作用是让thread2 set操作在thread1的输出之前执行 //如果线程之间是共用threadLocal，则thread2 set操作会覆盖掉thread1的set操作 //从而两者的输出都是thread2 set的值 Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; System.out.println(e); &#125; System.out.println(threadLocal.get()); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; MyRunnable sharedRunnableInstance = new MyRunnable(); Thread thread1 = new Thread(sharedRunnableInstance); Thread thread2 = new Thread(sharedRunnableInstance); thread1.start(); thread2.start(); thread1.join(); //wait for thread 1 to terminate thread2.join(); //wait for thread 2 to terminate &#125;&#125; 输出结果： 123456thread1 startthread2 start38thread1 join78thread2 join MyRunnable run中sleep两秒的作用是让thread2 set操作在thread1的输出之前执行，如果线程之间是共用threadLocal，则thread2 set操作会覆盖掉thread1的set操作，两者的输出都是thread2 set的值，从而输出的应该是同一个值。 但从代码执行结果来看，thread1、thread2的threadLocal是不同的，也就是实现了线程隔离。 ThreadLocal实例在线程间是如何独立的？看一眼ThreadLocal set方法： 12345678910111213141516171819public void set(T value) &#123; //currentThread是个native方法，会返回对当前执行线程对象的引用。 Thread t = Thread.currentThread(); //getMap 返回线程自身的threadLocals ThreadLocalMap map = getMap(t); if (map != null) &#123; //把value set到线程自身的ThreadLocalMap中了 map.set(this, value); &#125; else &#123; //线程自身的ThreadLocalMap未初始化，则先初始化，再set createMap(t, value); &#125;&#125;ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125;//Thread类中//ThreadLocalMapset的set方法未执行深拷贝，需要注意传递值的类型ThreadLocal.ThreadLocalMap threadLocals = null; 从代码中可以看到，在set的时候，会根据Thread对象的引用来将值添加到各自线程中。但set的值value还是同一个对象,既然传递的是同一个对象，那就涉及到另一个问题：参数值传递、引用传递的问题了。 基本类型123456789101112131415161718192021222324252627282930313233343536373839public class ThreadLocalExample &#123; public static class MyRunnable implements Runnable &#123; private ThreadLocal&lt;Object&gt; threadLocal = new ThreadLocal&lt;&gt;(); // MyRunnable 全局变量 int random; @Override public void run() &#123; random = (int) (Math.random() * 100D); threadLocal.set(random); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; System.out.println(e); &#125; System.out.println(threadLocal.get()); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; MyRunnable sharedRunnableInstance = new MyRunnable(); Thread thread1 = new Thread(sharedRunnableInstance); Thread thread2 = new Thread(sharedRunnableInstance); thread1.start(); System.out.println(&quot;thread1 start&quot;); thread2.start(); System.out.println(&quot;thread2 start&quot;); thread1.join(); //wait for thread 1 to terminate System.out.println(&quot;thread1 join&quot;); thread2.join(); //wait for thread 2 to terminate System.out.println(&quot;thread2 join&quot;); &#125;&#125; 输出结果： 1234567thread1 startthread2 start//两个值不同16thread1 join75thread2 join 从输出可以看出两者隔离了。 引用类型全局引用12345678910111213141516171819202122232425262728293031323334353637383940414243public class ThreadLocalExample &#123; public static class MyRunnable implements Runnable &#123; private ThreadLocal&lt;Object&gt; threadLocal = new ThreadLocal&lt;&gt;(); // MyRunnable 全局变量 Obj obj = new Obj(); @Override public void run() &#123; obj.value = (int) (Math.random() * 100D); threadLocal.set(obj); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; System.out.println(e); &#125; System.out.println(((Obj) threadLocal.get()).value); &#125; class Obj &#123; int value; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; MyRunnable sharedRunnableInstance = new MyRunnable(); Thread thread1 = new Thread(sharedRunnableInstance); Thread thread2 = new Thread(sharedRunnableInstance); thread1.start(); System.out.println(&quot;thread1 start&quot;); thread2.start(); System.out.println(&quot;thread2 start&quot;); thread1.join(); //wait for thread 1 to terminate System.out.println(&quot;thread1 join&quot;); thread2.join(); //wait for thread 2 to terminate System.out.println(&quot;thread2 join&quot;); &#125;&#125; 输出结果： 1234567thread1 startthread2 start//两个值相同3636thread1 jointhread2 join 从输出结果来看，当set操作的值是MyRunnable的全局变量，并且是引用类型的时候，无法起到隔离的作用。 局部引用123456789101112131415161718192021222324252627282930313233343536373839404142public class ThreadLocalExample &#123; public static class MyRunnable implements Runnable &#123; private ThreadLocal&lt;Object&gt; threadLocal = new ThreadLocal&lt;&gt;(); //Obj obj = new Obj(); @Override public void run() &#123; Obj obj = new Obj(); obj.value = (int) (Math.random() * 100D); threadLocal.set(obj); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; System.out.println(e); &#125; System.out.println(((Obj) threadLocal.get()).value); &#125; class Obj &#123; int value; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; MyRunnable sharedRunnableInstance = new MyRunnable(); Thread thread1 = new Thread(sharedRunnableInstance); Thread thread2 = new Thread(sharedRunnableInstance); thread1.start(); System.out.println(&quot;thread1 start&quot;); thread2.start(); System.out.println(&quot;thread2 start&quot;); thread1.join(); //wait for thread 1 to terminate System.out.println(&quot;thread1 join&quot;); thread2.join(); //wait for thread 2 to terminate System.out.println(&quot;thread2 join&quot;); &#125;&#125; 输出结果： 1234567thread1 startthread2 start//两个值不同1219thread1 jointhread2 join 从输出结果看，局部引用，可以相互隔离。 到这里可以看出ThreadLocal，只是把set值或引用绑定到了当前线程，但却没有进行相应的深拷贝，所以ThreadLocal要想做的线程隔离，必须是基本类型或者run的局部变量。 ThreadLocal OOM ？看一下ThreadLocalMap内部Entry： 123456789static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125; 从代码中看到，Entry继承了WeakReference，并将ThreadLocal设置为了WeakReference，value设置为强引用。也就是：当没有强引用指向ThreadLocal变量时，它可被回收。 但是，还有一个问题：ThreadLocalMap维护ThreadLocal变量与具体实例的映射，当ThreadLocal变量被回收后，该映射的key变为 null，而该Entry还是在ThreadLocalMap中，从而这些无法清理的Entry，会造成内存泄漏。 ThreadLocal自带的remove、set方法，都无法处理ThreadLocal自身为null的情况，因为代码中都直接取ThreadLocal的threadLocalHashCode属性了，所以如果ThreadLocal自身已经是null，这时调用remove、set会报空指针异常（java.lang.NullPointerException）的。 所以，在使用ThreadLocal的时候，在使用完毕记得remove（remove方法会将Entry的value及Entry自身设置为null并进行清理）。 JDK 12 ThreadLocal代码地址：https://github.com/jiankunking/openjdk12/blob/master/src/java.base/share/classes/java/lang/ThreadLocal.java]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>Java</tag>
        <tag>ThreadLocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[译 Java Concurrent Atomic Package详解]]></title>
    <url>%2Fjava-concurrent-atomic-package.html</url>
    <content type="text"><![CDATA[翻译自：Package java.util.concurrent.atomic 地址：https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/package-summary.html#package.description翻译JDK8而不是12的原因是JDK8对与内存语义部分讲解更加详细。 Package java.util.concurrent.atomic 对于单个变量支持无锁、线程安全操作的工具类。 类摘要： 类名 描述 AtomicBoolean 可以原子性更新的boolean值。 AtomicInteger 可以原子性更新的int值。 AtomicIntegerArray 一个int数组，其中的元素可以原子性更新。 AtomicIntegerFieldUpdater 基于反射，可以对指定类的指定 volatile int 字段进行原子更新。 AtomicLong 可以原子性更新的long值。 AtomicLongArray 一个long数组，其中的元素可以原子性更新。 AtomicLongFieldUpdater 基于反射，可以对指定类的指定 volatile long 字段进行原子更新。 AtomicMarkableReference 维护带有标记位的对象引用，可以原子方式对其进行更新。 AtomicReference 可以原子性更新的对象引用 AtomicReferenceArray 一个对象引用数组，其中的元素可以原子性更新。 AtomicReferenceFieldUpdater&lt;T,V&gt; 基于反射，可以对指定类的指定 volatile reference 字段进行原子更新。 AtomicStampedReference 维护带有整数版本标志的对象引用，可以原子方式对其进行更新。 DoubleAccumulator One or more variables that together maintain a running double value updated using a supplied function. DoubleAdder One or more variables that together maintain an initially zero double sum. LongAccumulator One or more variables that together maintain a running long value updated using a supplied function. LongAdder One or more variables that together maintain an initially zero long sum. DoubleAccumulator、DoubleAdder、LongAccumulator、LongAdder 均是Striped64的子类，内部维护的是一个数组，当并发更新时，每个线程操作的是数组中的元素，从而降低锁的粒度。 本质上，该package下的类扩展了volatile值、字段、数组元素的概念，提供以下形式的原子更新操作： 1boolean compareAndSet(expectedValue, updateValue); 该方法（在不同类中的参数类型不同）原子地将变量设置为updateValue，如果它当前持有expectedValue，更新成功返回true。该包中的类还包含获取和无条件设置（set）值的方法。 这些方法的规范使实现能够采用当代处理器上可用的高效机器级原子指令。 然而，在某些平台上，支持可能需要某种形式的内部锁定。因此，这些方法不是严格保证是非阻塞的（线程可能在执行操作之前暂时阻塞）。 AtomicBoolean、AtomicInteger、AtomicLong和AtomicReference类的实例都提供对对应类型的单个变量的访问和更新。每个类还提供了适用于该类型的实用方法。例如，类AtomicLong和AtomicInteger提供原子增量方法。 一个应用是生成序列号，如： 123456class Sequencer &#123; private final AtomicLong sequenceNumber = new AtomicLong(0); public long next() &#123; return sequenceNumber.getAndIncrement(); &#125;&#125; 原子性访问和更新内存效果，与volatiles遵循同样的规则，如The Java Language Specification (17.4 Memory Model)所述： get与volatile读效果一样 set与volatile写效果一样 lazySet与写入(分配)volatile变量的效果一样【写操作不会与之前的任何写操作重新排序】，但它可能被后续操作重排【也就是，在volatile写或者同步操作之前，可能对于其它线程不可见】。 compareAndSet和所有其他读取和更新操作(如getAndIncrement)一样，与volatile变量读取和写入具有相同的内存效果。 lazySet是使用Unsafe.putOrderedObject方法，这个方法在对低延迟代码是很有用的，它能够实现非阻塞的写入，这些写入不会被Java的JIT重新排序指令(instruction reordering)，这样它使用快速的存储-存储(store-store) barrier, 而不是较慢的存储-加载(store-load) barrier, 后者总是用在volatile的写操作上，这种性能提升是有代价的，虽然便宜，也就是写后结果并不会被其他线程看到，甚至是自己的线程，通常是几纳秒后被其他线程看到，这个时间比较短，所以代价可以忍受。设想如下场景: 设置一个 volatile 变量为 null，让这个对象被 GC 掉，volatile write 是消耗比较大（store-load 屏障）的，但是 putOrderedInt 只会加 store-store 屏障，损耗会小一些。 添加lazySet方法的原因:https://bugs.java.com/bugdatabase/view_bug.do?bug_id=6275329 123456789101112131415161718192021222324252627As probably the last little JSR166 follow-up for Mustang,we added a &quot;lazySet&quot; method to the Atomic classes(AtomicInteger, AtomicReference, etc). This is a nichemethod that is sometimes useful when fine-tuning code usingnon-blocking data structures. The semantics arethat the write is guaranteed not to be re-ordered with anyprevious write, but may be reordered with subsequent operations(or equivalently, might not be visible to other threads) untilsome other volatile write or synchronizing action occurs).The main use case is for nulling out fields of nodes innon-blocking data structures solely for the sake of avoidinglong-term garbage retention; it applies when it is harmlessif other threads see non-null values for a while, but you&apos;dlike to ensure that structures are eventually GCable. In suchcases, you can get better performance by avoidingthe costs of the null volatile-write. There are a fewother use cases along these lines for non-reference-basedatomics as well, so the method is supported across all of theAtomicX classes.For people who like to think of these operations in terms ofmachine-level barriers on common multiprocessors, lazySetprovides a preceeding store-store barrier (which is eithera no-op or very cheap on current platforms), but nostore-load barrier (which is usually the expensive partof a volatile-write). weakCompareAndSet JDK 9之后Deprecated，本文已跳过。 除了表示单个值的类之外，package中还包含Updater类，可用于在类的volatile字段上执行compareAndSet操作。 AtomicReferenceFieldUpdater、AtomicIntegerFieldUpdater和AtomicLongFieldUpdater是基于反射的，可提供对相关字段类型的访问。这些主要用于原子数据结构，同一节点的几个volatile字段（例如，树节点的链接）独立地原子更新。这些类在如何以及何时使用原子更新方面提供了更大的灵活性，但代价是更加笨拙的基于反射的设置、更不方便的使用和更弱的保证。 AtomicIntegerArray、AtomicLongArray和AtomicReferenceArray类进一步将原子操作支持扩展到这些类型的数组。这些类还提供了数组元素的volatile访问语义，这是普通数组不支持的。 AtomicMarkableReference类将单个布尔值与引用相关联。 例如，该位可能在数据结构中使用，表示被引用的对象在逻辑上已被删除。 AtomicStampedReference类将整数值与引用相关联。 例如，这可以用于表示与一系列更新相对应的版本号。 原子类主要设计为用于实现非阻塞数据结构和相关基础结构类的构建。 compareAndSet方法不是锁定的一般替代方法。 仅当对象的关键更新仅限于单个变量时，它才适用。 原子类不是java.lang.Integer和相关类的通用替换。它们没有定义equals，hashCode和compareTo等方法（由于原子变量预期会发生变化，所以它们不适合作为哈希表键）。 后续会出文章解析：AtomicLong、Striped64、LongAdder本文的目的主要是从大体上了解atomic及其内存语义 JDK 12https://docs.oracle.com/en/java/javase/12/docs/api/java.base/java/util/concurrent/atomic/package-summary.html]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>JDK</tag>
        <tag>Concurrent</tag>
        <tag>Atomic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java JUC Atomic AtomicLong]]></title>
    <url>%2Fjava-juc-atomic-atomiclong.html</url>
    <content type="text"><![CDATA[基于OpenJDK 12本文的目的是为后续文章解析LongAdder做一个引子，以便两者对比。 Atomic Package解析参考（比如lazySet原理解析） AtomicLong的常用方法如下： long addAndGet(long delta)：以原子方式将输入的数值与实例中的值（AtomicLong里的value）相加，并返回结果。 compareAndSet(long expectedValue, long newValue)：如果输入的数值等于预期值，则以原子方式将该值设置为输入的值。 long getAndIncrement()：以原子方式将当前值加1，注意，这里返回的是自增前的值。 void lazySet(long newValue)：最终会设置成newValue，使用lazySet设置值后，可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 long getAndSet(long newValue)：以原子方式设置为newValue的值，并返回旧值。 AtomicLong示例代码如下： 1234567public class AtomicLongTest &#123; static AtomicLong ai = new AtomicLong(1); public static void main(String[] args) &#123; System.out.println(ai.getAndIncrement()); System.out.println(ai.get()); &#125;&#125; 输出结果如下: 1212 那么getAndIncrement是如何实现原子操作的呢？让我们一起分析其实现原理，getAndIncrement的源码如下： 12345678910111213141516171819202122232425262728293031323334public final long getAndIncrement() &#123; return U.getAndAddLong(this, VALUE, 1L);&#125;@HotSpotIntrinsicCandidatepublic final long getAndAddLong(Object o, long offset, long delta) &#123; long v; do &#123; v = getLongVolatile(o, offset); &#125; while (!weakCompareAndSetLong(o, offset, v, v + delta)); return v;&#125;/** Volatile version of &#123;@link #getLong(Object, long)&#125; */@HotSpotIntrinsicCandidatepublic native long getLongVolatile(Object o, long offset);@HotSpotIntrinsicCandidatepublic final boolean weakCompareAndSetLong(Object o, long offset, long expected, ong x) &#123; return compareAndSetLong(o, offset, expected, x);&#125;/** * Atomically updates Java variable to &#123;@code x&#125; if it is currently * holding &#123;@code expected&#125;. * * &lt;p&gt;This operation has memory semantics of a &#123;@code volatile&#125; read * and write. Corresponds to C11 atomic_compare_exchange_strong. * * @return &#123;@code true&#125; if successful */@HotSpotIntrinsicCandidatepublic final native boolean compareAndSetLong(Object o, long offset, long expected, long x); @HotSpotIntrinsicCandidate JDK的源码中，被@HotSpotIntrinsicCandidate标注的方法，在HotSpot中都有一套高效的实现，该高效实现基于CPU指令，运行时，HotSpot维护的高效实现会替代JDK的源码实现，从而获得更高的效率。 源码getAndAddLong(Object o, long offset, long delta)中do while循环体是实现的关键所在，其逻辑是：第一步先取得AtomicLong里存储的数值，第二步对AtomicLong的当前数值进行加1操作，第三步调用weakCompareAndSetLong方法来进行原子更新操作，该方法先检查当前数值是否等于v，等于意味着AtomicLong的值没有被其他线程修改过，则将weakCompareAndSetLong的当前数值更新成v+delta的值，如果不等于v，weakCompareAndSetLong方法会返回false，程序会进入do while循环重新进行weakCompareAndSetLong操作。 这里隐含了一个问题，当对于共享变量（假设变量名字是a）的竞争非常激烈的时候，在当前线程读取a、改变a之间，a的值会被别的线程改变，从而导致当前线程一直重试（自旋），一直占用CPU。 这就引出另一个问题，对于锁抢占很激烈的时候，串行是最好的解决办法。比如使用synchronized。 java.util.concurrent.atomic中的原子操作基本是基于Unsafe或者VarHandle实现的。 AtomicLong源码 本文参考 《Java并发编程的艺术》 作者：方腾飞 魏鹏 程晓明]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>JDK</tag>
        <tag>Concurrent</tag>
        <tag>Atomic</tag>
        <tag>AtomicLong</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java JUC Atomic LongAdder]]></title>
    <url>%2Fjava-juc-atomic-longadder.html</url>
    <content type="text"><![CDATA[基于OpenJDK 12 阅读本文前，推荐先阅读以下两篇文章，以便能更好的对比理解： 译-Java-Concurrent-Atomic-Package-详解 Java-JUC-Atomic-AtomicLong LongAdder是JDK 1.8 新增的原子类，基于Striped64实现。 从官方文档看，LongAdder在高并发的场景下会比AtomicLong 具有更好的性能，代价是消耗更多的内存空间： This class is usually preferable to AtomicLong when multiple threads update a common sum that is used for purposes such as collecting statistics, not for fine-grained synchronization control. Under low update contention, the two classes have similar characteristics. But under high contention, expected throughput of this class is significantly higher, at the expense of higher space consumption. 那么LongAdder是怎么实现的？ 先看一下LongAdder的类图： 基类Number，Number是一个抽象类其中没有任何逻辑，该类是byte、double、float、int、long、short的基类。 Striped64设计思想该部分翻译自Striped64源码注释，可以略过，概括起来就是： 分散热点，将value值分散到一个数组中，不同线程会命中到数组的不同槽中，各个线程只对自己槽中的那个值进行CAS操作，这样热点就被分散了，冲突的概率就小很多。如果要获取真正的long值，只要将各个槽中的变量值累加返回。 从Striped64类注释可以看到： Striped64是package内使用的，对于在64位元素上动态分片提供统一实现（感觉有点像：AbstractQueuedSynchronizer）Striped64继承了Number类，这也就是说具体实现的子类也必须实现相关的内容 该类维护一个原子更新变量的延迟初始化表，以及一个额外的“base”字段。表的大小是2的幂。索引使用掩码下的每个线程的hash code。这个类中的几乎所有声明都是package私有的，由子类直接访问。 表格内的元素是Cell类，Cell类是一个为了减少缓存争用而填充的AtomicLong的变种。填充对于大多数原子来说是多余的，因为它们通常不规则地分散在内存中，因此彼此之间不会有太多的干扰。但是，驻留在数组中的原子对象往往是彼此相邻的，因此在没有这种预防措施的情况下，最常见的情况是共享高速缓存线(这对性能有很大的负面影响)。 在某种程度上，因为Cell类相对较大，只有他们真正被需要的时候，我们才创建。如果没有竞争，那么所有的更新操作将对base字段实现。当发生第一次争用（也就是说如果第一次对base字段的CAS操作失败），初始化为大小是2的表格。当进一步的争用发生的时候,表的大小会加倍，直到达到等于大于cpu的数量。表在未使用之前一直为null。 利用一个自旋锁（cellsBusy）来初始化和调整表的大小，以及用新Cells填充slots。这个地方没有必要使用阻塞锁，如果锁不可达，线程可以尝试其他的slots，或者尝试base字段。在这些重试期间，竞争加剧，但是降低了局部性，这仍然比阻塞锁来得好。 通过ThreadLocalRandom维护的Thread.probe字段用作每个线程的哈希码。我们让它们保持未初始化（为零）(如果它们以这种方式出现)，直到它们在插槽0竞争。出现竞争后初始化为通常不会和其他的的值冲突的值，比如线程的哈希码。在执行更新时发生CAS操作失败意味着出现了争用或者表碰撞，或两者都有。在发生冲突时，如果表的大小小于容量，那么它的大小将加倍，除非其他线程持有锁。如果哈希后的slot为空，并且锁可用，则创建一个新单元格。如果存在了那么会进行CAS尝试。通过双重哈希进行重试，利用一个辅助哈希（Marsaglia XorShift随机数算法）来尝试寻找一个空闲的slot。 表的大小是有上限的，因为当线程多于CPU时，假设每个线程都绑定到一个CPU，就会有一个完美的散列函数将线程映射到插槽，从而消除冲突。当我们达到容量时，我们通过随机改变冲突线程的哈希代码来搜索这个映射。因为搜索是随机的，冲突只有通过CAS失败才知道，收敛可能会很慢，而且因为线程通常不会永远绑定到CPU，所以根本不会发生。然而，尽管有这些限制，在这些情况下观察到的竞争率通常很低。 Cell可能会出现不可用的情况，包括进行哈希的线程终止，或者由于table扩容导致线程哈希不正确。我们不尝试检测或删除这样的单元格，假设对于长时间运行的实例，争用会再次发生，因此最终将再次需要这些单元格;而对于短时间运行的实例，花费时间去销毁又没有什么必要。 Cell类Atomiclong的变体，仅支持原始访问和CAS。 Cell类被注解@jdk.internal.vm.annotation.Contended修饰。Contended的作用（详细信息参见：JEP 142）：Define a way to specify that one or more fields in an object are likely to be highly contended across processor cores so that the VM can arrange for them not to share cache lines with other fields, or other objects, that are likely to be independently accessed. 示意图 具体实现Striped64的核心方法是longAccumulate、doubleAccumulate，两者类似，下面主要看一下longAccumulate，对于这种代码，个人建议是理解思路即可，毕竟咱们又不是过来修改JDK的，如果真的要修改了或者有类似的需求了，再回来细看即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107//x 元素//fn 更新函数，如果是add可以为null（这个约定避免了longadder中定义额外的变量或者函数）//wasUncontended 如果CAS在调用之前失败了，这个值为falsefinal void longAccumulate(long x, LongBinaryOperator fn, boolean wasUncontended) &#123; int h; //获取当前线程的probe值，如果为0，则需要初始化该线程的probe值 if ((h = getProbe()) == 0) &#123; ThreadLocalRandom.current(); // force initialization h = getProbe(); wasUncontended = true; &#125; boolean collide = false; // True if last slot nonempty done: for (;;) &#123; Cell[] cs; Cell c; int n; long v; //Cells不为空，进行操作 if ((cs = cells) != null &amp;&amp; (n = cs.length) &gt; 0) &#123; //通过（hashCode &amp; (length - 1)）这种算法来实现取模 有种看到HashMap代码的感觉 //如果当前位置为null说明需要初始化 if ((c = cs[(n - 1) &amp; h]) == null) &#123; //判断锁状态 if (cellsBusy == 0) &#123; // Try to attach new Cell Cell r = new Cell(x); // Optimistically create //再次判断锁状态，同时获取锁 if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; try &#123; // Recheck under lock Cell[] rs; int m, j; if ((rs = cells) != null &amp;&amp; (m = rs.length) &gt; 0 &amp;&amp; rs[j = (m - 1) &amp; h] == null) &#123; rs[j] = r; //创建成功跳出 break done; &#125; &#125; finally &#123; //释放锁 cellsBusy = 0; &#125; continue; // Slot is now non-empty &#125; &#125; collide = false; &#125; //运行到此说明cell的对应位置上已经有相应的Cell了， //不需要初始化了 //CAS操作已经失败了，出现了竞争 else if (!wasUncontended) // CAS already known to fail wasUncontended = true; // Continue after rehash //这里尝试将x值加到a的value上 else if (c.cas(v = c.value, (fn == null) ? v + x : fn.applyAsLong(v, x))) //如果尝试成功，跳出循环，方法退出 break; //cell数组最大为cpu的数量， //cells != as表明cells数组已经被更新了 //标记为最大状态或者说是过期状态 else if (n &gt;= NCPU || cells != cs) collide = false; // At max size or stale else if (!collide) collide = true; //扩容 当前容量 * 2 else if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; try &#123; if (cells == cs) // Expand table unless stale cells = Arrays.copyOf(cs, n &lt;&lt; 1); &#125; finally &#123; cellsBusy = 0; &#125; collide = false; continue; // Retry with expanded table &#125; h = advanceProbe(h); &#125; //尝试获取锁之后扩大Cells else if (cellsBusy == 0 &amp;&amp; cells == cs &amp;&amp; casCellsBusy()) &#123; try &#123; // Initialize table if (cells == cs) &#123; //初始化cell表，初始容量为2。 Cell[] rs = new Cell[2]; rs[h &amp; 1] = new Cell(x); cells = rs; break done; &#125; &#125; finally &#123; //释放cellsBusy锁 cellsBusy = 0; &#125; &#125; //如果创建cell表由于竞争导致失败，尝试将x累加到base上 // Fall back on using base else if (casBase(v = base, (fn == null) ? v + x : fn.applyAsLong(v, x))) break done; &#125;&#125;/** * CASes the cellsBusy field from 0 to 1 to acquire lock. */final boolean casCellsBusy() &#123; return CELLSBUSY.compareAndSet(this, 0, 1);&#125;/** * CASes the base field. */final boolean casBase(long cmp, long val) &#123; return BASE.compareAndSet(this, cmp, val);&#125; 这一段的核心是这样的： longAccumulate会根据当前线程来计算一个哈希值，然后根据(hashCode &amp; (length - 1))取模，以定位到该线程被分散到的Cell数组中的位置 如果Cell数组还没有被创建，那么就去获取cellBusy这个锁（相当于锁，但是更为轻量级），如果获取成功，则初始化Cell数组，初始容量为2，初始化完成之后将x包装成一个Cell，哈希计算之后分散到相应的index上。如果获取cellBusy失败，那么会试图将x累计到base上，更新失败会重新尝试直到成功。 如果Cell数组已经被初始化过了，那么就根据线程的哈希值分散到一个Cell数组元素上，获取这个位置上的Cell并且赋值给变量a，如果a为null，说明该位置还没有被初始化，那么就初始化，当然在初始化之前需要竞争cellBusy变量。 如果Cell数组的大小已经最大了（大于等于CPU的数量），那么就需要重新计算哈希，来重新分散当前线程到另外一个Cell位置上再走一遍该方法的逻辑，否则就需要对Cell数组进行扩容，然后将原来的计数内容迁移过去。由于Cell里面保存的是计数值，所以扩容后没有必要做其他处理，直接根据index将旧的Cell数组内容复制到新的Cell数组中。 LongAdder LongAdder的基本思路就是分散热点，将value值分散到一个数组中，不同线程会命中到数组的不同槽中，各个线程只对自己槽中的那个值进行CAS操作，这样热点就被分散了，冲突的概率就小很多。如果要获取真正的long值，只要将各个槽中的变量值累加返回。 说明保持一个或者多个变量，初始值设置为零用于求和。当更新出现多个线程竞争时，变量集合动态增长以减少争用。最后当需要求和的时候或者说需要这个Long型的值时，可以通过把当前这些变量求和，合并后得出最终的和。 LongAdder在一些高并发场景下表现要比AtomicLong好，比如多个线程同时更新一个求和的变量，比如统计集合的数量，但是不能用于细粒度同步控制，换句话说这个是可能有误差的（因为更新与读取是并行的）。在低并发场景场景下LongAdder和AtomicLong的性能表现没什么差别，但是当高并发竞争的时候，这个类将具备更好的吞吐性能，但是相应的也会耗费相当的空间。 LongAdder继承了Number抽象类，但是并没有实现一些方法例如: equals、hashCode、compareTo，因为LongAdder实例的预期用途是进行一些比较频繁的变化，所以也不适合作为集合的key。 具体实现看一下LongAdder有哪些方法： 下面主要解析LongAdder increment、sum方法，先看一下源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Equivalent to &#123;@code add(1)&#125;. */public void increment() &#123; add(1L);&#125;/*** Adds the given value.** @param x the value to add*/public void add(long x) &#123; Cell[] cs; long b, v; int m; Cell c; if ((cs = cells) != null || !casBase(b = base, b + x)) &#123; //到了这里 表明cs不为null or 线程有并发冲突，导致caseBase失败 boolean uncontended = true; if (cs == null || // cells 为null (m = cs.length - 1) &lt; 0 || // cells 不为null 但只有一个元素 (c = cs[getProbe() &amp; m]) == null || //哈希取模 对应位置元素为null !(uncontended = c.cas(v = c.value, v + x))) //cas 替换失败（并发竞争） longAccumulate(x, null, uncontended); &#125;&#125;/*** CASes the base field (Striped64类中的方法)*/final boolean casBase(long cmp, long val) &#123; return BASE.compareAndSet(this, cmp, val);&#125;//当在sum的过程中，有可能别的线程正在操作cells（因为没有加锁）//sum取的值，不一定准确public long sum() &#123; Cell[] cs = cells; long sum = base; if (cs != null) &#123; for (Cell c : cs) if (c != null) sum += c.value; &#125; return sum;&#125; LongAdder vs AtomicLong PerformanceJava 8 Performance Improvements: LongAdder vs AtomicLong 对比LongAccumulatorLongAdder类可以看做是LongAccumulator的一个特例，LongAccumulator提供了比LongAdder更强大、灵活的功能。 1234567891011121314151617181920212223/** * Creates a new instance using the given accumulator function * and identity element. * @param accumulatorFunction a side-effect-free function of two arguments * @param identity identity (initial value) for the accumulator function*/public LongAccumulator(LongBinaryOperator accumulatorFunction, long identity) &#123; this.function = accumulatorFunction; base = this.identity = identity;&#125;@FunctionalInterfacepublic interface LongBinaryOperator &#123;/** * Applies this operator to the given operands. * * @param left the first operand * @param right the second operand * @return the operator result*/long applyAsLong(long left, long right);&#125; 构造函数其中accumulatorFunction一个双目运算接口，根据输入的两个参数返回一个计算值，identity则是LongAccumulator累加器的初始值。 accumulatorFunction主要用于Striped64 longAccumulate中使用，如果fn==null，则默认是相加，否则会调用fn.applyAsLong(v, x) LongAccumulator相比于LongAdder，可以为累加器提供非0的初始值，而LongAdder只能提供默认的0值。另外，LongAccumulator还可以指定累加规则，比如累加或者相乘，只需要在构造LongAccumulator时，传入自定义的双目运算器即可，后者则内置累加规则。 Referencehttps://github.com/jiankunking/openjdk12/blob/master/src/java.base/share/classes/java/util/concurrent/atomic/LongAdder.java https://github.com/jiankunking/openjdk12/blob/master/src/java.base/share/classes/jdk/internal/vm/annotation/Contended.java https://www.jianshu.com/p/9a7de5644dd4 http://openjdk.java.net/jeps/142 http://mail.openjdk.java.net/pipermail/hotspot-dev/2012-November/007309.html]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>JDK</tag>
        <tag>Concurrent</tag>
        <tag>Atomic</tag>
        <tag>LongAdder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JRockit权威指南深入理解JVM 笔记]]></title>
    <url>%2Fjava-jrockit-note.html</url>
    <content type="text"><![CDATA[本文整理自：《JRockit权威指南深入理解JVM》 作者：Marcus Hirt , Marcus Lagergren 出版时间：2018-12-10 起步将应用程序迁移到JRockit命令行选项在 JRockit JVM中,主要有3类命令行选项,分别是系统属性、标准选项(以-X开头)和非标准选项(以-XX开头) 1、系统属性 设置JVM启动参数的方式有多种。以-D开头的参数会作为系统属性使用,这些属性可以为Java类库(如RMI等)提供相关的配置信息。例如,在启动的时候,如果设置了-Dcom.Rockin.mc.debug=true参数,则JRockit Mission Control会打印出调试信息。不过,R28之后的JRockit JVM版本废弃了很多之前使用过的系统属性,转而采用非标准选项和类似 HotSpot中虚拟机标志(VM flag)的方式设置相关选项 2、标准选项 以-X开头的选项是大部分JVM厂商都支持的通用设置。例如,用于设置堆大小最大值的选项-Xmx在包括 JRockit在内的大部分JVM中都是相同的。当然,也存在例外,如JRockit中的选项-Xverbose会打印出可选的子模块日志信息,而在 HotSpot中,类似的(但实际上有更多的限制)选项是-verbose 3、非标准选项 以-XX开头的命令行选项是各个JVM厂商自己定制的。这些选项可能会在将来的某个版本中被废弃或修改。如果JVM的参数配置中包含了以-XX开头的命令行选项,则在将Java应用程序从一种JVM迁移到另一种时,应该在启动M之前去除这些非标准选项确定了新的VM选项后才可以启动Java应用程序。 自适应代码生成Java虚拟机字节码格式Opcodes for the Java Virtual Machine 常量池程序,包含数据和代码两部分,其中数据作为操作数使用。对于字节码程序来说,如果操作数非常小或者很常用(如常量0),则这些操作数是直接内嵌在字节码指令中的。 较大块的数据,例如常量字符串或比较大的数字,是存储在class文件开始部分的常量池(constant pool)中的。当使用这类数据作为操作数时,使用的是常量池中数据的索引位置,而不是实际数据本身。以字符串数据aVeryLong FunctionName为例,如果在编译方法时每次都要重新编码这个字符串的话,那字节码就谈不上压缩存储了。 此外,Java程序中的方法、属性和类的元数据等也作为clas文件的组成部分,存储在常量池中。 自适应代码生成优化动态程序在汇编代码中,方法调用是通过call指令完成的。不同平台上call指令的具体形式不尽相同,不同类型的call指令,其具体格式也不尽相同。 在面向对象的语言中,虚拟方法分派通常被编译为对分派表(dispatch table)中地址的间接调用(indirect call,即需要从内存中读取真正的调用地址)。这是因为,根据不同的类继承结构分派虚拟调用时可能会有多个接收者。每个类中都有一个分派表,其中包含了其虚拟调用的接收者信息。静态方法和确知只有一个接收者的虚拟方法可以被编译为对固定调用地址的直接调用(direct call)。一般来说,这可以大大加快执行速度。 假设应用程序是使用C++开发的,对代码生成器来说,在编译时已经可以获取到程序的所有结构性信息。例如,由于在程序运行过程中,代码不会发生变化,所以在编译时就可以从代码中判断出,某个虚拟方法是否只有一种实现。正因如此,编译器不仅不需要因为废弃代码而记录额外的信息,还可以将那些只有一种实现的虚拟方法转化为静态调用。 假如应用程序是使用Java开发的,起初某个虚拟方法可能只有一种实现,但Java允许在程序运行过程中修改方法实现。当JIT编译器需要编译某个虚拟方法时,更喜欢的是那些永远只存在一种实现的,这样编译器就可以像前面提到的C+编译器一样做很多优化,例如将虚拟调用转化为直接调用。但是,由于Java允许在程序运行期间修改代码,如果某个方法没有声明final修饰符,那它就有可能在运行期间被修改,即使它看起来几乎不可能有其他实现,编译器也不能将之优化为直接调用。 在Java世界中,有一些场景现在看起来一切正常,编译器可以大力优化代码,但是如果某天程序发生了改变的话,就需要将相关的优化全部撤销。对于Java来说,为了能够媲美C++程序的执行速度,就需要一些特殊的优化措施。 JVM使用的策略就是“赌”。JVM代码生成策略的假设条件是,正在运行的代码永远不变。事实上,大部分时间里确实如此。但如果正在运行的代码发生了变化,违反了代码优化的假设条件,就会触发其簿记系统(bookkeeping system)的回调功能。此时,基于原先假设条件生成的代码就需要被废弃掉,重新生成,例如为已经转化为直接调用的虚拟调用重新生成相关代码。因此,“赌输”的代价是很大的,但如果“赌赢”的概率非常高,则从中获得的性能提升就会非常大,值得一试。 一般来说,JVM和JIT编译器所做的典型假设包括以下几点： 虚拟方法不会被覆盖。由于某个虚拟方法只存在一种实现,就可以将之优化为一个直接调用。 浮点数的值永远不会是NaN。大部分情况下,可以使用硬件指令来替换对本地浮点数函数库的调用。 某些try语句块中几乎不会抛出异常。因此,可以将catch语句块中的代码作为冷方法对待。 对于大多数三角函数来说,硬件指令fsin都能够达到精度要求。如果真的达不到,就抛出异常,调用本地浮点数函数库完成计算。 锁竞争并不会太激烈,初期可以使用自旋锁(spinlock)替代。 锁可能会周期性地被同一个线程获取和释放,所以,可以将对锁的重复获取操作和重复释放操作直接省略掉。 深入JIT编译器优化字节码 有些时候,对Java源代码做优化会适得其反。绝大部分写出可读性很差的代码的人都声称是为了优化性能,其实就是照着一些基准测试报告的结论写代码,而这些性能测试往往只涉及了字节码解释执行,没有经过JIT编译器优化,所以并不能代表应用程序在运行时的真实表现。例如,某个服务器端应用程序中包含了大量对数组元素的迭代访问操作,程序员参考了那些报告中的结论,没有设置循环条件,而是写一个无限for循环,置于try语句块中,并在catch语句块中捕获ArrayIndexOutOfBoundsException异常。这种糟糕的写法不仅使代码可读性极差,而且一旦运行时对之优化编译的话,其执行效率反而比普通循环方式低得多。原因在于,JVM的基本假设之一就是“异常是很少发生的”。基于这种假设,JVM会做一些相关优化,所以当真的发生异常时,处理成本就很高。 代码流水线代码生成概述 在生成优化代码时,如何分配寄存器非常重要。编译器教材上都将寄存器分配问题作为图的着色问题处理,这是因为同时用到的两个变量不能共享同一个寄存器,从这点上讲,与着色问题相同。同时使用的多个变量可以用图中相连接的节点来表示,这样,寄存器分配问题就可以被抽象为“如何为图中的节点着色,5能使相连节点有不同的颜色”。这里可用颜色的数量等于指定平台上可用寄存器的数量。不过,遗憾的是,从计算复杂性上讲,着色问题是NP-hard的,也就是说现在还没有一个高效的算法(指可以在多项式时间内完成计算)能解决这个问题。但是,着色问题可以在线性对数时间内给出近似解,因此大多数编译器都使用着色算法的某个变种来处理寄存器分配问题。 自适应内存管理堆管理基础对象的分配与释放一般来说,为对象分配内存时,并不会直接在堆上划分内存,而是先在线程局部缓冲(thread local buffer)或其他类似的结构中找地方放置对象,然后随着应用程序的运行、新对象的不断分配,垃圾回收逐次执行,这些对象可能最终会被提升到堆中保存,也有可能会当作垃圾被释放掉。 为了能够在堆中给新创建的对象找一个合适的位置,内存管理系统必须知道堆中有哪些地方是空闲的,即还没有存活对象占用。内存管理系统使用空闲列表(free list)—串联起内存中可用内存块的链表,来管理内存中可用的空闲区域,并按照某个维度的优先级排序。 在空闲列表中搜索足够存储新对象的空闲块时,可以选择大小最适合的空闲块,也可以选择第一个放得下的空闲块。这其中会用到几种不同的算法去实现,各有优劣,后文会详细讨论。 垃圾回收算法在后文中,根集合(root set)专指上述搜索算法的初始输入集合,即开始执行引用跟踪时的存活对象集合。一般情况下,根集合中包括了因为执行垃圾回收而暂停的应用程序的当前栈帧中所有的对象,包含了可以从当前线程上下文的用户栈和寄存器中能得到的所有信息。此外,根集合中还包含全局数据,例如类的静态属性。简单来说就是,根集合中包含了所有无须跟踪引用就可以得到的对象。 Java使用的是准确式垃圾回收器(exact garbage collector),可以将对象指针类型数据和其他类型的数据区分开,只需要将元数据信息告知垃圾回收器即可,这些元数据信息,一般可以从Java方法的代码中得到。 近些年,使用信号来暂停线程的方式受到颇多争议。实践发现,在某些操作系统上,尤以Linux为例,应用程序对信号的使用和测试很不到位,还有一些第三方的本地库不遵守信号约定,导致信号冲突等事件的发生。因此,与信号相关的外部依赖已经不再可靠。 分代垃圾回收事实上,将堆划分为两个或多个称为代(generation)的空间,并分别存放具有不同长度生命周期的对象,可以提升垃圾回收的执行效率。在JRockit中,新创建(young)的对象存放在称为新生代(nursery)的空间中,一般来说,它的大小会比老年代(old collections)小很多,随着垃圾回收的重复执行,生命周期较长的对象会被提升(promote)到老年代中。因此,新生代垃圾回收和老年代垃圾回收两种不同的垃圾回收方式应运而生,分别用于对各自空间中的对象执行垃圾回收。 新生代垃圾回收的速度比老年代快几个数量级,即使新生代垃圾回收的频率更高,执行效率也仍然比老年代垃圾回收强,这是因为大多数对象的生命周期都很短,根本无须提升到老年代。理想情况下,新生代垃圾回收可以大大提升系统的吞吐量,并消除潜在的内存碎片。 写屏障在实现分代式垃圾回收时,大部分JVM都是用名为写屏障(write barrier)的技术来记录执行垃圾回收时需要遍历堆的哪些部分。当对象A指向对象B时,即对象B成为对象A的属性的值时,就会触发写屏障,在完成属性域赋值后执行一些辅助操作。 写屏障的传统实现方式是将堆划分成多个小的连续空间(例如每块512字节),每块空间称为卡片(card),于是,堆被映射为一个粗粒度的卡表(card table)。当Java应用程序将某个对象赋值给对象引用时,会通过写屏障设置脏标志位(dirty bit),将该对象所在的卡片标记为脏。 这样,遍历从老年代指向新生代的引用时间得以缩短,垃圾回收器在做新生代垃圾回收时只需要检查老年代中被标记为脏的卡片所对应的内存区域即可。 JRockit中的垃圾回收老年代垃圾回收JRockit不仅将卡表应用于分代式垃圾回收,还用在并发标记阶段结束时的清理工作,避免搜索整个存活对象图。这是因为JRockit需要找出在执行并发标记操作时,应用程序又创建了哪些对象。修改引用关系时通过写屏障可以更新卡表,存活对象图中的每个区域使用卡表中的一个卡片表示,卡片的状态可以是干净或者脏,有新对象创建或者对象引用关系修改了的卡片会被标记为脏。在并发标记阶段结束时,垃圾回收器只需要检查那些标记为脏的卡片所对应的堆中区域即可,这样就可以找到在并发标记期间新创建的和被更新过引用关系的对象 性能与伸缩性线程局部分配在JRockit中,使用了名为线程局部分配(thread local allocation)的技术来大幅加速对象的分配过程。正常情况下,在线程内的缓冲区中为对象分配内存要比直接在需要同步操作的堆上分配内存快得多。垃圾回收器在堆上直接分配内存时是需要对整个堆加锁的,对于多线程竞争激烈的应用程序来说,这将会是一场灾难。因此,如果每个Java线程能够有一块局部对象缓冲区那么绝大部分的对象分配操作只需要移动一下指针即可完成,在大多数硬件平台上,只需要一条汇编指令就行了。这块转为分配对象而保留的区域,就称为线程局部缓冲区(thread local area,TLA)。 为了更好地利用缓存,达到更高的性能,一般情况下,TLA的大小介于16KB到128KB之间,当然,也可以通过命令行参数显式指定。当TLA被填满时,垃圾回收器会将TLA中的内容提升到堆中。因此,可以将TLA看作是线程中的新生代内存空间。 当Java源代码中有new操作符,并且JIT编译器对内存分配执行高级优化之后,内存分配的伪代码如下所示： 1234567891011object allocateNewobject(Class objectclass)&#123; Thread current getcurrentThread(): int objectSize=alignedSize(objectclass) if(current.nextTLAOffset+objectSize&gt; TLA_SIZE)&#123; current.promoteTLAToHeap();//慢,而且是同步操作 current.nextTLAOffset=0; &#125; Object ptr= current.TLAStart+current.nextTLAOffset: current.nextTLAOffset + objectSize; return ptr: &#125; 为了说明内存分配问題,在上面的伪代码中省略了很多其他关联操作。例如如果待分配的对象非常大,超过了某个阈值,或对象太大导致无法存放在TLA中,则会直接在堆中为对象分配内存。 NUMA架构NUMA(non-uniform memory access,非统一内存访问模型)架构的出现为垃圾回收带来了更多挑战。在NUMA架构下,不同的处理器核心通常访问各自的内存地址空间,这是为了避免因多个CPU核心访问同一内存地址造成的总线延迟。每个CPU核心都配有专用的内存和总线因此CPU核心在访问其专有内存时速度很快,而要访问相邻CPU核心的内存时就会相对慢些,CPU核心相距越远,访问速度越慢(也依赖于具体配置)传统上,多核CPU是按照UMA(uniform memory access,统一内存访问模型)架构运行的,所有的CPU核心按照统一的模式无差别地访问所有内存。 为了更好地利用NUMA架构,垃圾回收器线程的组织结构应该做相应的调整。如果某个CPU核心正在运行标记线程,那么该线程所要访问的那部分堆内存最好能够放置在该CPU的专有内存中,这样才能发挥NUMA架构的最大威力。在最坏情况下,如果标记线程所要访问的对象位于其他NUMA节点的专有内存中,这时垃圾回收器通常需要一个启发式对象移动算法。这是为了保证使用时间上相近的对象在存储位置上也能相近,如果这个算法能够正确工作,还是可以带来不小的性能提升的。这里所面临的主要问题是如何避免对象在不同NUMA节点的专有内存中重复移动。理论上,自适应运行时系统应该可以很好地处理这个问题。 大内存页内存分配是通过操作系统及其所使用的页表完成的。操作系统将物理内存划分成多个页来管理,从操作系统层面讲,页是实际分配内存的最小单位。传统上,页的大小是以4KB为基本单位划分的,页操作对进程来说是透明的,进程所使用的是虚拟地址空间,并非真正的物理地址。为了便于将虚拟页面转换为实际的物理内存地址,可使用名为旁路转换缓冲(translation lookaside buffer,TLB)的缓存来加速地址的转换操作。从实现上看,如果页面的容量非常小的话,会导致频繁出现旁路转换缓冲丢失的情况。 修复这个问题的一种方法就是将页面的容量调大几个数量级,例如以MB为基本单位。现代操作系统普遍倾向于支持这种大内存页机制。 很明显,当多个进程分别在各自的寻址空间中分配内存,而页面的容量又比较大时,随着使用的页面数量越来越多,碎片化的问题就愈发严重,像进程要分配的内存比页面容量稍微大一点的情况,就会浪费很多存储空间。对于在进程内自己管理内存分配回收、并有大量内存空间可用的运行时来说,这不算什么问题,因为运行时可以通过抽象出不同大小的虚拟页面来解决。 通常情况下,对于那些内存分配和回收频繁的应用程序来说,使用大内存页可以使系统的整体性能至少提升10%。 JRockit对大内存页有很好的支持。 近实时垃圾回收JRockit Real Time低延迟的代价是垃圾回收整体时间的延长。相比于并行垃圾回收,在程序运行的同时并发垃圾回收的难度更大,而频繁中断垃圾回收则可能带来更多的麻烦。事实上,这并非什么大问题,因为大多数使用JRockit Real Time的用户更关心系统的可预测性,而不是减少垃圾回收的总体时间。大多数用户认为暂停时间的突然增长比垃圾回收总体时间的延长更具危害性。 软实时的有效性软实时是JRockit Real Time的核心机制。但非确定性系统如何提供指定程度的确定性,例如像垃圾回收器这样的系统如何保证应用程序的暂停时间不会超过某个阈值?严格来说,无法提供这样的保证,但由于这样的极端案例很少,所以也就无关紧要了。 当然,没有什么万全之策,确实存在无法保证暂停时间的场景。但实践证明,对于那些堆中存活对象约占30%-50%的应用程序来说, JRockit Real Time的表现可以满足服务需要,而且随着JRockit Real Time各个版本的发行,30%-50%这个阈值在不断提升,可支持的暂停时间阈值则不断降低。 工作原理 高效的并行执行 细分垃圾回收过程,将之变成几个可回滚、可中断的子任务(work packet) 高效的启发式算法 事实上,实现低延迟的关键仍是尽可能多让Java应用程序运行,保持堆的使用率和碎片化程度在一个较低的水平。在这一点上, JRockit Real Time使用的是贪心策略,即尽可能推迟STW式的垃圾回收操作,希望问题能够由应用程序自身解决,或者能够减少不得不执行STW式操作的情况,最好在具体执行的时候需要处理的对象也尽可能少一些。 JRockit Real Time中,垃圾回收器的工作被划分为几个子任务。如果在执行其中某个子任务时(例如整理堆中的某一部分内存),应用程序的暂停时间超过了阈值,那么就放弃该子任务恢复应用程序的执行。用户根据业务需要指定可用于完成垃圾回收的总体时间,有些时候,某些子任务已经完成,但没有足够的时间完成整个垃圾回收工作,这时为了保证应用程序的运行,不得不废弃还未完成的子任务,待到下次垃圾回收的时候再重新执行,指定的响应时间越短,则废弃的子任务可能越多。 前面介绍过的标记阶段的工作比较容易调整,可以与应用程序并发执行。但清理和整理阶段则需要暂停应用程序线程(STW)。幸运的是,标记阶段会占到垃圾回收总体时间的90%。如果暂停应用程序的时间过长,则不得不终止当前垃圾回收任务,重新并发执行,期望问题可以自动解决。之所以将垃圾回收划分为几个子任务就是为了便于这一目标的实现。 内存操作相关API析构方法 Java中的析构函数的设计就是一个失误,应避免使用。 这不仅仅是我们的意见,也是Java社区的一致意见。 JVM的行为差异对于JVM来说,一定谨记,编程语言只能提醒垃圾回收器工作。就Java而言,在设计上它本身并不能精确控制内存系统。例如,假设两个ⅣM厂商所实现软引用在缓存中具有相同的存活时间,这本就是不切实际的。 另外一个问题就是大量用户对System.gc()方法的错误使用。System.gc()方法仅仅是提醒运行时“现在可以做垃圾回收了”。在某些JVM实现中,频繁调用该方法导致了频繁的垃圾回收操作,而在某些JVM实现中,大部分时间忽略了该调用。 我过去任职为性能顾问期间,多次看到该方法被滥用。很多时候,只是去掉对 System.gc方法的几次调用就可以大幅提升性能,这也是 JRock中会有命令行参数-xx:AllowSystemGC=False来禁用System,gc方法的原因。 陷阱与伪优化部分开发人员在写代码时,有时会写一些“经过优化的”的代码,期望可以帮助完成垃圾回收的工作,但实际上,这只是他们的错觉。记住,过早优化是万恶之源。就Java来说,很难在语言层面控制垃圾回收的行为。这里的主要问题时,开发人员误以为垃圾回收器有固定的运行模式,并妄图去控制它。 除了垃圾回收外,对象池(object poll)也是Java中常见的伪优化(false optimization)。有人认为,保留一个存活对象池来重新使用已创建的对象可以提升垃圾回收的性能,但实际上,对象池不仅增加了应用程序的复杂度,还很容易出错。对于现代垃圾收集器来说,使用java.lang.ref.Reference系列类实现缓存,或者直接将无用对象的引用置为null就好了,不用多操心。 事实上,基于现代VM,如果能够合理利用书本上的技巧,例如正确使用java.lang.ref.Reference系列类,注意Java的动态特性,完全可以写出运行良好的应用程序。如果应用程序真的有实时性要求,那么一开始就不该用Java编写,而应该使用那些由程序员手动控制内存的静态编程语言来实现应用程序。 JRockit中的内存管理需要注意的是,花大力气鼓捣JVM参数并不一定会使应用程序性能有多么大的提升,而且反而可能会干扰JVM的正常运行。 线程与同步基本概念每个对象都持有与同步操作相关的信息,例如当前对象是否作为锁使用,以及锁的具体实现等。一般情况下,为了便于快速访问,这些信息被保存在每个对象的对象头的锁字(lock word)中。JRockit使用锁字中的一些位来存储垃圾回收状态信息,虽然其中包含了垃圾回收信息,但是本书还是称之为锁字。 对象头还包含了指向类型信息的指针,在 JRockit中,这称为类块(class block)下图是 JRockit中Java对象在不同的CPU平台上的内存布局。为了节省内存,并加速解引用操作,对象头中所有字的长度是32位。类块是一个32位的指针,指向另一个外部结构,该结构包含了当前对象的类型信息和虚分派表(virtual dispatch table)等信息。 就目前来看,在绝大部分JVM(包括JRockit)中,对象头是使用两个32位长的字来表示的。在JRockit中,偏移为0的对象指针指向当前对象的类型信息,接下来是4字节的锁字。在SPARC平台上,对象头的布局刚好反过来,因为在使用原子指令操作指针时,如果没有偏移的话,效率会更高。与锁字不同,类块并不为原子操作所使用,因此在SPARC平台上,类块被放在锁字后面。 原子操作(atomic operation)是指全部执行或全部不执行的本地指令。当原子指令全部执行时,其操作结果需要对所有潜在访问者可见。 原子操作用于读写锁字,具有排他性,这是实现JVM中同步块的基础。 难以调试 死锁是指两个线程都在等待对方释放自己所需的资源,结果导致两个线程都进入休眠状态。很明显,它们再也醒不过来了。活锁的概念与死锁类似,区别在于线程在竟争时会采取主动操作,但无法获取锁。这就像两个人面对面前进,在一个很窄的走廊相遇,为了能继续前进,他们都向侧面移动,但由于移动的方向相反导致还是无法前进。 Java APIsynchronized关键字在Java中,关键字synchronized用于定义一个临界区,既可以是一段代码块,也可以是个完整的方法,如下所示: 123public synchronized void setGadget(Gadget g)&#123; this.gadget = g;&#125; 上面的方法定义中包含synchronized关键字,因此每次只能有一个线程修改给定对象的gadget域。 在同步方法中,监视器对象是隐式的,即当前对象this,而对静态同步方法来说,监视器对象是当前对象的类对象。上面的示例代码与下面的代码是等效的： 12345public void setGadget(Gadget g)&#123; synchronized(this)&#123; this.gadget = g; &#125;&#125; java.lang.Thread类Java中的线程也有优先级概念,但是否真的起作用取决于JVM的具体实现。setPriority方法用于设置线程的优先级,提示JVM该线程更加重要或不怎么重要。当然,对于大多数JVM来说,显式地修改线程优先级没什么大帮助。当运行时“有更好的方案”时, JRockit JVM甚至会忽略Java线程的优先级。 正在运行的线程可以通过调用yield方法主动放弃剩余的时间片,以便其他线程运行,自身休眠(调用wait方法)或等待其他线程结束再运行(调用join方法)。 volatile 关键字在多线程环境下,对某个属性域或内存地址进行写操作后,其他正在运行的线程未必能立即看到这个结果。在某些场景中,要求所有线程在执行时需要得知某个属性最新的值,为此,Java提供了关键字volatile来解决此问题。 使用volatile修饰属性后,可以保证对该属性域的写操作会直接作用到内存中。原本,数据操作仅仅将数据写到CPU缓存中,过一会再写到内存中,正因如此,在同一个属性域上,不同的线程可能看到不同的值。目前,JVM在实现volatile关键字时,是通过在写属性操作后插入内存屏障代码来实现的,只不过这种方法有一点性能损耗。 人们常常难以理解“为什么不同的线程会在同一个属性域上看到不同的值”。一般来说,目前的机器的内存模型已经足够强,或者应用程序的本身结构就不容易使非volatile属性出现这个问题。但是,考虑到JIT优化编译器可能会对程序做较大改动,如果开发人员不留心的话,还是会出现问题的。下面的示例代码解释了在Java程序中,为什么内存语义如此重要,尤其是当问题还没表现出来的时候。 1234567891011public class My Thread extends Thread&#123; private volatile boolean finished; public void run()&#123; while(!finished)&#123; // &#125; &#125; public void signalDone()&#123; this.finished = true &#125;&#125; 如果定义变量finished时没有加上volatile关键字,那么在理论上,JIT编译器在优化时,可能会将之修改为只在循环开始前加载一次finished的值,但这就改变了代码原本的含义如果finished的值是false,那么程序就会陷入无限循环,即使其他线程调用了signalDone方法也没用。Java语言规范指明,如果编译器认为合适的话,可以为非 volatile变量在线程内创建副本以便后续使用。 由于一般会使用内存屏障来实现volatile关键字的语义,会导致CPU缓存失效,降低应用程序整体性能,使用的时候要谨慎。 Java中线程与同步机制的实现Java内存模型现在CPU架构中,普遍使用了数据缓存机制以大幅提升CPU对数据的读写速度,减轻处理器总线的竞争程度。正如所有的缓存系统一样,这里也存在一致性问题,对于多处理器系统来说尤其重要,因为多个处理器有可能同时访问内存中同一位置的数据内存模型定义了不同的CPU,在同时访问内存中同一位置时,是否会看到相同的值的情况。 强内存模型(例如x86平台)是指,当某个CPU修改了某个内存位置的值后,其他的CPU几乎自动就可以看到这个刚刚保存的值。在这种内存模型之下,内存写操作的执行顺序与代码中的排列顺序相同。弱内存模型(例如IA-64平台)是指,当某个CPU修改了某个内存位置的值后其他的CPU不一定可以看到这个刚刚保存的值(除非CPU在执行写操作时附有特殊的内存屏障类指令),更普遍低说,所有由Java程序引起的内存访问都应该对其他所有CPU可见,但事实上却不能保证立即可见。 同步的实现原生机制从计算机最底层CPU结构来说,同步是使用原子指令实现的,各个平台的具体实现可能有所不同。以x86平台为例,它使用了专门的锁前缀(lock prefix)来实现多处理器环境中指令的原子性。 在大多数CPU架构中,标准指令(例如加法和减法指令)都可以实现为原子指令。 在微架构( micro- architecture)层面,原子指令的执行方式在各个平台上不尽相同。一般情况下,它会暂停CPU流水线的指令分派,直到所有已有的指令都完成执行,并将操作结果刷入到内存中。此外,该CPU还会阻止其他CPU对相关缓存行的访问,直到该原子指令结束执行。在现代x86硬件平台上,如果屏障指令(fence instruction)中断了比较复杂的指令执行,则该原子指令可能需要等上很多个时钟周期才能完成执行。因此,不仅是过多的临界区会影响系统性能锁的具体实现也会影响性能,当频繁对较小的临界区执行加锁、解锁操作时,性能损耗更是巨大。 同步在字节码中的实现Java字节码中有两条用于实现同步的指令,分别是monitorenter和monitorexit,它们都会从执行栈中弹出一个对象作为其操作数。使用javac编译源代码时,若遇到显式使用监视器对象的同步代码,则为之生成相应的monitorenter指令和monitorexit指令。 对于线程与同步的优化锁膨胀与锁收缩 默认情况下, JRockit使用一个小的自旋锁来实现刚膨胀的胖锁,只持续很短的时间。乍看之下,这不太符合常理,但这么做确实是很有益处的。如果锁的竟争确实非常激烈,而导致线程长时间自旋的话,可以使用命令行参数-XX:UseFatSpin=false禁用此方式。作为胖锁的一部分,自旋锁也可以利用自适应运行时获取到的反馈信息,这部分功能默认是禁用的,可以使用命令行参数-XX:UseAdaptiveFatSpin=true来开启。 延迟解锁如何分析很多线程局部的解锁,以及重新加锁的操作只会降低程序执行效率?这是否是程序运行的常态?运行时是否可以假设每个单独的解锁操作实际上都是不必要的? 如果某个锁每次被释放后又立刻都被同一个线程获取,则运行时可以做上述假设。但只要有另外某个线程试图获取这个看起来像是未被加锁的监视器对象(这种情况是符合语义的),这种假设就不再成立了。这时为了使这个监视器对象看起来像是一切正常,原本持有该监视器对象的线程需要强行释放该锁。这种实现方式称为延迟解锁,在某些描述中也称为偏向锁(biased locking)。 即使某个锁完全没有竞争,执行加锁和解锁操作的开销仍旧比什么都不做要大。而使用原子指令会使该指令周围的Java代码都产生额外的执行开销。 从以上可以看出,假设大部分锁都只在线程局部起作用而不会出现竞争情况,是有道理的。在这种情况下,使用延迟解锁的优化方式可以提升系统性能。当然,天下没有免费的午餐,如果某个线程试图获取某个已经延迟解锁优化的监视器对象,这时的执行开销会被直接获取普通监视器对象大得多,因为这个看似未加锁的监视器对象必须要先被强行释放掉因此,不能一直假设解锁操作是不必要的,需要对不同的运行时行为做针对性的优化。 1.实现 实现延迟解锁的语义其实很简单。 实现 monitorenter指令。 如果对象是未锁定的,则加锁成功的线程将继续持有该锁,并标记该对象为延迟加锁的。 如果对象已经被标记为延迟加锁的： 如果对象是被同一个线程加锁的,则什么也不做(大体上是一个递归锁) 如果对象是被另一个线程加锁的,则暂停该线程对锁的持有状态,检查该对象真实的加锁状态,即是已加锁的还是未加锁的,这一步操作代价高昂,需要遍历调用栈。如果对象是已加锁的,则将该锁转换为瘦锁,否则强制释放该锁,以便可以被新线程获取到。 实现monitorexit指令:如果是延迟加锁的对象,则什么也不做,保留其已加锁状态,即执行延迟解锁。 为了能解除线程对锁的持有状态,必须要先暂停该线程的执行,这个操作有不小的开销。在释放锁之后,锁的实际状态会通过检查线程栈中的锁符号来确定。延迟解锁使用自己的锁符号,以表示“该对象是被延迟锁定的”。 如果延迟锁定的对象从来也没有被撤销过,即所有的锁都只在线程局部内发挥作用,那么使用延迟锁定就可以大幅提升系统性能。但在实际应用中,如果我们的假设不成立,运行时就不得不一遍又一遍地释放已经被延迟加锁的对象,这种性能消耗实在承受不起。因此,运行时需要记录下监视器对象被不同线程获取到的次数,这部分信息存储在监视器对象的锁字中,称为转移位(transfer bit)。 如果监视器对象在不同的线程之间转移的次数过多,那么该对象、其类对象或者其类的所有实例都可能会被禁用延迟加锁,只会使用标准的胖锁和瘦锁来处理加锁或解锁操作。 正如之前介绍过的,对象首先是未加锁状态的,然后线程T1执行monitorenter指令,使之进入延迟加锁状态。但如果线程T1在该对象上执行了monitorexit指令,这时系统会假装已经解锁了,但实际上仍是锁定状态,锁对象的锁字中仍记录着线程T1的线程ID。在此之后线程T1如果再执行加锁操作,就不用再执行相关操作了。 如果另一个线程T2试图获取同一个锁,则之前所做“该锁绝大部分被线T1程使用”的假设不再成立,会受到性能惩罚,将锁字中的线程ID由线程T1的ID替换为线程T2的。如果这情况经常出现,那么可能会禁用该对象作为延迟锁,并将该对象作为普通的瘦锁使用。 陷阱与伪优化Thread.stop、Thread.resume和Thread.suspend永远不要使用Thread.stop方法、Thread.resume方法或Thread.suspend方法并小心处理使用这些方法的历史遗留代码。 普遍建议使用wait方法、notify方法或volatile变量来做线程间的同步处理。 双检查锁如果对内存模型和CPU架构缺乏理解的话,即使使用平遇到问题。以下面的代码为例,其目的是实现单例模式。 123456789public class Gadget Holder&#123; private Gadget theGadget; public synchronized Gadget cetGadget()&#123; if (this.theGadget == null)&#123; this.theGadget = new Gadget(); &#125; return this.theGadget; &#125;&#125; 上面的代码是线程安全的,因为getGadget方法是同步但当Gadget类的构造函数已经执行过一次之后,再执行同优化性能,将之改造为下面的代码。 12345678910public Gadget getGadget()&#123; if (this.theGadget == null)&#123; synchronized(this)&#123; if(this.theGadget == null))&#123; this.theGadget = new Gadget(); &#125; &#125; &#125; return this.theGadget;&#125; 上面的代码使用了一个看起来很“聪明”的技巧,如果行同步操作,而是直接返回已有的对象;如果对象还未创建值。这样可以保证“线程安全”。 上述代码就是所谓的双检查锁(double checked locking),下面分析一下这段代码的问题。假设某个线程经过内层的空值检查,开始初始化theGadget字段的值,该线程需要为新对象分配内存,并对theGadget字段赋值。可是,这一系列操作并不是原子的,且执行顺序无法保证。如果在此时正好发生线程上下文切换,则另一个线程看到的theGadget字段的值可能是未经完整初始化的,有可能会导致外层的控制检查失效,并返回这个未经完整初始化的对象。不仅仅是创建对象可能会出问题,处理其他类型数据时也要小心。例如,在32位平台上,写入一个long型数据通常需要执行两次32位数据的写操作,而写入int数据则无此顾虑。 上述问题可以通过将 theGadget字段声明为 volatile来解决(注意,只在新版本的内存模型下才有效),增加的执行开销尽管比使用synchronized方法的小,但还是有的。如果不确定当前版本的内存模型是否实现正确,不要使用双检查锁。网上有很多文章介绍了为什么不应该使用双检查锁,不仅限于Java,其他语言也是。 双检查锁的危险之处在于,在强内存模型下,它很少会使程序崩溃。Intel IA-64平台就是个典型示例,其弱内存模型臭名远扬,原本好好运行的Java应用程序却出现故障。如果某个应用程序在x86平台运行良好,在x64平台却出问题,人们很容易怀疑是JVM的bug,却忽视了有可能是Java应用程序自身的问题。 使用静态属性来实现单例模式可以实现同样的语义,而无须使用双检查锁,如下所示: 123public class GadgetMaker&#123; public static Gadget theGadget= new Gadget();&#125; Java语言保证类的初始化是原子操作, GadgetMaker类中没有其他的域,因此,在首次主动使用该类时会自动创建 Gadget类的实例。并赋值给theGadget字段。这种方法在新旧两种内存模型下均可正常工作。 总之,使用Java做并行程序开发有很多需要小心的地方,如果能够正确理解Java内存模型那么是可以避开这些陷阱的。开发人员往往不太关心当前的硬件架构,但如果不能理解Java内存模型,迟早会搬起石头砸自己的脚。 基准测试与性能调优wait方法、notify方法与胖锁Java并非万能的Java是一门强大的通用编程语言,因其友好的语义和内建的内的开发进度,但Java不是万能的,这里来谈谈不宜使用Java解决的场景： 要开发一个有近实时性要求的电信应用程序,并且其中会有其中会有成千上万的线程并发执行。 应用程序的数据库层所返回的数据经常是20MB的字节数组。 应用程序性能和行为的确定性,完全依赖于底层操作系统的调度器，即使调度器有微小变化也会对应用程序性能产生较大影响。 开发设备驱动程序。 使用 C/Fortran/COBOL等语言开发的历史遗留代码太多,目前团队手中还没有好用的工具可以将这些代码转换为Java代码。 除了上面的示例外,还有其他很多场景不适宜使用Java。通过JvM对底层操作系统的抽象Java实现了“一次编写,到处运行”,也因此受到了广泛关注。但夸大一点说,ANSI C也能做到这一点,只不过在编写源代码时,要花很多精力来应对可移植性问题。因此要结合实际场景选择合适的工具。Java是好用,但也不要滥用。 PDF书籍下载地址：https://github.com/jiankunking/books-recommendation/tree/master/Java]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>读书笔记</tag>
        <tag>GC</tag>
        <tag>JVM</tag>
        <tag>JRockit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解JVM＆G1GC 笔记]]></title>
    <url>%2Fjava-jvm-gc-g1-note.html</url>
    <content type="text"><![CDATA[本文整理自：《深入理解JVM＆G1GC》 作者：周明耀本书很一般出版时间：2017-06-01 JVM GC基本知识引言G1内部主要有四个操作阶段： 年轻代回收(A Young Collection) 运行在后台的并行循环(A Background,Concurrent Cycle) 混合回收(A Mixed Collection) 全量回收(A Full GC) 基本术语Java相关术语Interned Strings在Java语言中有8种基本类型和一种比较特殊的类型 String这些类型为了使它们在运行过程中速度更快、更节省内存,都提供了一种常量池的概念。常量池就类似一个Java系统级别提供的缓存。8种基本类型的常量池都是系统协调的,String类型的常量池比较特殊。它的主要使用方法有两种。 直接使用双引号声明出来的String对象会直接存储在常量池中 如果不是用双引号声明的String对象,可以使用String提供的Intern方法。 intern方法会从字符串常量池中查询当前字符串是否存在,若不存在就会将当前字符串放入常量池中。 通俗点讲,Interned String就是确保字符串在内存里只有一份拷贝,这样可以节约内存空间,加快字符串操作任务的执行速度。注意,这个值会被存放在字符串内部池(String Intern Pool)。 Java 7中Oracle的工程师对字符串池的逻辑做了很大的改变,即将字符串池的位置调整到Java堆内,这个改动意味着你再也不会被固定的内存空间限制了。所有的字符串都保存在堆(Heap)中,和其他普通对象一样,这样可以让你在进行调优应用时仅需要调整堆大小就可以了。字符串池概念原本使用得比较多,但是这个改动使得我们有足够的理由让我们重新考虑在Java7中使用 String intern()。 Java 对象头在HotSpot虚拟机中,对象在内存中的布局可以分成对象头、实例数据、对齐填充三部分。 对象头:它主要包括对象自身的运行行元数据,比如哈希码、GC分代年龄、锁状态标志等,同时还包含一个类型指针,指向类元数据,表明该对象所属的类型。 实例数据:它是对象真正存储的有效信息,包括程序代码中定义的各种类型的字段(包括从父类继承下来的和本身拥有的字段)。 对齐填充:它不是必要存在的,仅仅起着占位符的作用 对象头大小在32位HotSpot VM和64位 HotSpot VM之间是不一样的,对象头在32位系统上占用8yte,在64位系统上占用16yte。我们可以通过Java对象布局工具获取头大小,这个工具简称为JOL。 G1 涉及术语MetaspaceJDK8 HotSpot JVM使用本地内存来存储类元数据信息并称为元空间(Metaspace)。 默认情况下,大部分类元数据都在本地内存中分配,类元数据只受可用的本地内存限制(容量取决于是32位或是64位操作系统的可用虚拟内存大小)。新参数(MaxMetaspace Size)用于限制本地内存分配给类元数据的大小。如果没有指定这个参数,元空间会在运行时根据需要动态调整。 一般情况下,适时地监控和调整元空间对于减小垃圾回收频率和减少延时是很有必要的。持续的元空间垃圾回收情况如果频繁发生,说明可能存在类、类加载器导致的内存泄漏或是大小设置不合适。 G1 GC与Metaspace相关的选项如下： -XX:MetaspaceSize:初始化元空间的大小(默认12 Mbytes在32bit client VM and 16 Mbytes在32bit server VM,在64 bit VM上会更大些)。 -XX:MaxMetaspaceSize:最大元空间的大小(默认本地内存)。 -XX:MinMetaspaceFreeRatio:扩大空间的最小比率,当GC后,内存占用超过这一比率,就会扩大空间。 -XX:MaxMetaspaceFreeRatio:缩小空间的最小比率,当GC后,内存占用低于这一比率,就会缩小空间。 Mixed GC Event即混合GC事件,在这个事件内部,所有的年轻代Region和一部分老年代Region一起被回收。混合GC事件一定是跟在Minor GC之后的,并且混合GC只有在存活对象元数据存在的情况下才会触发。 ReclaimableGl GC为了能够回收,创建了一系列专门用于存放可回收对象的Region。这些Region都在个链表队列里面,这个队列只包含存活率小于-XX: GIMixedGCLiveThresholdPercent(默认85%)的Region。Region的值除以整个Java堆区,如果大于-XX:G1HeapWastePercen(默认5%),则启动回收机制。 Rset全称Remembered Set,简称Rset,即跟踪指向某个堆区(Region)内的对象引用。 在标记存活对象时,G1使用RememberSet的概念,将每个分区外指向分区内的引用记录在该分区的RememberSet中,避免了对整个Heap的扫描,使得各个分区的GC更加独立。堆内存中的每个区都有一个RSet,Rset的作用是让堆区能并行独立地进行垃圾集合。RSet所占用的JVM内存小于总大小的5%。在这样的背景下,可以看出G1GC大大提高了触发 Full Gc时的Heap占用率,同时也使得 Minor GC的暂停时间更加可控,对于内存较大的环境非常友好。 G1 GC引入了一些新的选项。G1RSetUpdatingPauseTimePercent设置STW阶段(独占阶段)为G1收集器指定更新RememberSet的时间占总STW时间的期望比例,默认为10。而G1ConcRefinementThreads则是在程序运行时维护RememberSet的线程数目。通过对这两个值的对应调整,我们可以把STW阶段的RememberSet更新工作压力更多地移到并行阶段。 CSet全称Collection Set,简称CSet,即收集集合,保存一次GC中将执行垃圾回收的区间(Region)。GC时在CSet中的所有存活数据(Live Data)都会被转移(复制/移动)。集合中的堆区可以是Eden, Survivor和/或Old Generation。CSets所占用的JVM内存小于总大小的1%。 从这里可以知道,实际上CSet相当于一个大圈,里面包含了很多的小圈(Rset),这些圈圈都是需要被回收的信息。这样可以把CSet比作垃圾场,RSet是垃圾场里面一个个绿色的可回收垃圾桶。 PLAB全称为Promotion Local Allocation Buffers,它被用于年轻代回收。PLAB的作用是避免多线程竞争相同的数据,处理方式是每个线程拥有独立的PLAB,用于针对幸存者和老年空间。当应用开启的线程较多时,最好使用-XX:-ResizePlaB来关闭PLAB()的大小调整,以避免大量的线程通信所导致的性能下降。 TLAB全称为Thread Local Allocation Buffers,即线程本地分配缓存,是一个线程专用的内存分配区域。 总的来说,TLAB是为了加速对象分配而生的。由于对象一般会分配在堆上,而堆是全局共享的。因此在同一时间,可能会有多个线程在堆上申请空间。因此,每一次对象分配都必须要进行同步,而在竞争激烈的场合分配的效率又会进一步下降。考虑到对象分配几乎是Java最最常用的操作,所以JVM就使用了TLAB这种线程专属的区间来避免多线程冲突,提高对象分配的效率。TLAB本身占用了Eden区的空间,即JVM会为每一个Java线程分配一块TLAB空间。 对于G1 GC来说,TLAB是Eden的一个Region,被一个单一线程用于分配资源。主要用途是让一个线程通过栈操作方式独享内存空间,用于对象分配,这样比多个线程之间共享资源要快很多。如果每个线程的分配内存不够,那么它会去全局内存池申请新的内存。这样也就是说,如果TLAB值设置过小,容易造成频繁申请,也就会造成GC性能下降。反之,如果设置过大,会造成TLAB使用不完,也就是说内存浪费。 Region从字面上来说, Region表示一个区域,每个区域里面的字母代表不同的分代内存空间类型(如[E]Eden,[O]Old,[S]Survivor),空白的区块不属于任何一个分区。G1可以在需要的时候任意指定这个区域属于Eden或是O区之类的。 Ergonomics Heuristic Decision在很多英文书里都能看到这串单词,特别是Ergonomics Heuristi,它们的字面意思是人体工程学,可以理解为适合人类理解的行为、习惯。GC日志里面看到Ergonomics这个单词,它后面一般跟着的是G1 GC相关的详细描述,比如堆内存日志、CSet划分等,通常采用选项-XX:+PrintAdaptiveSizePolicy时会看到这个单词。 Top-at-mark-start每个区间记录着两个TAMS指针(Top-at-mark-start),分别为prevTAMS和nextTAMS在TAMS以上的对象是新分配的,因而被视为隐式标记。 JVM&amp;GC 深入知识Java虚拟机内存模型 程序计数器程序计数器,英文全称Program Counter Register,它是一块很小的内存空间,它是运行速度最快的存储区域,这是因为它位于不同于其他存储区的地方—处理器内部。寄存器的数量极其有限,所以寄存器由编译器根据需求进行分配。实际上在Jaa应用程序内部不能直接控制寄存器,也不能在程序中感觉到寄存器存在的任何迹象。可以把程序计数器看作当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里,字节码解释器的工作就是通过改变程序计数器的值来选择下一条需要执行的字节码指令,分支、循环、跳转、异常处理、线程恢复等基础功能都要依赖这个计数器来完成。 简单概括上面的描述,即在多线程环境下,为了让线程切换后能恢复到正确的执行位置,每个线程都需要有一个独立的程序计数器,各个线程之间互不影响、独立存储,因此这块内存是线程私有的。JVM中的寄存器类似于物理寄存器的一种抽象模拟,正如前面说的,它是线程私有的,所以生命周期与线程的生命周期保持一致。 根据Java虚拟机定义来看,程序寄存器区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemory Error情况的区域。 虚拟机栈JVM的架构是基于栈的,即程序指令的每一个操作都要经过入栈和出栈这样的组合型操作才能完成。 总的来说,栈的优势是访问速度比堆要快,它仅次于寄存器,并且栈数据是可以被共享的栈的缺点是存储在栈里面的数据大小与生存期必须是确定的,从这一点来看,栈明显缺乏灵活性。虚拟机栈内主要被用来存放一些基本类型的变量,例如int、 short、long、byte、foat、 double、boolea、char,以及对象引用。 前面说过,虚拟机栈有一个很重要的特殊性,就是存放在栈内的数据可以共享。假设同时定义: 12int a=1;int b=1; 对于上面的代码,虚拟机处理第一条语句,首先它会在栈内创建一个变量为a的引用,然后查找栈内是否有1这个值,如果没找到,就将1存放进来,然后将a指向1。接下来处理第二条语句,在创建完b的引用变量后,因为在栈内已经有1这个值,便将b直接指向1。这样,就出现了a与b同时均指向1的情况。这时,如果存在第三条语句,它针对a再次定义为a=4,那么编译器会重新搜索栈内是否有4值,如果没有,则将4存放进来,并令a指向4,如果已经有了,则直接将a指向这个地址,因此a值的改变不会影响到b的值。要注意这种数据的共享与两个对象的引用同时指向一个对象的这种共享的方式存在明显的不同,因为这种情况a的修改并不会影响到b,它是由虚拟机完成的,这样的做法有利于节省空间。而一个对象引用变量修改了这个对象的内部状态,会影响到另一个对象引用变量。 与程序计数器一样,Java虚拟机栈也是线程私有的内存空间,它和Java线程在同一时间创建,它保存方法的局部变量、部分结果,并参与方法的调用和返回。 虚拟机栈在运行时使用一种叫作栈帧的数据结构保存上下文数据,栈帧里面存放了方法的局部变量表、操作数栈、动态连接方法和返回地址等信息。每一个方法的调用都伴随着栈帧的入栈操作,相应地,方法的返回则表示栈帧的出栈操作。 使用JClassLib工具可以查看Class文件中每个方法所分配的最大局部变量区的容量。JClassLib工具是开源软件,它可以用于查看 Class文件的结构,包括常量池、接口、属性、方法,还可以用于查看文件的字节码。 Java堆Java堆区在JVM启动的时候即被创建,它只要求逻辑上是连续的,在物理空间上可以是不连续。所有的线程共享Java堆,在这里可以划分线程私有的缓冲区(Thread Local Allocation Buffer,TLAB)。 正是因为Java堆区是GC的重点回收区域,所以GC极有可能会在大内存的使用和频繁进行垃圾回收过程上成为系统性能瓶颈。为了解决这个问题,JVM的设计者们开始考虑是否一定需要将对象实例存储到Java堆区内。基于OpenJDK深度定制的TaobaoJVM,其中创新的GCIH(GC invisible heap)技术实现了off-heap,即将生命周期较长的Java对象从heap中移到heap之外,并且GC不能管理GCH内部的Java对象,以此达到降低GC的回收频率和提升GC的回收效率的目的。 方法区方法区主要保存的信息是类的元数据。方法区与堆空间类似,它也是被JVM中所有的线程共享的区域。如下图所示,方法区中最为重要的是类的类型信息、常量池、域信息、方法信息。类型信息包括类的完整名称、父类的完整名称、类型修饰符(public/protected/private)和类型的直接接口类表。 常量池包括类方法、域等信息所引用的常量信息。域信息包括域名称、域类型和域修饰符方法信息包括方法名称、返回类型、方法参数、方法修饰符、方法字节码、操作数栈和方法栈帧的局部变量区大小以及异常表。方法区是线程间共享的,当两个线程同时需要加载一个类型时,只有一个类会请求ClassLoader加载,另一个线程则会等待。总而言之,方法区内保存的信息大部分来自于Class件,是Java应用程序运行必不可少的重要数据。 在Hotspot虚拟机中,方法区也被称为永久区,是一块独立于Java堆的内存空间。虽然被叫作永久区,但是在永久区中的对象同样也是可以被GC回收的,只是对于GC的对应策略与Java堆空间略有不同。 GC针对永久区的回收,通常主要从两个方面分析:一是GC对永久区常量池的回收,二是永久区对类元数据的回收。HotSpot虚拟机对常量池的回收策略是很明确的,只要常量池中的常量没有被任何地方引用，就可以被回收。 垃圾收集算法根搜索算法在HotSpot中,根对象集合中包含了5个元素,Java栈内的对象引用、本地方法栈内的对象引用、运行时常量池中的对象引用、方法区中类静态属性的对象引用以及与一个类对应的唯一数据类型的Class对象。 这部分了解一下就好注意,在根搜索算法中不可达的对象,也并非是“非死不可”的,这时候它们暂时处于“缓刑”阶段,要真正宣告一个对象死亡,至少要经历两次标记过程。如果对象在进行根搜索后发现没有与GC Roots相连接的引用链,那它将会被第一次标记并且进行一次筛选,筛选的条件是此对象是否有必要执行finalize方法。当对象没有覆盖finalize方法,或者finalized方法已经被虚拟机调用过,虚拟机将这两种情况都视为“没有必要执行”。如果这个对象被判定为有必要执行finalize方法,那么这个对象将会被放置在一个名为F-Queue的队列之中,并在稍后由条由虚拟机自动建立的、低优先级的Finalizer线程去执行。这里所谓的“执行”是指虚拟机会触发这个方法,但并不承诺会等待它运行结束。这样做的原因是,如果一个对象在finalize方法中执行缓慢,或者发生了死循环(更极端的情况),很可能会导致F-Queue队列中的其他对象永久处于等待状态,甚至导致整个内存回收系统崩溃。finalize()方法是对象逃脱死亡命运的最后一次机会,稍后GC将对F-Queue中的对象进行第二次小规模的标记,如果对象要在finalized中成功拯救自己—只要重新与引用链上的任何一个对象建立关联即可,譬如把自己(this关键字)赋值给某个类变量或对象的成员变量,那在第二次标记时它将被移除出“即将回收”的集合。如果对象这时候还没有逃脱,那它就真的离死不远了。 标记清除算法(Mark-Sweep)算法涉及几个概念,先来了解一下mutator和collector,这两个名词经常在垃圾收集算法中出现,collector指的就是垃圾收集器,而 mutator是指除了垃圾收集器之外的部分,比如说我们的应用程序本身。mutator的职责一般是NEW(分配内存)、READ(从内存中读取内容)、WRITE(将内容写入内存),而collector则就是回收不再使用的内存来供mutator进行NEW操作的使用。mutator根对象一般指的是分配在堆内存之外,可以直接被mutator直接访问到的对象,一般是指静态/全局变量以及ThreadLocal变量。 复制算法(Copying)基于分代的概念,Java堆区如果还要更进一步细分的话,还可以划分为年轻代(Young Gen)和老年代(OldEn),其中年轻代又可以被划分为Eden空间、From Survivor空间和To Survivor空间。在HotSpot中,Eden空间和另外两个Survivor空间默认所占的比例是8:1,当然开发人员可以通过选项“-XX:SurvivorRatio”调整这个空间比例。当执行一次Minor GC(年轻代的垃圾回收),Eden空间中的存活对象会被复制到To空间内,并且之前已经经历过一次 Minor GC并在From空间中存活下来的对象如果还年轻的话同样也会被复制到To空间内。需要注意的是,在满足两种特殊情况下,Eden和From空间中的存活对象将不会被复制到To空间内。首先是如果存活对象的分代年龄超过选项“-XX:MaxTenuringThreshold”所指定的阈值时,将会直接晋升到老年代中。其次当To空间的容量达到阈值时,存活对象同样也是直接晋升到老年代中。当所有的存活对象都被复制到To空间或者晋升到老年代后,剩下的均为垃圾对象,这就意味着GC可以对这些已经死亡了的对象执行一次Minor GC,释放掉其所占用的内存空间。 标记压缩算法(Mark-Compact)在HotSpot中,基于分代的概念,GC所使用的内存回收算法必须结合年轻代和老年代各自的特点。简单来说,就是针对不同的代空间,从而结合使用不同的垃圾收集算法。为年轻代选择的垃圾收集算法通常是以速度优先,因为年轻代中所存储的瞬时对象生命周期非常短暂,可以有针对性地使用复制算法,因此执行Minor GC时,一定要保持高效和快速。而年轻代中的生存空间通常都比较小,所以回收年轻代时一定会非常频繁。但老年代通常使用更节省内存的回收算法,因为老年代中所存储的对象生命周期都非常长,并且老年代占据了大部分的堆空间所以老年代的Full GC并不会跟年轻代的Minor GC一样频繁,不过一旦程序中发生一次Full GC,将会耗费更长的时间来完成,那么在老年代中使用标记-清除算法或者标记-压缩算法执行垃圾回收将会是不错的选择。 Garbage CollectionGC 概念在许多情况下,GC不应该成为影响系统性能的瓶颈,可以根据以下六点来评估一款GC的性能。 吞吐量：程序的运行时间(程序的运行时间+内存回收的时间)。 垃圾收集开销：吞吐量的补数,垃圾收集器所占时间与总时间的比例。 暂停时间：执行垃圾收集时,程序的工作线程被暂停的时间。 收集频率：相对于应用程序的执行,收集操作发生的频率。 堆空间：Java堆区所占的内存大小。 快速：一个对象从诞生到被回收所经历的时间。 Parallel收集器需要注意的是,垃圾收集器中吞吐量和低延迟这两个目标本身是相互矛盾的,因为如果选择以吞吐量优先,那么必然需要降低内存回收的执行频率,但是这样会导致GC需要更长的暂停时间来执行内存回收。相反的,如果选择以低延迟优先为原则,那么为了降低每次执行内存回收时的暂停时间,也只能频繁地执行内存回收,但这又引起了年轻代内存的缩减和导致程序吞吐量的下降。 举个例子,在60s的JVM总运行时间里,GC的执行频率是20秒/次,那么60s内一共会执行3次内存回收,按照每次GC耗时100ms来计算,最终一共会有300ms(3×100)被用于执行垃圾回收。但是如果我们将选项“-XX:MaxGCPauseMills”的值调小后,年轻代的内存空间也会自动调整,内存空间越小就越容易被耗尽,也就越容易造成GC的执行频繁发生。之前在60s的JVM总运行时间里,最终会有300ms被用于执行内存回收,而如今GC的执行频率却是10s/次,60s内将会执行6次内存回收,按照每次GC耗时60ms来计算,虽然看上去暂停时间更短了,但最终会耗时360ms(6×60)用于执行内存回收,很明显程序的吞吐量下降了。所以大家在设置这两个选项时,一定需要注意控制在一个折中的范围之内。Parallel收集器还提供个“-XX:UseAdaptiveSizePolicy”选项用于设置GC的自动分代大小调节策略,一旦设置这个选项后,就意味着开发人员将不再需要显式地设置年轻代中的一些细节参数,JVM会根据自身当前的运行情况动态调整这些相关参数。 Garbage First (G1) GCG1很重视老年代的垃圾回收,一旦整个堆空间占有率达到指定的阈值(启动时可配置),G1会立即启动一个独占的并行初始标记阶段(inil-mark phase)进行垃圾回收。在G1 GC,判断的是整个Java堆内部老年代的占有率,足以见G1对老年代的重视。 初始标记阶段一般和年轻代GC一起运行,一旦初始标记阶段结束,并行多线程的标记阶段就开始启动去标记所有老年代还存活的对象,注意这个标记阶段不是独占式的,它允许应用程序线程和它并行执行。当这个标记阶段运行完毕之后,为了再次确认是否有逃过扫描的对象,“启动一个独占式的再次标记阶段(remark phase),尝试标记所有遗漏的对象。在这个再次标记阶段结束之后,G1就掌握了所有的老年代 Region的标记信息,这和国家的户口统计方式差不多。一旦老年代的某些Region内部存在任何的存活对象,它就可以在下一个阶段,即清除阶段(cleanup phase)被清除了,就是可以销户了,又被放回了可用Region队列。同样地,再次标记阶段结束后就可以对一些老年代执行收集动作。 前面提到了CSet概念,一个CSet里面可以包含多少Region取决于多少空间可以被释放、G1停顿目标时间这两个因素。前面说起过混合GC(Mixed GC),这里就要具体说明一下了。当CSet被确定之后,会在接下来的一个年轻代回收过程当中对CSet进行回收,通过年轻代GC的几个阶段,一部分的老年代Region会被回收并放入年轻代使用。这个概念很灵活,即G1只关注你有没有存活对象了,如果没有,无论你属于老年代,还是属于年轻代,你都会被回收并放入可用Region队列,下一次你被分配到哪里就不确定了。也正是因为Region、混合收集这些特性,让G1对老年代的垃圾收集方式有别于Serial GC、Parallel GC和CMS GC,G1采用Region方式让对象之间的联系存在于虚拟地址之上,这样就不需要针对老年代的压缩和回收动作对整个Java堆执行扫描,为老年代回收节约了时间。 G1 设计思路Gl把整个Java堆划分为若干个区间(Region)。每个Region大小为2的倍数,范围在MB32MB之间,可能为1MB、2MB、4MB、8MB、16MB、32MB。所有的Region有一样的大小,在JVM生命周期内不会被改变。 注意,在年轻代、混合代、Full GC这三个阶段,年轻代的Eden Region和Survivor Region的数量会随时变化。Humongous Region(大对象 Region)是老年代Region的一部分,里面的对象超过每个Region的50%空间,这一点有别于一般对象Region。 从之前的介绍我们知道没有必要去刻意区分Region的用途,因为G1设计Region的分配原则是很灵活的。一开始G1会从可用 Region队列里面挑选出Region并设置为Eden Region,一个Eden Region里面填满对象以后,又会从可用Region队列里再挑出一个。当所有的Eden Region都被填满时,一个年轻代GC收集就会开始执行了,在这个收集阶段,我们会收集Eden和Survivor Region,所有的存活对象要么进入到下一个Survivor region,要么进入老年代Region。 G1提供了一个选项-XX:InitiatingHeapOccupancyPercent,默认值是Java堆空间的45%,这个选项决定了是否开始一次老年代回收动作,即年轻代GC结束之后,G1会评估剩余的对象是否达到了45%这个阈值。 如果标记阶段(Marking Phase)结束后一个老年代的Region已经不存在对象,那么它会被放回可用Region队列,反之,它会被放入混合收集器。 由于标记阶段不是一个独占式的多线程并行程序,这样应用程序线程就会和它一起并行执行。为了避免标记阶段占用过多的CPU资源,G1采用时间片方式分段执行操作,即在时间片内全力运行,然后休息一段时间,这个休息时间就是让应用程序尽可能多地使用CPU资源运行。 大对象(Humongous Object)大对象Region属于老年代的一部分,它只包含一个对象。当并行标记阶段发现没有存活对象时,G1会回收这个大对象 Region,注意这个动作可以是一个批量回收。 全垃圾收集(Full Garbage Collection)G1的Full GC和Serial go的FuGC采用的是同一种算法。Full GC会对整个Java堆进行压缩。G1的Full GC是单线程的,会引起较长的停顿时间,因此G1的设计目标是减少Full GC的发生次数。 并行循环(Concurrent Cycle)一个G1并行循环包括几个阶段的活动:初始标记(Initial Marking)、并行Root区间扫描(Concurrent Root Region Scanning)、并行标记(Concurrent Marking)、重标记(Remarking)和清除(Cleanup)。除了最后的Cleanup阶段以外,其余阶段都属于标记存活对象阶段。 初始标记阶段的目的是收集所有GC根(Roots)。 Roots是一个对象的起源指针。为了收集根引用,从应用线程开始,应用线程必须停下来,所以初始标记阶段是一个独占式的。由于个年轻代GC必须收集所有的Roots,所以G1的初始标记在一个年轻代GC里完成。 并行根区间扫描阶段必须扫描和标记所有幸存者区间的对象引用,这一阶段所有的应用程序线程都可以并行执行,唯一的约束是扫描必须在下一个GC开始前完成。这一约束的原因是个新的GC事件会产生一堆新的幸存者对象集合,这些对象和初始化标记阶段的幸存者对象不一样,容易发生混淆。 并行标记阶段完成了几乎所有的标记工作。在这一阶段,利用多线程并行标记存活对象及对应的逻辑地图。这一阶段允许所有的Java线程并行执行,但是对应用程序来说总体的吞吐量可能会下降。其实任何一个系统都和人体循环一样,当没有外部干扰时,系统可以正常运行，如果受到外部干扰,人体系统也会出现混乱,甚至出现短时间的休克。 重标记阶段是一个独占式阶段,通常是一个很短的停顿,这个阶段会完成所有的标记工作。 最后一个并行标记步骤是清除阶段。在这个阶段,没有包含存活对象的Region会被回收,并随即被加入可用Region队列。这个阶段的重要意义是最终决定了哪些 Region可以进入混合GC。在G1内部,混合GC是非常重要的释放内存机制,避免了G1出现没有可用Region的情况发生,否则就会触发Full GC事件。 堆大小（Heap Sizing）G1在以下几种情况下可能会增大堆内存大小： Full GC阶段。 Young或Mixed GC发生时,G1计算GC花费的时间与Java线程的花费时间比例,如果-XX:GCTimeRatio设置GC花费时间很长,则堆大小会增大,这样的设计思路是希望G1发生GC的频率降低,这样GC花费时间和Java线程花费时间比例也会相应下降。 -XX:GCTimeRatio选项的默认值是9,所有其他HotSpot GC的默认值是99。这个值越大,代表Java堆空间大小增长越偏激,即越容易扩大堆空间大小,这样也是为了达到降低GC花费时间的设计目标。 如果一个对象分配失败,即便一个GC刚刚结束,G1采用的策略不是立即重复Full GC，而是通过增大堆内存大小,确保对象分配成功。这样的设计理念符合G1的避免Full GC发生的最初思想。 和第3条一样,如果出现一个大对象分配失败,前面说过,大对象需要几个连续的Region区间才能确保对象分配成功。如果发生这种分配失败的情况,采用的设计理念也不是调用Full GC,而是扩大堆内存。 当GC申请加入一个新的Region时。 引用一段在StackOverfall.com上看到的经验分享,”我在一个真实的、较大规模的应用程序中使用过G1:大约分配有60B-70GB内存,存活对象大约在20GB50GB之间。服务器运行Linux操作系统,JDK版本为6u22。G1与PS/PS Old相比,最大的好处是停顿时间更加可控可预测。如果我在PS中设置一个很低的最大允许GC时间,譬如期望50ms内完成GC(-XX:MaxGCPauseMillis=50),但在65GB的Java堆下有可能得到的直接结果是一次长达30s至2min的漫长的Stop-the-World过程。而Gl与CMS相比,它们都立足于低停顿时间,CMS仍然是我现在的选择,但是随着Oracle对G1的持续改进,我相信Gl会是最终的胜利者。如果你现在采用的收集器没有出现问题,那么就没有任何理由现在去选择G1;如果你的应用追求低停顿,那么G1现在己经可以作为一个可尝试的选择:如果你的应用追求吞吐量,那么G1并不会为你带来什么特别的好处。” G1 GC应用示例G1 GC给我们提供了很多的命令行选项,也就是参数,这些参数一类以布尔类型打头,“+”表示启用该选项,“-”表示关闭该选项。另一类采用数字赋值,不需要布尔类型打头。 选项解释及应用首先在cmd命令行模式下输入java -X,,如C:Users\Administrator&gt; java -X,输出如代码如下： -XX:+PrintGCDetails该选项用于记录GC运行时的详细数据信息并输出,是最基本、使用最普遍的一个选项这个选项适用于所有GC,输出内容主要包括新生成对象占用内存大小以及耗费时间、各个年龄代的情况、每次回收的对应数据等。 -Xloggc如果想要以文件形式保存这些GC日志,可以在启动参数中输入-XX:+PrintGCDetails -verbose:gc -XLoggc:gc.log,运行后我们会发现生成了一个 gc.log文件。 -Xloggc:example_gc.log （设置垃圾回收日志打印的文件，文件名称可以自定义） -XX:initialHeapSize和-XX:MaxHeapSize就是我们比较熟悉的-Xms和-Xmx,它们允许我们指定JVM的初始和最大堆内存大小-XX:+UseCompressedClassPointers、XX:+UseCompressedOops以及-XX:-UseLargePagesIndividualAllocation这三个选项和OOP有关。OOP的全称是Ordinary Object Pointer,即普通对象指针。通常64位JVM消耗的内存会比32位的大1.5倍,这是因为对象指针在64位架构下,长度会翻倍(更宽的寻址)。对于那些将要从32位平台移植到64位的应用来说,平白无故多了1/2的内存占用,作为开发者一定不愿意看到这种场景。所以,从JDK1.6 update4开始,64 bit JVM正式支持了-XX:+UseCompressedOops这个可以压缩指针,起到节约内存占用的选项。CompressedOops的实现方式是在机器码中植入压缩与解压指令,可能会给JVM增加额外的开销。-XX:+UseCompressedClassPointers选项是在JDK8出现的,也是在永久区消失之后出现的新的选项,主要用于对类的元数据进行压缩。-XX:UseLargePagesIndividualAllocation和oops是一起使用的,在大页内存使用发生时这个选项也会自动启用。 -XX:+PrintGCApplicationStoppedTime打印垃圾回收期间程序暂停的时间,如果使用该选项,会输出GC造成应用程序暂停的时间。一般和-XX:+PrintGCApplicationConcurrentTime组合起来一起使用,这样比较有利于查看输出。 -XX:ConcGCThreads这个选项用来设置与Java应用程序线程并行执行的GC线程数量,默认为GC独占时运行线程的1/4。这个选项设置过大会导致Java应用程序可以使用的CPU资源减少,如果小一点则会对应用程序有利,但是过小就会增加GC并行循环的执行时间,反过来减少Java应用程序的运行时间(因为独占期时间拉长)。 -XX:G1HeapRegionSize这是G1GC独有的选项,它是专门针对Region这个概念的对应设置选项,后续GC应该会继续采用 Region这个概念。 Region的大小默认为堆大小的1/200,.也可以设置为1MB、2MB、4MB、8MB、16MB,以及32MB,这六个划分档次。 增大Region块的大小有利于处理大对象。前面介绍过,大对象没有按照普通对象方式进行管理和分配空间,如果增大Region块的大小,则一些原本走特殊处理通道的大对象就可以被纳入普通处理通道了。这就好比我们在机场安检,飞行员、空姐可以走特殊通道,乘客如果也搞特殊化,一部分人去特殊通道处理,那么特殊通道就得増加几个,相应的普通通道就得减少了,对效率就起了降低作用。反之,如果Region大小设置过小,则会降低G1的灵活性,对于各个年龄代的大小都会造成分配问题。 -XX:G1HeapWastePercent这个选项控制G1 GC不会回收的空闲内存比例,默认是堆内存的5%。G1 GC在回收过程中会回收所有Region的内存,并持续地做这个工作直到空闲内存比例达到设置的这个值为止,所以对于设置了较大值的堆内存来说,需要采用比较低的比例,这样可以确保较小部分的内存不被回收。这个很容易理解,城市越大就越容易出现一些死角,出于性能的原因可以不去关注那里,但是这个比例不能大。 -XX:G1MixedGCCountTarget老年代Region的回收时间通常来说比年轻代Region稍长一些,这个选项可以设置一个并行循环之后启动多少个混合GC,默认值是8个。设置一个比较大的值可以让G1 GC在老年代Region回收时多花一些时间,如果一个混合GC的停顿时间很长,说明它要做的事情很多,所以可以增大这个值的设置,但是如果这个值过大,也会造成并行循环等待混合GC完成的时间相应的增加。 当占用内存超过InitiatingHeapOccupancyPercent阀值时, 最多通过多少次Mixed GC来将内存控制在阀值之下。 -XX:+G1PrintRegionLivenessInfo由于开启这个选项会在标记循环阶段完成之后输出详细信息,专业一点的叫法是诊断选项,所以在使用前需要开启选项UnlockDiagnosticVMOptions。这个选项启用后会打印堆内存内部每个Region里面的存活对象信息,这些信息包括使用率、RSet大小、回收一个Region的价值(Region内部回收价值评估,即性价比)。 这个选项输出的信息对于调试堆内Region是很有效的,不过对于一个很大的堆内存来说,由于每个 Region信息都输出了,所以信息量也是挺大的。 -XX:G1ReservePercent每个年龄代都会有一些对象可以进入下一个阶段,为了确保这个提升过程正常完成,我们允许G1GC保留一些内存,这样就可以避免出现“ to space exhausted”错误,这个选项就是为了这个用途。 这个选项默认保留堆内存的10%。注意,这个预留内存空间不能用于年轻代。 对于一个拥有大内存的堆内存来说,这个值不能过大,因为它不能用于年轻代,这就意味着年轻代可用内存降低了。减小这个值有助于给年轻代留出更大的内存空间、更长的GC时间,这对提升性能吞吐量有好处。 -XX:+G1SummarizeRSetStats和GIPrintRegionLivenessInfo选项一样,这个选项也是一个诊断选项,所以也需要开启UnlockDiagnosticVMOptions选项后才能使用,这也就意味着-XX:+UnlockDiagnosticVMOptions选项需要放在-XX:+G1SummarizeRSetStats选项的前面。 这个选项和-XX:G1SummarizePeriod一起使用的时候会阶段性地打印RSets的详细信息,这有助于找到RSet里面存在的问题。 -XX:+G1TraceConcRefinement这是一个诊断选项。如果启动这个诊断选项,那么并行Refinement线程相关的信息会被打印。注意,线程启动和结束时的信息都会被打印。 这里提到了Refinement线程,我们来提前梳理这个概念。请看每一代GC对应的GC线程: Garbage Collector Worker Threads Used Parallel GC ParallelGCThreads CMS GC ParallelGCThreadsConcGCThreads G1 GC ParallelGCThreadsConcGCThreadsG1ConcRefinementThreads 上面列出了三类GC线程,分别是ParallelGCThreads、ConcGCThreads和G1ConcRefinementThreads。关于这三个线程的区别： 名称 选项控制 作用 ParallelGC Thread -XX:ParallelGCThreads GC的并行工作线程,专门用于独占阶段的工作,比如拷贝存活对象 ParallelMarkingThreads -XX:ConcGCThreads 并行标记阶段的并行线程,它由一个主控(Master)线程和一些工作(Worker)线程组成,可以和应用程序并行执行 G1ConcurrentRefinementThreads -XX:G1ConcRefinementThreads 和应用程序一起运行,用于更新RSet,如果ConcurrentRefinementThreads没有设置,那么默认为ParallelGCThreads+1 -XX:+G1UseAdaptiveConcRefinement这个选项默认是开启的。它会动态地对每一次GC中XX:G1ConcRefinementGreenZone、-XX:G1ConcRefinementYellowZone、-XX:G1ConcRefinementRedZone的值进行重新计算。 并行Refinement线程是持续运行的,并且会随着update log buffer积累的数量而动态调节。前面说到的三个配置选项-XX:G1ConcRefinementGreenZone、-XX:G1ConcRefinementYellowZone、-XX:G1ConcRefinementRedZone,是被用来根据不同的 buffer使用不同的Refinement线程,目的就是为了保证 Refinement线程一定要尽可能地跟上update log buffer产生的步伐。但是这个Refinement线程不是无限增加的,一旦出现 Refinement线程跟不上update log buffer产生的速度、update log buffer开始出现积压的情况,Mutator线程(即应用业务线程)就会协助Refinement线程执行RSet的更新工作。这个 Mutator线程实际上就是应用业务线程,当业务线程去参与Rset修改时,系统性能一定会受到影响,所以需要尽力去避免这种状况。 -XX:GCTimeRatio这个选项代表Java应用线程花费的时间与GC线程花费时间的比率。通过这个比率值可以调节Java应用线程或者GC线程的工作时间,保障两者的执行时间. HotSpot VM转换这个值为一个百分比,公式是100/(1+GCTimeRatio),默认值是9,表示花费在GC工作量上的时间占总时间的10%。 -XX:+HeapDumpBeforeFullGC/-XX:+HeapDumpAfterFullGC这个选项启用之后,在Full GC开始之前有一个hprof文件会被创建。建议这个选项和-XX:+HeapDumpAfterFullGC一起使用,可以通过对Full GC发生前后的Java堆内存进行对比,找出内存泄漏和其他问题。 获取full GC前后的heap dump -XX:InitiatingHeapOccypancyPercent该选项的默认值是45,表示G1 GC并行循环初始设置的堆大小值,这个值决定了一个并行循环是不是要开始执行。它的逻辑是在一次GC完成后,比较老年代占用的空间和整个Java堆之间的比例。如果大于这个值,则预约下一次GC开始一个并行循环回收垃圾,从初始标记阶段开始。这个值越小,GC越频繁,反之,值越大,可以让应用程序执行时间更长。不过在内存消耗很快的情况下,我认为早运行并行循环比晚运行要好,看病要趁早。 -XX:+UseStringDeduplication该选项启动Java String对象的去重工作。JDK8u20开始引入该选项,默认为不启用。我们知道一个判断Java String对象值是否一样的语句“Stringl equals(String2)tue”,如果开启了该选项,并且如果两个对象包含相同的内容,即返回“tue”,则两个String对象只会共享一个字符数组。这个选项是G1GC独有的,也可以和其他GC一起使用。 延伸一点我们的知识面,一个去重对象的必备条件有如下三点: Java.lang String对象的一个实例。 这个对象在年轻代堆区间。 这个对象的年龄达到去重年龄代,或者这个对象已经在老年代堆区间并且对象年龄比去重年龄小。选项-XX:StringDeduplicationAgeThreshold设置了这个年龄界限。 前面介绍过的可修改和不可修改字符串的处理方式有所不同,不可修改字符串默认就是去重的,在插入到HotSpot VM的String Table时已经注明了是去重的,这样就避免了HotSpot服务器JIT编译优化措施。 -XX:StringDeduplicationAgeThreshold这个选项是针对-XX:+UseStringDeduplication选项的,默认值是3。它的意思是一个字符串对象的年龄超过设定的阈值,或者提升到G1 GC老年代Region之后,就会成为字符串去重的候选对象,去重操作只会有一次。 -XX:+PrintStringDeduplicationStatistics这个选项挺有用的,能够帮助我们通过读取输出的统计资料来了解是否字符串去重后节约了大量的堆内存空间,默认是关闭的,就是说不会输出字符串去重的统计资料。 -XX:+G1UseAdaptiveIHOPJDK9提供的新的选项。这个选项的作用是通过动态调节标记阶段开始的时间,以达到提升应用程序吞吐量的目标,主要通过尽可能迟地触发标记循环方式来避免消耗老年代空间。 这个选项的值在VM刚开始启动时和-XX:InitiatingHeapOccupancyPercent的值一样,如果出现标记循环阶段内存不够用,则它会自动调节大小,确保标记循环启用更多的堆内存。 注意,-XX:+G1UseAdaptiveIHOP这个选项会在JDK9里默认启用,即-XX:InitiatingHeapOccupancyPercent和XX:+GIUseAdaptivelHOP在JDK9之后只需要启用一个就可以了。 JDK8环境下运行该选项会输出:“Unrecognized VM option ‘G1UseAdaptivelHOP’” -XX:+MaxGCPauseMills这个选项比较重要。它设置了G1的目标停顿时间,单位是ms,默认值为200ms。这个值是一个目标时间,而不是最大停顿时间。G1 GC尽最大努力确保年轻代的回收时间可以控制在这个目标停顿时间范围里面,在G1GC使用过程中,这个选项和-Xms、Xmx两个选项一起使用,它们三个也最好在JVM启动时就一起配置好。 -XX:+MinHeapFreeRatio这个选项设置堆内存里可以空闲的最小的内存空间大小,默认值为堆内存的40%。当空闲堆内存大小小于这个设置的值时,我们需要判断-Xms和-Xmx这两个值的初始化设置值,如果-Xms和-Xmx不一样,那么我们就有机会扩展堆内存,否则就无法扩展。 -XX:+MaxHeapFreeRatio这个选项设置最大空闲空间大小,默认值为堆内存的70%。这个选项和上面那个最小堆内存空闲大小刚好相反,当大于这个空闲比率时,G1 GC会自动减少堆内存大小。需要判断-Xms和-Xmx这两个值的初始化设置值,如果-Xms和-Xmx不一样,那么就有机会减小堆内存,否则就无法减小。 -XX:+PrintAdaptiveSizePolicy这个选项决定是否开启堆内存大小变化的相应记录信息打印,即是否打印这些信息到GC日志里面。这个信息对于Parallel GC和G1 GC都很有用。 -XX:+ResizePLABGC使用的本地线程分配缓存块采用动态值还是静态值进行设置是由这个选项决定的,它默认是开启的,这个设置对应的是GC在提升对象时是否会调整PLAB的大小。 这个选项大家还是慎用,据说会出现性能问题,启用后可能会增加GC的停顿时间。当应用开启的线程较多时,最好使用-XX:ResizePlaB来关闭PLAB()的大小调整,以避免大量的线程通信所导致的性能下降。 -XX:+ResizeTLABJava应用线程使用的本地线程分配缓存块采用动态值还是静态值进行设置是由这个选项决定的,它默认是开启的,即TLAB值会被动态调整。 -XX:+ClassUnloadingWithConcurrentMark这个选项开启在G1 GC并行循环阶段卸载类,尤其是在老年代的并行回收阶段,默认是开启的。这个选项开启后会在并行循环的重标记阶段卸载JVM没有用到的类,这些工作也可以放在Full GC里面去做,但是提前做了有很大的好处。但因为开启它意味着重标记阶段的GC停顿时间会拉长,这时候我们就要判断性价比了,如果GC停顿时间比我们设置的最大GC停顿目标时间还长,并且需要卸载的类也不多,那还是关闭这个选项吧。 -XX:+ClassUnloading默认值是Ture,决定了JVM是否会卸载所有无用的类,如果关闭了这个选项,无论是并行回收循环,还是Full GC,都不会再卸载这些类了,所以需谨慎关闭。 -XX:+UnlockDiagnosticVMOptions这个选项决定是否开启诊断选项,默认值是False,即不开启在GC里面有一些选项称之为诊断选项(Diagnostic Options),通过-XX:+PrintFlagsFinal 和XX:+Unlock。DiagnosticVMOptions这两个选项组合起来运行,就可以输出并查看这些选项。 -XX:+UnlockExperimentalVMOptions除了之前说的诊断选项以外,JVM还有一些叫作试验选项(Experimental Options),这些选项也需要通过XX:+UnlockExperimentalVMOptions这个选项开启,默认是关闭的。 和诊断选项一样,也可以和-XX:+PrintFlagsFinal选项联合使用,即-XX:+PrintFlagsFinal和-XX:+UnlockExperimental VMOptions这两个选项联合使用时可以输出日志,输出的日志已经包含在了前一个选项-XX:+UnlockDiagnosticVMOptions的运行输出里,这里就不再重复。 总的来说,这些试验选项对整体应用性能可能会有些好处,但是它们并没有经历完整的测试环节,所以称为试验选项。 -XX:+UnlockCommercialFeatures这个选项判断是否使用 Oracle特有的特性,默认是关闭的。 有一些属性是Oracle公司针对Oracle的Java运行时独有的,没有被包含在OpenJDK里面。举个例子,比如说 Oracle的监控和管理工具Java Mission Control,它有一个特性叫作Java Flight Recorder,这个特性作为Java Mission Control的一部分,属于事件回收框架,可以被用来显示应用程序和JVM的底层信息。 深入G1 GCG1 GC概念简介背景知识G1使用了全新的分区算法,其特点如下所示： 并行性：G1在回收期间,可以有多个GC线程同时工作,可以有效利用多核的计算能力 并发性：G1拥有与应用程序交替执行的能力,部分工作可以和应用程序同时执行,因此,一般来说,不会在整个回收阶段发生完全阻塞应用程序的情况。 分代GC：G1依然是一个分代收集器,但是和之前的各类回收器不同,它同时兼顾了年轻代和老年代。对比其他回收器,它们或者工作在年轻代,或者工作在老年代。 空间整理：G1在回收过程中,会进行适当的对象移动,不像CMS那样只是简单地标记清理对象。在若干次GC后,CMS必须进行一次碎片整理。而G1不同,它每次回收都会有效地复制对象,减少空间碎片,进而提升内部循环速度。 可预见性：由于分区的原因,G1可以只选取部分区域进行内存回收,这样缩小了回收的范围,因此对于全局停顿情况的发生也能得到较好的控制。 随着G1 GC的出现,GC从传统的连续堆内存布局逐渐走向了不连续内存块布局,这是通过引入Region概念实现的,也就是说,由一堆不连续的Region组成了堆内存。其实也不能说是不连续的,只是它从传统的物理连续逐渐改变为逻辑上的连续,这是通过Region的动态分配方式实现的,可以把一个Region分配给Eden、Surviⅳvor、老年代、大对象区间、空闲区间等区间的任意一个,而不是固定它的作用,因为越是固定,越是呆板。 G1的区间设计灵感在G1中,堆被平均分成若干个大小相等的区域(Region)。每个Region都有个关联的Remembered Set(简称RS),RS的数据结构是Hash表,里面的数据是Card Table(堆中每512byte映射在card table 1byte)。简单地说,RS里面存在的是Region中存活对象的指针。当Region中数据发生变化时,首先反映到Card Table中的一个或多个Card上,RS通过扫描内部的Card Table得知Region中内存使用情况和存活对象。在使用Region过程中,如果Region 被填满了,分配内存的线程会重新选择一个新的Region,空闲Region被组织到一个基于链表的数据结构(LinkedList里面,这样可以快速找到新的Region。 G1 GC分代管理年轻代除非我们显示地通过命令行方式声明了年轻代的初始化值和最大值的大小,否则，一般来说,初始化值默认是整个Java堆大小的5%(通过选项-XX:G1NewSizePercent设置),最大值默认是整个Java堆大小的60%(通过选项-XX:G1MaxNewSizePercent设置)。 回收集合及其重要性任何一次垃圾回收都会释放CSet里面的所有区间。一个CSet由一系列的等待回收的区间所组成。在一次垃圾回收过程中,这些回收候选区间的存活对象会被整体评估,并且在回收结束后这些区间会被加入到空闲区间队列(LinkedList队列)。在一次年轻代回收过程中,cset只会包含年轻代区间,而在一个混合回收过程中,CSet会在年轻代区间基础上再包含一些老年代区间,这就是新增的混合回收概念,不再对年轻代和老年代完全切分。 G1 GC提供了两个选项用于帮助选择进入CSet的候选老年代区间: -XX:G1MixedGCLiveThresholdPercent:JDK8u45默认值为一个G1 GC区间的85%。这个值是一个存活对象的阈值,并且起到了从混合回收的CSet里排除一些老年代区间的作用,即可以理解为G1 GC限制CSet仅包含低于这个阈值(默认85%)的老年代区间,这样可以减少垃圾回收过程中拷贝对象所消耗的时间。 -XX:G1OldCSetRegionThresholdPercent:JDK8u45默认值为整个Java堆区的10%。这个值设置了可以被用于一次混合回收暂停所回收的最大老年代区间数量。这个阈值取决于JVM进程所能使用的Java堆的空闲空间。 RSet及其重要性一个RSet是一个数据结构,这个数据结构帮助维护和跟踪在它们单元内部的对象引用信息,在G1 GC里,这个单元就是区间(Region),也就是说,G1 GC里每一个RSet对应的是一个区间内部的对象引用情况。有了RSet,就不需要扫描整个堆内存了,当G1 GC执行STW独占回收(年轻代、混合代回收)时,只需要扫描每一个区间内部的RSet就可以了。因为所有RSet都保存在CSet里面,即Region-RSet-CSet这样的概念,所以一旦区间内部的存活对象被移除,RSet里面保存的引用信息也会立即被更新。这样我们就能够理解RSet就是一张虚拟的对象引用表了,每个区间内部都有这么一张表存在,帮助对区间内部的对象存活情况、基本信息做有序高效的管理。 G1 GC的年轻代回收或者混合回收阶段,由于年轻代被尽可能地设计为最大量的回收,这样的设计方式减少了对于RSet的依赖,即减弱了对于年轻代里面存储的跟踪引用信息的依赖程度,进而减弱了多余RSet的消耗。G1 GC只在以下两个场景依赖RSet。 老年代到年轻代的引用：G1 GC维护了从老年代区间到年轻代区间的指针,这个指针保存在年轻代的RSet里面。 老年代到老年代的引用：G1 GC维护了从老年代区间到老年代区间的指针,这个指针保存在老年代的RSet里面。 每一个区间只会有一个RSet由于对于对象的引用是基于Java应用程序的需求的,所以有可能会出现RSet内部的“热点”,即一个区间出现很多次的引用更新,都出现在同一个位置的情况。 对于一个访问很频繁的区间来说,这样的方式会影响RSet的扫描时间。 注意,区间(Region)并不是最小单元,每个区间会被进一步划分为若干个块(Chunks)。在G1 GC区间里,最小的单元是一个512个字节的堆内存块(Card)。G1 GC为每个区间设置了一个全局内存块表来帮助维护所有的堆内存块，如下图所示： 当一个指针引用到了RSet里面的一个区间时,包含该指针的堆内存块就会在PRT里面被标记。如果需要快速地扫描一张数据表,最好的方式是建立索引,一个粗粒度的PRT就是基于哈希表建立的。对于一个细粒度的PRT来说,哈希表内部的每一个入口对应一个区间,而区间内部的内存块索引也是存储在位图里面的。当细粒度PRT的最大值被突破的时候,我们就会开始采用粗粒度方式处理PRT。 在垃圾回收过程中,当扫描RSet并且内存块确实存在于PRT里时,G1 GC会在全局堆内存块数据表里标记对应的入口,这种做法避免了重新扫描这个内存块。G1 GC会在回收循环阶段默认清除内存堆表,在GC线程的并行工作(主要包括根外部扫描、更新和扫描RSet、对象拷贝、终止协议等)完成之后紧跟着的就是清除堆内存表标记(Clear CT)阶段。Update RS和Scan RS对应的是RSet的更新和扫描动作。 RSet的作用是很明显的,但是在使用过程中我们也遇到了写保护和并行更新线程的维护成本。 OpenJDK HotSpot的并行老年代和CMS GC都在执行JVM的一个对象引用写操作时使用了写保护机制,如代码object field = some_other_object。还记得我们对于每个区间是采用针对最小单元堆内存块进行管理的吗?这个写保护机制也会通过更新一个类似于堆内存块表的数据结构来跟踪跨年代引用。堆内存表在最小垃圾回收时会被扫描。写保护算法基于Urs Holzle的快速写保护算法,这个算法减少了编译代码时的外部指令消耗。 当跨越区间的更新发生的时候,G1 GC会将这些对应的堆内存块放入一个缓存,我们可以称这个缓存为“更新日志缓存”,写入该缓存的方式和写入队列的方式一样。G1 GC会使用一个专门的线程组去维持RSet信息,它们的职责是扫描“更新日志缓存”,然后更新RSet。JDK8u45采用选项-XX:G1ConcRefinementThreads设置这个线程组的数量,如果你没有设置,那么默认采用-XX:ParallelGCThreads选项。 一旦“更新日志缓存”达到了最大可用,它会被放入全局化的满载队列并启用一个新的缓存块。一旦更新线程在全局满载队列里面发现了入口,它们就开始并行处理整个满载缓存队列。 G1 GC针对并行更新线程采用的是分层方法,为了保证更新速度会加入更多的线程,如果实在跟不上速度,Java应用程序线程也会加入战斗,但尽量不要出现这样的情况,这种情况是发生了线程窃取,会造成应用程序花费了本可以用于自身程序算法运行的能力。 并行标记循环并行标记循环的过程是初始标记阶段→根区间扫描阶段→并行标记阶段→重标记阶段→清除阶段,其中一部分是可以与应用程序并行执行的,一部分是独占式的。 1.初始标记阶段这个阶段是独占式的,它会停止所有的Java线程,然后开始标记根节点可及的所有对象。这个阶段可以和年轻代回收同时执行,这样的设计方式主要是为了加快独占阶段的执行速度。 在这个阶段,每一个区间的NATMS值会被设置在区间的顶部。 2.根区间扫描阶段设置了每个区间的TAMS值之后,Java应用程序线程重新开始执行,根区间扫描阶段也会和Java应用程序线程并行执行。基于标记算法原理,在年轻代回收的初始标记阶段拷贝到幸存者区间的对象需要被扫描并被当作标记根元素,相应地,G1 GC因此开始扫描幸存者区间。任何从幸存者区间过来的引用都会被标记,基于这个原理,幸存者区间也被称为根区间。 根区间扫描阶段必须在下一个垃圾回收暂停之前完成,这是因为所有从幸存者区间来的引用需要在整个堆区间扫描之前完成标记工作。 3.并行标记阶段首先可以明确的是,并行标记阶段是一个并行的且多线程的阶段,可以通过选项-XX:ConcGCThreads来设置并行线程的数量。默认情况下,G1 GC设置并行标记阶段线程数量为选项-XX:ParallelGCThreads(并行GC线程)的1/4。并行标记线程一次只扫描一个区间,扫描完毕后会通过标记位方式标记该区间已经扫描完毕为了满足SATB并行标记算法的要求,G1 GC采用一个写前barrier执行相应的动作。 4.重标记阶段重标记阶段是整个标记阶段的最后一环。这个阶段是一个独占式阶段,在整个独占式过程中,G1 GC完全处理了遗留的SATB日志缓存、更新。这个阶段主要的目标是统计存活对象的数量,同时也对引用对象进行处理。 G1 GC采用多线程方式加快并行处理日志缓存文件,这样可以节省下来很多时间,通过选项-XX:ParallelGCThreads可以设置GC数量。 注意,如果你的应用程序使用了大量的引用对象,例如弱引用、软引用、虚引用、强引用,那么这个重标记阶段的耗时会有所增加。 5.清除阶段前面各个阶段在做的主要事情就是为了标记对象,那么为什么需要针对每一个区间进行标记呢?这是因为如果我们知道了每个区间的存活对象数量,如果这个区间没有一个存活对象,那么就可以很快地清除RSet,并且立即放入空闲区间队列,而不是将这个区间放入排队序列,等待一个混合垃圾回收暂停阶段的回收。RSet也可以被用来帮助检测过期引用,例如,如果标记阶段发现所有在特定堆块上的对象都已经死亡,那么RSet可以快速清除这块堆块。 一句话总结,清除阶段会识别并清理完全空闲的区域。它是并发的清理,不会引起停顿。 评估失败和完全回收如果在年轻代区间或者老年代区间执行拷贝存活对象操作的时候,找不到一个空闲的区间,那么这个时候就可以在GC日志里看到诸如“to-space exhausted”这样的错误日志打印。 发生这个错误的同时,G1 GC会尝试去扩展可用的Java堆内存大小。如果扩展失败,G1 GC会触发它的失败保护机制并且启动单线程的完全回收动作。 在这个完全回收阶段,单线程会针对整个堆内存里的所有区间进行标记、清除、压缩等动作。在完成回收后,堆内存就完全由存活对象填充,并且所有的年龄代对应的区间都已经完成了压缩任务。 也正是因为这个完全回收是单线程执行的,所以当堆内存很大时势必耗时很长,所以需要谨慎使用,最好不要让它经常发生,以避免不必要的长时间的应用程序暂停。 G1 GC使用场景如果应用程序具有如下的一个或多个特征,那么将垃圾收集器从CMS或ParallelOldGC切换到G1将会大大提升性能: Full GC次数太频繁或者消耗时间太长 对象分配的频率或代数提升(promotion)显著变化。 受够了太长的垃圾回收或内存整理时间(超过0.5~1s) 注意,如果正在使用CMS或ParallelOldGC,而应用程序的垃圾收集停顿时间并不长,那么继续使用现在的垃圾收集器是个好主意。 G1 GC性能优化方案G1的年轻代回收External Root Regions外部根区间扫描指的是从根部开始扫描通过JNI中本地的类中调用Malloc函数分配出的内存。这个步骤是并行任务的第一个任务。这个阶段堆外(off-heap)根节点被开始扫描,这些扫描范围包括JVM系统字典、VM数据结构、JNI线程句柄、硬件注册器、全局变量,以及线程栈根部等,这个过程主要是为了找到并行暂停阶段是否存在指向当前收集集合(CSet)的指针。 这里还有一个情况需要引起大家的重视,就是查看工作线程是否在处理一个单一的根节点时耗时过长,导致感觉类似挂起的现象。这个现象可以通过查看工作线程对应的“termination”日志看出来。如果存在这个现象,你需要去查看是否存在比较大的系统字典(JVM System Dictionary),如果这个系统字典被当成了一个单一根节点进行处理,那么当存在大量的加载类时就会出现较长时间的耗时。 Rememebered Sets and Processed BuffersRset帮助维护和跟踪指向G1区间的引用,而这些区间本身拥有这些RSet。还记得我们在第4章介绍过的并行Refinement线程吗?这些线程的任务是扫描更新日志缓存,并且更新区间的RSet。为了更加有效地支援这些Refinement线程的工作,在并行回收阶段,所有未被处理的缓存(已经有日志写在里面了)都会被工作线程拿来处理,这些缓存也被称为日志里面的处理缓存。 为了限制花费在更新RSet上的时间,G1通过选项-XX:MaxGCPauseMills设置了目标暂停时间,采用相对于整个停顿目标时间百分比的方式,限制了更新RSet花费的总时长,让评估暂停阶段把最大量的时候花费在拷贝存活对象上。这个目标时间默认为整个停顿时间的10%,例如整个停顿时间是10s,那么花费在更新RSet上的时间最大为ls。G1 GC的设计目标是让更多的停顿时间花费在拷贝存活对象上面,因此暂停时间的10%被用于更新RSet也是比较合理的,百分比大了,花在干具体业务(各阶段拷贝存活对象)上的时间也就少了。 如果你发现这个值不太准确或者不符合你的实际需求,这里可以通过更新选项-XX:G1RSetUpdatingPauseTimePercent来改变这个更新RSet的目标时间值。切记,如果你改变了花费在更新RSet上的时间,那你必须有把握工作线程可以在回收暂停阶段完成它们的工作,如果不能,那这部分工作会被放到并行Refinement线程里面去执行,这会导致并行工作量增加、并行回收次数增多。最坏的情况是如果并行Refinement线程也不能完成任务,那么Java应用程序就会被暂停,原本负责执行Java应用程序的资源就会直接接手任务,这个画面“太美”不敢看!大家要尽量避免这种情况发生。 注意,-XX:G1ConcRefinementThreads选项的值默认和-XX:ParallelGCThreads的值一样,这意味着对于-XX:ParallelGCThreads选项的修改会同样改变-XX:G1ConcRefinementThreads选项的值。 在当前CSet里面回收之前,CSet内部的每个区间的Rset都需要被扫描,主要目的是找到CSet区间内部的引用关系。一个有较多存活对象的区间容易导致Rset的粒度变细,即每个区间对应的表格会从粗粒度变为细粒度,也可以理解为里面对象增多后扫描一个Rset需要更长的扫描时间,这样你就会看到更多的时间被花费在了扫描RSet上面。也可以理解为扫描时间取决于RSet数据结构的粗细粒度。 Summarizing Remembered SetsXX:+G1SummarizeRSetStats选项用于统计RSet的密度数量(细粒度或者粗粒度),这个密度帮助决定是否并行Refinement线程有能力去应对更新缓存的工作,并且收集更多关于Nmethods的信息。这个选项每隔n次GC暂停收集一次RSet的统计信息,这个n次由选项-XX:G1SummarizeRSetStatsPeriod=n决定,也是需要通过选项进行设置的。 注意,-XX:+G1SummarizeRSetStats选项是一个诊断选项,因此必须启用-XX:+UnlockDiagnosticVMOptions选项才可以启用-XX:+G1SummarizeRSetStats选项。 PDF书籍下载地址：https://github.com/jiankunking/books-recommendation/tree/master/Java]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>读书笔记</tag>
        <tag>GC</tag>
        <tag>G1</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库索引设计与优化 笔记]]></title>
    <url>%2Frelational-database-index-design-and-the-optimizers-note.html</url>
    <content type="text"><![CDATA[本文整理自：《数据库索引设计与优化》 作者：Tapio Lahdenmaki，Michael Leach 出版时间：2015-06-01 概述索引误区误区1：索引层级不要超过5层由于非叶子页通常都会留在内存或者读缓存中，所以通常索引任意一个叶子页的时间为10ms~20ms，这是固定的。所以，对索引层数的限制是没有什么意义的。 误区2：单表的索引数不要超过6个我建议不要给表的索引数目设置上限。 保证所有的SQL语句都能够流畅运行是设计的底线。我们总能找到一种方法来达到这一点。如果为了达到这一点需要在表上创建10个索引，那么你就应该在表上建立10个索引。 误区3：不应该索引不稳定的列索引行是按索引键的顺序存储的，所以当索引键中存一列被更新时，DBMS可能不得不把相应的行从旧的索引位置移到新的位置来保持这一顺序。这个新的位迓可能与旧的位晋位于相同的叶子贞上，在这种情况下，只有一个页会受到影响。然而，如果被修改的键是第一列或唯一的列，那么新的索引行可能必须被迁移到一个不同的叶子页上，即DBMS必须更新两个叶子页。三十年前，如果这个索引为一个4层索引，这也许需要6次磁盘随机读取：3次常规读取，即2次非叶子页读取和1次叶子页读取，加上新的位置所涉及的3次随机读取。当一次随机读取耗时30ms 时，迁移一个索引行可能会给该更新操作额外增加6 x 30ms =180ms 的响应时间。因此，不稳定的列很少被索引就不足为奇了。 现在，当四层索引中三个层级的非叶子页保留在内存中时，一次磁盘随机读取需要 l0 ms ，响应的时间变成了 2 x 10ms = 20ms 。此外，许多索引为多列索引，也称作复合或组合索引，它通常包含多列，以使得索引键值唯一。当不稳定的列为复合索引的尾列更新这个不稳定的列绝不会导致其迁移到新的叶子页。因此，在当前的磁盘条件下，更新一个不稳定的列只会对该更新操作增加10ms的响应时间。 在当前磁盘条件下，只有在更新频率多于10次/秒的情况下，不稳定列才可能成为问题。 创建索引的目的应该是在硬件容量限制的前提下保证所有的数据库调用运行的足够快。 系统化的索引设计 找到由于索引不合适而导致运行太慢的查询语句 最差输入：导致执行时间最长的变量值 设计索引，使所有查询语句都运行的足够快 表的维护（插入、更新、删除）也必须足够快 表和索引结构DBMS 会意识到多个索引或表页需要被顺序地读取，且能识別出那些不在缓冲池中的页。随后，它将发出多页I/O请求，每次请求的页的数量由DBMS决定。只有那些不在缓冲池中的页会被从磁盘服务器上读取，因为那些已经在缓冲池中的页中可能包含了尚未被写人磁盘的更新数据。 SQL处理过程谓词WHERE子句由一个或者多个谓词（搜索参数）组成。 123456# SQL 3.1WHERE SEX = &apos;M&apos; AND (WEIGHT &gt; 90 OR HEIGHT &gt; 190) SQL 3.1中有三个简单谓词，它们是： SEX = ‘M’ WEIGHT &gt; 90 HEIGHT &gt; 190 同样，它们也可以被认为是两个组合谓词： WEIGHT &gt; 90 OR HEIGHT &gt; 190 SEX = ‘M’ AND ( WEIGHT &gt; 90 OR HEIGHT &gt; 190 ) 谓词表达式是索引设计的主要入手点。如果一个索引能够满足SELECT查询语句的所有谓词表达式，那么优化器就很有可能建立起一个高效的访问路径。 核实确认访问路径（执行计划） 为SELETE语句创建理想的索引【重点】 很多调优人员（尽管没经验）认为，如果一个SQL语句使用了索引，那这个 SQL就是被很好地优化过的，我对此感到很惊讶。你应该总是问自己，”这是不是可用的最好的索引？” 或 “再添加另外一个索引能否提升响应性能？”，又或者 “全表扫描会不会更快地返回结果？” 三星索引 三星索引：查询语句的理想索引。 1234567DECLARE CURSOR41 CURSOR FORSELECT CNO, FNAMEFROM CUSTWHERE LNAME = :LNAME AND CITY = :CITYORDER BY FNAME 星级是如何给定的如果与一个查询相关的索引行是相邻的，或者至少相距足够靠近的话，那这个索引就可以被标记上第一颗星。这最小化了必须扫描的索引片的宽度。 如果索引行的顺序与查询语句的需求一致，则索引可以被标记上第二颗星。这排除了排序操作。 如果索引行包含的查询语句中的所有列，那么索引就可以被标记上第三颗星。这避免了访问表的操作：仅访问索引就可以了。 对于这三颗星，第三颗通常是最重要的。将一个列排除在索引之外可能会导致许多速度较慢的磁盘随机读。我们把一个至少包含第三颗星的索引称为对应查询语句的宽索引。 宽索引：宽索引是指一个至少满足第三颗星的索引。该索引包含了SELECT语句所涉及的所有列，因而能够使得查询只需访问索引而无需访问表。 为了满足第一颗星首先取出所有等值谓词的列（WHERE COL=…）。把这些列作为索引最开头的列——以任意顺序都可以。对于CURSOR41来说，三星索引可以以LNAME、CITY或者以CITY、LNAME开头。在这两种情况下，必须扫描的索引片宽度将被缩减至最窄。 为了满足第二颗星将ORDER BY列加入到索引中。不要改变这些列的顺序，但是忽略那些在第一步中已经加入索引的列。例如，如果CURSOR41在ORDER BY 中有重复的列，如ORDER BY LNAME、FNAME或者是ORDER BY FNAME、CITY，只有FNAME需要在这步中被加入到索引中去。当FNAME是索引的第三列时，结果集中的记录无须排序就已经是以正确的顺序排列的了。第一次读取操作将返回FNAME值最小的那一行。 为了满足第三颗星将查询语句中剩余的列加到索引中去，列在索引中添加的顺序对查询语句的性能没有影响，但是将易变的列放在最后能降低更新的成本。现在，索引已包含了满足无须回表的访问路径所需的所有列。 最终三星索引将会是：(LNAME, CITY, FNAME, CNO) 或 (CITY, LNAME, FNAME, CNO) CURSOR41在以下方面是最为挑剔的： WHERE 条件不包含范围谓词（BETWEEN、&gt;、&gt;=等） FROM 语句只涉及单表 所有谓词对于优化器来说都足够简单 范围谓词与三星索引1234567DECLARE CURSOR43 CURSOR FORSELECT CNO, FNAMEFROM CUSTWHERE LNAME BETWEEN :LNAME1 AND :LNAME2 AND CITY = :CITY ORDER BY FNAME 让我们尝试为这个 CURSOR 设计一个三星索引。大部分的推论与 CURSOR41 相同，但是“BETWEEN 谓词”将“=谓词”替代后将会有很大的影响。我们将会以相反的顺序依次考虑三颗星，按理说，这代表了理解的难度。 首先是最简单的星（虽然非常重要），第三颗星。按照先前所述，确保查询语句中的所有列都在索引中就能满足第三颗星。这样不需要访问表，那么同步读也就不会造成问题。 添加 ORDER BY 列能使索引满足第二颗星，但是这个仅在将其放在 BETWEEN 谓词列 LNAME 之前的情况下才成立，如索引 (CITY, FNAME, LNAME)。由于 CITY 的值只有一个（=谓词），所以使用这个索引可以使结果集以 FNAME 的顺序排列，而不需要额外的排序。但是如果 ORDER BY 字段加在 BETWEEN 谓词列 LNAME 后面，如索引 (CITY, LNAME, FNAME)，那么索引行不是按 FNAME 顺序排列的，因而就需要进行排序操作。因此，为了满足第二颗星，FNAME 必须在 BETWEEN 谓词列 LNAME 前面，如索引 (FNAME, …) 或索引 (CITY, FNAME, …)。 再考虑第一颗星，如果 CITY 是索引的第一个列，那我们将会有一个相对较窄的索引片需要扫描（MC=1），这取决于 CITY 的过滤因子。但是如果用索引 (CITY, LNAME, …) 的话，索引片会更窄，这样在有两个匹配列的情况下我们只需要访问真正需要的索引行。但是，为了做到这样，并从一个很窄的索引片中获益，其他列（如 FNAME）就不能放在这两列之间。 MC:match column（匹配列） 所以我们的理想索引会有几颗星呢？首先它一定能有第三颗星，但是，正如我们刚才所说，我们只能有第一颗星或者第二颗星，而不能同时拥有两者！换句话说，我们只能二选一： 避免排序 — 拥有第二颗星 拥有可能的最窄索引片，不仅将需要处理的索引行数降至最低，而且将后续处理量，特别是表中数据行的同步读，减少到最少 — 拥有第一颗星 在这个例子中，BETWEEN 谓词或者任何其他范围谓词的出现，意味着我们不能同时拥有第一颗星和第二颗星。也就是说我们不能拥有一个三星索引。这就意味着我们需要在第一颗星和第二颗星中做出选择。通常这不是一个困难的选择，因为第一颗星一般比第二颗星更重要，虽然并不总是这样。 为查询语句设计最佳索引的算法根据以上的讨论，理想的索引是一个三星索引。然而，正如我们所见，当存在范围谓词时，这是不可能实现的。我们（也许）不得不牺牲第二颗星来满足一个更窄的索引片（第一颗星），这样，最佳索引就只拥有两颗星。这也就是为什么我们需要仔细区分理想和最佳。在这个例子中理想索引是不可能实现的。将这层因素考虑在内，我们可以对所有情况下创建最佳索引（也许不是理想索引）的过程公式化。创建出的索引将拥有三颗星或者两颗星。 首先设计一个索引片尽可能窄（第一颗星）的宽索引（第三颗星）。如果查询使用这个索引时不需要排序（第二颗星），那这个索引就是三星索引。否则这个索引只能是二星索引，牺牲第二颗星。或者采用另一种选择，避免排序，牺牲第一颗星保留第二颗星。这种二星索引中的一个将会是相应查询语句的最佳索引。 为查询语句创建最佳索引的算法候选 A 取出对于优化器来说不过分复杂的等值谓词列。将这些列作为索引的前导列(以任意顺序皆可)。 将选择性最好的范围谓词作为索引的下一个列，如果存在的话。最好的选择性是指对于最差的输入值有最低的过滤因子。只考虑对于优化器来说不过分复杂的范围谓词。 以正确的顺序添加ORDER BY语列，忽略在第一步或者第二步已添加的列。 以任意顺序将 SELECT 语句中其余的列添加至索引中（但是需要以不易变的列开始）。 举例：CURSOR43 候选 A 为 (CITY, LNAME, FNAME, NCNO)。 由于 FNAME 在范围谓词列 LNAME 的后面，候选 A 引起了 CURSOR43 的一次排序操作。 候选 B如果候选 A 引起了所给查询语句的一次排序操作，那么还可以设计候选 B。根据定义，对于候选 B 来说第二颗星比第一颗星更重要。 取出对于优化器来说不过分复杂的等值谓词列。将这些列作为索引的前导列(以任意顺序皆可)。 以正确顺序添加 ORDER BY 列（如果 ORDER BY 列有 DESC 的话，加上 DESC）。忽略在第1步中已经添加的列。 以任意顺序将 SELECT 语句中其余的列添加至索引中（但是需要以不易变的列开始）。 举例：CURSOR43 候选 B 为 (CITY, FNAME, LNAME, CNO)。 需要注意的是，到目前为止，我们所做的只是设计理想索引或是最佳索引。但是这是否是实际可行的，我们在这个阶段还不好说。 前瞻性的索引设计基本概念访问根据定义，DBMS读取一个索引行或一个表行的成本称为一次访问 : 索引访问或表访问。如果DBMS扫描索引或表的一个片段(被读取的行在物理上是彼此相邻的)，那么第一行的读取即为一次随机访问。对于后续行的读取，每行都是一次顺序访问。在当前的硬件条件下，顺序访问的成本比随机访问的成本低得多。一次索引访问的成本与一次表访问的成本基本上是相同的。 读取一组连续的索引行物理上彼此相邻是什么意思? 索引上的所有行都通过指针链接在一起，链接的先后顺序由索引的键值严格定义。当几个索引行的键值相同时，就根据索引行存储的指针值进行链接。在传统的索引设计(从某个角度看，是理想化的)中，链表从LP1(叶子页1)开始，随后链接LP2，以此类推。这样(假设每个磁道可以放12个叶子页，当前的硬件通常可以容纳更多)，叶子页就组成了一个连续的文件，LP1至LP12存储在磁盘柱面的第一个磁道，LP13至LP24存储在下一个磁道，如此继续，当第一个柱面存满后，下一组LP就会被存储在下一个柱面的首个磁道上。换句话说，就是叶子页之间没有其他页。 现在，读取一个连续的索引行(即一个索引片，或者包含了单个键值或者一个范围的键值所对应的索引行)就非常快了。一次磁盘旋转会将多个叶子页读取进内存中，而且只有在磁盘指针移到下一个柱面时才需要进行一次短暂的寻址、 不过，这个完美的顺序还是会被打破的，至少有以下三个影响因素 : 如果一个叶子页没有足够的空间存储新插入的索引行，那么叶子页就必须被分裂。之后链表仍会按照正确的顺序链接索引行，但是这与底层的物理存储顺序就不再一致了，一些按道理应该是顺序的访问就变成随机访问了。不过索引的充足可以再次恢复最理想的顺序。 意向不到的数据增长可能会填满原本连续的空间(区或类似的概念)。操作系统于是就会寻找另外有一个连续的空间，并将它连接到原来空间的后面。这时候从第一个区跨到第二个区访问就会产生一次随机访问，不过这种情况影响不大。 RAID 5条带会将前几个叶子页存储在一个驱动器上，将后面的叶子页存放在另外的驱动器上。这就会产生额外的随机读，但实际上条带的积极作用要大过随机读带来的性能恶化，一个智能的磁盘服务器可以将后续的叶子页并行的从多个驱动器上读取至磁盘缓存中，从而大大降低了单个叶子页的I/O时间。此外，在RAID 5条带策略下，一个被频繁访问的索引的不太可能导致某一个磁盘负载过高，因为I/O请求会被均匀低分布到RAID 5阵列内的多个磁盘驱动器。 忽略上述情况，我们仍然假设，如果两个索引行在链表上彼此相邻(或者在唯一索引中，相同键值的行指针意味着彼此相邻)，那么我们就认为这两行在物理上也相邻。这就意味着QUBE认为所有的索引都有最理想的顺序。 读取一组连续的表行读取一组连续的表行有如下两种情况： 全表扫描从TP1(表页1)开始，读取该页上所有的记录，然后再访问TP2，一次类推。按照记录在表页中存储的顺序进行读取，没有其他特殊的顺序。 聚簇索引扫描读取索引片上第一个索引行，然后获取相应的表行，再访问第二个索引行，以此类推。如果索引行与对应的表行记录顺序完全一致(聚簇率为100%)，那么除了第一次之外的所有表访问就都是顺序访问。表记录的链接方式跟索引不一样。单个表页中记录的顺序无关紧要，只要访问的下一个表记录在同一个表页或者相邻的下一个表页内就可以了。 同索引一样，存储表的传统方式也是将所有表页保留在一个连续的空间内。引起顺序杂乱或碎片化的因素也和索引中的相似，但又两个地方不同： 如果往表中插入的记录在聚簇索引所定义的主页中装不下，则通常不会移动现有的行，而是会将新插入的记录存储到离主页尽可能近的表页中。对第二个页的随机I/O会使聚簇索引扫描变得更慢，但是如果这条记录离主页很近，这些额外的开销就可以被避免，因为顺预读功能会一次性将多个表页装载到数据库缓存中。即使顺序预读功能没有使用，也只有当该页在数据库缓存被覆盖的情况下才会发生额外的随机I/O。 一条记录被更新后，可能因为表行过长导致其无法再存储于当前的表页中。这是DBMA就必须将该行记录迁移至另外一个表页中，同时在原有的表页中存储指向新表页的指针。当该行被访问时，会引入额外的随机访问。表可以通过重组来还原行记录的顺序，从而减少不必要的随机访问。 计算访问次数随机访问我们首先思考一下磁盘读与访问的区别。一次磁盘读所访问的对象是一个页，而一次访问的访问对象则是一行。一次随机磁盘读会将一整页(通常会包含很多行)读取至数据库的缓冲池中，但是根据定义，前后两次随机读不太可能会访问到同一个页。 使用满足需求的成本最低的索引还是所能达到的最有索引当有多个等值谓词作为匹配列时，我们需要考虑这些列在索引上的先后顺序。经常变化的列应当尽可能的排在后面。 更改现有索引列的顺序和在现有索引列之间添加新列同样危险。在这两种情况下，现有的select的执行速度都可能会急剧下降，因为匹配列减少了，或者引入了排序(导致过早产生结果集) 半宽索引(最大化索引过滤)在现有索引的末端添加缺少的谓词列可以消除大量的随机访问，因为这样能引入索引过滤过程。 影响索引设计过程的因素I/O时间估算: 随机读取 10ms(页的大小为4KB或8KB) 顺序读取 40MB/s 这些数据是假定系统使用当前硬件并在一个合理的负载下运行时的值。一些系统可能运行的更慢或处理超负荷状态。 困难谓词大体上，假设一个谓词的判定结果为false，而这时如果不检查其他谓词就不能确定地将一行记录排除在外，那么这类谓词对优化器而言就是太过困难的。 过滤因子隐患当以下三个条件同时满足时，这种过滤因子隐患可能会产生 : 访问路径中没有排序 第一屏结果一建立就回应 不是所有的谓词字段都参与定义带扫描的索引片–换句话说就是，不是所有的字段都是匹配字段。 被动式索引设计 被动式的方法与莱特兄弟创造守架飞机的经历非常相似。本质上就是把查询放在一起，推下悬崖，然后看他能否起飞。换句话说，就是为应用设计一个没有索引的原型，然后开始运行一些查询。又或者，创建原始索引集，然后通过运行应用来看那些索引被用到，那些没有被用到。即使是一个小型的数据库系统，运行速度慢的查询也会被很快凸显出来。 被动式调优的方法也被用来理解和调优一个性能没有满足预期的已有应用。 对结果集排序除了全表扫描和全索引扫描，结果集的排序就是最有用的警示信号了。引起排序的原因可能有以下两种: 没有可使查询语句避免排序的索引。 优化器所选择的访问路径包含了一次多余的排序。 有很多数据库顾问将排序视为敌人。我们认为，哪些强调随机I/O带来致命影响的顾问更值得信任。 成本估算一些数据库管理系统的EXPLAIN功能显示了优化器对所选访问路径的本地响应时间的估算，或至少显示了对CPU时间的估算。 不幸的是，以下两个严重问题限制了使用成本估算方法的价值: 优化器所做出的的本地响应时间估算可能与实际相差很大 当谓词使用绑定变量时(显然这是很普遍的)，优化器对过滤因子的估算是基于平均输入值的，或更差情况下，基于默认值。为了获取更有价值的最差情况估值，EXPLAIN中的绑定变量必须用最差情况下的输入值来代替。这是一个需要应用知识的累人操作。 为表连接设计索引预测表的访问顺序在大部分情况下，可以使用以下经验法则来预测最佳的表访问顺序 : 将包含最低数量本地行的表作为外层表。 本地行的数量是指最大过滤因子过滤本地谓词之后所剩余的行数。 经验法则忽略了以下因素: 排序。 很小的表。非常小的表及其索引可能长期存在于数据库的缓冲池中，至少在一个连接查询中，没有页会被读取多次。在这样的表和索引上进行随机读取所耗费的时间小于0.1ms，至少在页被第一次读取之后是这样的。所以，当这样的表不是外层表时，对其大量的随机读取也不会称为问题。 聚簇比例。索引中行的顺序和表中行的顺序的关联性(对于聚簇索引而言，该关联性在表重组后为100%)，可能会影响对最佳访问顺序的选择，当然，除非索引是宽索引。 最好的基于成本的优化器在进行路径选择时会把这些因素考虑进来。因此，他找出的访问顺序可能比我们基于本地行的数量的经验所得出的结果更优。 合并扫描连接和哈希连接合并扫描连接执行过程如下 执行表或索引扫描以找出满足本地谓词的所有行。 随后可能会进行排序，如果这些扫描未按所要求的顺序提供结果集。 对前两者生成的临时表进行合并。 在以下情况下，合并扫描会比嵌套循环快。 用于连接的字段上没有可用的索引。在这种情况下，若使用嵌套循环，那么内层表可能需要被扫描很多次。在实际情况中，用于连接的列上面没有索引的情况很少见，因为大部分连接谓词都是基于“主键等于外键”这一条件的。 结果表很大。在这种情况下，若使用嵌套循环连接，可能会导致相同的页被不断的重复访问。 连接查询中不止一张表的过滤因子很低。如我们所见，嵌套循环可能导致对内层表(或者内层表索引)的大量随机访问。 例如： 12345678DECLARE CURSOR81 CURSOR FORSELECT CNAME, CTYPE, INO, IEURFROM CUST, INVOICEWHERE CUST.CTYPE = :CTYPE AND IDATE &gt; :IDATE AND CUST.CNO = INVOICE.CNO 哈希连接哈希连接本质上是用哈希算法代替排序算法的合并扫描连接。首先，对较小的结果集用哈希算法计算其连接字段，并将其保存在一个临时表中；然后，再扫描其他的表(或索引片)，并通过(计算得到的)哈希值将满足本地谓词条件的每一行记录与临时表中相应的行进行匹配。 若结果行集已在索引中满足了所要求的顺序，那么合并扫描的速度将更快。若合并扫描需要进行排序，那么哈希连接的速度可能更快，尤其是当其中一个行集能够全部留在内存中时(对一个哈希表进行一次随机访问所花费的CPU时间，通常会比排序和合并一行所花费的时间少)。如果两个行集很大，那么哈希表会根据可用内存的大小对哈希表进行分区(同一时间内存中只有一个分区)，而另外一个行集会被扫描多次。 为什么连接的性能表现较差模糊的索引设计“在连接字段上建索引”是最古老的索引建议之一。事实上，这是基于建议的一个扩展 : “为主键创建一个索引，并且为每一个外键创建一个由此外键作为前导列的索引”。在连接谓词上建索引使得嵌套循环称为一个可行的方案，但包含连接谓词列并不一定能够提供完全可接受的响应时间。连接谓词列上的索引和本地谓词上的索引通常都需要是宽索引。而且，不同的表访问顺序可能导致完全不同的索引需求。 为子查询设计索引从性能的角度看，子查询与连接十分相似。实际上，现今的优化器通常会在进行访问路径的选择之前，先将子查询重写为一个连接。若优化器没有进行重写，那么子查询的类型本身可能就决定了表访问顺序。内外层无关联的子查询通常会从最内层的SELECT开始执行。结果集被保存在一张临时表中，等待下一个SELECT的访问。内外层有关联的子查询通常会从最内层的SELECT开始执行。无论是何种情况，同连接一样，应当基于能够形成最快访问路径的表访问顺序进行索引设计。若最佳的表访问顺序未被选中，那么程序开发人员可能需要对语句进行重写，在某些情况下还可能要使用连接。 为UNION语句设计索引通过UNION或UNION ALL连接的SELECT语句是逐个分别进行优化和指向的。因此，应该为每个独立的SELECT设计合适的索引。需要注意一点，带ORDER BY的UNION可能会导致提前物化。 对于表设计的思考冗余数据有两种通过冗余数据优化连接速度的方法： 将某列拷贝至依赖表(向下反范式法)。 将汇总数据添加至父表(向上反范式法)。 向下反范式化不过，总体而言，当我们考虑引入向下反范式化时，需要预测一下冗余字段更新时可能会导致最差情况下的索引随机访问次数。 反范式化的成本考虑性能时最令人关注的通常是，为了更新表及索引上的冗余字段锁带来的I/O时间。在向下反范式化中，这可能需要移动大量的索引行，从而导致一个简单的UPDATE运行的很慢。向上反范式化不太可能因为一次简单的更新操作而引发I/O剧增。不过INSERT，UPDATE和DELETE可能导致父表及其索引上的一些额外I/O。在极端情况下，如每秒10次以上的INSERT或UPDATE，由这些I/O带来的磁盘负载可能会成为问题。 嵌套循环连接和合并扫描连接/哈希连接 VS 反范式化许多数据库专家不愿意将冗余列添加至事务型表上，这是可以理解的。反范式化不仅仅是查询速度和更新速度之间的一个权衡，在某种程度上，他还是性能和数据完整性之间的一个权衡，即使在使用触发器来维护冗余数据的情况下。然而，当嵌套循环引入了过多的随机访问，且MS/HJ耗费了过多的CPU时间时，反范式化可能成为唯一的选择。尽管如此，在决定采用这一极端的方案之前，我们必须确保所有能够避免这一方案的方法都已经考虑过了。 无意识的表设计从性能的角度看，我们非常难以理解为何有那么多的数据库中存在具有1 : 1或1: C(C = 有条件的;即0或1)关系的表。 为何要建四张表而非只建一张CUST表？只要关系永远不会变成1 : M，那么灵活性就不会成为问题。在本例中，客户要么是公司，要么是个人，且不会有客户死亡两次! 将这四张表合成一张部分字段为空的表(对于每一行，要么公司相关的字段为空，要么个人相关的字段为空；同样，所有活着的客户的死亡相关的字段为空)，这是存储空间和性能(随机访问的次数)之间的权衡。为空的数据并不违反范式。 在不考虑硬件性能的情况下设计表可能会有如下问题 : 即便是在最佳索引条件下，随机访问的次数仍可能会很高。 复杂连接可能使得索引设计变得非常困难。 优化器可能对复杂连接做出错误的访问路径选择。 星型连接介绍星型连接与普通连接的差别主要有两个方面： 如下图所示，位于星型结构中心位置的表称为事实表，它的数据量远大于它周围的表—维度表。 最佳的访问路径通常是包含维度表的笛卡尔积，这意味着他们没有相同的冗余列，满足本地谓词的维度表数据行都会参与连接。 SALES 销售记录STORE 出售的店铺ITEM 出售的商品DATE 出售的时间CUST 出售的客户 在星型连接中，事实表的数据量通过都比较大。在这种情况下，至少依据经验法则，事实表应该作为嵌套循环连接方式中最内层的表。一般情况下未读表都没有共同的列，所以这种链接顺序意味着是笛卡尔连接。 事实表的索引事实上，宽索引通常比事实表还大，原因有两个 : 表通常都进行了压缩处理，而索引没有。 新增一条记录通常都追加到表的尾部，因此事实表并不需要分散的空闲空间。但插入到索引(除了聚簇索引)上的位置是随机的。为了避免频繁的索引重组，在当前硬件条件下，通常10亿行的表将花费几个小时，大多数的索引在叶子节点上都需要留出足够的空闲空间，可能为30% ~ 40%。 汇总表即使是在理想索引的情况下，一些针对10亿条记录的事实表进行的查询也会导致大量的I/O访问。提高这类查询的性能的唯一方式就是使用汇总表(查询表)。这类表是反范式化的事实表。如果表不是特别大(比如只包含几百万行数据)，那么这是一个比较可行的方案，因为反模式化查询只需针对汇总表，而不需要多表关联。 如果频繁查询每周的销售情况，那么可以针对每周的消费记录建立一张汇总表。比较好的汇总表设计是根据周，商品，商店汇总一条记录。汇总表根据周和商品进行汇总后，数据量可以大幅度减少。在这种情况下，查询的响应时间可能降低到不到1秒钟。 汇总表上的索引通常会比较小，他唯一的显示因素就是刷新此表所需的时间。如同所有新鲜事物一样，汇总表的方案也会带来新的问题。 如果用户的查询需求多样，汇总表的设计会比索引的设计更困难，不过，已经有一些工具基于查询日志来协助汇总表的设计。 如果优化器不能选择正确的汇总表，那么汇总表的意义就不大；我们不能指望用户来指定查询使用某个合适的汇总表。最好的优化器已经在试着访问合适的表了，虽然SELECT语句实际上查询的是事实表。如果优化器不具备这个能力，或者它的正确率不够高，那么可能就不得不强制指定每个用户只能访问一个汇总表。用户不得不自己选择要使用的汇总表。优化器能自动识别的汇总表通常被称为自动汇总表或物化视图。 多索引访问简介许多数据库管理系统支持从一张表的多个索引处收集制作，或是从单个索引的几个索引片收集，然后比较这些指针集并访问满足WHERE语句中所有谓词条件的数据行。这一能力被称为多索引访问，或被称为索引与(索引交集)和索引或(索引并集)。 多索引访问的思想是：对表中数据分别使用各个索引，最后将满足条件的进行交集或并集操作。 索引和索引重组DBMS如何查找索引行在当前的硬件条件下，非叶子页很可能已经被缓存在数据库缓冲池中，或者至少在磁盘的读缓存中，因为它们经常被频繁的访问。 插入一行会发生什么？如果一张表有一个聚簇索引，那么DBMS会根据聚簇索引的键值尝试将插入的记录放在它所属的表页(主页)中。如果这行记录在主页里放不下，或者当前页被锁住，那么DBMS将会检查邻近的页。在最坏的情况下，新的行会被插入到标的最坏一页。依赖于DBMS和表的类型，已经插入的行通常都不会被移动，否则这将意味着更新表上已经建立的所有索引上的相关指针。当有许多表行未能存在在主页中时，如果表行的顺序很重要，则需要对这个表进行重组—对于那些涉及多张大表的大规模批处理任务而言，通常需要这么做。 当往表中插入一条记录时，DBMS会尝试将索引行添加至其索引建所属的叶子页上，但是该索引页可能没有足够的空闲空间来存放这个索引行，在这种情况下，DBMS将会分裂该叶子页。其中一半的行将被移动到一个新的叶子页上，并尽可能地靠近被分裂的页，但是在最坏的情况下，这个索引页可能会被放置在索引的末尾。除了在每个叶子页上预留部分比例的空闲空间外，也许可以在索引被创建或重组时，每n个页面预留一个空页—当索引分裂无法避免时，这会是一个不错的办法。 当一个索引有一个不断增长的键值时，新行将被添加到索引页的最后，索引页可能永远也不会进行分裂,这样的索引可能不需要任何空闲空间。 叶子页的分裂严重吗？分裂一个索引页只需一次额外的同步读，约10ms。除了两个叶子页以外，DBMS通常还必须更新一个非叶子页，而它很可能已经在内存或者读缓存中了。 在叶子页分裂后，查询任何一条索引行的速度很可能同之前一样快。在最坏情况下，一次分裂会创建一个新的索引层级，但是如果不是从磁盘读取非叶子页的话，这只会增加很少的CPU时间。 然而，叶子页的分裂会导致一个索引片变得更慢。目前为止，我们在所有的场景中都假设串联索引行的链指针总是指向同一页或者下一页，这些页可能已被DBMS预读取。在索引被创建或者重组后，这种假设是接近真实情况的，但是索引片上的每处叶子页分裂都可能会增加额外的两次随机访问—一次是为了查找索引行被移动至的索引页，一次是为了返回到扫描的原始位置。其中第一次随机访问很可能会导致一次磁盘的随机读取(10ms)。 什么时候应该对索引进行重组？插入模式索引重组是为了恢复索引行正确的物理位置，他对于索引片扫描和全索引扫描的性能而言很重要。因为插入模式的不同，增加的索引行可能会以无序的方式来创建。我们需要记住，更新一个列意味着需要删除旧的索引行，并增加一个新的索引行，新索引行的位置由新的索引键值来确定。 下文对三种基本插入模式的讨论基于如下假设 索引是唯一索引。 被删除的索引行锁腾出的空间在重组之前可以被新的索引行重用。 新索引行被添加至索引的尾部(永远递增的键)假设插入了一个索引行，其索引键值比任何已经存在的索引键值都要大，则DBMS就不会分类最后的叶子页，那么就不需要空闲的空间或者进行索引重组了。然而，如果在索引前面的索引行被定期地删除，那么为了回收空闲的空间，索引可能不得不进行重组(一个“爬行”的索引)。 随机插入模式我们稍后将会看到，尽管考虑了空闲空间和重组，对于不同的索引行长度(短，中，长)的处理也是不同的。越长的索引行越难处理，越短的越好处理。 有一个重要的例外场景 : 如果索引行是变长的，那么就需要有空闲的空间去适应任何索引行的增长。 索引重组的代价一个索引可以以多种方式进行重组： 全索引扫描（随机访问且无须排序，或者顺序访问并排序）。 全表扫描（类似 CREATE INDEX; 顺序访问及一次排序）。 由索引重组产生的锁等待依赖于具体的数据库和选项。如果使用的是简易的工具,那么当表或者索引正在被扫描吋，整个表可能被加上一个S锁(更新操作会被阻塞）。如果工具能在扫描期间将更新操作保存下来，并在排序前将它们应用到数据行上，那么锁等待的时间将会缩短很多。 有些时候，大的索引可能不得不在不合适的时间被重组。在最坏情况下，锁的问题可能会导致频繁的重组无法实现。在这种情况下 ， 一些易变的索引可能必需被强制缓存在内存中（将其固定在内存中）。 数据库管理系统相关的索引限制索引列的数量能够复制到索引上的列的个数上限在16至64之间。并非每个人都将这视为一个问题。 Gulutzan 和 Pelzer提出了一个出人意料的建议，如下: 针对所有数据库管理系统的总体建议为：在一个复合索引中最多使用5列。虽然你能够确定数据库管理系统至少能够支持16列，但是5列是一些专家所认为的合理上限。 索引列的总长度复制到索引的列的总长度存在一个上限，该上限的值取决于数据库管理系统。随着宽索引变得越来越流行，这一上限在数据库筲理系统的新版本中在变大。 单表索引数量上限在单表索引数量限制方面，许多数据库产品要么没有上限，要么上限太高以至于无关紧要。 索引大小上限典型的索引大小上限为几GB,而且这一上限正在持续增大。就像大表一样，大索引通常是分区的，这样能够使执行维护程序的成本最小化，并且能将索引分散到多个磁盘驱动器或RAID组上。 索引锁定从更新的时间点到提交的时间点内，如果数据库管理系统给一个索引页或者一个索引页的一部分（如一个子页）加了锁，那么该索引页或子页很能会成为瓶颈，因为插入操作将会变为顺序的。例如，SQL Server 2000就是这样做的，但如果上锁的粒度仅为一行，那么这不可能会成一个问题。 在DB2数据库的z/OS版本中，使用闩锁来保证索引页的物理完幣性。当用闩锁对一个缓冲池中的页加锁时，实际上是在数据库缓冲池中进行了一次置位操作，当释放闩锁时再进行重置。一个页只有在读取或修改时才会被加上闩锁，在当前的处理器条件下耗费时间不到一微秒。而数据完幣性是通过对索引行所指向的表页或表行加上普通锁来保证的（仅对数据上锁）。当程序修改一个表行或表页。这些锁一直不会释放，直至修改被提交。 数据库索引选项索引键决定了这一索引行在索引结构中的位置。当索引键被修改后,DBMS会删除原来的索引行，并将其插入到新的位置上。在最差情况下，索引行会被移动到其他的叶子页上。 其他评估事项宽索引还是理想索引单纯从响应时间来看，理想的索引并非具有完全的优势，我们给出了如下结论 : 虽然三星索引有一定的优势，尤其是在结果集为空的情况下，但是这个优势并不也别明显，而且会带来与新增索引相关的额外开销。 组织索引设计过程简介在大公司里，一个合理的折中方案是聘用50/50的专家，他们会将50%的时间用来应用程序开发，另外50%的时间用于协助同事进行索引评估及其他性能问题的处理（比如根据EXPLAIN的输出内容解决某个优化器问题）。经验显示，为每5至10位应用开发者配一名50/50专家的方式效果很好。 索引设计需要同事掌握技术技能以及应用系统知识。相比让数据库专家熟悉应用系统的细节而言，教会开发人员索引技能是更容易的。 PDF书籍下载地址：https://github.com/jiankunking/books-recommendation/tree/master/Database]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>Design</tag>
        <tag>MySQL</tag>
        <tag>读书笔记</tag>
        <tag>Database</tag>
        <tag>Oracle</tag>
        <tag>SQL Server</tag>
        <tag>DB2</tag>
        <tag>Index</tag>
        <tag>Optimizers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java性能优化权威指南 笔记]]></title>
    <url>%2Fjava-performance-note.html</url>
    <content type="text"><![CDATA[本文整理自：《Java性能优化权威指南》 作者：Charlie Hunt / Binu John 出版时间：2014-03 操作系统性能监控CPU使用率大多数的操作系统的CPU使用率分为用户态CPU使用率和系统态CPU使用率。 用户态CPU使用率是指执行应用程序代码的时间占总CPU时间的百分比。 系统态CPU使用率是指应用执行操作系统调用的时间占总CPU时间的百分比。系统态CPU使用率高意味着共享资源有竞争或者I/O设备之间有大量的交互。 既然原本用于执行操作系统内核调用的CPU周期也可以用来执行应用代码，所以理想情况下，应用达到最高性能和扩展性时，它的系统态CPU使用率为0%，所以提高应用性能和扩展性的一个目标是尽可能降低系统态CPU使用率。 对于计算密集型应用来说，不仅要监控用户态和系统态CPU使用率，还要进一步监控每时钟指令数（Instructions Per Clock，IPC）或每指令时钟周期（Cycles Per Instruction，CPI）等指标。这两个指标对于计算密集型应用来说很重要，因为现代操作系统自带的CPU使用率监控工具只能报告CPU使用率，而没有CPU执行指令占用CPU时钟周期的百分比，这意味着，即便CPU在等待着内存中的数据，操作系统工具仍然会报告CPU繁忙。这种情况通常被称为停滞。当CPU执行指令所用的操作数据不在寄存器或者缓存中时，就会发生停滞，由于指令执行前必须等待数据从内存中装入CPU寄存器，所以一旦发生停滞，就会浪费时钟周期。CPU停滞通常会等待好几百个时钟周期，因此提高计算密集型应用性能的策略就是减少停滞或者改善CPU高速缓存使用率，从而减少CPU在等待内存数据时浪费的时钟周期。 CPU调度程序运行队列监控CPU调度程序运行队列对于分辨系统是否满负荷也有重要意义。运行队列中就是那些已准备好运行、正等待可用CPU的轻量级进程。如果准备运行的轻量级进程数超过系统所能处理的上限，运行队列就会很长。运行队列长表明系统负载可能饱和。系统运行队列长度等于虚拟机处理器的个数时，用户不会明显感觉到性能下降。此处虚拟处理器的个数就是系统硬件线程的个数，也是Java API Runtime.availableProcessors()的返回值。当运行队列长度达到虚拟处理的4倍或者更多时，系统的响应就非常迟缓了。 一般性的指导原则是：如果在很长一段时间里，运行队列的长度一直都超过虚拟处理器个数的1倍，就需要关注了，只是暂时还不需要立刻采取行动。如果在很长一段时间里，运行队列长度达到虚拟处理器个数的3~4倍或更高，则需要立刻引起注意和采取行动。 内存使用率系统在进行页面交换或者使用虚拟内存时，Java应用或JVM会表现出明显的性能问题。当应用运行所需的内存超过可用物理内存时，就会发生页面交换。为了应对这种可能出现的情况，通常要为系统配置swap空间。swap空间一般会在一个独立的磁盘分区上。当应用耗尽内存时，操作系统会将应用的一部分置换到磁盘上的swap空间。通常是应用中最少运行的部分，以免影响整个应用或者应用最忙的那部分。当访问应用中被置换出去的部分时，就必须将它从磁盘置换进内存，而这种置换活动会对应用的响应性和吞吐量造成很大影响。 JVM垃圾收集器在系统页面交换时的性能也很差，这是由于垃圾收集器为了回收不可达对象所占用的空间，需要访问大量的内存。如果Java堆得一部分被置换出去，就必须先置换进内存以便垃圾收集器扫描存活对象，这会增加垃圾收集的持续时间。垃圾收集是一种Stop-The-World操作，即停止所有正在运行的应用线程，如果此时系统正在进行页面交换，则会引起JVM长时间的停顿。 监控抢占式上下文切换让步式上下文切换时指执行线程主动释放CPU，抢占式上下文切换时指线程因为分配的时间片用尽而被迫放弃CPU或者被其他优先级更高的线程锁抢占。pidstat的输出结果中cswch/s是每秒的让步式上下文切换，nvccswch/s是抢占式上下文切换。 监控线程迁移我们发现，待运行线程在处理器之前的迁移也会导致性能的下降。大多数操作系统的CPU调度程序会将待运行线程分配给上次运行它的虚拟处理器。如果这个虚拟处理器忙，调度程序就会将待处理线程迁移到其他可用的虚拟处理器。线程迁移会对应用性能造成影响，这是因为新的虚拟处理器缓存中可能没有待运行线程所需的数据或状态信息。多核系统上运行Java应用可能会发生大量的线程迁移，减少迁移的策略是创建处理器组并将应用分配给这些处理器组。一般性准则是，如果横跨多核或虚拟处理器的Java应用每秒迁移超过500次，将Java应用绑定在处理器组上就有好处。 网络I/O使用率应用性能改进的考虑单次读写数据量小而网络读写量大的应用会消耗大量的系统态CPU，产生大量的系统调用。对于这类应用，减少系统态CPU的策略是减少网络读写的系统调用。此外，使用非阻塞的Java NIO而不是阻塞的java.net.Socket，减少处理请求和发送相应的线程数，也可以改善应用性能。 从非阻塞Socket中读取数据的策略是，应用在每次读请求时尽可能多地读取数据。同样，当往Socket中写数据时，每个写调用应该尽可能多地写。 JVM概览HotSpot 运行时命令行选项HotSpot VM 命令行选项有3类： 标准选项（Standard option）：标准选项是Java virtual Machine Specification要求所有java JVM 都必须实现的选项。 非标准选项（NonStandard option）：非标准选项(以–X为前缀)，不保证也不强制所有JVM实现都必须支持。 非稳定选项（Developer option）：非稳定选项(以-XX为前缀),通常为了特定需要而对JVM的运行进行矫正。选项名称前+代表 true启用，-代表false关闭。 VM生命周期启动器启动HotSpot VM时会执行一系列操作。步骤概述如下： 解析命令行选项 设置堆的大小和JIT编译器如果命令行没有明确设置堆的大小和JIT编译器，启动器则通过自动优化进行设置。 设定环境变量如：LD_LIBRARY_PATH和CLASSPATH 如果命令行有-jar选项，启动器则从指定JAR的manifest中查找Main-Class，否则从命令行读取Main-Class 使用标准Java本地接口（Java Native Interface，JNI）方法JNI_CreateJavaVM在新创建的线程中创建HotSpot VM 一旦创建并初始化号HotSpot VM，就会加载Java Main-Class，启动器也会从Java Main-Class中取得Java main方法的参数 HotSpot VM通过JNI方法CallStartVoidMethod调用Java main方法，并将命令行选项传给它 VM类加载阶段类加载阶段对于给定的Java类或接口，类加载时会依据它的名字找到Java类的二进制类文件，定义Java类，然后创建代表这个类或者接口的java.lang.Class对象。如果没有找到Java类或接口的二进制表示就会抛出NoClassDefFound。此外，类加载阶段会对类的格式进行语法检查，如果有错，则会抛出ClassFormatError或UnsupportedClassVersionError。Java类加载前，HotSpot VM必须先加载它的所有超类和超接口，如果类的继承层次有错，例如Java类是它自己的超类或超接口（类层次递归）,HotSpot VM则会抛出ClassCircularityError。如果所引用的直接超接口本身并不是接口，或者直接超类实际上是接口，HotSpot VM则会抛出IncompatibleClassChangeError。 链接的第一步是验证，检查类文件的语义、常量池符号以及类型。如果检查有错，就会抛出VerifyError。链接的下一步是准备，它会创建静态字段，初始化为标准默认值，以及分配方法表。请注意，此时还没有执行任何Java代码。接下来解析符号引用，这一步是可选的。然后初始化类，运行类构造器。这是迄今为止，类中运行的第一段Java代码。值得注意的是，初始化类需要首先初始化超类（不会初始化超接口）。 如：int的标准默认值为0；public static int alue=123，准备阶段将其初始化为0而不是123，value=123的赋值操作在类构造器&lt;clinit&gt;()中。public static final int value=123，编译时会为value在字段属性表中生成ConstantValue，从而在准备阶段就被初始化成123。 Java Virtual Machine Specification规定首次使用类时进行类初始化，而Java Language Specification则允许在链接阶段符号解析时灵活处理，只要保持语言的语义不变，JVM依次执行加载、链接和初始化，保证及时抛出错误即可。出于性能优化的考虑，通常直到类初始化时HotspotVM才会加载和链接类。这意味着，类A引用类B。加载A不一定导致加载B（除非B需要验证）。执行B的第一条指令会导致初始化B，从而加载和链接B。 类加载器委派当请求类加载器查找和加载某个类时，该类加载器可以转而请求别的类加载器来加载。这被称为类加载器委派。类的首个类加找器称为初始类加载器（Initiating ClassLoader)，最终定义类的类加载器称为定义类加载器（Defining ClassLoader）。就字节码解析而言，某个类的初始类加载器是指对该类进行常量池符号解析的类加载器。 类加载器之间是层级化关系，每个类加载器都可以委派给上一级类加载器。这种委派关系定义了二进制类的查找顺序。Java SE类加载器的层级查找顺序为启动类加载器、扩展类加载器及系统类加载器。系统类加载器是默认的应用程序类加载器，它加载Java类的main方法并从classpath上加载类。应用程序类加载器可以是Java SE系统自带的类加载器，或者由应用程序开发人员提供。扩展类加载器则JavaSE系统实现，它负责从JRE(Java Runtime Environment,Java运行环境）的lib/ext目录下加载类。 启动类加载器启动类加载器是由HotSpot VM实现的，负责加载BOOTCLASSPATH路径中的类，如包含Java SE类库的rt.jar。为了加快启动速度，Client模式的HotSpot VM可以通过称为类教据共享（Class Data Sharing）的特性使用已经预加载的类。这个特性默认为开启，可由HotSpot VM命令行开关-Xshare:on开启，-Xshare:off关闭。到本书编写时为止，Server模式的HotSpot VM还不支持类数据共享，而且即便是Client模式，也只有使用Serial收集器时才支持该机制。 类型安全Java类或接口的名字为全限定名（包括包名）。Java的类型由全限定名和类加载器唯一确定。 HotSpot类元数据类加载时，HotSpot VM会在永久代创建类的内部表示instanceKlass或arrayKlass。instanceKlass应用了与之对应的java.lang.Class实例，后者是前者的Java镜像。HotSpot VM内部使用称为klassOop的数据结构访问instanceKlass。后缀“Oop”表示普通对象指针，所以klassOop是应用java.lang.Class的HotSpot内部抽象，它是指向Klass（与Java类对应的内部表示）的普通对象指针。 内部的类加载数据类加载过程中，HotSpot VM维护了3张散列表。SystemDictionary包含已加载的类，它将建立类名/类加载器（包括初始类加载器和定义类加载器）与klassOop对象之间的映射。目前只有在安全点事才能移除SystemDictionary中的元素。Placeholder-Table包含当前正在加载的类，它用于检查ClassCircularityError，多线程类加载器并行加载类时也会用到它。LoaderConstraintTable用于追踪类型安全检查的约束条件。这些散列表都需要加锁保证访问安全，在HotSpot VM中，这个锁称为SystemDictionary_lock。通常，HotSpot VM借助类加载器对象锁对加载类的过程进行序列化。 字节码验证Java是一门类型安全语言，官方标准的Java编译器（javac）可以生成合法的类文件和类型安全的字节码，但Java虚拟机无法确保字节码一定是由可信的javac编译器产生的，所以在链接时必须进行字节码验证以保障类型安全。 类数据共享类数据共享是Java 5引人的特性，以缩短Java程序（特別是小程序）的启动时间，同时也能减少它们的内存占用。使用Java HotSpot JRE安装程序在32位平台上安装Java运行环境（JRE)时，安装程序会加载系统jar中的部分类，变成私有的内部表示并转储成文件，称为共享文档(Shared Archive)。如果过没有使用Java HotSpot JRE安装程序，也可以手工生成该文件。之后调用Java虚拟机时，共享文档会映射到JVM内存中，从而减少减少加载这些类的开销，也使得这些类的大部分JVM允数椐能在多个JVM进程间共享。 解释器HotSpot VM解释器是一种基于模板的解释器。JVM启动时，HotSpot VM运行时系统利用内部TemplateTable中的信息在内存中生成解析器。TemplateTable包含于每个字节码对应的机器代码，每个模板描述一个字节码。 HotSpot VM解释器堪于模板的设计要好于传统的switch语句循环方式。switch语句需要重复执行比较操作，最差情况需要和所冇字节码比较。此外，switch语句必须使用单独的软件栈传递 Java 参数。HotSpot VM使用本地C栈传递 Java 参数。一些存储在C变量中的HotSpot VM内部变量，例如Java线程的程序计数器或栈指针，并不能保证总是存储在底层硬件寄存器中。结果，管理这些软件解释器数据结构就会占去总执行时间的相当大一部分。不过总体来说，HotSpot解释器显著缩短HotSpot VM和实体机之间的性能差距，解释速度也明显变快了，然而代价是大量与机器相关的代码。例如，Intel X86平台特定的代码大约有10000行，SPARC平台专用的代码大约打14000行。由于需要支持动态代码生成（JIT编译），整体的代码量和复杂度也显著变大。并且调 试动态生成的机器码（ JIT编译代码）比调试静态代码困难多了。虽然这些不利于运行时系统的改善，但也并非不可能完成的任务。 异常处理当与Java的语义约束冲突时，Java虚拟机会用异常通知程序。异常处理由HotSpot VM解释器、JIT编译器和其他HotSpot VM组件一起协作实现。异常处理主要有两种情形，同一方法中抛出和捕获异常，或由调用方法捕获异常。异常可以由抛出字节码、VM内部调用返回、JNI调用返回或Java调用返回所引发。 线程管理HotSpot VM通过协作、轮询的机制创建安全点。简中来说，线程会经常询问：“我该在安全点停住么？ ”高效地询问这个问题并不是件容易的事。线程在状态变迁的过程中，会经常询问这个问题，但并非所有的状态变迁都会如此询问，比如线程离开HotSpot VM进入本地代码的情况。此外，JIT编译代码从java方法中返回或正作循环迭代的某个阶段时，线程也会询问“我该在安全点停住吗？ ”。正在执行解释代码的线程通常不会询问它们是否该在安全点停住。相反，当解释器切换到不同的分配表时，会请求安全点。切换操作中包含一部分代码，用以询问何时离开安全点。当离开安全点时，分配表会再次切换回来。一旦请求了安全点，VMThread就必须在继续执行VM操作前等待，直到确定所行线程都已进入安全点保全状态为止。在安全点时，VMThread用Threads_lock阻塞所有正在运行的线程，VM操作完成后 , VMThread释放Threads_lock。 Java本地接口（JNI）切记，一旦在应用中使用JNI，就意味着丧失了Java平台的两个好处。首先，依赖JNI的Java应用难以在多种异构的硬件平台上运行。即便应用中Java语言编写的部分可以移植到多种硬件平台，采用本地编程语言的部分也需要重新编译。换句话说，一旦使用JNI就失去了Java承诺的特性，即“一次编写，到处运行”。其次，Java是强类型和安全的语言,本地语言如C或C++则不是。因此，Java开发者用JNI编写应用时必须格外小心。误用本地方法可能破坏整个应用。鉴于此，在调JNI方法前，Java应用常常需要安仝检查。额外的安全检查以及HotSpot VM在Java与JNI之间的数据复制会降低应用的性能。 HotSpot VM追踪正在执行本地方法的线程时必须特別小心。在HotSpot VM的某些活动过程中，尤其是垃圾收集的某些阶段，线程必须在安全点时暂停，以保证Java内存堆不被更改，确保垃圾收集的准确性。当HotSpot VM线程执行本地代码到达安全点时，线程可以继续执行本地代码，直到它Java代码或者发起JNI调用为止。 VM致命错误处理HotSpot内部使用信号进行通信。当无法识别信号时，将调用致命错误处理程序。在无法识别的情况下，它可能来自应用程序JNI代码，OS本地库，JRE本地库或JVM本身的错误。 HotSpot VM垃圾收集器分代垃圾收集垃圾收集器不需要扫描整个（可能比新生代更大）老年代就能识别新生代中的存活对象，从而缩短Minor GC的时间。HotSpot VM的垃圾收集器使用称为卡表（CardTable)的数据结构来达到这个目的。老年代以512字节为块划分成若十张卡（Card)。卡表是个单字节数组，每个数组元素对应堆中的一张卡。每次老年代对象中某个引用新生代的字段发生变化时，HotSpot VM就必须将该卡所对位的卡表元素设置为适当的值，从而将该引用字段所在的卡标记为脏。在Minor GC过程中，垃圾收集器只会在脏卡中扫描查找老年代-新生代引用。 HotSpot VM的字节码解释器和JIT编译器使用写屏障(Write Barrier)维护卡表。写屏障是一小段将卡状态设罝为脏的代码。解释器每次执行更新引用的字节码时，都会执行一段写屏障；JIT 编译器在生成更新引用的代码后，也会生成一段写屏障。虽然写屏障使得应用线程增加了一些性能开销，但Minor GC变快了许多，整天的垃圾收集效率也提高了许多。通常应用的吞吐量也会有所改善。 新生代需要指出的是，在Minor GC过程中，Survivor可能不足以容纳Eden和另一个Survivor中的存活对象。如果Survivor 中的存活对象溢出，多余的对象将被移到老年代。这称为过早提升(Premature Promotion)。这会导致老年代中短期存活对象的增长，可能会引发严重的性能问题。再进一步说，在Minor GC过程中，如果老年代满了而无法容纳更多的对象，Minor GC之后通常就会进行Full GC,这将导致遍历整个Java堆。这称为提升失败（Promotion Failure）。 快速内存分配对象内存分配器的操作需要和垃圾收集器紧密配合。垃圾收集器必须记录它冋收的空间，而分配器在重用堆空间之前需要找到可以满足其分配需求的空闲空间。垃圾收集器以复制方式回收HotSpot VM新生代，其好处在于回收以后Eden总为空，在Eden中运用被称为指计碰撞(Bump-the-Pointer)的技术就可以有效地分配空间。这种技术追踪最后一个分配的对象（常称为top),当有新的分配请求时，分配器只需要检查top和eden未端之间的空间是否能容纳。如果能容纳，top则跳到新近分配对象的未端。 重要的Java应用大多是多线程的，因此内存分配的操作需要考虑多线程安全。如果只用全局锁，在Eden中的分配操作就会成为瓶颈而降低性能。HotSpot VM没有采用这种方式，而是以一种称为线程本地分配缓冲区（thread-Local Allocation Buffer,TLAB)的技术，为每个线程设设置各自的缓冲区(即Eden的一小块），以此改善多线程分配的吞吐量。因为每个TLAB都只有一个线程从中分配对象，所以可以使用指针碰撞技术快速分配而不需要任何锁。然而当线程的TLAB填满需要获取新的空间时（不常见），它就需要采用多线程安全的方式了。大部分时候，HotSpot VM的new Object()操作只需要大约十条指令。垃圾收集器清空Eden区域，然后就可以支持快速内存分配了。 HotSpot VM JIT编译器经典的寄存器分配策略是图着色算法，通常可以使机器寄存器的使用率达到最高，而且多余的值很少会卸载到栈中。图表示的是同时有哪些变量在使用．以及哪些寄存器可以存放这些变虽。如果同时存活的变量数超过了可用的寄存器数，重要性最低的变量将被移到栈中，使得其他变量可以使用寄存器。指派某个变量给寄存器通常需要来回几次构建图和着色。这也导致了它的不足，图着色算法花费的时间、数据结构所需的空间都比较昂贵。 JVM性能监控垃圾收集重要的垃圾收集数据重要的垃圾收集数据包括： 当前使用的垃圾收集器 Java堆的大小 新生代和老年代的大小 永久代的大小 Minor GC的持续时间 Minor GC的频率 Minor GC的空间回收量 Full GC的持续时间 Full GC的频率 每个并发垃圾收集周期内的空间回收量 垃圾收集前后Java堆的占用量 垃圾收集前后新生代和老年代的占用量 垃圾收集前后永久代的占用量 是否老年代或永久代的占用触发了 Full GC 应用是否显式调用了 System.gc() 垃圾回收报告JIT编译器可以使用-XX:+PrintCompilation监控HotSpot JIT编译器。-XX:+PrintCompilation为每次编译生成一行日志。 日志样例如下: 123456787 java. lang String: indexOf (151 bytes)8% ! sun. awt. image. PNGImageDecoder: produceImage a 960 (1920 bytes)9 ! sun awt. image. PNGImageDecoder: produceImage (1920 bytes)10 java. lang. AbstractStringBuilder: append(40 bytes)11 n java. lang System: arraycopy (static)12 s java util. Hashtable: get (69 bytes)13 b java util. HashMap: indexFor (6 bytes)14 made zombie java. awt. geom. Path2DSIterator: isDone (20 bytes) JVM性能调优入门应用程序的系统需求吞吐量吞吐量是对单位时间内处理工作量的度量。设计吞吐量需求时,我们一般不考虑它对延迟或者响应时间的影响。通常情况下,增加吞吐量的代价是延迟的增加或内存使用的增加。 吞吐量性能需求的一个典型例子是,应用程序每秒需要完成2500次事务。 延迟或响应性延迟,或者响应性,是对应用程序收到指令开始工作直到完成该工作所消耗时间的度量。 定义延迟或响应性需求时并不考虑程序的吞吐量。通常情况下,提高响应性或缩小延迟的代价是更低的吞吐量、或者更多的内存消耗(或者二者同时发生) 延迟或响应需求的一个典型例子是,应用程序应该在60毫秒内完成交易请求的处理工作。 内存占用内存占用指在同等程度的吞吐量、延迟、可用性和可管理性前提下,运行应用程序所需的内存大小。内存占用通常以运行应用程序需要的Java堆大小或者运行应用程序需要的总内存大小来表述。一般情况下,通过增大Java堆的方式增加可用内存能够提高吞吐量、降低延迟或者兼顾二者。应用程序的可用内存减少时,吞吐量和延迟通常都会受到影响。应用程序的内存占用限制了固定内存的机器上能同时运行的应用程序实例数。 内存占用需求的一个典型例子是,应用程序需要在拥有8GB内存的系统上以单个实例方式运行或者在24GB内存的系统上以3个应用程序实例方式运行。 性能收集调优基础性能属性 吞吐量:是评价垃圾收集器能力的重要指标之一,指不考虑垃圾收集引起的停顿时间或内存消耗,垃圾收集器能支撑应用程序达到的最高性能指标。 延迟:也是评价垃圾收集器能力的重要指标,度量标准是缩短由于垃圾收集引起的停顿时间或完全消除因垃圾收集所引起的停顿,避免应用程序运行时发生抖动。 内存占用:垃圾收集器流畅运行所需要的内存数量。 这其中任何一个属性性能的提高几乎都是以另一个或两个属性性能的损失作代价的。换句话说,某一个属性上的性能提高总会牺牲另一个或两个属性。然而,对大多数的应用而言,极少出现这三个属性的重要程度都同等的情况。很多时候,某一个或两个属性的性能要比另一个重要。 我们需要了解对应用程序而言哪些系统需求是最重要的,也需要知道对应用程序而言这三个性能属性哪些是最重要的。确定哪些属性最重要,并将其映射到应用程序的系统需求,对应用程序而言非常重要。 原则谈到JVM垃圾收集器调优也有三个需要理解的基本原则。 每次Minor GC都尽可能多地收集垃圾对象。我们把这称作“Minor GC回收原则”。遵守这原则可以减少应用程序发生 Full GC的频率。Full GC的持续时间总是最长的,是应用程序无法达到其延迟或吞吐量要求的罪魁祸首。 处理吞吐量和延迟问题时,垃圾处理器能使用的内存越大,即Java堆空间越大,垃圾收集的效果越好,应用程序运行也越流畅。我们称之为“GC内存最大化原则”。 在这三个性能属性(吞吐量、延迟、内存占用)中任意选择两个进行JVM垃圾收集器调优。我们称之为“GC调优的3选2原则”。 调优JVM垃圾收集的过程中谨记这三条原则能帮助你更轻松地调优垃圾收集,达到应用程序的性能要求。 确定内存占用HotSpot VM堆布局通过-Xmn可以很方便地设定新生代空间的初始值和最大值。有一点需要特别注意,如果-Xms和-Ⅺmx并没有设定为同一个值,使用-Xmn选项时,Java堆的大小变化不会影响新生代空间,即新生代空间的大小总保持恒定,而不是随着Java堆大小的扩展或缩减做相应的调整。因此,请注意,只有在-Xms与-Xmx设定为同一值时才使用-Xmn选项。 老年代空间的大小会根据新生代的大小隐式设定。老年代空间的初始值为-Xmx的值减去XX: NewSize的值。老年代空间的最小值为-Xmx的值减去-XX:MaxNewSize的值。如果-Xms与Xmx设置为同一值,同时使用了-Xmn,或者-XX:NewSize与-XX:MaxNewsize一样,则老年代的大小为-Xmx(或-Xms)的值减去-Xmn。 实际上,当HotSpot VM发现当前可用空间不足以容纳下一次Minor GC提升的对象时就会进行Full GC。与因空间问题导致的Minor GC过程中的对象提升失败比较起来,这种方式的代价要小得多。从失败的对象提升中恢复是一个很昂贵的操作。永久代没有足够的空间存储新的VM或类元数据时也会发生Full GC。 如果Full GC缘于老年代空间已满,即使永久代空间并没有用尽,老年代和永久代都会进行垃圾收集。同样,如果Full GC由永久代空间用尽引起,老年代和永久代也都会进行垃圾收集,无论老年代是否还有空闲空间。开启-XX:+UseParallelGC或-XX:+UseParallelOldGC时,如果关闭-XX:-ScavengeBeforeFullGC, HotSpot VM在Full GC之前不会进行Minor GC,但Full GC过程中依然会收集新生代;如果开启-XX:+ScavengeBeforeFullGC, HotSpot VM在Full GC前会先做一次Minor GC,分担一部分Full GC原本要做的工作。 堆大小调优着眼点如果你使用的HotSpot VM不接受-XX:+UseParallelOldGC选项,可以使用-XX:+UseParallelGC代替。如果你很清楚Java应用程序要使用多大的Java堆空间,可以将Java堆大小作为调优的入手点,使用-Xmx和-Xms设置Java堆的大小。如果你不清楚Java应用程序到底需要使用多大的Java堆,可以利用HotSpot VM自动选取Java堆的大小。启动Java应用程序时不指定-Xmx或-Xms的值, HotSpot VM会自动设定Java堆大小的初始值。换句话说,这是一个起始点。随着调优过程,后面会逐渐调整Java堆的大小。 通过HotSpot命令行选项-XX:+PrintCommandLineFlags还可以查看堆的初始值及最大值。-XX:+PrintCommandlineFlags选项可以输出HotSpot VM初始化时使用-XX:InitialHeapSize=&lt;n&gt; -XX:MaxHeapSize=&lt;m&gt;指定的堆的初始值及最大值,其中&lt;n&gt;是以字节为单位的初始Java堆大小,&lt;m&gt;是以字节为单位的堆的最大值。 计算活跃数据大小活跃数据大小是应用程序运行于稳定态时,长期存活的对象在Java堆中占用的空间大小。换句话说,活跃数据大小是应用程序运行于稳定态,Full GC之后Java堆中老年代和永久代占用的空间大小。 Java应用的活跃数据大小可以通过GC日志收集。活跃数据大小包括下面的内容: 应用程序运行于稳定态时,老年代占用的Java堆大小; 应用程序运行于稳定态时,永久代占用的Java堆大小。 除了活跃数据大小,稳定态的Full GC也会对延迟带来严重影响。 为了更好地度量应用程序的活跃数据大小,最好在多次Full GC之后再查看Java堆的占用情况。另外,需要确保Full GC发生时,应用程序正处于稳定态。 如果应用程序没有发生Full GC或者不经常发生Full GC,你可以使用JVM监控工具Visual VM或JConsole人工触发Full GC。你也可以使用HotSpot JDK发行版中提供的jmap命令,通过命令行强制进行Full GC。为了实现这个目的,jmap需要使用- histo:live命令行选项及JVM进程号。JVM进程号可以通过JDK的js命令获得。例如,Java应用程序的JM进程号是348,使用jmap触发Full GC的命令行如下: 1jmap -histo: live 348 jmap命令触发 Full GC的同时也生成一份包含对象分配信息的堆分析文件。为了专注本步操作,你可以忽略生成的堆分析文件。 初始化堆大小配置推荐的做法是基于最差延迟进行估算。 空间 命令行选项 占用倍数 Java堆 -Xms和-Xmx 3-4倍Full GC后的老年代空间占用量 永久代 -XX:Permsize-XX:MaxPermSize 1.2-1.5倍Full GC后的永久代空间占用量 新生代 -Xmn 1~1.5倍 Full GC后的老年代空间占用量 老年代 Java堆大小减新生代大小 2-3倍Full GC后的老年代空间占用量 调优延迟/响应性输入这一步调优有多个输入,都源于应用程序的系统性需求。 应用程序可接受的平均停滞时间。平均停滞时间将与测量出的Minor GC持续时间进行比较。 可接受的Minor GC(会导致延迟)频率。 Minor GC的频率将与可容忍的值进行比较。对应用程序干系人而言,GC持续的时间往往比GC发生的频率更重要。 应用程序干系人可接受的应用程序的最大停顿时间。最大停顿时间将与最差情况下Full GC的持续时间进行比较。 应用程序干系人可接受的最大停顿发生的频率。最大停顿发生的频率基本上就是Full GC的频率。同样,对于大多数应用程序干系人而言,相对于GC的频率,他们更关心GC持续的平均停顿时间和最大停顿时间。 优化新生代的大小调整新生代空间时,需要谨记下面几个准则： 老年代空间大小不应该小于活跃数据大小的1.5倍。 新生代空间至少应为Java堆大小的10%,通过Xmx和-Xms可以设定该值。新生代过小可能适得其反,会导致频繁的Minor GC。 增大Java堆大小时,需要注意不要超过JVM可用的物理内存数。堆占用过多内存将导致底层系统交换到虚拟内存,反而会造成垃圾收集器和应用程序的性能低下。 监控晋升阈值最大晋升阈值可以通过HotSpot VM的命令行选项-XX: MaxTenuringThreshold=&lt;n&gt;设置。使用HotSpot VM的命令行选项 1-XX: +PrintTenuringDistribution 可以监控晋升的分布或者对象年龄分布,并以此为依据确定最优的最大晋升阈值值。 通过-XX:+PrintTenuringDistribution命令行选项可以观察Survivor空间中的对象是如何老化的。在-XX:+PrintTenuringDistribution生成的输出中,我们需要关注的是随着对象年龄的增加,各对象年龄上字节数减少的情况,以及 HotSpot VM计算出的晋升阈值是否等于或接近设置的最大晋升阈值。 XX:+PrintTenuringDistribution会输出每次Minor GC时晋升分布的情况。它也可以和其他的垃圾收集命令行选项,例如-XX:+PrintGCDateStamps、-XX:+PrintGcTime Stamps或-XX:+PrintgCDetails配合使用。对Survivor空间的有效对象老化进行微调时,应该使用选项XX:+PrintTenuringDistribution在垃圾收集日志中包含晋升分布的统计信息。同样,如果需要在生产环境中判断一个应用程序事件是否源于一次Stop-The-World压缩式垃圾收集,往往也需要获取晋升分布的日志信息,使用该选项是非常有帮助的。 通常情况下,观察到新的晋升阈值持续小于最大晋升阈值,或者观察到 Survivor空间大小小于总的存活对象大小都表明 Survivor空间过小。 调整 Survivor空间的容量调整Survivor空间容量一个应该谨记于心的重要原则:调整Survivor空间容量时,如果新生代空间大小不变,增大Survivor空间会减少Eden空间;而减少Eden空间会增加Minor GC的频率。因此,为了同时满足应用程序Minor GC频率的要求,就需要增大当前新生代空间的大小;即增大Survivor空间大小时,Eden空间的大小应该保持不变。换句话说,每当 Survivor空间增加时,新生代空间都应该增大。如果可以增大Minor GC的频率,你可以选择用一部分Eden空间来增大Survivor空间,或者直接增大新生代空间大小。如果内存足够,相对于减少Eden空间.增加新生代大小通常是更好的选择。保持Eden空间大小恒定, Minor GC的频率就不会由于Survivor空间增大而发生变化。 如果你观察到垃圾收集中晋升分布极少出现对象年龄为15的情况,并且也没有发生Surviⅳvor空间溢出,那么应该设置最大晋升阈值为其默认值15。这种场景下,对象都不是长期存活对象,在年龄很小的时候就被回收了,根本不会生存到最大晋升年限的年龄15。 调整目标Survivor空间占用目标Survivor空间占用是HotSpot VM尝试在Minor GC之后仍然维持的Survivor空间占用。通过 HotSpot VM的命令行选项-XX:TargetSurvivorRatio=&lt;percent&gt;可以对该值进行调整。通过命令行选项指定的参数实际上是Survivor空间占用的百分比而不是一个比率。它的默认值是50。 HotSpot VM研发团队对不同类型的应用程序进行了大量的负荷测试,结果表明50%的目标Survivor空间占用能适应大多数的应用程序,这是因为它能应对Minor GC时存活对象的急速增加。 极少发生需要对目标Survivor空间占用进行调优的情况。但是,如果应用程序有一个相对稳定的对象分配速率,可以考虑提高目标Survivor空间占用到80~90。这样可以减少用于老化对象的Survivor空间的数量。将-XX:TargetSurvivorRatio=&lt;percent&gt;设置得大于默认值会带来的冋题是不能很妤的适应迅速上涨的对象分配速率,导致提升对象的时机比预期更早。使用CMS时,如果对象提升过快会导致老年代占用增大,由于提升了一些非长期存活的对象,这些对象在将来的并发垃圾收集周期中一定会被回收,导致出现内存碎片的概率较高。碎片是我们要尽量避免的,因为它最终会导致Stop-The-World压缩式垃圾收集。 成功的CMS收集器调优要能以对象从新生代提升到老年代的同等速度对老年代中的对象进行垃圾收集。达不到这个标准则称之为“失速”(Lost the Race)失速的结果就会发生Stop-The-World压缩式垃圾收集。避免失速的关键是要结合足够大的老年代空间和足够快地初始化CMS垃圾收集周期,让它以比提升速率更快的速度回收空间。 CMS周期的初始化基于老年代空间的占用情况。如果CMS周期开始得太晚,就会发生失速。如果它无法以足够快的速度回收对象,就无法避免老年代空间用尽。但是CMS周期开始得过早又会引起无用的消耗,影响应用程序的吞吐量。通常,早启动CMS周期要比晚启动CMS好,因为启动太晚的结果比启动过早的结果要恶劣得多。 如果GC日志中发现concurrent mode failures字样,可以通过下面的命令行选项通知HotSpot在更早的时间启动CMS垃圾收集周期。 1-XX: CMSInitiatingOccupancyFraction=&lt;percent&gt; 设定的值是CMS垃圾收集周期在老年代空间占用达到多少百分比时启动。例如,如果你希望CMS周期在老年代空间占用达到65%时开始,可以设置-XX:CMSInitiatingOccupancyFraction=65。 另一个可以与-XX:CMSInitiatingOccupancyFraction=&lt;percent&gt;一起使用另一个Hotspot命令行选项是 1-XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCMSInitiatingOccupancyOnly告知HotSpot VM总是使用-XX:CMSInitiatingOccupancyFraction设定的值作为启动CMS周期的老年代空间占用阈值。不使用-XX:+UseRs InitiatingOccupancyOnly, HotSpot VM仅在启动的第一个CMS周期里使用-XX:CMSInitiatingOccupancyFraction设定的值作为占用比率,之后的周期中又转向自适应地启动CMS周期,即第一次CMS周期之后就不再使用-XX:CMSInitiatingoccupancy Fraction设定的值。 过选项设置何时启动CMS周期时,最好同时使用-XX:CMSInitiatingOccupancyFraction=&lt;percent&gt;和-XX:+UseCMSInitiatingOccupancyOnly 选项-XX:CMSInitiatingOccupancyFraction设定的空间占用值应该大于老年代占用空间和活跃数据大小之比。应用程序的活跃数据大小就是一次Full GC之后堆所占用的空间大小。如果使用-XX:CMSInitiatingOccupancyFraction设置的值小于活跃数据的占用百分比,CMS收集器一直运行陷入死循环。因此-XX:CMSInitiatingoccupancyFraction设置的一个通用原则是老年代占用百分比应该至少应该是活跃数据大小的1.5倍。 何时(提前或推迟)启动CMS周期取决于对象从新生代提升至老年代的速率,即老年代空间的增长率。如果老年代空间消耗得比较慢,可以在稍晩的时候启动CMS周期。如果老年代空间消耗迅速,你应该在较早的时候启动CMS周期,但是也不应低于活跃数据的占用的比率。不应该将启动CMS周期的值设置得比活跃数据的大小低,解决这个问题更好的方法是增大老年代空间的大小。 显式的垃圾收集使用CMS时,如果你观察到由显式调用System.gc()触发的Full GC,有2种处理的方法。 1、可以使用如下的 HotSpot VM命令行选项,指定 HotSpot VM以CMS垃圾收集周期的方式执行 -XX:+ExplicitGCInvokesConcurrent 或者 -XX:+ExplicitGCInvokesConcurrentAndUnloadsClasses 前者需要Java6及以上版本。后者需要Java6 Update4及以上版本。如果你的JK版本支持,最好使用-XX:+ExplicitGCInvokesConcurrentAndUnloadsClasses选项。 2、也可以使用下面的命令行通知HotSpot VM忽略显式的 System.gc()调用 -XX:+DisableExplicitGC 要留意的是,使用这个命令行选项也会导致其他 HotSpot VM的垃圾收集器忽略显式的System.gc()调用。 禁用显式的垃圾收集时应该慎重,它可能会对应用程序的性能造成较大影响。还有可能出现这样的场景,你需要及时对对象引用做处理,但与之对应的垃圾收集却跟不上其节奏。使用Java RMI的应用程序尤其容易碰到这种问题。我们建议除非有非常明确的理由,否则不要轻易地禁用显式的垃圾收集。与此同时,也建议只在有明确理由的情况下才在应用程序中使用System.gc()。 1234562010-12-16T23:04:39.452-0600:[Full GC(System)CMS:418061K-&gt;428608K(16384K),0.2539726 secs418749K-&gt;4288608K(31168K),[CMS Perm:32428K-&gt;32428K(65536K)],0.2540393 secs][Times: user=0. 12 sys=0. 01, real=0. 25 secs] 请留意Full GC之后的(System)标签,它表明System.gc()触发了本次 Full GC。如果在垃圾收集日志中发现了显式的Full GC,你需要先判断为什么它会发生,之后再决定是否要禁用,是否要把该调用从代码中移除,或者是否有必要指定一个条件来触发CMS并发垃圾收集周期。 应用程序吞吐量调优 一个通用原则是使用 Throughput收集器时,垃圾收集的开销应该小于5%。如果可以将垃圾收集的开销减少到1%甚至更少,那基本上就已经到了极限,进一步优化花费的代价很大。 调优并行垃圾收集线程并行垃圾收集器使用的线程数也应该依据系统上运行的应用程序数以及底层的硬件平台进行相应的调优。多个应用程序运行于同一个系统上时,建议通过命令行选项-XX:ParallelGCThreads=&lt;n&gt;将并行垃圾收集的线程数设置为小于其默认值。 否则,由于大量的垃圾收集线程同时运行,其他应用程序的性能将受到严重影响。截至Java6 Update23,默认情况下并行垃圾收集的线程数等于Java API Runtime.availableProcessors()的返回值(如果该返回值小于等于8),否则其等于8+(Runtime.availableProcessors()-8)*5/8。多个应用程序运行于同一系统上时设置并行垃圾收集线程的一个通用原则是用虚拟处理器的数目 (Runtime.availableProcessors()的返回值)除以该系统上运行的应用程序数。这里我们假设这些应用程序的负荷及堆大小的情况相差不大。如果应用程序的负荷及Java堆大小差异很大,那么为每个Java应用设置不同权重,并据此设置并行垃圾线程数是一个比较好的方法。 在NUMA系统上部署如果应用程序需要在NUMA(非一致性内存架构)系统上部署,还有一个可以与Throughput收集器一起使用的HotSpot命令行选项是: 1-XX:+UseNUMA 该命令行选项根据CPU与内存位置的关系在分配线程运行的本地内存中分配对象。这里依据的假设是分配对象的线程是近期最有可能访问该对象的线程。相对于远程的内存而言,在同一线程的本地内存中分配对象用更短的时间即能访问该对象的内容。 只有当JVM的部署跨CPU、不同CPU访问内存的拓扑有所不同,导致访问时间也有所差别的环境下才选择使用-XX:+UseNUMA选项。例如,虽然JVM部署到NUMA系统的一个处理器集上,但是这个处理器集并不存在跨CPU访问内存的拓扑,没有访问时间的差别,那么就不应该使用-XX:+UseNUMA选项。 简而言之,攴持NUMA的VM会根据NUMA节点划分堆,线程创建新的对象时,只会在该线程运行所在核的NUMA节点上分配对象,后续该线程如果需要使用这个对象,就直接从本地内存中访问。通常情况下,如果没有使用命令,臂如RHEL下使用numactl,设置CPU的亲和性(Affinity),默认就跨多个内存节点,满足-XX:+UseNUMA的使用条件。 注：Throughput garbage collector,实际指的是Parallel收集器 其它性能命令行选项大页面支持计算机系统的内存被划分成称为“页”的固定大小的块。程厅访问内存的过程中会将虚拟内存地址转换成物理内存地址。虚拟地址到物理地址的转换是通过表完成的。为了减少每次内存访问时访问页表的代价,通常的做法是使用一块快速缓存,对虚拟地址到物理地址的转换进行缓存。这块缓存被称为转译快查缓存(TLB)。 使用TLB完成从虚拟地址到物理地址的映射比遍历整个页表的方式要快得多。TLB通常只能容纳固定数量的条目。TLB中的一条记录就是按页面大小统计的一块内存地址区间的映射。因此系统的页面越大,每个条目能映射的内存地址区间越大,每个TLB能管理的空间也越大。TLB代表的地址区间越大,地址转译请求在TLB中失效的可能性就越小。当一个地址转译请求无法在TLB中找到匹配项时,我们称之发生了“TLB失效”。TLB失效事件发生时常常需要遍历内存中的页表,査找虚拟地址到物理地址的映射。与在TLB中查找地址映射比较起来,遍历页表是一项非常昂贵的操作。由此可见,使用大页面的好处是其减小了TLB失效的几率。 HotSpot虚拟机在Oracle solaris(这之后称为Solaris)、 Linux和 Windows上都支持大页面。页面大小还可能随着处理器的不同有所不同。另外,为了使用大页面还可能需要对操作系统进行配置。 Java 应用的基准测试基准测试所面临的挑战基准测试的预热阶段执行基准测试时使用HotSpot VM命令行选项-XX:+PrintCompilation是一个好习惯,使用该命令行选项的输岀可以判断JIT编译器何时完成了预热阶段,确保在 HotSpot编译器到达稳定态后,即已经完成它的优化工作(生成了适合基准测试的优化机器码)后再开始基准数据采样。-XX:+PrintCompilation选项通知VM为每个它优化或逆优化的函数输出一条日志。下面是在一段微基准测试中使用-XX:+PrintCompilation选项输出的日志片段: 123456789101112131411 java.util.Random:: nextInt (60 bytes)12 java.util.Random: next (47 bytes13 java.util.concurrent atomic Atomi CLong: get (5 bytes)14 java.util.HashSet: contains (9 bytes)15 java.util.HashMap: transfer (83 bytes)16 java.util.Arrays SArrayList: set (16 bytes)17 java.util.Arrays SArrayList: set (16 bytes)18 java.util.Collections: swap (25 bytes)19 java.util.Arrays SArrayList: get (7 bytes)20 java.lang.Long: &lt;init&gt; (10 bytes)21 java.lang.Integer: longvalue (6 bytes)22 java.lang.Long: valueOf (36 bytes)23 java.lang.Integer: stringSize (21 bytes)24 java.lang.Integer: getChars (131 bytes) 观察日志中已经不再有-XX:+PrintCompilation输出的信息(表明JIT编译器的优化工作已经完成)之后才正式开始采样数据。此外,基准测试还应该在不使用-XX:+PrintCompilation选项的情况下运行几次,比较其性能与使用-XX:+ PrintCompilation选项的结果是否一致。如果二者不一致,可能在创建基准测试或微基准测试时受到了其他因素的影响。 微基准测试中有一个惯例,即在开始采样间隔开始计时之前,先调用几次 System.gc()。多次调用 System.gc()的目的是希望通过Java对象的终结方法释放内存,而这往往需要进行多次垃圾收集才能完成。此外,当对象不可达,导致终结方法一直处于等待队列,或者部分执行队列中,调用System.runFinalization()接口可以请求JVM执行其fina1ize()方法,完成垃圾收集。 使用Java Time接口引入新的System.nanoTime()接口之前,大多数的Java基准测试或微基准测试都使用System.currentTimeMillis()接口获取采样间隔的开始和终止时间,根据终止时间与开始时间的间隔得到运行关注的代码所消耗的时间使用Java的System.currentTimeMillis()和System.nanoTime()接口都有一定程度的精度问题。虽然 System.currentTimeMillis()的返回值是以亳秒计的当前时间,但毫秒级的精度却取决于操作系统。Java API Specification中对于System.currentTimeMillis()有明确的陈述:虽然该接口的返回值是毫秒,但返回值的粒度取决于底层的操作系统。这一规范为操作系统使用自身的亳秒级系统接口提供了方便,但是可能存在这样的情况,尽管使用的是毫秒计数器,但是更新间隔却过大,譬如每30毫秒更新一次。这个规范有意地规定得比较宽松,试图让Java API尽可能地支持更多的操作系统,其中就包含一些无法提供毫秒级时钟精度的操作系统。使用Java API System. nanoTime()也有类似的问题。虽然该方法提供了纳秒级的精度,但接口并不保证提供纳秒级的精度: System.nanoTime()的 Java API Specification中明确提到不保证System.nanoTime()返回值的更新频度。 因此,使用System.currentTimeMillis()计算时间消耗时,采样的时间间隔应该足够大,尽量减少System, currentTimeMillis()精度带来的影响。也即是说,采样的时间间隔需要比毫秒大(譬如几秒、或者尽可能几分钟)同样的原则也适用于 System.nanoTime()。根据 jJava API Specification,System.nanoTime()依赖底层的操作系统,返回系统中可用的最精确时钟的当前值。然而,最精确的可用系统时钟可能也没有纳秒级的精度。进行基准测试时,建议首先摸清楚这两个Java API在对应平台或操作系统上的粒度或精度。如果你不是很清楚,但手里有源代码,可以通过查看这两个API的底层实现,了解其粒度和精度。如果你使用 System.currentTimeMillis(或System.nanoTime())。而且采样时间间隔很短(相对于毫秒或纳秒来讲),要特别注意这个问题。 微基准测试时,使用System.nanoTime()获取启动和终止时间计算采样间隔是一种好方法。接着计算终止与启动的时间差就可以得到微基准测试的耗时,以及每次操作选代所消耗的纳秒数或者每秒所发生的迭代次数。最重要的是要确保微基准测试运行的时间要足够长,确保应用程序运行已达稳定态且采样的时间也足够长。 剔除无效代码为了避免微基准测试中的代码被定性为无效代码,引发过度简化的问题,可以采用下面的编程实践： 让该方法变得必不可少 在釆样阶段结束时直接输出计算的结果,或者保存该计算结果,在采样阶段结束后输出该值 要使计算有意义,就要向被测方法传入参数,并从被测方法返回计算结果。此外,在基准测试采样阶段内或在多个不同的基准测试采样阶段间变换迭代次数也是一个不错的方法,然后比较每毫秒内发生的迭代次数,判断迭代次数是否保持恒定,同时使用-XX:+PrintCompilation选项追踪记录JIT编译器的状态。 内联HotSpot VM的Client和Server JIT编译器都能对方法进行内联。这意味着调用过程中,目标方法会被展开到调用方法中。这个过程是由J编译器完成的,J编译器通过降低方法调用的开销提升执行性能。此外,内联的代码可能提供更多的优化机会,整合后的代码可能更简单,或者消除了无效调用,而这些在不内联的情况下是无法实现的。内联在微基准测试中还可能实现让人眼前一亮的性能提升。 12-XX:+PrintInlining-XX:MaxInlineSize=N 逆优化JIT编译器以其执行优化的能力而著称于世。但是,某些场景下JIT编译器也会进行“逆优化”。譬如,Java应用一旦开始运行,方法调用变得频繁;JIT编译器就可以根据从程序过程中了解到的信息做出优化决策。有些时候,优化的决策在后续可能被证明是错误的。当JIT编译器发现之前的优化作了错误的优化决策时就会进行逆优化。很多时候,在JIT编译器逆优化不久之后(一旦达到一定的执行次数阈值)就会接着再次进行优化。忽视发生的逆优化可能得出错误的性能结论。 使用-XX:+PrintCompilation选项可以帮助确定是否发生了逆优化。-XX:+Printcompilation选项的输出中如果包含“mad not entrant”,即表明之前的编译优化被丢弃了,方法将通过解释器运行,直到该方法执行足够的次数再触发优化。 软件开发者应该专注于优秀的软件架构、设计以及实现,没有必要过度担忧现代JIT编译器的影响。如果对软件架构、设计或实现的修改是为了克服JIT编译器的一些性质,就应该考虑这是JIT编译器的缺陷或不足。 创建微基准测试的注意事项 明确你需要了解的性能指标是什么,设计相应的实验冋答你需要解决的冋题。不要受些无关痛痒的因素影响而忽略了你真正需要解决的问题。 确保采样阶段中每次使用同样的工作量。 计算并收集多种性能指标,譬如消耗时间、单位时冋迭代次数或每次迭代的消耗时间用在预热阶段之后,采样阶段期间记录的性能指标。留意度量时间的精度和粒度,特别是使用了System.currentTimeMi1lis()和 System.nanoTime()况。多次运行试验,并变换采样的周期数或釆样的持续时间。之后再比较其所消耗的时间,密切注意单位时间迭代次数或每迭代消耗时间指标的变化。微基准测试经历了足够的预热、达到稳定态时,后一个指标几乎应该与釆样阶段持续时间的变化保持一致。 开始釆样之前确认微基准测试已经到达稳定态。可遵循的一条通用原则是,确保微基准测试至少运行10秒以上。使用 HotSpot的-XX:+PrintCompilation选项通过插入表示微基准测试执行阶段的工具可以帮助确认基准测试已经到达稳定态。这一步的日的是确保在开始采样之前,微基准测试经过充分预热,在采样阶段不会发生进一步的优化或逆优化事件。 多次运行基准测试以确保观测的结果是可重复的。多次运行可以为你最终的结论提供有力支持。 运行实验及观测结果时特别要留意得到的结果是否合理。如果碰到无法解释或可疑的结果,要花时间去研究、回顾实验的设计,确保观察的结果合理。 通过传递随时变化的参数到关注的方法中、返回关注方法的执行结果、在采样周期之外打印输出计算结果使计算更有意义,避免在微基准测试中创建无效代码。 留意内联可能对微基准测试产生的影响。如果对结果存疑,可以通过XX:+PrintInlining和-XX:+PrintCompilation命令行选项,利用HotSpot Debug VM观察HotSpot JIT编译器进行内联决策的过程。 确保执行微基准测试时其他的应用程序不会对系统造成影响。执行微基准测试时即使向桌面窗口管理器中添加很小或者很简单的应用程序(譬如天气应用或者股票行情记录软件),都会对系统性能造成影响。 当你需要很明确地了解JIT编译器生成了什么样的优化代码时,可以使用Oracle Solaris Studio Performance Analyzer或者HotSpot Debug VM(使用-XX:+PrintOptoAssembly选项)查看生成的汇编代码。 采用小数据集或数据结构的微基准测试受缓存的影响很大。微基准测试的结果可能每次执行都不一样,在不同的机器上运行结果也差别很大。 对于采用多线程的微基准测试需要意识到线程调度可能不是确定性的,特别是在负荷较重的情况下。 PDF书籍下载地址：https://github.com/jiankunking/books-recommendation/tree/master/Java]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>读书笔记</tag>
        <tag>Performance</tag>
        <tag>GC</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Arthas 学习笔记]]></title>
    <url>%2Farthas-learning-notes.html</url>
    <content type="text"><![CDATA[阿里 Arthas 学习笔记 thread查看当前线程信息，查看线程的堆栈。 参数说明： 参数名称 参数说明 示例 id 线程id [n:] 指定最忙的前N个线程并打印堆栈 thread -n 3 [b] 找出当前阻塞其他线程的线程 thread -b [i ] 指定cpu占比统计的采样间隔，单位为毫秒 注意， 目前只支持找出synchronized关键字阻塞住的线程， 如果是java.util.concurrent.Lock， 目前还不支持。 jvm查看当前JVM信息。 使用参考： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293$ jvm RUNTIME ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- MACHINE-NAME 1@9db2689bb4d6 JVM-START-TIME 2019-07-16 18:18:06 MANAGEMENT-SPEC-VERSION 2.0 SPEC-NAME Java Virtual Machine Specification SPEC-VENDOR Oracle Corporation SPEC-VERSION 12 VM-NAME OpenJDK 64-Bit Server VM VM-VENDOR Oracle Corporation VM-VERSION 12.0.1+12 INPUT-ARGUMENTS -XX:G1PeriodicGCInterval=120000 -XX:G1PeriodicGCSystemLoadThreshold=0 CLASS-PATH application.jar BOOT-CLASS-PATH LIBRARY-PATH /usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- CLASS-LOADING ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- LOADED-CLASS-COUNT 11046 TOTAL-LOADED-CLASS-COUNT 11377 UNLOADED-CLASS-COUNT 331 IS-VERBOSE false ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- COMPILATION ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- NAME HotSpot 64-Bit Tiered Compilers TOTAL-COMPILE-TIME 74558(ms) ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- GARBAGE-COLLECTORS ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- G1 Young Generation 12027/52593(ms) [count/time] G1 Old Generation 0/0(ms) [count/time] ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- MEMORY-MANAGERS ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- CodeCacheManager CodeHeap &apos;non-nmethods&apos; CodeHeap &apos;profiled nmethods&apos; CodeHeap &apos;non-profiled nmethods&apos; Metaspace Manager Metaspace Compressed Class Space G1 Young Generation G1 Eden Space G1 Survivor Space G1 Old Gen G1 Old Generation G1 Eden Space G1 Survivor Space G1 Old Gen ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- MEMORY ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- HEAP-MEMORY-USAGE 184549376(176.00 MiB)/2113929216(1.97 GiB)/32178700288(29.97 GiB)/60812520(58.00 MiB) [committed/init/max/used] NO-HEAP-MEMORY-USAGE 115539968(110.19 MiB)/7667712(7.31 MiB)/-1(-1 B)/108227072(103.21 MiB) [committed/init/max/used] PENDING-FINALIZE-COUNT 0 ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- OPERATING-SYSTEM ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- OS Linux ARCH amd64 PROCESSORS-COUNT 32 LOAD-AVERAGE 2.25 VERSION 3.10.0-514.26.2.el7.x86_64 ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- THREAD ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- COUNT 1060 DAEMON-COUNT 25 PEAK-COUNT 1062 STARTED-COUNT 2463 DEADLOCK-COUNT 0 ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- FILE-DESCRIPTOR ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- MAX-FILE-DESCRIPTOR-COUNT -1 OPEN-FILE-DESCRIPTOR-COUNT -1 Affect(row-cnt:0) cost in 14 ms. sysprop查看当前JVM的系统属性(System Property)。 与jinfo 信息类似，不过sysprop提供修改单个属性值的功能。 查看单个属性，支持通过TAB键自动补全。 sysenv查看当前JVM的环境属性(System Environment Variables)。 查看单个属性，支持通过TAB键自动补全。 sc、sm、jadsc查看JVM已加载的类信息,可以package前缀模糊匹配。 sm查看已加载类的方法信息。 1234567891011121314$ monitor com.jiankunking.logsearch.services.LogSearchService queryStringByKeyWordPress Q or Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 90 ms. timestamp class method total success fail avg-rt(ms) fail-rate --------------------------------------------------------------------------------------------------------------------------------------------- 2019-08-02 10:37:50 com.jiankunking.logsearch.services.LogSearchService queryStringByKeyWord 61 61 0 96.68 0.00% timestamp class method total success fail avg-rt(ms) fail-rate --------------------------------------------------------------------------------------------------------------------------------------------- 2019-08-02 10:38:50 com.jiankunking.logsearch.services.LogSearchService queryStringByKeyWord 27 27 0 51.06 0.00% timestamp class method total success fail avg-rt(ms) fail-rate --------------------------------------------------------------------------------------------------------------------------------------------- 2019-08-02 10:39:50 com.jiankunking.logsearch.services.LogSearchService queryStringByKeyWord 0 0 0 0.00 0.00% jad反编译指定已加载类的源码。 jad 命令将 JVM 中实际运行的 class 的 byte code 反编译成 java 代码，便于你理解业务逻辑； 在 Arthas Console 上，反编译出来的源码是带语法高亮的，阅读更方便 当然，反编译出来的 java 代码可能会存在语法错误，但不影响你进行阅读理解 sc、sm、jad 三个命令可以结合使用。 classloader查看classloader的继承树，urls，类加载信息。 classloader 命令将 JVM 中所有的classloader的信息统计出来，并可以展示继承树，urls等。 可以让指定的classloader去getResources，打印出所有查找到的resources的url。对于ResourceNotFoundException比较有用。 mc、redefinemcMemory Compiler/内存编译器，编译.java文件生成.class。 redefine加载外部的.class文件，redefine jvm已加载的类。 mc、redefine 可以结合使用 monitor1234567891011121314$ monitor com.jiankunking.logsearch.services.LogSearchService queryStringByKeyWordPress Q or Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 90 ms. timestamp class method total success fail avg-rt(ms) fail-rate --------------------------------------------------------------------------------------------------------------------------------------------- 2019-08-02 10:37:50 com.jiankunking.logsearch.services.LogSearchService queryStringByKeyWord 61 61 0 96.68 0.00% timestamp class method total success fail avg-rt(ms) fail-rate --------------------------------------------------------------------------------------------------------------------------------------------- 2019-08-02 10:38:50 com.jiankunking.logsearch.services.LogSearchService queryStringByKeyWord 27 27 0 51.06 0.00% timestamp class method total success fail avg-rt(ms) fail-rate --------------------------------------------------------------------------------------------------------------------------------------------- 2019-08-02 10:39:50 com.jiankunking.logsearch.services.LogSearchService queryStringByKeyWord 0 0 0 0.00 0.00% watch方法执行数据观测。 让你能方便的观察到指定方法的调用情况。能观察到的范围为：返回值、抛出异常、入参，通过编写OGNL 表达式进行对应变量的查看。 这个有点强大啊 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152$ watch com.jiankunking.logsearch.services.LogSearchService queryStringByKeyWord &quot;&#123;params,target,returnObj&#125;&quot; -x 2 -b -s -n 2Press Q or Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 111 ms.ts=2019-08-02 10:48:48; [cost=0.006503ms] result=@ArrayList[ @Object[][ @String[monitor], @String[console], null, @String[all], @String[all], null, null, @Integer[100], @Long[1564713816522], @Long[1564714116522], null, @[desc], ], @LogSearchService[ log=@Logger[Logger[com.jiankunking.logsearch.services.LogSearchService]], esFilterService=@ESFilterService[com.jiankunking.logsearch.services.ESFilterService@58ebfd03], indexPrefixService=@IndexPrefixService[com.jiankunking.logsearch.services.IndexPrefixService@19fb8826], searchAfterSort=@String[][isEmpty=false;size=3], ], null,]ts=2019-08-02 10:48:48; [cost=91.878471ms] result=@ArrayList[ @Object[][ @String[monitor], @String[console], null, @String[all], @String[all], null, null, @Integer[100], @Long[1564713816522], @Long[1564714116522], null, @[desc], ], @LogSearchService[ log=@Logger[Logger[com.jiankunking.logsearch.services.LogSearchService]], esFilterService=@ESFilterService[com.jiankunking.logsearch.services.ESFilterService@58ebfd03], indexPrefixService=@IndexPrefixService[com.jiankunking.logsearch.services.IndexPrefixService@19fb8826], searchAfterSort=@String[][isEmpty=false;size=3], ], @SearchResult[ metadata=@MetaData[MetaData(total=21431)], items=@ArrayList[isEmpty=false;size=100], ],] trace方法内部调用路径，并输出方法路径上的每个节点上耗时 trace 命令能主动搜索 class-pattern／method-pattern 对应的方法调用路径，渲染和统计整个调用链路上的所有性能开销和追踪调用链路。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384$ trace com.jiankunking.logsearch.services.LogSearchService queryStringByKeyWord Press Q or Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 182 ms.`---ts=2019-08-02 10:53:36;thread_name=http-nio-8080-exec-8;id=42d;is_daemon=true;priority=5;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader@1095194 `---[416.520613ms] com.jiankunking.logsearch.services.LogSearchService:queryStringByKeyWord() +---[0.019415ms] org.elasticsearch.search.builder.SearchSourceBuilder:&lt;init&gt;() #78 +---[0.011853ms] org.elasticsearch.index.query.QueryBuilders:boolQuery() #79 +---[0.02617ms] com.jiankunking.logsearch.services.ESFilterService:addProjectFilter() #80 +---[0.005593ms] com.jiankunking.logsearch.util.StringUtils:isEmpty() #81 +---[0.006519ms] java.lang.String:equals() #81 +---[0.002157ms] com.jiankunking.logsearch.util.StringUtils:isEmpty() #84 +---[0.002046ms] java.lang.String:equals() #84 +---[0.004284ms] com.jiankunking.logsearch.util.StringUtils:isNotEmpty() #87 +---[0.002342ms] com.jiankunking.logsearch.util.StringUtils:isNotEmpty() #90 +---[0.002066ms] com.jiankunking.logsearch.util.StringUtils:isNotEmpty() #98 +---[0.005008ms] org.elasticsearch.index.query.QueryBuilders:rangeQuery() #114 +---[0.005904ms] com.jiankunking.logsearch.util.TimeUtils:toDate() #115 +---[0.006666ms] org.elasticsearch.index.query.RangeQueryBuilder:gte() #115 +---[0.002128ms] com.jiankunking.logsearch.util.TimeUtils:toDate() #116 +---[0.004161ms] org.elasticsearch.index.query.RangeQueryBuilder:lte() #116 +---[0.347063ms] org.elasticsearch.index.query.BoolQueryBuilder:must() #117 +---[min=0.002196ms,max=0.030129ms,total=0.036436ms,count=3] org.elasticsearch.search.builder.SearchSourceBuilder:sort() #126 +---[0.005683ms] org.elasticsearch.search.builder.SearchSourceBuilder:fetchSource() #131 +---[0.003406ms] org.elasticsearch.search.builder.SearchSourceBuilder:size() #133 +---[0.003613ms] org.elasticsearch.search.builder.SearchSourceBuilder:query() #133 +---[0.321264ms] org.elasticsearch.search.builder.SearchSourceBuilder:toString() #134 +---[0.00689ms] org.springframework.http.HttpMethod:name() #136 +---[0.172438ms] com.jiankunking.logsearch.services.IndexPrefixService:getIndexPrefix() #136 +---[0.012968ms] com.jiankunking.logsearch.util.ESQueryUtils:getEndpoint() #136 +---[398.953869ms] com.jiankunking.logsearch.util.ESQueryUtils:performRequest() #136 +---[0.336319ms] com.alibaba.fastjson.JSON:parse() #138 +---[min=0.002461ms,max=0.013798ms,total=0.016259ms,count=2] java.util.Map:get() #140 +---[0.006434ms] java.lang.Integer:intValue() #140 +---[min=0.001908ms,max=0.002073ms,total=0.003981ms,count=2] java.util.Map:get() #141 +---[0.007532ms] com.jiankunking.logsearch.dto.SearchResult:&lt;init&gt;() #145 +---[0.004286ms] java.util.ArrayList:&lt;init&gt;() #146 +---[0.005725ms] com.alibaba.fastjson.JSONArray:iterator() #148 +---[min=8.07E-4ms,max=0.010324ms,total=0.124433ms,count=101] java.util.Iterator:hasNext() #148 +---[min=8.16E-4ms,max=0.009674ms,total=0.114893ms,count=100] java.util.Iterator:next() #148 +---[min=8.38E-4ms,max=0.013835ms,total=0.130734ms,count=100] com.alibaba.fastjson.JSONObject:get() #150 +---[min=8.69E-4ms,max=0.005654ms,total=0.112518ms,count=100] com.jiankunking.logsearch.util.MapUtils:getSize() #151 +---[min=7.84E-4ms,max=0.005522ms,total=0.106345ms,count=100] java.util.HashMap:&lt;init&gt;() #151 +---[min=8.4E-4ms,max=0.016457ms,total=0.12907ms,count=100] com.alibaba.fastjson.JSONArray:iterator() #152 +---[min=8.21E-4ms,max=0.002612ms,total=0.106495ms,count=100] java.util.Iterator:hasNext() #152 +---[min=8.07E-4ms,max=0.018355ms,total=0.143992ms,count=100] java.util.Iterator:next() #152 +---[min=8.72E-4ms,max=0.008687ms,total=0.132524ms,count=100] java.lang.String:valueOf() #153 +---[min=8.07E-4ms,max=0.008049ms,total=0.110329ms,count=100] java.lang.String:contains() #153 +---[min=8.46E-4ms,max=0.002612ms,total=0.109295ms,count=100] java.lang.String:valueOf() #156 +---[min=8.86E-4ms,max=0.041239ms,total=0.178986ms,count=100] java.util.HashMap:put() #156 +---[min=7.96E-4ms,max=0.008521ms,total=0.113489ms,count=100] com.alibaba.fastjson.JSONObject:get() #159 +---[min=8.2E-4ms,max=0.016801ms,total=0.125154ms,count=100] java.util.Map:get() #159 +---[min=8.39E-4ms,max=0.002089ms,total=0.106842ms,count=100] java.util.HashMap:put() #159 +---[min=7.86E-4ms,max=0.002836ms,total=0.104852ms,count=100] com.alibaba.fastjson.JSONObject:get() #160 +---[min=8.4E-4ms,max=0.003068ms,total=0.10763ms,count=100] java.util.HashMap:put() #160 +---[min=8.02E-4ms,max=0.020112ms,total=0.122423ms,count=100] com.alibaba.fastjson.JSONObject:get() #161 +---[min=8.22E-4ms,max=0.001939ms,total=0.104809ms,count=100] java.util.HashMap:put() #161 +---[min=7.99E-4ms,max=0.00186ms,total=0.10316ms,count=100] com.alibaba.fastjson.JSONObject:get() #162 +---[min=8.17E-4ms,max=0.012238ms,total=0.115955ms,count=100] java.util.Map:get() #162 +---[min=8.14E-4ms,max=0.018342ms,total=0.124331ms,count=100] java.util.HashMap:put() #162 +---[min=8.47E-4ms,max=0.003852ms,total=0.113692ms,count=100] com.alibaba.fastjson.JSONObject:containsKey() #164 +---[min=8.11E-4ms,max=0.017145ms,total=0.122375ms,count=100] com.alibaba.fastjson.JSONObject:containsKey() #167 +---[min=7.85E-4ms,max=0.002459ms,total=0.10233ms,count=100] com.alibaba.fastjson.JSONObject:get() #167 +---[min=8.57E-4ms,max=0.005959ms,total=0.11281ms,count=100] java.util.Map:containsKey() #167 +---[min=7.86E-4ms,max=0.016889ms,total=0.116022ms,count=100] com.alibaba.fastjson.JSONObject:get() #171 +---[min=8.27E-4ms,max=0.007969ms,total=0.121529ms,count=100] java.util.Map:get() #171 +---[min=8.24E-4ms,max=0.017969ms,total=0.126264ms,count=100] java.util.Map:containsKey() #171 +---[min=7.83E-4ms,max=0.001924ms,total=0.101244ms,count=100] com.alibaba.fastjson.JSONObject:get() #172 +---[min=7.92E-4ms,max=0.247702ms,total=0.502606ms,count=200] java.util.Map:get() #172 +---[min=8.61E-4ms,max=0.010095ms,total=0.124106ms,count=100] java.util.HashMap:put() #172 +---[min=7.84E-4ms,max=0.002789ms,total=0.100561ms,count=100] com.alibaba.fastjson.JSONObject:get() #174 +---[min=8.18E-4ms,max=0.003825ms,total=0.105746ms,count=100] java.util.Map:get() #174 +---[min=8.17E-4ms,max=0.003044ms,total=0.108946ms,count=100] java.util.Map:containsKey() #174 +---[min=7.83E-4ms,max=0.010689ms,total=0.10957ms,count=100] com.alibaba.fastjson.JSONObject:get() #175 +---[min=7.92E-4ms,max=0.018299ms,total=0.225606ms,count=200] java.util.Map:get() #175 +---[min=8.53E-4ms,max=0.002182ms,total=0.108485ms,count=100] java.util.HashMap:put() #175 +---[min=7.84E-4ms,max=0.005874ms,total=0.103371ms,count=100] com.alibaba.fastjson.JSONObject:get() #177 +---[min=8.02E-4ms,max=0.002017ms,total=0.101695ms,count=100] java.util.Map:get() #177 +---[min=8.26E-4ms,max=0.002735ms,total=0.106121ms,count=100] java.util.Map:containsKey() #177 +---[min=8.0E-4ms,max=0.017176ms,total=0.130393ms,count=100] com.alibaba.fastjson.JSONObject:get() #178 +---[min=8.03E-4ms,max=0.003318ms,total=0.204767ms,count=200] java.util.Map:get() #178 +---[min=8.55E-4ms,max=0.008081ms,total=0.115006ms,count=100] java.util.HashMap:put() #178 +---[min=9.11E-4ms,max=0.014017ms,total=0.136197ms,count=100] java.util.List:add() #180 +---[0.005288ms] com.jiankunking.logsearch.dto.SearchResult:setItems() #182 `---[0.002643ms] com.jiankunking.logsearch.dto.SearchResult:setTotal() #183 stack输出当前方法被调用的调用路径。 很多时候我们都知道一个方法被执行，但这个方法被执行的路径非常多，或者你根本就不知道这个方法是从那里被执行了，此时你需要的是 stack 命令。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475$ stack com.jiankunking.logsearch.services.LogSearchService queryStringByKeyWord Press Q or Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 173 ms.ts=2019-08-02 10:55:44;thread_name=http-nio-8080-exec-1;id=426;is_daemon=true;priority=5;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader@1095194 @com.jiankunking.logsearch.controller.LogSearchController.searchByKeyWord() at com.jiankunking.logsearch.controller.LogSearchController$$FastClassBySpringCGLIB$$61775844.invoke(&lt;generated&gt;:-1) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:746) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163) at org.springframework.aop.aspectj.MethodInvocationProceedingJoinPoint.proceed(MethodInvocationProceedingJoinPoint.java:88) at com.jiankunking.logsearch.aspect.ControllerTimeConsumeAspect.doAround(ControllerTimeConsumeAspect.java:34) at jdk.internal.reflect.GeneratedMethodAccessor91.invoke(null:-1) at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:567) at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:644) at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethod(AbstractAspectJAdvice.java:633) at org.springframework.aop.aspectj.AspectJAroundAdvice.invoke(AspectJAroundAdvice.java:70) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:174) at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185) at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:688) at com.jiankunking.logsearch.controller.LogSearchController$$EnhancerBySpringCGLIB$$2f145034.searchByKeyWord(&lt;generated&gt;:-1) at jdk.internal.reflect.GeneratedMethodAccessor97.invoke(null:-1) at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:567) at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:209) at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:136) at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:102) at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:891) at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:797) at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87) at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:991) at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:925) at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:974) at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:866) at javax.servlet.http.HttpServlet.service(HttpServlet.java:635) at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:851) at javax.servlet.http.HttpServlet.service(HttpServlet.java:742) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:109) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:198) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96) at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:493) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:140) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:81) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342) at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:800) at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:806) at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1498) at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:835) tt方法执行数据的时空隧道，记录下指定方法每次调用的入参和返回信息，并能对这些不同的时间下调用进行观测。 options全局开关 名称 默认值 描述 unsafe false 是否支持对系统级别的类进行增强，打开该开关可能导致把JVM搞挂，请慎重选择！ dump false 是否支持被增强了的类dump到外部文件中，如果打开开关，class文件会被dump到/${application dir}/arthas-class-dump/目录下，具体位置详见控制台输出 batch-re-transform true 是否支持批量对匹配到的类执行retransform操作 json-format false 是否支持json化的输出 disable-sub-class false 是否禁用子类匹配，默认在匹配目标类的时候会默认匹配到其子类，如果想精确匹配，可以关闭此开关 debug-for-asm false 打印ASM相关的调试信息 save-result false 是否打开执行结果存日志功能，打开之后所有命令的运行结果都将保存到/home/admin/logs/arthas/arthas.log中 job-timeout id 异步后台任务的默认超时时间，超过这个时间，任务自动停止；比如设置 1d, 2h, 3m, 25s，分别代表天、小时、分、秒]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>JVM</tag>
        <tag>Arthas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序员常用词汇]]></title>
    <url>%2Fcoder-vocabulary.html</url>
    <content type="text"><![CDATA[下面是工作中，经常用到的一些词汇，持续更新 1, abbreviation [əbriːvɪ’eɪʃ(ə)n]n. 缩写；缩写词2, abruptlyadv. 突然地；唐突地3, absence [‘æbsns]n. 没有；缺乏；缺席；不注意4, absent [ˈæbsnt;(for v.)əbˈsɛnt]adj. 缺席的；缺少的；心不在焉的；茫然的vt. 使缺席5, absorber [əb’sɔːbə]n. 减震器；吸收器；吸收体6, accessible [ək’sesɪb(ə)l]adj. 易接近的；可进入的；可理解的7, accidental [æksɪ’dent(ə)l]adj. 意外的；偶然的；附属的；临时记号的;n. 次要方面；非主要的特性；临时记号8, accomplished [ə’kʌmplɪʃt; ə’kɒm-]adj. 完成的；熟练的，有技巧的；有修养的；有学问的9, accumulate [ə’kjumjəlet]vi. 累积；积聚vt. 积攒10, accumulation [ə,kjʊmjə’leʃən]n. 积聚，累积；堆积物11, accuracy [‘ækjʊrəsɪ]n. [数] 精确度，准确性12, actually [‘æktjʊəlɪ; -tʃʊ-]adv. 实际上；事实上13, Actually [‘æktjʊəlɪ; -tʃʊ-]adv. 实际上；事实上14, actuator [‘æktʃʊ,etɚ]n. [自] 执行机构；激励者；促动器15, Adaptee被适配者16, additionally [ə’dɪʃənəlɪ]adv. 此外；又，加之17, Additivity [,ædɪ’tɪvətɪ]n. 添加；相加性18, addon [addon]n. 插件19, adequate [ˈædɪkwət]adj. 充足的；适当的；胜任的20, adjacent [ə’dʒesnt]adj. 邻近的，毗连的21, adopt [ə’dɒpt]vt. 采取；接受；收养；正式通过;vi. 采取；过继22, Advice [əd’vaɪs]n. 建议；忠告；劝告；通知23, advisable [əd’vaɪzəb(ə)l]adj. 明智的，可取的，适当的24, advisor [əd’vaɪzə]n. 顾问；指导教师；劝告者25, affinity [ə’fɪnəti]n. 密切关系；吸引力；姻亲关系；类同26, affliction [ə’flɪkʃ(ə)n]n. 苦难；苦恼；折磨27, afford [ə’fɔrd]vt. 给予，提供；买得起n. (Afford)人名；(英)阿福德28, agenda [ə’dʒɛndə]n. 议程；日常工作事项29, aggregation [,ægrɪ’geʃən]n. [地质][数] 聚合，聚集；聚集体，集合体30, agnostic [æg’nɒstɪk]n. 不可知论者;adj. 不可知论的31, aligned [ə’laɪn]adj. 对齐的；均衡的v. 结盟（align的过去式）；使成一直线32, Alliance [ə’laɪəns]n. 联盟，联合；联姻33, allocation [,ælə’keʃən]n. 分配，配置；安置34, allowable [ə’laʊəbl]adj. 许可的；正当的；可承认的；可获宽免35, along with沿（顺）着；连同…一起；与…一道；随同…一起36, alternative [ɔːl’tɜːnətɪv; ɒl-]adj. 供选择的；选择性的；交替的;n. 二中择一；供替代的选择37, ambassador [æm’bæsədɚ]n. 大使；代表；使节38, ameliorate [ə’milɪə’ret]vt. 改善；减轻（痛苦等）；改良vi. 变得更好39, amend [ə’mend]vt. 修改；改善，改进vi. 改正，改善；改过自新n. (Amend)人名；(德、英)阿门德40, amendedadj. 修正的v. 修订；改进（amend的过去分词）41, amid [美/ə’mɪd/]prep. 在其中，在其间;n. (Amid)人名；(法、阿拉伯)阿米德;42, among [ə’mʌŋ]prep. 在…中间；在…之中43, analogous [ə’næləgəs]adj. 类似的；[昆] 同功的；可比拟的44, analysts [‘æn(ə)lɪsts]n. 分析师；分析家；[分化] 分析员（analyst的复数）45, Analyzer [‘ænəlaɪzə]n. [计] 分析器；分析者；检偏镜46, anatomy [ə’nætəmi]n. 解剖；解剖学；剖析；骨骼47, ancestor [‘ænsɛstɚ]n. 始祖，祖先；被继承人48, animate [‘ænɪmeɪt]vt. 使有生气；使活泼；鼓舞；推动;adj. 有生命的49, animation [,ænɪ’meʃən]n. 活泼，生气；激励；卡通片绘制50, Annotation [ænə’teɪʃ(ə)n]n. 注释；注解；释文51, annotations []释文52, announce [ə’naʊns]vt. 宣布；述说；预示；播报vi. 宣布参加竞选；当播音员53, anomaliesn. 异常现象，反常现象（anomaly 复数形式）54, approach [ə’protʃ]n. 方法；途径；接近vt. 接近；着手处理vi. 靠近55, appropriate [ə’prəʊprɪət]adj. 适当的；恰当的；合适的;vt. 占用，拨出56, approve [ə’pruːv]vt. 批准；赞成；为…提供证据;vi. 批准；赞成；满意57, approximately [ə’prɒksɪmətlɪ]adv. 大约，近似地；近于58, approximation [ə,prɒksɪ’meɪʃn]n. [数] 近似法；接近；[数] 近似值59, approximations [approximations][数] 近似值60, apsaran. (Apsara)人名；(法)阿普萨拉61, arbitrarily [,ɑrbə’trɛrəli]adv. 武断地；反复无常地；专横地62, arbitrary [‘ɑːbɪt(rə)rɪ]adj. [数] 任意的；武断的；专制的63, arbitrate [‘ɑːbɪtreɪt]vt. 仲裁；公断64, archetype [‘ɑːkɪtaɪp]n. 原型65, architectural [,ɑːkɪ’tektʃərəl]adj. 建筑学的；建筑上的；符合建筑法的66, archive [‘ɑrkaɪv]n. 档案馆；档案文件;vt. 把…存档67, Archive [‘ɑrkaɪv]n. 档案馆；档案文件;vt. 把…存档68, arguably [‘ɑrɡjuəbli]adv. 可论证地；可争辩地；正如可提出证据加以证明的那样地[广义用法]可能，大概69, ark [ɑrk]n. 约柜；方舟；（美）平底船；避难所n. (Ark)人名；(俄、土)阿尔克70, artifact [‘ɑ:təˌfækt]n. 人工制品；手工艺品71, artifacts [‘a:rtifækts]n. 史前古器物；人工产品72, Artifacts [‘a:rtifækts]n. 史前古器物；人工产品73, Artificial [ɑːtɪ’fɪʃ(ə)l]adj. 人造的；仿造的；虚伪的；非原产地的；武断的74, Aspect [‘æspekt]n. 方面；方向；形势；外貌75, assailed [ə’seɪl]vt. 攻击；质问；着手解决76, asset [‘æset]n. 资产；优点；有用的东西；有利条件；财产；有价值的人或物 n. （法）阿塞（人名）77, Assets []n. 资产；[经] 财产；有利条件（asset的复数）78, asshole [‘æshəʊl]n. 屁眼儿，肛门；令人讨厌的人adj. 愚蠢的；窝囊的；可恶的79, assign [ə’saɪn]vt. 分配；指派；[计][数] 赋值;vi. 将财产过户（尤指过户给债权人）80, assignee [æsɪ’niː; -saɪ-]n. 代理人；受托人；分配到任务的人81, association [əsəʊsɪ’eɪʃ(ə)n; -ʃɪ-]n. 协会，联盟，社团；联合；联想82, assume [ə’sjuːm]vt. 承担；假定；采取；呈现;vi. 装腔作势；多管闲事83, asterisk [‘æstərɪsk]n. 星号vt. 注上星号；用星号标出84, async [əˈsɪŋk]abbr. 异步，非同步（asynchronous）85, asynchronous [ə’sɪŋkrənəs; eɪ-]adj. [电] 异步的；不同时的；不同期的86, Asynchronous [ə’sɪŋkrənəs; eɪ-]adj. [电] 异步的；不同时的；不同期的87, atlassian 88, atop [ə’tɒp]prep. 在…的顶上;adv. 在顶上89, attach [ə’tætʃ]vt. 使依附；贴上；系上；使依恋;vi. 附加；附属；伴随90, attendance [ə’tend(ə)ns]n. 出席；到场；出席人数；考勤91, attendeesn. 参加者；出席者（attendee的复数）92, atypical [eɪ’tɪpɪk(ə)l; æ-]adj. 非典型的；不合规则的93, audit [‘ɔːdɪt]vi. 审计；[审计] 查账;n. 审计；[审计] 查账;vt. （美）旁听94, authentication [ɔː,θentɪ’keɪʃən]n. 证明；鉴定；证实95, Authentication [ɔː,θentɪ’keɪʃən]n. 证明；鉴定；证实96, authorization [ɔːθəraɪ’zeɪʃ(ə)n]n. 授权，认可；批准，委任97, Authorization [ɔːθəraɪ’zeɪʃ(ə)n]n. 授权，认可；批准，委任98, authorize [‘ɔθəraɪz]vt. 批准，认可；授权给；委托代替99, automata [ɔ’tɑmətə]n. 自动装置；机器人 （automaton的复数）100, Automation [ɔːtə’meɪʃ(ə)n]n. 自动化；自动操作101, autonomous [ɔː’tɒnəməs]adj. 自治的；自主的；自发的102, autowiren. 自动装配103, auxiliary [ɔːg’zɪlɪərɪ; ɒg-]n. 助动词；辅助者，辅助物；附属机构;adj. 辅助的；副的；附加的104, avail [ə’veɪl]n. 效用，利益;vt. 有益于，有益于；使对某人有利。;vi. 有益于，有益于；使对某人有利。105, await [ə’weɪt]vt. 等候，等待；期待106, Aware [ə’weə]adj. 意识到的；知道的；有…方面知识的；懂世故的;n. (Aware)人名；(阿拉伯、索)阿瓦雷107, awesome [‘ɔːs(ə)m]【网络】另人竟为的；生畏；真了不起；傲森108, Awesome [‘ɔːs(ə)m]adj. 令人敬畏的；使人畏惧的；可怕的；极好的109, axis [‘æksɪs]n. 轴；轴线；轴心国110, backslash [backslash]n. 反斜杠，反斜线符号111, backwards [‘bækwɚdz]adv. 倒；向后；逆112, Badge [bædʒ]n. 徽章；证章；标记;vt. 授给…徽章113, banner [‘bænɚ]n. 横幅图片的广告模式n. 旗帜，横幅,标语n. 人名(英、德、罗)班纳114, barge [bɑrdʒ]vi. 蹒跚；闯入n. 驳船；游艇vt. 用船运输；蛮不讲理地闯入或打扰某事物n. (Barge)人名；(英)巴奇；(德)巴格；(西)巴尔赫；(法、葡)巴尔热115, Barter [‘bɑːtə]vi. 进行易货贸易；[贸易] 作物物交换；讨价还价;vt. 以…作为交换；拿…进行易货贸易;n. 易货贸易；物物交换；实物交易;n. (Barter)人名；(英)巴特116, Bash [bæʃ]vt. 猛击，痛击；怒殴;n. 猛烈的一击，痛击;n. (Bash)人名；(英、俄、巴基)巴什117, be populated with用…填充118, be responsible for []对……负责；是……的原因Be responsible for: 负责 对 承担责任119, bear [bɛr]vt. 结果实，开花（正式）vt. 忍受；承受；具有；支撑n. 熊n. (Bear)人名；(英)贝尔120, bearer [‘bɛrɚ]n. 持票人；[建] 承木；[机] 托架；送信人；搬运工人121, beast [biːst]n. 野兽；畜生，人面兽心的人122, behalf [bɪ’hɑːf]n. 代表；利益123, benchmarks [benchmarks]n. [计] 基准；标竿；水准点；基准测试程序数值（benchmark的复数形式）v. 测定基准点（benchmark的三单形式）124, bend [bend]vt. 使弯曲；使屈服；使致力；使朝向;vi. 弯曲，转弯；屈服；倾向；专心于;n. 弯曲;n. (Bend)人名；(瑞典)本德125, beware [bɪ’wɛr]vi. 当心，小心vt. 注意，当心；提防126, bin [bɪn]n. 箱子，容器；二进制;vt. 把…放入箱中;n. (Bin)人名；(意、柬)宾；(日)敏(名)；(东南亚国家华语)民127, Binary [‘baɪnərɪ]adj. [数] 二进制的；二元的，二态的128, Binary Distributions二进制发行版129, bio [‘baɪəʊ]n. 个人简历，小传 n. (Bio)人名；(法、意、葡、土、刚(金)、塞拉)比奥130, bitwisen. 按位131, biz [bɪz]n. 商业（等于business）132, blazing [‘bleɪzɪŋ]adj. 燃烧的；强烈的；闪耀的;adv. 非常;v. 燃烧；闪耀；迸发；公开（blaze的ing形式）133, block [blɒk]n. 块；街区；大厦；障碍物;vt. 阻止；阻塞；限制；封盖;adj. 成批的，大块的；交通堵塞的;n. (Block)人名；(英、法、德、西、葡、芬、罗)布洛克134, Block [blɒk]n. 块；街区；大厦；障碍物;vt. 阻止；阻塞；限制;adj. 成批的，大块的；交通堵塞的;n. (Block)人名；(英、法、德、西、葡、芬、罗)布洛克135, blueprint [‘blʊ’prɪnt]vt. 计划；制成蓝图n. 蓝图，设计图；计划136, blur [blɜː]vt. 涂污；使…模糊不清；使暗淡；玷污;vi. 沾上污迹；变模糊;n. 污迹；模糊不清的事物137, board [bɔrd]n. 董事会；木板；甲板；膳食vt. 上（飞机、车、船等）；用板盖上；给提供膳宿vi. 寄宿n. (Board)人名；(英、西)博德138, boilerplate [‘bɔɪlɚplet]n. 样板文件；引用139, boilsn. 生疖，疮（boil复数）140, bold [bold]adj. 大胆的，英勇的；黑体的；厚颜无耻的；险峻的n. (Bold)人名；(英、德、罗、捷、瑞典)博尔德141, bolt [bəʊlt]n. 螺栓，螺钉；闪电，雷电；门闩；弩箭;（布的）一匹，一卷vt. 筛选；囫囵吞下；（把门、窗等）闩上；突然说出，脱口说出vi. （门窗等）闩上，拴住；冲出，跳出；（马等的）脱缰；囫囵吞下adv. 突然地；像箭似地；直立地142, boost [buːst]vt. 促进；增加；支援;vi. 宣扬；偷窃;n. 推动；帮助；宣扬;n. (Boost)人名；(英)布斯特；(德)博斯特143, boot [buːt]vt. 引导；踢；解雇；使穿靴;n. 靴子；踢；汽车行李箱;n. (Boot)人名；(英)布特；(德)博特144, bootstrap [‘buːtstræp]n. [计] 引导程序，辅助程序；解靴带145, Bootstrap [‘buːtstræp]n. [计] 引导程序，辅助程序；解靴带146, BootStrap [‘buːtstræp]n. [计] 引导程序，辅助程序；解靴带147, bootupn. 启动148, borrow [‘bɒrəʊ]vi. 借；借用；从其他语言中引入 vt. 借；借用 n. (Borrow)人名；(英)博罗149, bottleneck [‘bɑtlnɛk]n. 瓶颈；障碍物150, bound [baʊnd]adj. 有义务的；受约束的；装有封面的;vt. 束缚；使跳跃;n. 范围；跳跃;vi. 限制；弹起151, bourne [buən]n. 小溪；目的地；边界n. (Bourne)人名；(英)伯恩；(西)博尔内152, Bourne [buən]n. 小溪；目的地；边界;n. (Bourne)人名；(英)伯恩；(西)博尔内153, breakthrough [‘breɪkθruː]n. 突破；突破性进展154, brevity [‘brɛvəti]n. 简洁，简短；短暂，短促155, brew [brʊ]vt. 酿造；酝酿vi. 酿酒；被冲泡；即将发生n. 啤酒；质地n. (Brew)人名；(英)布鲁156, brief [brif]adj. 简短的，简洁的；短暂的，草率的n. 摘要，简报；概要，诉书vt. 简报，摘要；作…的提要n. (Brief)人名；(英)布里夫157, brilliant [‘brɪlj(ə)nt]adj. 灿烂的，闪耀的；杰出的；有才气的；精彩的，绝妙的158, brittle [‘brɪt(ə)l]adj. 易碎的，脆弱的；易生气的159, broker [‘brokɚ]n. 经纪人，掮客vi. 作为权力经纪人进行谈判vt. 以中间人等身分安排…n. (Broker)人名；(英)布罗克；(俄)布罗克尔160, bucket [‘bʌkɪt]n. 桶，水桶；铲斗；一桶的量v. 倾盆而下；颠簸着行进n. (Bucket)人名；(德)布克特161, buddy [‘bʌdɪ]n. 伙伴，好朋友；密友；小男孩;vi. 做好朋友，交朋友;n. (Buddy)人名；(英)巴迪162, budget [‘bʌdʒɪt]n. 预算，预算费vt. 安排，预定；把…编入预算vi. 编预算，做预算adj. 廉价的163, bulk [bʌlk]n. 体积，容量；大多数，大部分；大块vt. 使扩大，使形成大量；使显得重要n. (Bulk)人名；(土)布尔克164, bunch [bʌn(t)ʃ]n. 群；串；突出物 vi. 隆起；打褶；形成一串 vt. 使成一串；使打褶 n. (Bunch)人名；(英)邦奇165, bundle [‘bʌnd(ə)l]n. 束；捆;vt. 捆;vi. 匆忙离开166, Bundle [‘bʌnd(ə)l]n. 束；捆;vt. 捆;vi. 匆忙离开167, burst [bɜːst]vi. 爆发，突发；爆炸 vt. 爆发，突发；爆炸 n. 爆发，突发；爆炸 n. (Burst)人名；(德、罗)布尔斯特168, byzantine [baɪˈzntaɪn;ˈbɪzəntaɪn]adj. 拜占庭式的；东罗马帝国的n. 拜占庭人，拜占庭派的建筑师169, Callable [‘kɔːləb(ə)l]adj. 随时可偿还的；请求即付的170, Campaign [kæm’peɪn]vi. 作战；参加竞选；参加活动n. 运动；活动；战役171, canary [kə’nɛri]n. [鸟] 金丝雀；淡黄色n. (Canary)人名；(英)卡纳里172, candidate [‘kændɪdeɪt; -dət]n. 候选人，候补者；应试者173, canonical [kə’nɒnɪk(ə)l]adj. 依教规的；权威的；牧师的n. 牧师礼服174, Canonical [kə’nɒnɪk(ə)l]adj. 依教规的；权威的；牧师的;n. 牧师礼服175, Canvas [‘kænvəs]n. 帆布;vt. 用帆布覆盖，用帆布装备;adj. 帆布制的176, Capable [‘keɪpəb(ə)l]adj. 能干的，能胜任的；有才华的177, capitalized [capitalized]大写的178, capped [capped]v. 给…戴帽；去蒂；覆以…；除去盖子；胜过（cap的过去分词形式）adj. 包过的；加盖的；去蒂的179, cardinality [kɑːdɪ’nælɪtɪ]n. 基数；集的势180, caret [‘kærət]n. 脱字符号；插入符号;n. (Caret)人名；(法)卡雷；(英)卡雷特181, Caret [‘kærət]n. 脱字符号；插入符号;n. (Caret)人名；(法)卡雷；(英)卡雷特182, carrot [‘kærət]n. 胡萝卜;诱饵183, cascading [kæ’skeɪdɪŋ]n. [电] 级联；串接；阶式渗透 v. 瀑布般落下；串联；传递信息（cascade的ing形式）184, catalina [,kætə’li:nə]n. 远程轰炸机;n. (Catalina)人名；(法、西)卡塔利娜；(英)卡塔莉娜185, cave [keɪv]vt. 使凹陷，使塌落；在…挖洞穴vi. 凹陷，塌落；投降n. 洞穴，窑洞n. (Cave)人名；(西)卡韦；(英)凯夫；(法)卡夫186, caveat [‘kævɪæt; ‘ke-]n. 警告；中止诉讼手续的申请；货物出门概不退换；停止支付的广告187, caveatsn. 警告；说明（caveat的复数）188, cellular [‘sɛljəlɚ]adj. 细胞的；多孔的；由细胞组成的n. 移动电话；单元189, Celsius [‘sɛlsɪəs]adj. 摄氏的n. 摄氏度190, characteristic [kærəktə’rɪstɪk]adj. 典型的；特有的；表示特性的n. 特征；特性；特色191, Characteristics [,kærəktə’rɪstɪks]n. 特性，特征；特色（characteristic的复数）；特质192, charcuterie [ʃɑː’kuːt(ə)rɪ]n. 熟食店；猪肉店;n. (Charcuterie)人名；(法)沙尔屈特里193, chase [tʃeɪs]vt. 追逐；追捕；试图赢得；雕镂;vi. 追逐；追赶；奔跑;n. 追逐；追赶；追击;n. (Chase)人名；(英)蔡斯；(法)沙斯194, chaser [‘tʃeɪsə]n. 驱逐舰；猎人；饮烈酒后喝的饮料；[机] 螺纹梳刀195, chefs [ʃefs]n.大厨；主厨（chef的复数）196, Choreographies Processes组合流程197, choreography [,kɒrɪ’ɒgrəfɪ]n. 编舞；舞蹈艺术；舞艺198, chronicle [‘krɒnɪk(ə)l]n.编年史，年代记；记录vt.记录；把…载入编年史199, chronology [krə’nɒlədʒɪ]n. 年表；年代学Chronology: 年代学 年代学 年表200, chubby [‘tʃʌbi]adj. 圆胖的，丰满的201, chunk [tʃʌŋk]n.大块；矮胖的人或物202, churn [tʃɜːn]vi.搅拌；搅动vt.搅拌；搅动n.搅乳器203, cipher [‘saɪfə]n. 密码；暗号；零 vi. 使用密码；计算；做算术 vt. 计算；做算术；将…译成密码204, circuit [‘sɝkɪt]n. [电子] 电路，回路；巡回；一圈；环道 vi. 环行 vt. 绕回…环行205, circumstance [ˈsɝ​kəmˌstəns]n. 环境，情况；事件；境遇206, citrus [‘sɪtrəs]n. [园艺] 柑橘属果树；柑橘类的植物;adj. 柑橘属植物的207, claim [kleɪm]vi. 提出要求;vt. 要求；声称；需要；认领;n. 要求；声称；索赔；断言；值得208, clamp [klæmp]vt. 夹紧，固定住n. 夹钳，螺丝钳209, clarity [‘klærɪtɪ]n. 清楚，明晰；透明;n. (Clarity)人名；(英)克拉里蒂210, Clip [klɪp]vt. 修剪；夹牢；痛打;vi. 修剪;n. 修剪；夹子；回形针211, clove [kləʊv]n. [植][中医] 丁香v. 劈开（cleave的过去式）212, clue [klʊ]n. 线索；（故事等的）情节vt. 为…提供线索；为…提供情况213, cluster [‘klʌstə]n. 群；簇；丛；串vi. 群聚；丛生vt. 使聚集；聚集在某人的周围n. (Cluster)人名；(英)克拉斯特214, co-branding [kəu’brændiŋ]n. 联合品牌；合作品牌215, coalesced合并（coalesce的过去式和过去分词）216, coarse [kɔrs]adj. 粗糙的；粗俗的；下等的217, coerce [kəʊ’ɜːs]vt. 强制，迫使218, coexist [,kəʊɪg’zɪst]vi. 共存；和平共处219, cognitive [‘kɒɡnɪtɪv]adj. 认知的，认识的220, coherency [coherency]n. 附着；凝聚；联接；相参性221, coherent [kə(ʊ)’hɪər(ə)nt]adj. 连贯的，一致的；明了的；清晰的；凝聚性的；互相耦合的；粘在一起的222, cohesive [kəʊ’hiːsɪv]adj. 有结合力的；紧密结合的；有粘着力的223, collaboration [kəlæbə’reɪʃn]n. 合作；勾结；通敌224, collapsar [kə’læpsɑː]n. 黑洞（等于black hole）225, collapse [kə’læps]vi.倒塌；瓦解；暴跌vt.使倒塌，使崩溃；使萎陷；折叠n.倒塌；失败；衰竭226, collide [kə’laɪd]vi. 碰撞；抵触，冲突;vt. 使碰撞；使相撞227, collision [kə’lɪʒ(ə)n]n. 碰撞；冲突；（意见，看法）的抵触；（政党等的）倾轧228, collisions [collisions]n. [物] 碰撞；冲突；撞击（collision的复数形式）229, colloquially [kə’ləukwiəli]adv. 口语地；用通俗语230, collyvt. （英）把…弄黑n. 锅灰；煤灰n. (Colly)人名；(英、法)科利231, colon [‘kolən]n. [解剖] 结肠；冒号（用于引语、说明、例证等之前）；科郎（哥斯达黎加货币单位）232, columnar [kə’lʌmnə]adj. 柱状的；圆柱的；分纵栏印刷或书写的233, comma [‘kɑmə]n. 逗号；停顿234, Commerce [‘kɒmɜːs]n. 贸易，商业235, commercial [kə’mɜːʃ(ə)l]adj. 商业的；营利的；靠广告收入的;n. 商业广告236, communicating [kə’mjʊnə,ketɪŋ]adj. 交流的；通信的v. 交流；传播（communicate的现在分词）；传递237, comparison [kəm’pærɪs(ə)n]n. 比较；对照；比喻；比较关系238, compatible [kəm’pætɪb(ə)l]adj. 兼容的；能共处的；可并立的239, compensate [‘kɒmpenseɪt]vi. 补偿，赔偿；抵消;vt. 补偿，赔偿；付报酬240, Compensation [kɒmpen’seɪʃ(ə)n]n. 补偿；报酬；赔偿金241, comply [kəm’plaɪ]vi. 遵守；顺从，遵从；答应242, compound [‘kɒmpaʊnd]n. [化学] 化合物；混合物；复合词 adj. 复合的；混合的 v. 合成；混合；恶化，加重；和解，妥协243, comprehensive [kɒmprɪ’hensɪv]adj. 综合的；广泛的；有理解力的;n. 综合学校；专业综合测验244, Comprehensive [kɒmprɪ’hensɪv]adj. 综合的；广泛的；有理解力的;n. 综合学校；专业综合测验245, compulsive [kəm’pʌlsɪv]adj. 强制的；强迫的246, computation [kɒmpjʊ’teɪʃ(ə)n]n. 估计，计算247, concatenate [kɑn’kætə,net]vt. 连结；使连锁adj. 连结的；连锁的248, conclusions [kən’klʊʒənz]n. 结论，总结；决定（conclusion的复数）249, concrete [‘kɒŋkriːt]adj. 混凝土的；实在的，具体的；有形的;vi. 凝结;vt. 使凝固；用混凝土修筑;n. 具体物；凝结物250, Concrete [‘kɒŋkriːt]adj. 混凝土的；实在的，具体的；有形的;vi. 凝结;vt. 使凝固；用混凝土修筑;n. 具体物；凝结物251, condense [kən’dens]vi. 浓缩；凝结;vt. 使浓缩；使压缩252, confidentiality [,kɑnfɪ,dɛnʃɪ’æləti]n. 机密，[计] 机密性253, confinement [kən’faɪnmənt]n. 限制；监禁；分娩254, confluence [‘kɑnfluəns]n. （河流的）汇合、汇流点；（人或物的）聚集255, conjunction [kən’dʒʌŋkʃən]n. 结合；[语] 连接词；同时发生256, consecutive [kən’sekjʊtɪv]adj. 连贯的；连续不断的257, conservative [kən’sɝvətɪv]adj. 保守的n. 保守派，守旧者258, consistent [kən’sɪstənt]adj. 始终如一的，一致的；坚持的259, constant [‘kɒnst(ə)nt]adj. 不变的；恒定的；经常的;n. [数] 常数；恒量;n. (Constant)人名；(德)康斯坦特260, Constant [‘kɒnst(ə)nt]adj. 不变的；恒定的；经常的;n. [数] 常数；恒量;n. (Constant)人名；(德)康斯坦特261, consult [kən’sʌlt]vt. 查阅；商量；向…请教;vi. 请教；商议；当顾问262, consumption [kən’sʌm(p)ʃ(ə)n]n. 消费；消耗；肺痨263, contemporary [kən’temp(ə)r(ər)ɪ]adj. 发生（属）于同时期的；当代的 n. 同代人，同龄人；同时期的东西264, contend [kən’tɛnd]vi. 竞争；奋斗；斗争；争论vt. 主张；为…斗争265, contention [kən’tɛnʃən]n. 争论，争辩；争夺；论点266, contiguous [kən’tɪgjʊəs]adj. 连续的；邻近的；接触的267, continual [kən’tɪnjʊəl]adj. 持续不断的；频繁的268, continuations [continuations]n. 延续部分；（书籍的）附录；（文学作品的）续集（continuation的复数）269, convenient [kən’viːnɪənt]adj. 方便的；[废语]适当的；[口语]近便的；实用的270, Convenient [kən’viːnɪənt]adj. 方便的;方便;近便的;近便地271, convention [kən’venʃ(ə)n]n. 大会；[法] 惯例；[计] 约定；[法] 协定；习俗272, convergence [kən’vɜːdʒəns]n. [数] 收敛；会聚，集合 n. (Convergence)人名；(法)孔韦尔让斯273, conversely [‘kɑnvɝsli]adv. 相反地274, conveys [conveys]运输表达275, coordinate [kəʊ’ɔ:dɪneɪt]n. 坐标；同等的人或物;adj. 并列的；同等的;vt. 调整；整合;vi. 协调276, coordinator [ko’ɔrdn,etɚ]n. 协调者；[自] 协调器；同等的人或物277, cordon [‘kɔrdn]n. 警戒线；绶带；束带层vt. 用警戒线围住；包围隔离n. (Cordon)人名；(英、西)科登；(法、葡)科尔东278, cornerstone [‘kɔːnəstəʊn]n. 基础；柱石；地基279, coroutine [,kəuru:’ti:n]n. 协同程序280, correspond [kɒrɪ’spɒnd]vi. 符合，一致；相应；通信281, correspondingly [,kɔrə’spɑndɪŋli]adv. 相应地，相对地282, Cotton [‘kɒt(ə)n]n. 棉花；棉布；棉线vi. 一致；理解；和谐；亲近adj. 棉的；棉制的n. (Cotton)人名；(英、西、葡)科顿；(法)戈登283, counterintuitive [,kaʊntərɪn’tjuːɪtɪv]adj. 违反直觉的284, counterpart [‘kaʊntəpɑːt]n. 副本；配对物；极相似的人或物285, counterparts [‘kaʊntəpɑ:ts]n. （契约）副本（counterpart的复数）；相对物；相对应的人286, Courage [‘kʌrɪdʒ]n. 勇气；胆量;n. (Courage)人名；(英)卡里奇；(法)库拉热287, crap [kræp]n. 废话；废物；屎；拉屎vi. 掷骰子；拉屎n. (Crap)人名；(英)克拉普288, crater [‘kreɪtə]n. 火山口；弹坑;vt. 在…上形成坑；取消；毁坏;vi. 形成坑；消亡289, crawler [‘krɔlɚ]n. 爬行者；履带牵引装置290, creamy [‘kriːmɪ]adj.奶油色的；乳脂状的；含乳脂的291, crisp [krɪsp]adj. 脆的；新鲜的；易碎的;vt. 使卷曲；使发脆;vi. 卷曲；发脆;n. 松脆物；油炸马铃薯片;n. (Crisp)人名；(英)克里斯普292, criteria [kraɪ’tɪərɪə]n. 标准，条件（criterion的复数）293, Criteria [kraɪ’tɪərɪə]n. 标准，条件（criterion的复数）294, critical [‘krɪtɪk(ə)l]adj. 鉴定的；[核] 临界的；批评的，爱挑剔的；危险的；决定性的；评论的295, criticize [‘krɪtɪsaɪz]vt. 批评；评论；非难vi. 批评；评论；苛求296, cropper [‘krɒpə]n. 种植者；农作物；收割机；修剪机n. (Cropper)人名；(意)克罗珀尔；(英)克罗珀297, crucial [‘kruːʃ(ə)l]adj. 重要的；决定性的；定局的；决断的298, crunch [krʌn(t)ʃ]n. 咬碎，咬碎声；扎扎地踏vt. 压碎；嘎扎嘎扎的咬嚼；扎扎地踏过vi. 嘎吱作响地咀嚼；嘎吱嘎吱地踏过299, cryptography [krɪp’tɒgrəfɪ]n. 密码学；密码使用法300, Cryptography [krɪp’tɒgrəfɪ]n. 密码学；密码使用法301, cue [kjuː]n.提示，暗示；线索vt.给…暗示n.(Cue)人名；(西)库埃302, culprit [‘kʌlprɪt]n. 犯人，罪犯；被控犯罪的人303, cumbersome [‘kʌmbəs(ə)m]adj. 笨重的；累赘的；难处理的304, cumulative [‘kjumjəletɪv]adj. 累积的305, cumulativelyadv. 累积地；渐增地306, curator [kjʊ(ə)’reɪtə]n. 馆长；监护人；管理者307, curly [‘kɜːlɪ]adj. 卷曲的；卷毛的；（木材）有皱状纹理的；蜷缩的308, curry [‘kʌrɪ]vt. 用咖喱烧，给…加咖喱粉；梳刷；鞭打;n. 咖哩粉，咖喱；咖哩饭菜309, cutoff [‘kʌt,ɔːf]n. 切掉；中断；捷径 adj. 截止的；中断的310, cutting-edge [‘kʌtɪŋ’edʒ]n. (刀片的)刃口；尖端；前沿;adj. 先进的，尖端的311, Cycle [‘saɪk(ə)l]n. 循环；周期；自行车；整套；一段时间;vt. 使循环；使轮转;vi. 循环；骑自行车；轮转312, cyclone [‘saɪklon]n. 旋风；[气象] 气旋；飓风313, dabble [‘dæbl]vt. 溅湿；浸入水中vi. 涉猎；涉足；玩水314, daemon [‘diːmən]n. 守护进程；后台程序315, dapper [‘dæpɚ]adj. 短小精悍的；衣冠楚楚的；整洁的；整齐的n. (Dapper)人名；(英)达珀；(意)达珀尔；(德)达佩尔316, Dashboard [‘dæʃbɔːd]n. 汽车等的仪表板；马车等前部的挡泥板317, daunting [‘dɔntɪŋ]adj. 使人畏缩的；使人气馁的；令人怯步的318, deactivation [di:,ækti’veiʃən]n. [物化] 减活化作用；钝化作用319, dealt [delt]v. 处理（deal的过去式和过去分词）320, decade [‘dekeid]n. 十年，十年期；十321, decay [dɪ’ke]vi. 衰退，[核] 衰减；腐烂，腐朽n. 衰退，[核] 衰减；腐烂，腐朽vt. 使腐烂，使腐败；使衰退，使衰落n. (Decay)人名；(法)德凯322, decaying [dɪ’ke]n. [核] 衰减；颓坏；腐蚀作用v. 腐烂；衰退；消瘦（decay的ing形式）adj. 衰减的；腐烂的；消散的323, declarations [,deklə’reiʃənz]n. [法] 声明（declaration的复数）324, Declarative [dɪ’klærətɪv]adj. 宣言的；陈述的，说明的325, decommission [diːkə’mɪʃ(ə)n]vt. 使…退役；解除…的军职326, decomposition [,diːkɒmpə’zɪʃn]n. 分解，腐烂；变质327, decorate [‘dekəreɪt]vt. 装饰；布置；授勋给;vi. 装饰；布置328, decoration [dekə’reɪʃ(ə)n]n. 装饰，装潢；装饰品；奖章329, decorator [‘dɛkəretɚ]n. 装饰者；室内装潢师330, decouple [diː’kʌp(ə)l]vt. 减弱震波;n. [电] 去耦331, decoupling [di:’kʌpliŋ]n. 去耦v. 去耦（decouple的现在分词）332, dedicate [‘dɛdɪket]vt. 致力；献身；题献333, dedicated [‘dɛdə’ketɪd]adj. 专用的；专注的；献身的v. 以…奉献；把…用于（dedicate的过去式和过去分词）334, deem [diːm]vt. 认为，视作；相信 vi. 认为，持某种看法；作某种评价 n. (Deem)人名；(英)迪姆335, deemed [diːmd]v. 认为（deem的过去式）336, Defer [dɪ’fɜː]vi. 推迟；延期；服从;vt. 使推迟；使延期;n. (Defer)人名；(法)德费337, Deferred [dɪ’fɜːd]adj. 延期的;v. 推迟（defer的过去式及过去分词形式）338, deficiency [dɪ’fɪʃ(ə)nsɪ]n. 缺陷，缺点；缺乏；不足的数额339, deficient [dɪ’fɪʃ(ə)nt]adj. 不足的；有缺陷的；不充分的340, deflate [dɪ’flet]vt. 放气；使缩小；紧缩通货；打击；使泄气vi. 缩小；物价下降341, degree [dɪ’griː]n. 程度，等级；度；学位；阶层342, dejection [dɪ’dʒekʃ(ə)n]n. 沮丧；粪便343, delimiter [dɪ’lɪmɪtɚ]n. [计] 定界符344, delta [‘dɛltə]n. （河流的）三角洲；德耳塔（希腊字母的第四个字）n. (Delta)人名；(英、罗、葡)德尔塔345, demand [dɪ’mɑːnd]vt. 要求；需要；查询;vi. 需要；请求；查问;n. [经] 需求；要求；需要;n. (Demand)人名；(德)德曼德346, demarcate [‘diːmɑːkeɪt]vt. 划分界线；区别347, demarcation [‘dimɑr’keʃən]n. 划分；划界；限界348, democracy [dɪˈmɑkrəsi]n. 民主，民主主义；民主政治349, demonstrate [‘demənstreɪt]vt. 证明；展示；论证vi. 示威350, demote [diː’məʊt]vt. 使降级；使降职351, denial [dɪ’naɪəl]n. 否认；拒绝；节制；背弃352, denoted表示，指示（denote的过去式和过去分词）353, Denotes []为…的符号，表示，指示（denote的第三人称单数）354, density [‘dɛnsəti]n. 密度355, dentry目录项356, depict [dɪ’pɪkt]vt. 描述；描画357, Deploy [dɪ’plɒɪ]vt. 配置；展开；使疏开;vi. 部署；展开;n. 部署358, depot [‘depəʊ]n. 仓库；停车场；航空站;vt. 把…存放在储藏处;adj. 药性持久的;n. (Depot)人名；(刚(布))德波特359, deprecation [,depri’keiʃən]n. 祈免；贬低；反对360, derivative [dɪ’rɪvətɪv]n. [化学] 衍生物，派生物；导数adj. 派生的；引出的361, descendant [dɪ’sɛndənt]adj. 下降的；祖传的n. 后裔；子孙362, descended [dɪ’sendɪd]adj. 出身于…的；从一个祖先传下来的363, designate [‘dezɪgneɪt]vt. 指定；指派；标出；把…定名为;adj. 指定的；选定的364, designated [‘dɛzɪg,netɪd]adj. 指定的；特指的365, desirable [dɪ’zaɪərəbl]adj. 令人满意的；值得要的 n. 合意的人或事物366, desire [dɪ’zaɪɚ]n. 欲望；要求，心愿；性欲vt. 想要；要求；希望得到…vi. 渴望n. (Desire)人名；(刚(布)、英)德西雷367, desired [dɪ’zaɪrd]adj. 渴望的；想得到的v. 渴望，要求（desire的过去分词形式）368, destruction [dɪ’strʌkʃ(ə)n]n. 破坏，毁灭；摧毁369, detach [dɪ’tætʃ]vt. 分离；派遣；使超然370, detached [dɪ’tætʃt]adj. 分离的，分开的；超然的v. 分离371, Detail [‘diːteɪl]n. 细节，详情;vt. 详述；选派;vi. 画详图372, deviation [‘divɪ’eʃən]n. 偏差；误差；背离373, diagnose [‘daɪəgnəʊz; -‘nəʊz]vt. 诊断；断定 vi. 诊断；判断374, diagram [‘daɪəgræm]n. 图表；图解vt. 用图解法表示diagram: 图表 图解 简图375, dial [‘daɪəl]n. 转盘；刻度盘；钟面vi. 拨号vt. 给…拨号打电话n. (Dial)人名；(英)戴尔376, dialect [‘daɪəlekt]n. 方言，土话；同源语；行话；个人用语特征 adj. 方言的377, Dialect [‘daɪəlekt]n. 方言，土话；同源语；行话；个人用语特征;adj. 方言的378, digest [daɪ’dʒɛst]vt. 消化；吸收；融会贯通vi. 消化n. 文摘；摘要379, Digest [daɪ’dʒest; dɪ-]vt. 消化；吸收；融会贯通;vi. 消化;n. 文摘；摘要380, dimension [dɪ’menʃ(ə)n; daɪ-]n. 方面;[数] 维；尺寸；次元；容积 vt. 标出尺寸;adj. 规格的;adj. 规格的381, dimensional [dɪ’menʃənəl]adj. 空间的；尺寸的382, dimensionality [daɪmɛnʃə’næləti]n. 维度；幅员；广延383, dimensions [dɪ’mɛnʃənz]n. 规模，大小384, directive [daɪ’rɛktɪv]n. 指示；指令adj. 指导的；管理的385, disambiguate [dɪsæm’bɪgjʊeɪt]vt. 消除（字句等的）含糊意义，消除…的虚无主义386, disastrous [dɪ’zɑːstrəs]adj. 灾难性的；损失惨重的；悲伤的387, discrepancy [dɪs’krep(ə)nsɪ]n. 不符；矛盾；相差388, discretionary [dɪ’skreʃ(ə)n(ə)rɪ]adj. 任意的；自由决定的389, discriminator [dɪ’skrɪmɪneɪtə]n. [电子] 鉴别器；辨别者390, discuss [dɪ’skʌs]vt. 讨论；论述，辩论391, dismiss [dɪs’mɪs]vt. 解散；解雇；开除；让…离开；不予理会、不予考虑;vi. 解散392, Dismissal [dɪs’mɪsl]n. 解雇；免职393, Dispatcher [dɪs’pætʃə]n. 调度员；[计] 调度程序；[计] 分配器394, disperse [dɪ’spɜːs]vt. 分散；使散开；传播;vi. 分散;adj. 分散的395, Displaced [dɪs’ples]adj. 无家可归的；位移的；被取代的n. 无家可归者v. 取代（displace的过去分词）；移动…的位置；撤换396, disruption [dɪs’rʌpʃən]n. 破坏，毁坏；分裂，瓦解397, disruptor [disruptor]n. 破坏者，分裂者398, Disruptor [dɪsˈrʌpt]n. 破坏者，分裂者399, dissect [dɪˈsɛkt, daɪ-, ˈdaɪˌsɛkt]vt. 切细；仔细分析vi. 进行解剖；进行详细分析400, Dissector [dɪ’sektə]n. 解剖器；析象器401, Dissipator [‘disipeitə]n. 浪子，放荡者402, distinguish [dɪ’stɪŋgwɪʃ]vi. 区别，区分；辨别vt. 区分；辨别；使杰出，使表现突出403, Distributed [dɪ’strɪbjʊtɪd]adj. 分布式的，分散式的404, Distributions [,dɪstrə’bjʊʃən]n. 分派；分派；分销（distribution的复数形式）405, Distro [Distro]n. 发行版406, divergence [daɪ’vɜːdʒ(ə)ns]n. 分歧407, diverse [daɪ’vɜːs; ‘daɪvɜːs]adj. 不同的，相异的；多种多样的，形形色色的408, Doctypen. 文档类型409, Domestic [də’mestɪk]adj. 国内的；家庭的；驯养的；一心只管家务的n. 国货；佣人410, dominant [‘dɒmɪnənt]adj. 显性的；占优势的；支配的，统治的n. 显性411, Donations [doʊ’neɪʃnz]n. [经] 捐赠（donation的复数）；捐款412, dozensn. 许多（dozen的复数）413, drain [dren]vi. 排水；流干vt. 喝光，耗尽；使流出；排掉水n. 排水；下水道，排水管；消耗n. (Drain)人名；(英)德雷恩；(法)德兰414, dramatically [drə’mætɪkəlɪ]adv. 戏剧地；引人注目地;adv. 显著地，剧烈地415, drastically [‘dræstikəli]adv. 彻底地；激烈地416, drawback [‘drɔbæk]n. 缺点，不利条件；退税417, drench [dren(t)ʃ]vt. 使湿透；给（牲畜）灌药;n. 滂沱大雨；浸液418, drill [drɪl]n. 训练；钻孔机；钻子；播种机vi. 钻孔；训练vt. 钻孔；训练；条播n. (Drill)人名；(德、英)德里尔419, drip [drɪp]vi. 滴下；充满；漏下n. 水滴，滴水声；静脉滴注；使人厌烦的人vt. 使滴下；溢出，发出420, dual [‘djuːəl]adj. 双的；双重的;n. 双数；双数词;n. (Dual)人名；(法)迪阿尔421, duckweed [‘dʌkwiːd]n. 浮萍；水萍422, Duckweed [‘dʌkwiːd]n. 浮萍；水萍423, ducky [‘dʌkɪ]adj. 可喜的；极好的；极为愉快的424, dumb [dʌm]adj. 哑的，无说话能力的；不说话的，无声音的425, dummy [‘dʌmi]adj. 虚拟的；假的n. 傀儡；哑巴；仿制品426, dust [dʌst]n. 灰尘；尘埃；尘土;vt. 撒；拂去灰尘;vi. 拂去灰尘；化为粉末;n. (Dust)人名；(德、俄)杜斯特427, Dutch [dʌtʃ]adj. 荷兰的；荷兰人的；荷兰语的n. 荷兰人；荷兰语adv. 费用平摊地；各自付账地428, dutiful [‘djuːtɪfʊl; -f(ə)l]adj. 忠实的；顺从的；守本分的429, E-Commerce [‘i:,kɒmɜ:s]n. 电子商务430, ebook [‘i:buk]n. 电子书431, echo [‘ɛko]vt. 反射；重复vi. 随声附和；发出回声n. 回音；效仿432, ecosystem [‘ɛko,sɪstəm]n. 生态系统433, eden [‘i:dən]n. 伊甸园（《圣经》中亚当和夏娃最初居住的地方）434, edgesight 435, ego [‘iːgəʊ; ‘e-]n. 自我；自负；自我意识;n. (Ego)人名；(日)依怙 (姓)；(法)埃戈436, elaborate [ɪ’læbəret]adj. 精心制作的；详尽的；煞费苦心的vt. 精心制作；详细阐述；从简单成分合成（复杂有机物）vi. 详细描述；变复杂437, elapse [ɪ’læps]vi. 消逝；时间过去n. 流逝；时间的过去438, elapsedv. 时间过去；消逝（elapse的过去分词）439, elastic [ɪ’læstɪk]adj. 有弹性的；灵活的；易伸缩的;n. 松紧带；橡皮圈440, election [ɪ’lekʃ(ə)n]n. 选举；当选；选择权；上帝的选拔441, elector [ɪ’lɛktɚ]n. 选举人；有选举权的人；总统选举人442, elide [ɪ’laɪd]vt. 省略；取消；删去；不予考虑删节n. (Elide)人名；(意)埃利德443, eligible [‘elɪdʒɪb(ə)l]adj. 合格的，合适的；符合条件的；有资格当选的n. 合格者；适任者；有资格者444, eliminate [ɪ’lɪmɪnet]vt. 消除；排除445, elimination [ɪ,lɪmɪ’neɪʃən]n. 消除；淘汰；除去446, embed [ɪm’bɛd]vt. 栽种；使嵌入，使插入；使深留脑中447, embedded [ɪmˈbɛdɪd]adj. 嵌入式的；植入的；内含的v. 嵌入（embed的过去式和过去分词形式）448, Embedded [ɪm’bedɪd]adj. 嵌入式的；植入的；内含的;v. 嵌入（embed的过去式和过去分词形式）449, embellishment [ɪm’belɪʃmənt]n. 装饰，修饰；润色450, emergency [ɪ’mɜːdʒ(ə)nsɪ]n. 紧急情况；突发事件；非常时刻adj. 紧急的；备用的451, emit [ɪ’mɪt]vt. 发出，放射；发行；发表452, empathy [‘empəθɪ]n. 神入；移情作用；执着n. 感同身受；同感；共鸣453, emphasis [‘ɛmfəsɪs]n. 重点；强调；加强语气454, emphasis on []着重于；对…的强调emphasis on: 对 强调 对……的强调455, emphasize [‘ɛmfəsaɪz]vt. 强调，着重456, employ [ɪm’plɒɪ; em-]vt. 使用，采用；雇用；使忙于，使从事于 n. 使用；雇用457, empower [ɪm’paʊə; em-]vt. 授权，允许；使能够458, emulation [,emjʊ’leɪʃən]n. [计] 仿真；竞争；效法459, encapsulate [ɪn’kæpsjʊleɪt; en-]vt. 压缩；将…装入胶囊；将…封进内部；概述;vi. 形成胶囊460, encapsulated [ɪn’kæpsəleɪtɪd]adj. 密封的；包在荚膜内的 v. 压缩（encapsulate的过去分词）；封进内部；装入胶囊461, encapsulates [ɪn’kæpsjʊleɪt; en-]vt. 压缩；将…装入胶囊；将…封进内部;vi. 形成胶囊462, encapsulation [ɪn,kæpsə’leʃən]n. 封装；包装463, Encapsulation [ɪn,kæpsə’leɪʃən]n. 封装；包装464, encompass [ɪn’kʌmpəs; en-]vt. 包含；包围，环绕；完成465, encore [‘ɒŋkɔː]n. 再演唱的要求；经要求而再唱vt. 要求再演或唱int. 再来一个466, encounter [ɪn’kaʊntə; en-]vt. 遭遇，邂逅；遇到;n. 遭遇，偶然碰见;vi. 遭遇；偶然相遇467, encryptedv. 把…编码；把…加密（encrypt的过去分词）468, endeavour [ɪn’devə; en-]n. 尽力，竭力;vt. 竭力做到，试图或力图（做某事）;vi. 竭力；企图469, enormous [ɪ’nɔːməs]adj. 庞大的，巨大的；凶暴的，极恶的470, entail [ɪn’tel]vt. 使需要，必需；承担；遗传给；蕴含n. 引起；需要；继承471, enthusiasm [ɪn’θjuːzɪæz(ə)m; en-]n. 热心，热忱，热情472, enthusiastic [ɪn,θjuːzɪ’æstɪk; en-]adj. 热情的；热心的；狂热的473, entrance [‘entr(ə)ns]n. 入口；进入;vt. 使出神，使入迷474, entrypoint [entrypoint]n. 进入点；入境点；入口点475, envelope [‘envələʊp; ‘ɒn-]n. 信封，封皮；包膜；[天] 包层；包迹476, ephemeral [ə’fɛmərəl]adj. 短暂的；朝生暮死的n. 只生存一天的事物477, epoch [‘ɛpək]n. [地质] 世；新纪元；新时代；时间上的一点478, equate [ɪ’kweɪt]vt. 使相等；视为平等;vi. 等同479, equates [ɪ’kweɪt]vt. 使相等；视为平等 vi. 等同480, equipment [ɪ’kwɪpm(ə)nt]n. 设备，装备；器材481, equivalent [ɪ’kwɪv(ə)l(ə)nt]adj. 等价的，相等的；同意义的;n. 等价物，相等物482, erase [ɪ’reɪz]vt. 抹去；擦除vi. 被擦去，被抹掉483, erasure [ɪ’reɪʒə(r)]n. 消除；涂擦的痕迹；消磁484, essence [‘es(ə)ns]n. 本质，实质；精华；香精 n. (Essence)人名；(英)埃森丝485, estimate [‘ɛstə,met]vi. 估计，估价n. 估计，估价；判断，看法vt. 估计，估量；判断，评价486, Estimate [‘estɪmeɪt]vi. 估计，估价;n. 估计，估价；判断，看法;vt. 估计，估量；判断，评价487, estimated [ɛstəˌmetɪd]adj. 估计的；预计的；估算的488, estimation [,ɛstɪ’meʃən]n. 估计；尊重489, ethereal [ɪ’θɪərɪəl]adj. 优雅的；轻飘的；缥缈的；超凡的490, ethernet [‘iθɚnɛt]n. [计] 以太网491, Ethernet [Ethernet]n. [计] 以太网492, evacuate [ɪ’vækjuet]vt. 疏散，撤退；排泄vi. 疏散；撤退；排泄493, evacuated [ɪ’vækjʊ,et]adj. 疏散；排空的，撤退者的v. 疏散；撤出；排泄（evacuate的过去式和过去分词形式）494, evacuation [ɪ,vækjʊ’eɪʃ(ə)n]n. 疏散；撤离；排泄495, eval [ɪ’væl]n. 重新运算求出参数的内容496, evaluate [ɪ’væljʊeɪt]vt. 评价；估价；求…的值;vi. 评价；估价497, Evaluate [ɪ’væljʊeɪt]vt. 评价；估价；求…的值;vi. 评价；估价498, evaluatedvi. 评估；估…的价（evaluate的过去分词形式）499, even [‘iːv(ə)n]adj. [数] 偶数的；平坦的；相等的;adv. 甚至；即使；还；实际上;vt. 使平坦；使相等;vi. 变平；变得可比较；成为相等;n. (Even)人名；(法)埃旺；(德)埃文；(英)埃文500, eventually [ɪ’ventʃʊəlɪ]adv. 最后，终于501, evict [ɪ’vɪkt]vt. 驱逐；逐出502, eviction [ɪ’vɪkʃ(ə)n]n. 逐出；赶出；收回503, Eviction [ɪ’vɪkʃ(ə)n]n. 逐出；赶出；收回504, evolve [ɪ’vɒlv]vt. 发展，进化；进化；使逐步形成；推断出;vi. 发展，进展；进化；逐步形成505, Evolve [ɪ’vɒlv]vt. 发展，进化；进化；使逐步形成；推断出;vi. 发展，进展；进化；逐步形成506, evolvedadj. 进化了的;vt. 使逐步形成（evolve的过去分词）507, exact [ɪg’zækt; eg-]adj. 准确的，精密的；精确的;vt. 要求；强求；急需;vi. 勒索钱财508, examine [ɪg’zæmɪn; eg-]vt. 检查；调查； 检测；考试;vi. 检查；调查509, excitable [ɪk’saɪtəb(ə)l; ek-]adj. 易激动的；易兴奋的；易怒的510, Exclusive [ɪk’skluːsɪv; ek-]adj. 独有的；排外的；专一的;n. 独家新闻；独家经营的项目；排外者511, exhausted [ɪɡ’zɔːstɪd]adj. 筋疲力尽的，疲惫不堪的；耗尽的，枯竭的 v. 使……精疲力尽；耗尽，用尽；详尽讨论（exhaust 的过去式和过去分词）512, exhaustive [ɪg’zɔːstɪv; eg-]adj. 详尽的；彻底的；消耗的513, exotic [ɪg’zɒtɪk; eg-]adj. 异国的；外来的；异国情调的514, experimental [ek,speri’mentəl; ek’s-]adj. 实验的；根据实验的；试验性的515, explanatory [ɪk’splænətɔri]adj. 解释的；说明的516, explicit [ɪk’splɪsɪt; ek-]adj. 明确的；清楚的；直率的；详述的517, explicitly [ɪk’splɪsɪtli]adv. 明确地；明白地518, explode [ɪk’spləʊd; ek-]vi. 爆炸，爆发；激增;vt. 使爆炸；爆炸；推翻519, explosion [ɪk’spləʊʒ(ə)n; ek-]n. 爆炸；爆发；激增 n. （Explosion）《引爆者》（一部中国动作、犯罪电影）。520, exponential [‘ɛkspə’nɛnʃəl]adj. 指数的n. 指数521, exponentially [,ɛkspo’nɛnʃəli]adv. 以指数方式522, exponentiation [,ekspənenʃɪ’eɪʃ(ə)n]n. [数] 取幂，求幂；乘方523, expose [ɪk’spəʊz; ek-]vt. 揭露，揭发；使曝光；显示524, expresso [ek’spresəʊ]n. （蒸汽加压煮出的）浓咖啡525, expunge [ɪk’spʌndʒ]vt. 擦去；删掉526, Expunge [ɪk’spʌn(d)ʒ; ek-]vt. 擦去；删掉527, expunges [ɪk’spʌn(d)ʒ; ek-]vt. 擦去；删掉528, extensible [ek’stensɪbl; ɪk’stensɪb(ə)l]adj. 可延长的；可扩张的529, Extensible [ek’stensɪbl; ɪk’stensɪb(ə)l]adj. 可延长的；可扩张的530, extent [ɪk’stent; ek-]n. 程度；范围；长度531, extract [ˈekstrækt]vt. 提取；取出；摘录；榨取 n. 汁；摘录；榨出物；选粹532, extrapolate [ɪk’stræpə’let]vt. 外推；推断vi. 外推；进行推断过去式:extrapolated; 过去分词:extrapolated; 现在分词:extrapolating533, extrapolatedv. 推测（extrapolate的过去分词）adj. 推测的534, Extras [‘ɛkstrə]n. 附加设备；额外部分；另外收费的部分（extra的复数）535, facade [fə’sɑːd]n. 正面；表面；外观536, facilitate [fə’sɪlɪteɪt]vt. 促进；帮助；使容易537, faint [feɪnt]adj. 模糊的；头晕的；虚弱的；[医] 衰弱的 vi. 昏倒；变得微弱；变得没气力 n. [中医] 昏厥，昏倒538, Fake [feɪk]n. 假货；骗子；假动作;vt. 捏造；假装…的样子;vi. 假装；做假动作;adj. 伪造的;n. (Fake)人名；(英)费克539, Fakes []n. 赝品；云母板状岩；假冒商品（fake的复数）;v. 佯装；假装（fake的三单形式）;n. (Fakes)人名；(法)法克540, fallback [‘fɔlbæk]n. 可依靠的东西；后备物品；撤退，退却541, fame [feɪm]n. 名声，名望；传闻，传说;vt. 使闻名，使有名望542, fanatical [fə’nætɪkəl]adj. 狂热的543, fancy [‘fænsɪ]n. 幻想；想象力；爱好;adj. 想象的；奇特的；昂贵的；精选的;vt. 想象；喜爱；设想；自负;vi. 幻想；想象;n. (Fancy)人名；(法)方西544, fanout [fanout]n. 扇出；展开；分列（账户）545, fatal [‘fetl]adj. 致命的；重大的；毁灭性的；命中注定的 n. (Fatal)人名；(葡、芬)法塔尔546, fault [fɔːlt; fɒlt]n. 故障；[地质] 断层；错误；缺点；毛病；（网球等）发球失误 vi. 弄错；产生断层 vt. （通常用于疑问句或否定句）挑剔547, favor [‘feɪvə]vt. 赞成；喜欢；像；赐予；证实;n. 喜爱；欢心；好感548, federate [‘fedəreɪt]adj. 同盟的；联邦制度下的；联合的vt. 使结成同盟；使结成联邦vi. 结成联邦549, federated [‘fɛdə,retɪd]adj. 联邦的；联合的；结成同盟的 v. 结成同盟；按联邦制组织起来（federate的过去分词）550, federation [fedə’reɪʃ(ə)n]n. 联合；联邦；联盟；联邦政府551, fence [fens]n. 栅栏；围墙；剑术vt. 防护；用篱笆围住；练习剑术vi. 击剑；搪塞；围以栅栏；跳过栅栏552, fetch [fetʃ]vt. 取来；接来；到达；吸引;vi. 拿；取物；卖得;n. 取得；诡计553, fictional [‘fɪkʃənl]adj. 虚构的；小说的554, fictitious [fɪk’tɪʃəs]adj. 虚构的；假想的；编造的；假装的555, final [‘faɪn(ə)l]adj. 最终的；决定性的；不可更改的;n. 决赛；期末考试；当日报纸的末版556, finer-grained [finer-grained]细粒度557, firework [‘faɪɚwɝk]n. 烟火；激烈情绪558, flap [flæp]n. 拍打，拍打声；神经紧张；[航] 襟翼vi. 拍动；神经紧张；鼓翼而飞；（帽边等）垂下vt. 拍打；扔；拉下帽边；飘动559, flat [flæt]adj. 平的；单调的；不景气的；干脆的；平坦的；扁平的；浅的;adv. （尤指贴着另一表面）平直地；断然地；水平地；直接地，完全地;n. 平地；公寓；平面;vt. 使变平；[音乐]使（音调）下降，尤指降半音;vi. 逐渐变平；[音乐]以降调唱（或奏）;n. （法）弗拉特（人名）；（英）弗莱特（人名）560, flatten [‘flætn]vt. 击败，摧毁；使……平坦 vi. 变平；变单调 n. (Flatten)人名；(德)弗拉滕561, flatter [‘flætə]vt. 奉承；谄媚；使高兴562, flavorsn. 风味调料（flavor复数）v. 添加味道（flavor的三单形式）563, flavourn. 香味；滋味vt. 给……调味；给……增添风趣564, fledged [fledʒd]adj. 成熟的；快会飞的；羽毛丰满的565, flood [flʌd]vt. 淹没；充满；溢出vi. 涌出；涌进；为水淹没n. 洪水；泛滥；一大批n. (Flood)人名；(英)弗勒德；(瑞典、芬)弗洛德566, fluent [‘fluənt]adj. 流畅的，流利的；液态的；畅流的567, flute [fluːt]n. 长笛；【工程设计】(刀具的)出屑槽vt. 用长笛吹奏vi. 吹长笛568, forcibly [‘fɔːsɪblɪ]adv. 用力地；强制地；有说服力地569, foremost [‘fɔːməʊst]adj. 最重要的；最先的adv. 首先；居于首位地570, forgesn. 轻便锻炉；熔炉（forge的复数） v. 伪造（forge的第三人称单数）；打制 n. (Forges)人名；(法)福尔热571, fork [fɔːk]n. 叉；餐叉；耙;vt. 叉起；使成叉状;vi. 分叉；分歧;n. (Fork)人名；(英、德)福克572, formerly [‘fɔːməlɪ]adv. 以前；原来573, Fortress [‘fɔːtrɪs]n. 堡垒；要塞;vt. 筑要塞；以要塞防守574, Forwarding [‘fɔːwədɪŋ]v. 促进；寄发；装订；转递（forward的ing形式）;n. 转发；促进；运输业务；各项装订工序;adj. 运输的；转发的575, foundry [‘faʊndri]n. 铸造，铸造类；[机] 铸造厂576, fractional [‘frækʃənl]adj. 部分的；[数] 分数的，小数的577, fragile [‘frædʒəl]adj. 脆的；易碎的578, fragments [‘frægmənt]n. 碎片（fragment的复数）；片断；[计] 分段;v. 破碎（fragment的三单形式）；打碎579, frequency [‘frikwənsi]n. 频率；频繁580, fresh [freʃ]adj. 新鲜的；清新的；淡水的；无经验的;n. 开始；新生；泛滥;adv. 刚刚，才；最新地581, frugal [‘fruːg(ə)l]adj. 节俭的；朴素的；花钱少的582, frustrate [frʌ’streɪt; ‘frʌs-]vt. 挫败；阻挠；使感到灰心;vi. 失败；受挫;adj. 挫败的；无益的583, fudge [fʌdʒ]n. 软糖；胡说；谎话vt. 捏造；粗制滥造；回避vi. 逃避责任；欺骗；蒙混int. 胡说八道！n. (Fudge)人名；(英)富奇584, functionality [fʌŋkʃə’nælətɪ]n. 功能；[数] 泛函性，函数性585, fundamental [fʌndə’ment(ə)l]adj. 基本的，根本的n. 基本原理；基本原则586, fuzzy [‘fʌzi]adj. 模糊的；失真的；有绒毛的n. (Fuzzy)人名；(英)富齐[ 比较级:fuzzier 最高级:fuzziest ]587, gain [geɪn]n. 增加；利润；收获;vt. 获得；增加；赚到;vi. 增加；获利;n. (Gain)人名；(英、匈、法)盖恩588, gauge [gedʒ]n. 计量器；标准尺寸；容量规格vt. 测量；估计；给…定规格589, gear [ɡɪr]n. 齿轮；装置，工具；传动装置 vi. 适合；搭上齿轮；开始工作 vt. 开动；搭上齿轮；使……适合；使……准备好 adj. 好极了 n. (Gear)人名；(英)吉尔590, gem [dʒɛm]n. 宝石，珍宝； 精华；受人重视者vi. 点缀； 用宝石装饰； 饰以宝石adj. 最佳品质的n. (Gem)人名；(英)杰姆591, geographically [dʒɪə’græfɪkli]adv. 在地理上；地理学上592, gigabytesn. 千兆字节（gigabyte的复数）；十亿位元组593, glimpse [glɪm(p)s]n. 一瞥，一看 vi. 瞥见 vt. 瞥见594, glossary [‘glɒs(ə)rɪ]n. 术语（特殊用语）表；词汇表；专业词典595, glossed [glɒs]n. 光彩；注释；假象;vt. 使光彩；掩盖；注释;n. (Gloss)人名；(德、西、捷)格洛斯596, goals [gəʊlz]n. 目标，[心理] 目的；进球，射中次数597, gradient [‘greɪdɪənt]n. [数][物] 梯度；坡度；倾斜度;adj. 倾斜的；步行的598, gradientsn. 渐变，[数][物] 梯度（gradient复数形式）599, gradually [‘grædʒʊəli]adv. 逐步地；渐渐地600, grant [grɑːnt]vt. 授予；允许；承认 vi. 同意 n. 拨款；[法] 授予物 n. (Grant)人名；(瑞典、葡、西、俄、罗、英、塞、德、意)格兰特；(法)格朗601, granularityn. 间隔尺寸，[岩] 粒度602, graphical [‘græfɪk(ə)l]adj. 图解的；绘画的；生动的603, greedy [‘griːdɪ]adj. 贪婪的；贪吃的；渴望的604, grep [grep]n. UNIX工具程序；可做文件内的字符串查找;n. (Grep)人名；(苏里)格雷普605, grok [ɡrɑk]vt. 凭直觉深刻了解；欣赏；神交vi. 心意相通；与…神交606, gross [ɡros]adj. 总共的；粗野的；恶劣的；显而易见的vt. 总共收入n. 总额，总数n. (Gross)人名；(英、法、德、意、葡、西、俄、芬、罗、捷、匈)格罗斯607, grumpy [‘ɡrʌmpi]adj. 脾气暴躁的；性情乖戾的n. 脾气坏的人；爱抱怨的人608, guarantee [,ɡærən’ti]n. 保证；担保；保证人；保证书；抵押品vt. 保证；担保609, guaranteed [‘gærən’tid]adj. 有保证的，；有人担保的;v. 担保（guarantee的过去式和过去分词）610, guidance [‘gaɪd(ə)ns]n. 指导，引导；领导611, guide [gaɪd]n. 指南；向导；入门书;vt. 引导；带领；操纵;vi. 担任向导;n. (Guide)人名；(法、葡)吉德612, Guvnor [‘ɡʌvnə]n. 雇主；老板613, gym [dʒɪm]n. 健身房；体育；体育馆614, hall [hɔːl]n. 过道，门厅，走廊；会堂；食堂；学生宿舍；大厅，前厅；娱乐中心，会所;n. （土）哈勒（人名）；（德、波、丹、芬、瑞典）哈尔（人名）；（英）霍尔（人名）；（法）阿尔（人名）615, hallway [‘hɔːlweɪ]n. 走廊；门厅；玄关616, haltedv. 停止，停顿（halt的过去式，过去分词）617, halve [hɑːv]vt. 二等分；把……减半 n. (Halve)人名；(芬)哈尔韦618, handlers []n. 操作者；经理人；陶器工人（handler的复数）619, handshake [‘hæn(d)ʃeɪk]n. 握手620, hashtag [ˈhæʃˌtæɡ]n. 标签621, hazard [‘hæzəd]vt. 赌运气；冒…的危险，使遭受危险n. 危险，冒险；冒险的事n. (Hazard)人名；(法)阿扎尔；(英)哈泽德622, headroom [‘hedruːm; -rʊm]n. 净空；净空高度；头上空间623, heap [hiːp]n. 堆；许多；累积;vt. 堆；堆积;vi. 堆起来;n. (Heap)人名；(芬)海亚普；(东南亚国家华语)协；(英)希普624, heck [hek]int.真见鬼（hell的委婉说法）n.饲草架n.(Heck)人名；(德)黑克；(葡、西、法)埃克；(英)赫克625, Hence [hens]adv. 因此；今后626, hereby [hɪə’baɪ]adv. 以此方式，据此；特此627, hesitant [‘hɛzɪtənt]adj. 迟疑的；踌躇的；犹豫不定的628, hesitate [ˈhɛzətet]vi. 踌躇，犹豫；不愿vt. 踌躇，犹豫；有疑虑，不愿意629, heterogeneous [,het(ə)rə(ʊ)’dʒiːnɪəs; -‘dʒen-]adj. [化学] 多相的；异种的；[化学] 不均匀的；由不同成分形成的630, heuristics [hju’rɪstɪks]n. 启发法；启发式教学法631, hexadecimal [,hɛksə’dɛsɪml]adj. 十六进制的n. 十六进制632, hierarchical [haɪə’rɑːkɪk(ə)l]adj. 分层的；等级体系的633, hierarchy [‘haɪərɑːkɪ]n. 层级；等级制度634, hint [hɪnt]n. 暗示；线索vt. 暗示；示意vi. 示意n. (Hint)人名；(英)欣特；(法)安特635, histogram [‘hɪstəɡræm]n. [统计] 直方图；柱状图636, hitchhiker [‘hɪtʃhaɪkə]n. 搭便车的旅行者；顺便插入的广告637, hive [haɪv]vi. 群居；入蜂房；生活在蜂房中n. 蜂房，蜂巢；热闹的场所；熙攘喧闹的人群vt. 入蜂箱；贮备638, hoist [hɒɪst]n. 起重机；升起，吊起;vi. 升起；吊起;vt. （用绳索，起重机等）使升起;n. (Hoist)人名；(英)霍伊斯特639, hole [həʊl]n. 洞，孔；洞穴，穴；突破口vi. 凿洞，穿孔；（高尔夫球等）进洞vt. 凿洞n. (Hole)人名；(瑞典、挪)霍勒；(英)霍尔640, holt [holt]n. 小林；林丘；杂木林n. (Holt)人名；(英、德、罗、匈、瑞典)霍尔特；(法、西)奥尔特641, homebrew [homebrew]n. 自酿（啤）酒；公司自产自用642, honestly [‘ɒnɪstlɪ]adv. 真诚地；公正地643, hood [hʊd]n. 头巾；覆盖；兜帽n. (Hood)人名；(英)胡德；(德、荷)霍德vt. 罩上；以头巾覆盖644, hostile [‘hɒstaɪl]adj. 敌对的，敌方的；怀敌意的;n. 敌对645, hourly [‘aʊɚli]adv. 每小时地；频繁地，随时adj. 每小时的，以钟点计算的；频繁的646, hydrant [‘haɪdr(ə)nt]n. 消防栓；水龙头；给水栓647, ideally [aɪ’diəli]adv. 理想地；观念上地648, idempotent [,aɪdem’pəʊt(ə)nt; aɪ’dempət(ə)nt]adj. 幂等的 n. [数] 幂等649, identify [aɪ’dentɪfaɪ]vt. 确定；鉴定；识别，辨认出；使参与；把…看成一样 vi. 确定；认同；一致;vi. 确定；认同；一致650, idiom [‘ɪdɪəm]n. 成语，习语；土话651, idiomatic [,ɪdɪə’mætɪk]adj. 惯用的；符合语言习惯的；通顺的652, idle [‘aɪdl]adj. 闲置的；懒惰的；停顿的 vi. 无所事事；虚度；空转 vt. 虚度；使空转653, Idle [‘aɪd(ə)l]adj. 闲置的；懒惰的；停顿的;vi. 无所事事；虚度；空转;vt. 虚度；使空转654, ignite [ɪɡ’naɪt]vt. 点燃；使燃烧；使激动vi. 点火；燃烧655, illegal [ɪ’liːg(ə)l]n. 非法移民；间谍;adj. [法] 非法的；违法的；违反规则的656, illustrate [‘ɪləstreɪt]vi. 举例vt. 阐明，举例说明；图解657, immutability [immutability]n. 不变；永恒性；不变性658, immutable [ɪ’mjuːtəb(ə)l]adj. 不变的；不可变的；不能变的659, implement [‘ɪmplɪm(ə)nt]n. 工具，器具；手段vt. 实施，执行；实现，使生效660, implementation [ɪmplɪmen’teɪʃ(ə)n]n. [计] 实现；履行；安装启用661, implementing []n. 实施，执行；实现v. 贯彻，执行（implement的现在分词）662, implicit [ɪm’plɪsɪt]adj. 含蓄的；暗示的；盲从的663, inception [ɪn’sɛpʃən]n. 起初；获得学位n. 《盗梦空间》（电影名）664, inconvenience [ɪnkən’viːnɪəns]n. 不便；麻烦;vt. 麻烦；打扰665, incorporate [ɪn’kɔːpəreɪt]vt. 包含，吸收；体现；把……合并;vi. 合并；混合；组成公司;adj. 合并的；一体化的；组成公司的666, incubator [‘ɪŋkjʊbeɪtə]n. [禽] 孵卵器；[儿科] 保温箱；早产儿保育器；细菌培养器667, indicate [‘ɪndɪkeɪt]vt. 表明；指出；预示；象征668, indicating [‘ɪndɪkeɪtɪŋ]n. 表明；指示v. 表明；指示；要求（indicate的ing形式）adj. 指示的669, indicator [‘ɪndɪketɚ]n. 指示器；[试剂] 指示剂；[计] 指示符；压力计670, indicen. 指数；标记体671, individual [ɪndɪ’vɪdjʊ(ə)l]adj. 个人的；个别的；独特的;n. 个人，个体672, individually [ˌɪndɪˈvɪdʒuəli]adv. 个别地，单独地673, industry [‘ɪndəstrɪ]n. 产业；工业；勤勉674, inevitably [ɪ’nevɪtəblɪ; ɪn’evɪtəblɪ]adv. 不可避免地；必然地675, infection [ɪn’fɛkʃən]n. 感染；传染；影响；传染病676, infer [ɪn’fɝ]vt. 推断；推论vi. 推断；作出推论677, Infinite [‘ɪnfɪnət]adj. 无限的，无穷的；无数的；极大的;n. 无限；[数] 无穷大；无限的东西（如空间，时间）678, infrastructure [‘ɪnfrəstrʌktʃə]n. 基础设施；公共建设；下部构造679, ingest [ɪn’dʒest]vt. 摄取；咽下；吸收；接待680, ingress [‘ɪngres]n. 进入；入口；准许进入；入境n. (Ingress)人名；(英)英格雷斯681, inherently [ɪn’hɪrəntli]adv. 内在地；固有地；天性地682, inhibition [ɪn(h)ɪ’bɪʃ(ə)n]n. 抑制；压抑；禁止683, Initiative [ɪ’nɪʃɪətɪv; -ʃə-]n. 主动权；首创精神;adj. 主动的；自发的；起始的684, initiator [ɪ’nɪʃɪeɪtə]n. 发起人，创始者；教导者；[计] 启动程序；引爆器685, innocent [‘ɪnəs(ə)nt]adj. 无辜的；无罪的；无知的n. 天真的人；笨蛋n. (Innocent)人名；(英、西)因诺森特；(法)伊诺桑686, inplacen. 原地687, insight [‘ɪnsaɪt]n. 洞察力；洞悉n. (Insight)人名；(英)因赛特688, inspect [ɪn’spekt]vt. 检查；视察；检阅;vi. 进行检查；进行视察689, instant [‘ɪnstənt]adj. 立即的；紧急的；紧迫的n. 瞬间；立即；片刻690, instantaneous [,ɪnstən’tenɪəs]adj. 瞬间的；即时的；猝发的691, instantiate [ɪn’stænʃɪeɪt]vt. 例示，举例说明692, instrument [‘ɪnstrʊm(ə)nt]n. 仪器；工具；乐器；手段；器械693, instrumental [,ɪnstrə’mɛntl]adj. 乐器的；有帮助的；仪器的，器械的n. 器乐曲；工具字，工具格694, insulation [ɪnsjʊ’leɪʃ(ə)n]n. 绝缘；隔离，孤立695, intact [ɪn’tækt]adj. 完整的；原封不动的；未受损伤的696, integrate [‘ɪntɪgreɪt]vt. 使…完整；使…成整体；求…的积分；表示…的总和;vi. 求积分；取消隔离；成为一体;adj. 整合的；完全的;n. 一体化；集成体697, integration [ɪntɪ’greɪʃ(ə)n]n. 集成；综合698, Integration [ɪntɪ’greɪʃ(ə)n]n. 集成；综合699, intense [ɪn’tɛns]adj. 强烈的；紧张的；非常的；热情的[ 比较级:more intense或 intenser 最高级:most intense或 intensest ]700, intent [ɪn’tent]n. 意图；目的；含义;adj. 专心的；急切的；坚决的701, intention [ɪn’tenʃ(ə)n]n. 意图；目的；意向；愈合702, intentionallyadv. 故意地，有意地703, interacting [,ɪntə’rækt]n. 相互作用；相互制约;v. 互相影响；互相作用（interact的ing形式）704, interaction [ɪntər’ækʃ(ə)n]n. 相互作用；[数] 交互作用n. 互动interaction: 相互作用 交互 交互作用705, interactive [ɪntər’æktɪv]adj. 交互式的；相互作用的706, Interactive [ɪntər’æktɪv]adj. 交互式的；相互作用的707, intercept [,ɪntə’sept]vt. 拦截；截断；窃听;n. 拦截；[数] 截距；截获的情报708, interference [ɪntə’fɪər(ə)ns]n. 干扰，冲突；干涉709, interleavingn. [计] 交错；交叉 v. 插入；[计] 交叉存取（interleave的ing形式）710, Intermediate [,ɪntə’miːdɪət]vi. 起媒介作用;adj. 中间的，中级的;n. [化学] 中间物；媒介711, internallyadv. 内部地；国内地；内在地712, Internationalization [‘ɪntə,næʃənəlaɪ’zeɪʃən]n. 国际化713, Interopn. 交互操作；Interop贸易展示会（每年在美国举办的全球最大的网络专业展览会）714, Interoperability [‘ɪntər,ɒpərə’bɪlətɪ]n. [计] 互操作性；互用性715, interoperableadj. 彼此协作的；能共同操作的；能共同使用的716, interpolate [ɪn’tɝpəlet]vt. 篡改；插入新语句vi. 插入；篡改717, interpolation [ɪn,tɚpə’leʃən]n. 插入；篡改；添写718, Interpretable [in’tə:prətəbl]adj. 可说明的；可判断的；可翻译的719, interrupt [ɪntə’rʌpt]vt. 中断；打断；插嘴；妨碍vi. 打断；打扰n. 中断720, interview [‘ɪntəvjuː]n. 接见，采访；面试，面谈;vt. 采访；接见；对…进行面谈；对某人进行面试721, intoxication [ɪn,tɒksɪ’keɪʃn]n. [内科] 中毒；陶醉；喝醉722, intrinsic [ɪn’trɪnsɪk]adj. 本质的，固有的723, introspection [ɪntrə(ʊ)’spekʃ(ə)n]n. 内省；反省724, introspective [ɪntrə’spektɪv]adj. 内省的；反省的725, intuitive [ɪn’tuɪtɪv]adj. 直觉的；凭直觉获知的726, intuitively [ɪn’tjʊɪtɪvli]adv. 直观地；直觉地727, invalidate [ɪn’vælɪdeɪt]vt. 使无效；使无价值728, invariant [ɪn’vɛrɪənt]adj. 不变的n. [数] 不变量；[计] 不变式729, invasive [ɪn’veɪsɪv]adj. 侵略性的；攻击性的730, inventory [‘ɪnv(ə)nt(ə)rɪ]n. 存货，存货清单；详细目录；财产清册731, Inventory [‘ɪnv(ə)nt(ə)rɪ]n. 存货，存货清单；详细目录；财产清册732, Inversion [ɪn’vɜːʃ(ə)n]n. 倒置；反向；倒转733, inverted [ɪn’vɜːtɪd]adj. 倒转的，反向的v. 颠倒（invert的过去分词）；使…反向734, investigate [ɪn’vestɪgeɪt]v. 调查；研究735, invitation [ˌɪnvɪˈteɪʃn]n. 邀请；引诱736, invite [ɪn’vaɪt]vt. 邀请，招待；招致;n. 邀请737, invocation [,ɪnvə(ʊ)’keɪʃ(ə)n]n. 祈祷；符咒；【法律】(法院对另案的)文件调取；(法权的)行使738, Invocation [,ɪnvə(ʊ)’keɪʃ(ə)n]n. 祈祷；符咒739, invoice [‘ɪnvɒɪs]n. 发票；货物；发货单 vt. 开发票；记清单740, irrespective [,ɪrɪ’spɛktɪv]adj. 无关的；不考虑的；不顾的741, iteration [ɪtə’reɪʃ(ə)n]n.[数] 迭代；反复；重复742, iteratively [‘itə,reitivli]adv. 迭代地；反复地743, iterator [ɪtə’retɚ]n. 迭代器；迭代程序744, Iterator [ɪtə’reɪtə]n. 迭代器；迭代程序745, jet [jɛt]n. 喷射，喷嘴；喷气式飞机；黑玉adj. 墨黑的vt. 射出vi. 射出；[航] 乘喷气式飞机746, journal [‘dʒɜːn(ə)l]n. 日报，杂志；日记；分类账747, Jumbotron [‘dʒʌmbəutrɔn]n. 电视机的超大屏幕748, Kestrel [‘kestr(ə)l]n. （产于欧洲的）茶隼749, kick [kɪk]n. 踢；反冲，后座力vt. 踢；反冲，朝后座vi. 踢；反冲n. (Kick)人名；(德)基克750, knobsn. 把手（knob的复数）；凸岩751, knuckle [‘nʌk(ə)l]n. 关节；指关节；指节；膝关节；肘vi. 开始认真工作vt. 用指关节敲打752, lack [læk]vt. 缺乏；不足；没有；需要;vi. 缺乏；不足；没有;n. 缺乏；不足;n. (Lack)人名；(老)拉；(英、法、意、葡、匈)拉克；(匈)洛克753, landscape [‘læn(d)skeɪp]n. 风景；风景画；景色；山水画；乡村风景画；地形；（文件的）横向打印格式;vt. 对…做景观美化，给…做园林美化；从事庭园设计;vi. 美化（环境等），使景色宜人；从事景观美化工作，做庭园设计师754, lane [leɪn]n. 小巷；[航][水运] 航线；车道；罚球区n. (Lane)人名；(英、俄)莱恩；(老)兰；(德、法、意、葡、塞、瑞典)拉内755, Lantern [‘læntən]n. 灯笼；提灯；灯笼式天窗756, Laptops []笔记本电脑757, lark [lɑːk]n. 云雀；百灵鸟；欢乐vi. 骑马玩乐；嬉耍vt. 愚弄n. (Lark)人名；(东南亚国家华语)六758, latch [lætʃ]vi. 占有，抓住；闭锁vt. 闩上；纠缠住某人n. 门闩n. (Latch)人名；(英)拉奇759, Latch [lætʃ]vi. 占有，抓住；闭锁;vt. 闩上；纠缠住某人;n. 门闩;n. (Latch)人名；(英)拉奇760, latency [‘letnsi]n. 潜伏；潜在因素761, latent [‘letnt]adj. 潜在的；潜伏的；隐藏的762, launching [‘lɔːntʃɪŋ]n. 发射；下水；创设;v. 发射；发动；开始从事（launch的ing形式）763, lean [lin]vi. 倾斜；倚靠；倾向；依赖adj. 瘦的；贫乏的，歉收的vt. 使倾斜n. 瘦肉；倾斜；倾斜度n. (Lean)人名；(西)莱安；(柬)连；(英)利恩[ :过去式leaned或 leant 过去分词:leaned或 leant 现在分词:leaning ]764, leap [lip]vi. 跳，跳跃n. 飞跃；跳跃vt. 跳跃，跳过；使跃过n. (Leap)人名；(法)莱亚765, ledger [‘ledʒə]n.总账，分户总账；[会计] 分类账；账簿；底账；（手脚架上的）横木n.(Ledger)人名；(英)莱杰766, legacy [‘legəsɪ]n. 遗赠，遗产767, Legacy [‘legəsɪ]n. 遗赠，遗产768, lenient [‘liːnɪənt]adj. 宽大的；仁慈的 n. (Lenient)人名；(法)勒尼安769, leverage [‘lɛvərɪdʒ]n. 手段，影响力；杠杆作用；杠杆效率v. 利用；举债经营770, Lexical [‘leksɪk(ə)l]adj. 词汇的；[语] 词典的；词典编纂的771, lexicographically 772, line up排列起；整队773, linguistic [lɪŋ’gwɪstɪk]adj. 语言的；语言学的774, literal [‘lɪt(ə)r(ə)l]adj. 文字的；逐字的；无夸张的775, loan [ləʊn]n. 贷款；借款vi. 借出vt. 借；借给776, lobster [‘lɒbstə]n. 龙虾龙虾肉777, lockup [‘lɒkʌp]n. 拘留所，监狱；锁住；监禁778, looks up 779, lookup [‘lʊkʌp]n. 查找；检查780, loop [luːp]vi. 打环；翻筋斗;n. 环；圈；弯曲部分；翻筋斗;vt. 使成环；以环连结；使翻筋斗781, Loop [luːp]vi. 打环；翻筋斗;n. 环；圈；弯曲部分；翻筋斗;vt. 使成环；以环连结；使翻筋斗782, loopback [loopback]n. [计] 回送；回路783, luggage [‘lʌgɪdʒ]n. 行李；皮箱784, lumberjack [‘lʌmbədʒæk]n. 伐木工人；木材商的佣工；短茄克衫785, macro [‘mækro]adj. 巨大的，大量的n. 宏，巨（计算机术语）n. (Macro)人名；(意)马克罗786, malformed [,mæl’fɔrmd]adj. 畸形的，难看的787, malicious [mə’lɪʃəs]adj. 恶意的；恶毒的；蓄意的；怀恨的788, malwaren. 恶意软件789, mandatory [‘mændət(ə)rɪ]adj. 强制的；托管的；命令的 n. 受托者（等于mandatary）790, manifest [‘mænɪfest]vt. 证明，表明；显示vi. 显示，出现n. 载货单，货单；旅客名单adj. 显然的，明显的；明白的791, manipulate [mə’nɪpjʊleɪt]vt. 操纵；操作；巧妙地处理；篡改792, Manipulation [mə,nɪpjʊ’leɪʃ(ə)n]n. 操纵；操作；处理；篡改793, manner [‘mænə]n. 方式；习惯；种类；规矩；风俗;n. (Manner)人名；(德、芬、瑞典)曼纳794, manufacture [mænjʊ’fæktʃə]n. 制造；产品；制造业vt. 制造；加工；捏造vi. 制造795, margin [‘mɑːdʒɪn]n. 边缘；利润，余裕；页边的空白;vt. 加边于；加旁注于;n. (Margin)人名；(俄、意)马尔金796, marginally [‘mɑrdʒɪnəli]adv. 少量地；最低限度地；在栏外；在页边797, Markup [‘mɑːkʌp]n. 涨价；利润；审定;n. (Markup)人名；(捷、匈)马尔库普798, marshalln. 元帅；典礼官；执法官；法官的随行官员（等于judge’s marshal）vt. 安排；引领；统帅vi. 各就各位；按次序排列成形（等于marshal）799, marshalled [marshalled]整理（marshal的过去式与过去分词形式）使排列（marshal的过去式与过去分词形式）800, maskableadj. 可屏蔽的801, master [‘mɑːstə]vt. 控制；精通；征服;n. 硕士；主人；大师；教师;adj. 主人的；主要的；熟练的;n. (Master)人名；(英)马斯特802, mate [meɪt]n. 助手，大副；配偶；同事；配对物vt. 使配对；使一致；结伴vi. 交配；成配偶；紧密配合n. (Mate)人名；(日)蛏(姓)；(西、意、塞)马特；(波黑)马特；(罗、俄)马泰803, material [mə’tɪrɪəl]adj. 重要的；物质的，实质性的；肉体的n. 材料，原料；物资；布料804, matrix [‘metrɪks]n. [数] 矩阵；模型；[生物][地质] 基质；母体；子宫；[地质] 脉石805, Matrix [‘meɪtrɪks]n. [数] 矩阵；模型；[生物][地质] 基质；母体；子宫；[地质] 脉石806, mean [min]adj. 平均的；卑鄙的；低劣的；吝啬的vt. 意味；想要；意欲n. 平均值vi. 用意n. (Mean)人名；(柬)棉807, Measurable [‘meʒ(ə)rəb(ə)l]adj. 可测量的；重要的；重大的808, mechanism [‘mek(ə)nɪz(ə)m]n. 机制；原理，途径；进程；机械装置；技巧809, mechanisms [‘mɛkənɪzəmz]n. 机制；[机] 机构（mechanism的复数）；机械；[机] 机构学810, median [‘midɪən]n. n. 中值，中位数；三角形中线；梯形中位线adj. 中值的；中央的n. (Median)人名；(阿拉伯)迈迪安；(罗)梅迪安811, mediocre [,miːdɪ’əʊkə]adj. 普通的；平凡的；中等的812, mediocrity [miːdɪ’ɒkrɪtɪ]n. 平庸之才；平常813, mega [‘mɛɡə]n. 百万adj. 许多；宏大的adv. 非常n. (Mega)人名；(葡、意、捷、塞)梅加814, megabyte [‘megəbaɪt]n. [计] 兆字节815, melon [‘melən]n. 瓜；甜瓜；大肚子；圆鼓鼓像瓜似的东西 n. (Melon)人名；(意、西、葡)梅隆816, memorization [,mɛmərɪ’zeʃən]n. 记住；暗记817, mentor [‘mɛn’tɔr]n. 指导者，良师益友;vt. 指导818, merchant [‘mɝtʃənt]n. 商人，批发商；店主adj. 商业的，商人的n. (Merchant)人名；(英)麦钱特819, merry [‘mɛri]adj. 愉快的；微醉的；嬉戏作乐的 n. 甜樱桃 n. (Merry)人名；(英、法、西)梅里820, mesh [mɛʃ]n. 网眼；网丝；圈套vi. 相啮合vt. [机] 啮合；以网捕捉821, mess [mes]n.混乱；食堂，伙食团；困境；脏乱的东西vt.弄乱，弄脏；毁坏；使就餐vi.把事情弄糟；制造脏乱；玩弄n.(Mess)人名；(德、罗)梅斯822, messy [‘mesɪ]adj. 凌乱的，散乱的；肮脏的，污秽的；麻烦的823, metric [‘mɛtrɪk]adj. 公制的；米制的；公尺的n. 度量标准824, metrics [‘mɛtrɪks]n. 度量；作诗法；韵律学825, Metrics [‘metrɪks]n. 度量；作诗法；韵律学826, migrate [maɪ’greɪt; ‘maɪgreɪt]vi. 移动；随季节而移居；移往vt. 使移居；使移植827, Migrate [maɪ’greɪt; ‘maɪgreɪt]vi. 移动；随季节而移居；移往;vt. 使移居；使移植828, migration [maɪ’greɪʃ(ə)n]n. 迁移；移民；移动829, milestonesn. 里程碑；时间表830, mimic [‘mɪmɪk]vt. 模仿，摹拟n. 效颦者，模仿者；仿制品；小丑adj. 模仿的，模拟的；假装的831, mind [maɪnd]n. 理智，精神；意见；智力；记忆力vt. 介意；专心于；照料vi. 介意；注意832, minify [‘mɪnɪfaɪ]vt. 使变小；贬低833, minor [‘maɪnə]adj. 未成年的；次要的；较小的；小调的；二流的;n. 未成年人；小调；副修科目;vi. 副修;n. (Minor)人名；(英)迈纳；(德、法、波、俄)米诺尔834, Misc [‘mɪsk]abbr. 混杂的；各色各样混在一起；多才多艺的（miscellaneous）835, miscellaneous [‘mɪsə’lenɪəs]adj. 混杂的，各种各样的；多方面的，多才多艺的836, misconception [mɪskən’sepʃ(ə)n]n. 误解；错觉；错误想法837, misfire [‘mɪs’faɪr]vi. 失败；不发火n. 失败；不发火838, mitigated [‘mɪtɪgeɪt]vt. 使缓和，使减轻 vi. 减轻，缓和下来839, mitigation [mɪtɪ’geɪʃ(ə)n]n. 减轻；缓和；平静840, mix [mɪks]vt. 配制；混淆；使混和；使结交vi. 参与；相混合；交往n. 混合；混合物；混乱n. (Mix)人名；(德、英)米克斯841, mock [mɒk]n. 英国模拟考试（mocks）vt. 愚弄，嘲弄adj. 仿制的，模拟的，虚假的，不诚实的vt. 不尊重，蔑视842, Modal [‘məʊd(ə)l]adj. 模式的；情态的；形式的;n. 莫代尔（一种新型纤维素纤维）843, Mon amour []我的爱844, monial [‘məuniəl]n.竖框（等于mullion）845, monolith [‘mɒn(ə)lɪθ]n. 整块石料；庞然大物846, monolithic [mɒnə’lɪθɪk]adj. 整体的；巨石的，庞大的；完全统一的 n. 单块集成电路，单片电路847, monopolize [mə’nɑpə’laɪz]vt. 垄断；独占；拥有…的专卖权848, monotonically [mɒnə’tɒnɪklɪ]adv. 单调地849, monotonicity [,mɔnətə’nisəti]n. [数] 单调性850, mood [muːd]n. 情绪，语气；心境；气氛;n. (Mood)人名；(英)穆德；(瑞典)莫德851, Moreover [mɔːr’əʊvə]adv. 而且；此外852, mount [maʊnt]vt. 增加；爬上；使骑上马；安装，架置；镶嵌，嵌入；准备上演；成立（军队等）;vi. 爬；增加；上升;n. 山峰；底座；乘骑用马；攀，登；运载工具；底座;n. （英）芒特（人名）;v. 登上；骑上853, mprotect 854, mule [mjuːl]n. 骡；倔强之人，顽固的人；杂交种动物;n. (Mule)人名；(意)穆莱；(英)米尔855, multiplexer [‘mʌltɪ,plɛksɚ]n. [计][通信] 多工器；多路器；[通信] 多路转接器856, multiplicity [,mʌltɪ’plɪsəti]n. 多样性；[物] 多重性857, mutable [‘mjutəbl]adj. 易变的，不定的；性情不定的858, mutate [‘mjutet]vi. 变化，产生突变vt. 改变，使突变859, mutative [‘mjuːtətɪv]adj. 变化的；生物突变的860, mutator [‘mju:teitə]n. 增变基因861, mute [mjuːt]adj. 哑的；沉默的；无声的;vt. 减弱……的声音；使……柔和;n. 哑巴；弱音器；闭锁音;n. (Mute)人名；(塞)穆特862, mutex []n. 互斥；互斥元，互斥体；互斥量863, mutual [‘mjutʃuəl]adj. 共同的；相互的，彼此的864, mutually [ˈmjuːtʃuəli]adv. 互相地；互助865, negate [nɪ’geɪt]vt. 否定；取消；使无效;vi. 否定；否认；无效;n. 对立面；反面866, negligible [‘neglɪdʒɪb(ə)l]adj. 微不足道的，可以忽略的867, Negotiate [nɪ’gəʊʃɪeɪt]vt. 谈判，商议；转让；越过;vi. 谈判，交涉868, nest [nest]n. 巢，窝；安乐窝；温床;vt. 筑巢；嵌套;vi. 筑巢；找鸟巢;n. (Nest)人名；(德)内丝特；(英)内丝特(女子教名 Agnes 威尔士语的昵称)869, newbie [‘njuːbɪ]n. 网络新手；新兵870, nick [nɪk]vt. 刻痕于；挑毛病；用刻痕记 n. 刻痕；缺口 vi. 刻痕；狙击871, nimbus [‘nɪmbəs]n. （人和物产生的）灵气；光轮；[气象] 雨云；光辉灿烂的气氛872, noisy [ˈnɔɪzɪ]adj. 嘈杂的；喧闹的；聒噪的n. 响声；嘈杂声873, nonce [nɒns]n. 目前；特定场合；强奸犯 adj. 临时的；特定场合的874, nope [nəʊp]adv. 不是，没有；不875, norm [nɔrm]n. 标准，规范876, notably [‘notəbli]adv. 显著地；尤其877, notation [no’teʃən]n. 符号；乐谱；注释；记号法878, Notation [nəʊ’teɪʃ(ə)n]n. 符号；乐谱；注释；记号法879, noticeable [‘nəʊtɪsəb(ə)l]adj. 显而易见的，显著的；值得注意的880, notion [‘nəʊʃ(ə)n]n. 概念；见解；打算881, notoriously [noˈtɔːriəsli]adv. 众所周知地；声名狼藉地；恶名昭彰地882, noverflow 883, obey [ə’be]vt. 服从，听从；按照……行动vi. 服从，顺从；听话n. (Obey)人名；(英、法)奥贝884, obligation [,ɑblɪ’ɡeʃən]n. 义务；职责；债务885, observer [əb’zɜːvə]n. 观察者；[天] 观测者；遵守者886, obsession [əb’seʃ(ə)n]n. 痴迷；困扰；[内科][心理] 强迫观念887, obtain [əb’teɪn]vi. 获得；流行;vt. 获得888, obtuse [əb’tjuːs]adj. 迟钝的；圆头的；不锋利的889, occupied [‘ɒkjʊpaɪd]adj. 已占用的；使用中的；无空闲的v. 占有（occupy的过去分词）890, occurrence [ə’kʌr(ə)ns]n. 发生；出现；事件；发现891, odevity [odevity]奇偶性892, offloading []n. 卸载；卸货;vt. 卸载（offload的现在分词形式）893, omit [ə(ʊ)’mɪt]vt. 省略；遗漏；删除；疏忽894, Onboardingadv. 在船上；在飞机上；在板上adj. 随车携带的895, onset [‘ɑnsɛt]n. 开始，着手；发作；攻击，进攻896, onwards [‘ɒnwədz]adv. 向前；在前面897, opaque [o’pek]adj. 不透明的；不传热的；迟钝的n. 不透明物vt. 使不透明；使不反光898, opinion [ə’pɪnjən]n. 意见；主张899, opinionated [ə’pɪnjənetɪd]adj. 固执己见的；武断的900, optimized [‘ɒptɪmaɪzd]adj. 最佳化的；尽量充分利用901, optimizer [‘ɒptɪmaɪzə]n. [计] 优化程序；最优控制器902, orchestrating精心策划 给 … 配管弦乐曲 使协调地结合在一起903, orchestration [,ɔkɛs’treʃən]n. 管弦乐编曲；和谐的结合904, orchestratorn. 管弦乐演奏家；管弦乐编曲家905, ordinarily [‘ɔːd(ə),n(ə)rɪlɪ; ,ɔːdɪ’nerɪlɪ]adv. 通常地；一般地906, Oriented [‘ɔːrɪentɪd]adj. 导向的；定向的；以…为方向的;v. 调整；使朝向（orient的过去分词）；确定…的方位907, orphans [‘ɔrfən]n. 孤儿（orphan的复数）v. 成为孤儿（orphan的三单形式）908, outage [‘aʊtɪdʒ]n. 储运损耗；中断供应；运行中断909, outgoing [‘aʊtgəʊɪŋ]adj. 对人友好的，开朗的；出发的，外出的；即将离职的；乐于助人的;n. 外出；流出；开支;v. 超过；优于（outgo的ing形式）910, outnumber [aʊt’nʌmbə]vt. 数目超过；比…多911, outright [‘aʊtraɪt]adv. 全部地；立刻地；率直地；一直向前；痛快地adj. 完全的，彻底的；直率的；总共的912, outstanding [aʊt’stændɪŋ]adj. 杰出的；显著的；未解决的；未偿付的;n. 未偿贷款913, overkill [‘əʊvəkɪl]n. 过犹不及，过分行为；超量毁伤 v. 过度地杀伤914, overlap [əʊvə’læp]n. 重叠；重复 vi. 部分重叠；部分的同时发生 vt. 与…重叠；与…同时发生915, overlay [‘ovəlɛi]n. 覆盖图；覆盖物vt. 在表面上铺一薄层，镀916, overpopulate [əʊvə’pɒpjʊleɪt]vt. 使人口过剩；使人口过密917, overwhelm [ovɚˈhwɛlm]vt. 淹没；压倒；受打击；覆盖；压垮918, pacesetter [‘pessɛtɚ]n. 标兵；先导者，步调调整者919, pad [pæd]n. 衬垫；护具；便笺簿；填补vi. 步行；放轻脚步走vt. 填补；走n. (Pad)人名；(英)帕德(男子教名 Patrick 的昵称)920, padding [‘pædɪŋ]n. 填料；垫料;v. 填补（pad的ing形式）921, paddle [‘pædl]n. 划桨；明轮翼vt. 拌；搅；用桨划vi. 划桨；戏水；涉水922, padlock [‘pædlɒk]n. 挂锁；关闭；禁止进入;vt. 用挂锁锁上；关闭923, Page View访问量924, Pagination [,pædʒɪ’neɪʃ(ə)n]n. 标记页数；页码925, painful [‘penfl]adj. 痛苦的；疼痛的；令人不快的926, palette [‘pælət]n. 调色板；颜料n. (Palette)人名；(法)帕莱特927, pampas [‘pæmpəs]n. 草原；蒲苇（pampa的复数形式）n. (Pampas)人名；(瑞典)潘帕斯928, pan [美/pæn/]n. 平底锅；盘状的器皿；淘盘子，金盘，秤盘;vt. 淘金；在浅锅中烹调（食物）；[非正式用语]严厉的批评;vi. 淘金；在淘洗中收获金子;929, pane [peɪn]n. 窗格；边；面；窗格玻璃；嵌板;vt. 装窗玻璃于；镶嵌板于;n. (Pane)人名；(老)班；(英)潘恩；(德、西、意、塞、印尼)帕内930, panic [‘pænɪk]n. 恐慌，惊慌；大恐慌;adj. 恐慌的；没有理由的;vt. 使恐慌;vi. 十分惊慌;n. (Panic)人名；(罗)帕尼克931, paradise [‘pærədaɪs]n. 天堂至福境地932, Paradise [‘pærədaɪs]n. 天堂;至福境地933, paramount [‘pærəmaʊnt]adj. 最重要的，主要的；至高无上的 n. 最高统治者934, parasitic [,pærə’sɪtɪk]adj. 寄生的（等于parasitical）935, parentheses [pə’rɛnθəsɪz]n. parenthesis的复数形式936, parenthesis [pə’rɛnθəsɪs]n. 插入语,插入成分n. 圆括号n. 间歇，插曲937, parlance [‘pɑːl(ə)ns]n. 说法；用语；语调；发言938, participant [pɑː’tɪsɪp(ə)nt]n. 参与者；关系者adj. 参与的；有关系的939, passenger [‘pæsɪndʒə]n. 旅客；乘客；过路人；碍手碍脚的人940, passive [‘pæsɪv]adj. 被动的，消极的；被动语态的 n. 被动语态941, passphrase [passphrase]n. 密码；通行码；口令短语942, peculiar [pɪ’kjulɪɚ]adj. 特殊的；独特的；奇怪的；罕见的n. 特权；特有财产943, peek [pik]n. 偷看；一瞥，看一眼vi. 窥视，偷看n. (Peek)人名；(英)皮克944, peer [pɪə]n. 贵族；同等的人；同龄人 vi. 凝视，盯着看；窥视 vt. 封为贵族；与…同等 n. (Peer)人名；(英、巴基)皮尔945, pending [‘pendɪŋ]adj. 未决定的；行将发生的;prep. 在…期间；直到…时为止；在等待…之际;v. 待定；悬而不决（pend的ing形式）;n. (Pending)人名；(瑞典)彭丁946, Pending [] 947, percentile [pɚ’sɛntaɪl]adj. 百分率的；按百等分排列的n. 百分位948, percolate [‘pɜːkəleɪt]vi. 过滤；渗出；浸透 vt. 使渗出；使过滤 n. 滤过液；渗出液949, percolator [‘pɜːkəleɪtə]n. 过滤器；渗滤器；咖啡渗滤壶950, perform [pə’fɔːm]vt. 执行；完成；演奏vi. 执行，机器运转；表演perform: 执行 表演 履行951, Perform [pə’fɔːm]vt. 执行；完成；演奏;vi. 执行，机器运转；表演952, performance [pə’fɔːm(ə)ns]n. 性能；绩效；表演；执行953, performing [pə’fɔːmɪŋ]v. 表演（perform的ing形式）；履行adj. 表演的；履行的performing: 执行 表演性 表现954, period [‘pɪərɪəd]n. 周期，期间；时期；月经；课时；（语法学）句点，句号 adj. 某一时代的955, periodic [,pɪərɪ’ɒdɪk]adj. 周期的；定期的956, periodically [pɪrɪˈɑdɪklɪ]adv. 定期地；周期性地；偶尔；间歇957, peripheral [pə’rɪfərəl]adj. 外围的；次要的；（神经）末梢区域的n. 外部设备958, permanent [‘pɜːm(ə)nənt]adj.永久的，永恒的；不变的n.烫发（等于permanent wave）959, permission [pə’mɪʃ(ə)n]n. 允许，许可960, Permission [pə’mɪʃ(ə)n]n. 允许，许可961, permutation [‘pɝmjʊ’teʃən]n. [数] 排列；[数] 置换962, perpetual [pɚ’pɛtʃuəl]adj. 永久的；不断的；四季开花的；无期限的963, personality [pɜːsə’nælɪtɪ]n. 个性；品格；名人964, perspective [pə’spektɪv]n. 观点；远景；透视图;adj. 透视的965, pertain [pə’teɪn]vi. 属于；关于；适合966, phantom [‘fæntəm]n. 幽灵；幻影；虚位 adj. 幽灵的；幻觉的；有名无实的967, Phantom [‘fæntəm]n. 幽灵；幻影；虚位;adj. 幽灵的；幻觉的；有名无实的968, phase [feɪz]n. 相；阶段；[天] 位相;vt. 使定相；逐步执行;vi. 逐步前进969, phases [‘feɪsiːz]n. 阶段，时期（phase的复数形式）;v. 逐步实行（phase的三单形式）970, philosophies [fɪ’lɒsəfɪ]n. 哲学；哲理；人生观971, phrase [frez]n. 短语, 习语, 措辞, 乐句vt. 措词, 将(乐曲)分成乐句972, PHRASE [freɪz]n. 短语, 习语, 措辞, 乐句;vt. 措词, 将(乐曲)分成乐句973, pier [pɪr]n. 码头，直码头；桥墩；窗间壁n. (Pier)人名；(英、德)皮尔；(西)彼尔；(意)皮耶尔974, piggyback [‘pɪɡɪbæk]adj. 背着的；[航] 在背肩上的adv. 在背肩上n. 肩扛；背负式运输vt. 背负式装运975, pilot [kə’lekt]vt. 收集；募捐vi. 收集；聚集；募捐adv. 由收件人付款地adj. 由收件人付款的n. (Collect)人名；(英)科莱克特976, pin [pɪn]n. 大头针，别针，针；栓；琐碎物vt. 钉住；压住；将……用针别住977, pinned [‘pind]adj. 用针别住的；被压制的v. 牵制；用针别住；刺穿（pin的过去分词）978, pipeline [‘paɪplaɪn]n. 管道；输油管；传递途径过去式:pipelined; 过去分词:pipelined; 现在分词:pipelining979, placement [‘pleɪsmənt]n. 布置；定位球；人员配置980, plain [plen]adj. 平的；简单的；朴素的；清晰的n. 平原；无格式；朴实无华的东西adv. 清楚地；平易地n. (Plain)人名；(英)普莱恩；(法)普兰981, planet [‘plænɪt]n. 行星n. (Planet)人名；(法)普拉内；(西、葡)普拉内特982, Plastic [‘plæstɪk]adj. 塑料的；（外科）造型的；可塑的n. 塑料制品；整形；可塑体983, plenty [‘plentɪ]n. 丰富，大量；充足;adj. 足够的，很多的;adv. 足够;n. (Plenty)人名；(英)普伦蒂984, plot [plɑt]n. 情节；图；阴谋vt. 密谋；绘图；划分；标绘vi. 密谋；策划；绘制n. (Plot)人名；(捷)普洛特；(法)普洛985, plotted [plotted]adj. 标绘的v. 策划（plot的过去分词）；划分；绘制…的地图986, plug [plʌg]n. 插头；塞子；栓;vi. 塞住；用插头将与电源接通;vt. 插入；塞住；接插头987, plug in []插入；插上电源988, pluggable [plʌgəbl]adj. 插接式连接，[电] 可插的989, Pluggable [plʌgəbl]adj. 插接式连接，[电] 可插的990, Plugin [plʌgɪn]n. 插件；相关插件;n. (Plugin)人名；(俄)普卢金991, plural [‘plʊrəl]adj. 复数的n. 复数992, poet [‘pəʊɪt]n. 诗人993, poetry [‘pəʊɪtrɪ]n. 诗；诗意，诗情；诗歌艺术994, poison [‘pɔɪzn]vt. 污染；使中毒，放毒于；败坏；阻碍vi. 放毒，下毒n. 毒药，毒物；酒；有毒害的事物；[助剂] 抑制剂adj. 有毒的n. (Poison)人名；(西)波伊松995, polar [‘pəʊlə]adj. 极地的；两极的；正好相反的 n. 极面；极线 n. (Polar)人名；(德、西)波拉尔996, pollution [pə’luʃən]n. 污染污染物997, polymorphism [,pɑlɪ’mɔrfɪzm]n. 多态性；多形性；同质多晶998, popover [‘pɒpəʊvə]n. 一种淡烤的酥饼999, populate [‘pɒpjʊleɪt]vt. 居住于；构成人口；移民于；殖民于1000, populated [ˈpɑːpjuleɪtid]adj. 粒子数增加的;v. 居住于…中；构成…的人口（populate的过去分词）1001, portable [‘pɔrtəbl]adj. 手提的，便携式的；轻便的n. 手提式打字机1002, portal [‘pɔːt(ə)l]n. 大门，入口;n. (Portal)人名；(法、西、葡)波塔尔；(英)波特尔1003, portion [‘pɔːʃ(ə)n]n. 部分；一份；命运 vt. 分配；给…嫁妆1004, portrait [‘pɔrtrɪt]n. 肖像；描写；半身雕塑像n. (Portrait)人名；(法)波特雷1005, postpone [pəʊs(t)’pəʊn; pə’spəʊn]vt. 使…延期；把…放在次要地位；把…放在后面;vi. 延缓，延迟；延缓发作1006, pot [美/pɑt/]n. 壶；盆；罐;vt. 把…装罐；射击；节略;vi. 随手射击;n. (Pot)人名；(柬)布特；(捷)波特；(法)波;1007, potentially [pə’tɛnʃəli]adv.可能地，潜在地1008, practical [‘præktɪk(ə)l]adj. 实际的；实用性的1009, pragmatic [præg’mætɪk]adj. 实际的；实用主义的；国事的1010, prank [præŋk]n. 恶作剧，开玩笑；戏谑;vt. 装饰；打扮;vi. 炫耀自己；胡闹1011, precaution [prɪ’kɔːʃ(ə)n]n. 预防，警惕；预防措施;vt. 警惕；预先警告1012, precede [prɪ’siːd]vt. 领先，在…之前；优于，高于;vi. 领先，在前面1013, precedence [‘presɪd(ə)ns; prɪ’siːd(ə)ns]n. 优先；居先1014, precision [prɪ’sɪʒn]n. 精度，[数] 精密度；精确adj. 精密的，精确的1015, preclude [prɪ’kluːd]vt. 排除；妨碍；阻止1016, predecessor [‘priːdɪsesə]n. 前任，前辈1017, predictability [pri,diktə’biliti]n. 可预测性；可预言1018, preempt [,primpt]vt. 先占；先取；以先买权获得1019, preference [‘pref(ə)r(ə)ns]n. 偏爱，倾向；优先权1020, prefetch [prefetch]v. 预取1021, preliminary [prɪ’lɪmɪn(ə)rɪ]n. 准备；预赛；初步措施adj. 初步的；开始的；预备的1022, premiered []（戏剧、电影等）首次公演（premiere的过去式和过去分词）1023, premises [‘premɪsɪz]n.前提；经营场址；上述房屋；契约前言（premise的复数） v.提出…为前提；预述（premise的第三人称单数）1024, premium [‘priːmɪəm]n. 额外费用；奖金；保险费;(商)溢价;adj. 高价的；优质的1025, preppy [‘prɛpi]n. 预科生（等于preppie）1026, prerequisite [priː’rekwɪzɪt]n. 先决条件;adj. 首要必备的1027, present [‘prez(ə)nt]vt. 提出；介绍；呈现；赠送;vi. 举枪瞄准;adj. 现在的；出席的;n. 现在；礼物；瞄准1028, presentation [prez(ə)n’teɪʃ(ə)n]n. 展示；描述，陈述；介绍；赠送1029, Presentation [prez(ə)n’teɪʃ(ə)n]n. 描述，陈述；介绍；赠送1030, presenter [prɪ’zɛntɚ]n. 提出者；推荐者；赠送者；任命者；主持人1031, prevent [prɪ’vent]vt. 预防，防止；阻止;vi. 妨碍，阻止1032, primary [‘praɪmɛri]adj. 主要的；初级的；基本的n. 原色；最主要者1033, Primordial [praɪ’mɔːdɪəl]adj. 原始的；根本的；原生的1034, principal [‘prɪnsəp(ə)l]adj. 主要的；资本的;n. 首长；校长；资本；当事人1035, Principal [‘prɪnsəp(ə)l]adj. 主要的；资本的;n. 首长；校长；资本；当事人1036, principle [‘prɪnsɪp(ə)l]n. 原理，原则；主义，道义；本质，本义；根源，源泉1037, Principle [‘prɪnsɪp(ə)l]n. 原理，原则；主义，道义；本质，本义；根源，源泉1038, principle engineer【网络】首席工程师1039, priority [praɪ’ɔrəti]n. 优先；优先权；[数] 优先次序；优先考虑的事1040, Priority [praɪ’ɒrɪtɪ]n. 优先；优先权；[数] 优先次序；优先考虑的事1041, proactive [prəʊ’æktɪv]adj. 前摄的（前一活动中的因素对后一活动造成影响的）；有前瞻性的，先行一步的；积极主动的1042, probe [prəʊb]n. 探针；调查vi. 调查；探测vt. 探查；用探针探测n. (Probe)人名；(法)普罗布1043, proceed [pro’sid]vi. 开始；继续进行；发生；行进n. 收入，获利1044, Processing [‘prɑsɛs]v. 加工；[自] 处理；对…起诉（process的ing形式）1045, Processors [prɔ:’sesəz]n. [计] 处理器，[计] 处理机（processor复数）1046, profile [‘profaɪl]n. 侧面；轮廓；外形；剖面；简况vt. 描…的轮廓；扼要描述vi. 给出轮廓1047, profiled []v.描绘…轮廓，评论人物（profile的过去式）1048, progressively [prə’ɡrɛsɪvli]adv. 渐进地；日益增多地1049, projection [prə’dʒɛkʃən]n. 投射；规划；突出；发射；推测1050, promote [prə’məʊt]vt. 促进；提升；推销；发扬vi. 成为王后或其他大于卒的子1051, pronounce [prə’naʊns]vt. 发音；宣判；断言;vi. 发音；作出判断1052, propagation [,prɒpə’ɡeɪʃən]n. 传播；繁殖；增殖1053, PROPAGATION [,prɒpə’ɡeɪʃən]n. 传播；繁殖；增殖1054, proper [‘prɑpɚ]adj. 适当的；本身的；特有的；正派的adv. 完全地n. (Proper)人名；(英、德)普罗珀1055, proportional [prə’pɔːʃ(ə)n(ə)l]adj. 比例的，成比例的；相称的，均衡的;n. [数] 比例项1056, proposal [prə’pəʊz(ə)l]n. 提议，建议；求婚1057, prospector [prəʊ’spektə]n. 探勘者；采矿者1058, protocol [‘prəʊtəkɒl]n. 协议；草案；礼仪;vt. 拟定;vi. 拟定1059, Protocol [‘prəʊtəkɒl]n. 协议；草案；礼仪;vt. 拟定;vi. 拟定1060, provision [prə’vɪʒ(ə)n]n. 规定；条款；准备；[经] 供应品vt. 供给…食物及必需品1061, proximallyadv. 最近地1062, proxy [‘prɒksɪ]n. 代理人；委托书；代用品1063, prune [pruːn]vi. 删除；减少vt. 修剪；删除；剪去n. 深紫红色；傻瓜；李子干n. (Prune)人名；(罗)普鲁内1064, pseudo [‘sjuːdəʊ]n. 伪君子；假冒的人;adj. 冒充的，假的1065, pump [pʌmp]vt. 打气；用抽水机抽… n. 泵，抽水机；打气筒 vi. 抽水 n. (Pump)人名；(捷)蓬普；(英)庞普1066, punch above [punch+above]超越1067, punctuation [‘pʌŋktʃʊ’eʃən]n. 标点；标点符号1068, puppet [‘pʌpɪt]n. 木偶；傀儡；受他人操纵的人1069, Purchase [‘pɜ:tʃəs]n. 购买；紧握；起重装置;vt. 购买；赢得;vi. 购买东西1070, purge [pɝdʒ]vi. 净化；通便vt. 净化；清洗；通便n. 净化；泻药1071, puzzle [‘pʌzl]vt. 使…困惑；使…为难；苦思而得出vi. 迷惑；冥思苦想n. 谜；难题；迷惑1072, quack [kwæk]n. 庸医；鸭叫声vi. （鸭子）嘎嘎叫；吹嘘；大声闲聊adj. 骗人的；冒牌医生的n. (Quack)人名；(德)夸克1073, quadword四倍长字1074, qualifier [‘kwɒlɪfaɪə(r)]n. [语] 修饰语n. 资格赛，预选赛；取得资格的人1075, quantilen. [计] 分位数；分位点1076, Quantity [‘kwɒntɪtɪ]n. 量，数量；大量；总量1077, quiescent [kwɪ’ɛsnt]adj. 静止的；不活动的；沉寂的1078, quorum [‘kwɔrəm]n. 法定人数1079, rack [ræk]n. [机] 齿条；架子；拷问台 vi. 变形；随风飘；小步跑 vt. 折磨；榨取 n. (Rack)人名；(法、德、意、匈)拉克1080, radically [‘rædɪkli]adv. 根本上；彻底地；以激进的方式1081, radius [‘reɪdɪəs]n. 半径，半径范围；[解剖] 桡骨；辐射光线；有效航程;n. (Radius)人名；(意、印尼)拉迪乌斯；(英)雷迪厄斯；(法)拉迪于斯1082, raft [rɑːft]n. 筏；救生艇；（美）大量vt. 筏运；制成筏vi. 乘筏n. (Raft)人名；(匈、瑞典)拉夫特1083, raid [reɪd]n. 袭击；突袭；搜捕；抢劫vi. 突袭vt. 袭击，突袭n. (Raid)人名；(阿拉伯)拉伊德1084, rake [rek]vi. 搜索；用耙子耙；掠过，擦过vt. 倾斜；搜索；掠过；用耙子耙n. 耙子；斜度；钱耙；放荡的人，浪子n. (Rake)人名；(英)雷克；(塞)拉凯1085, rally [‘rælɪ]vi. 团结；重整；恢复；（网球等）连续对打 vt. 团结；集合；恢复健康、力量等 n. 集会；回复；公路赛车会 n. (Rally)人名；(罗)拉利1086, rancher [‘ræntʃɚ]n. 大农场经营者；大农场工人n. (Rancher)人名；(意)兰凯尔1087, Range [reɪn(d)ʒ]n. 范围；幅度；排；山脉;vi. 平行，列为一行；延伸；漫游；射程达到;vt. 漫游；放牧；使并列；归类于；来回走动;n. (Range)人名；(葡)兰热；(瑞典、德)朗格1088, rare [reə]adj. 稀有的；稀薄的；半熟的adj. 杰出的；极度的；非常好的adv. 非常；极其vi. 用后腿站起；渴望1089, rather [‘rɑːðə]adv. 宁可，宁愿；相当;int. 当然啦（回答问题时用）;n. (Rather)人名；(英)拉瑟1090, raw [rɔː]adj. 生的；未加工的；阴冷的；刺痛的；擦掉皮的；无经验的；（在艺术等方面）不成熟的;n. 擦伤处;vt. 擦伤;n. (Raw)人名；(英)罗1091, Raw [rɔː]adj. 生的；未加工的；阴冷的；刺痛的；擦掉皮的；无经验的；（在艺术等方面）不成熟的;n. 擦伤处;vt. 擦伤;n. (Raw)人名；(英)罗1092, react [rɪ’ækt]vi. 反应；影响；反抗；起反作用;vt. 使发生相互作用；使起化学反应1093, Reactive [rɪ’æktɪv]adj. 反应的；电抗的；反动的1094, realistic [rɪə’lɪstɪk]adj. 现实的；现实主义的；逼真的；实在论的1095, realm [relm]n. 领域，范围；王国1096, Realm [relm]n. 领域，范围；王国1097, receiver [rɪ’siːvə]n. 接收器；接受者；收信机；收款员，接待者1098, reciprocate [rɪ’sɪprəkeɪt]vt. 报答；互换；互给vi. 往复运动；互换；酬答；互给1099, reclamation [,reklə’meɪʃən]n. 开垦；收回；再利用；矫正1100, recognition [rekəg’nɪʃ(ə)n]n. 识别；承认，认出；重视；赞誉；公认1101, recommend [rekə’mend]vt. 推荐，介绍；劝告；使受欢迎；托付;vi. 推荐；建议1102, reconcile [‘rɛkənsaɪl]vt. 使一致；使和解；调停，调解；使顺从1103, recruiter [rɪ’kruːtə]n. 招聘人员，征兵人员1104, rectangle [‘rektæŋg(ə)l]n. 矩形；长方形1105, rectify [‘rektɪfaɪ]vt. 改正；精馏；整流1106, recursen. 递归（算法）v. 递归1107, recursion [rɪ’kɜːʃ(ə)n]n. [数] 递归，循环；递归式1108, recursive [rɪ’kɜːsɪv]adj. [数] 递归的；循环的1109, Reduce [rɪ’djuːs]vt. 减少；降低；使处于；把…分解;vi. 减少；缩小；归纳为1110, redundant [rɪ’dʌnd(ə)nt]adj. 多余的，过剩的；被解雇的，失业的；冗长的，累赘的1111, ReentrantLock重入锁1112, reflexive [rɪ’fleksɪv]n. 反身代词；反身动词;adj. 反身的；[物] 反射的1113, regardless [rɪ’gɑːdlɪs]adj. 不管的；不顾的；不注意的;adv. 不顾后果地；不管怎样，无论如何；不惜费用地1114, regardless of不顾，不管regardless of: 不顾 | 不管 | 不惜1115, register [‘rɛdʒɪstɚ]vt. 登记；注册；记录；挂号邮寄；把…挂号；正式提出 vi. 登记；注册；挂号 n. 登记；注册；记录；寄存器；登记簿 n. (Register)人名；(英)雷吉斯特1116, registry [‘rɛdʒɪstri]n. 注册；登记处；挂号处；船舶的国籍1117, regular [‘regjʊlə]adj. 定期的；有规律的；合格的；整齐的；普通的 n. 常客；正式队员；中坚分子 adv. 定期地；经常地 n. (Regular)人名；(以)雷古拉尔1118, reinforce [riːɪn’fɔːs]vt. 加强，加固；强化；补充;vi. 求援；得到增援；给予更多的支持;n. 加强；加固物；加固材料1119, relevance [‘rɛləvəns]n. 关联；适当；中肯1120, remedy [‘remɪdɪ]vt. 补救；治疗；纠正 n. 补救；治疗；赔偿1121, render [‘rendə]vt. 致使；提出；实施；着色；以…回报;vi. 给予补偿;n. 打底；交纳；粉刷;n. (Render)人名；(英、德)伦德尔1122, rendering [‘rend(ə)rɪŋ]n. 翻译；表现；表演；描写；打底；（建筑物等）透视图vt. 致使；表演；打底（render的ing形式）vi. 给予补偿（render的ing形式）1123, renouncement [renouncement]n. 否认；拒绝；放弃1124, replica [‘rɛplɪkə]n. 复制品，复制物1125, replicas [replicas][古] 复制品复型1126, repository [rɪ’pɒzɪt(ə)rɪ]n. 贮藏室，仓库；知识库；智囊团1127, represent [reprɪ’zent]vt. 代表；表现；描绘；回忆；再赠送vi. 代表；提出异议1128, representation [,reprɪzen’teɪʃ(ə)n]n. 代表；表现；表示法；陈述1129, reproducibleadj. 可再生的；可繁殖的；可复写的1130, REQUIRED [rɪ’kwaɪəd]adj. 必需的；（美）必修的;v. 需要（require的过去式及过去分词形式）；要求1131, reroute [riː’ruːt]vt. 变更旅程；按新的特定路线运送1132, rescue [‘reskjuː]vt. 营救；援救;n. 营救；援救；解救1133, reservation [rezə’veɪʃ(ə)n]n. 预约，预订；保留1134, reservoir [‘rɛzɚ,vɔr]n. 水库；蓄水池1135, resilience [rɪ’zɪlɪəns]n. 恢复力；弹力；顺应力1136, resiliency [rɪ’zɪlɪənsɪ]n. 弹性；跳回1137, Resiliency [rɪ’zɪlɪənsɪ]n. 弹性；跳回1138, resilient [rɪ’zɪlɪənt]adj. 弹回的，有弹力的adj. 能复原的；有复原力的1139, resist [rɪ’zɪst]vi. 抵抗，抗拒；忍耐;vt. 抵抗；忍耐，忍住;n. [助剂] 抗蚀剂；防染剂1140, resolute [‘rezəluːt]adj. 坚决的；果断的1141, resolution [,rɛzə’luʃən]n. [物] 分辨率；决议；解决；决心1142, resolve [rɪ’zɒlv]vt. 决定；溶解；使…分解；决心要做…;vi. 解决；决心；分解;n. 坚决；决定要做的事1143, resolved [rɪ’zɒlvd]adj. 下定决心的；已解决的；断然的;v. 解决；决定；分解；转变（resolve的过去分词）1144, Resolved [rɪ’zɒlvd]adj. 下定决心的；已解决的；断然的;v. 解决；决定；分解；转变（resolve的过去分词）1145, Resolver [riː’zɒlvə]n. 溶剂；[电子] 分解器；下决心者1146, respect [rɪ’spekt]n. 尊敬，尊重；方面；敬意;vt. 尊敬，尊重；遵守1147, response [rɪ’spɒns]n. 响应；反应；回答1148, responsible [rɪ’spɒnsɪb(ə)l]adj. 负责的，可靠的；有责任的1149, responsive [rɪ’spɒnsɪv]adj. 响应的；应答的；回答的1150, restored [ri’stɔ:d]adj. 精力充沛的；精力恢复的;v. 修复（restore的过去式）；恢复健康1151, restrict [rɪ’strɪkt]vt. 限制；约束；限定1152, restriction [rɪ’strɪkʃ(ə)n]n. 限制；约束；束缚1153, restrictions [rɪ’strɪkʃən]n. 限制；限制条件（restriction的复数形式）1154, retention [rɪ’tenʃ(ə)n]n. 保留；扣留，滞留；记忆力；闭尿1155, Retention [rɪ’tenʃ(ə)n]n. 保留；扣留，滞留；记忆力；闭尿1156, retrieval [rɪ’triːvl]n. 检索；恢复；取回；拯救1157, retrieve [rɪ’triːv]vt. [计] 检索；恢复；重新得到;vi. 找回猎物;n. [计] 检索；恢复，取回1158, Retype [ri:’taip]vt. 重新输入1159, Reuse [riː’juːz]n. 重新使用，再用;vt. 再使用1160, revision [rɪ’vɪʒ(ə)n]n. [印刷] 修正；复习；修订本1161, revisions []n. 校订，[印刷] 修正；复习（revision的复数形式）1162, revoke [rɪ’vəʊk]vt. 撤回，取消；废除 vi. 有牌不跟 n. 有牌不跟1163, rewind [,ri’waɪnd]n. 重绕；倒带器vt. 倒回；重绕vi. 倒回；重绕1164, rid [rɪd]vt. 使摆脱；使去掉;n. (Rid)人名；(英)里德1165, riddle [‘rɪdl]vt. 解谜；给…出谜；充满于n. 谜语；粗筛；谜一般的人、东西、事情等vi. 出谜n. (Riddle)人名；(英)里德尔1166, ridiculous [rɪ’dɪkjʊləs]adj. 可笑的；荒谬的1167, rigid [‘rɪdʒɪd]adj. 严格的；僵硬的，死板的；坚硬的；精确的1168, rigorously [‘rigərəsli]adv.严厉地；残酷地1169, ripped [rɪpt]adj. 喝醉的；受毒品麻醉的;v. 撕；扯（rip的过去分词）1170, rival [‘raɪvl]n. 对手；竞争者;vt. 与…竞争；比得上某人;vi. 竞争;adj. 竞争的;n. (Rival)人名；(英、法、西)里瓦尔1171, robust [rə(ʊ)’bʌst]adj. 强健的；健康的；粗野的；粗鲁的1172, role [rəʊl]n. 角色；任务;n. (Role)人名；(意、塞、赤几)罗莱1173, rollover [‘rolovɚ]n. 翻转；（车）翻覆；延期付款1174, rotate [ˌæplɪ’keɪʃ(ə)n]n. 应用；申请；应用程序；敷用；（对事物、学习等）投入1175, rotation [ro’teʃən]n. 旋转；循环，轮流1176, routesn. [计] 路由，[通信] 路径；[交] 线路1177, rush [rʌʃ]n. 冲进；匆促；急流；灯心草 adj. 急需的 vt. 使冲；突袭；匆忙地做；飞跃 vi. 冲；奔；闯；赶紧；涌现 n. (Rush)人名；(英)拉什1178, saber [‘seɪbə]n. 军刀；佩剑；骑兵vt. 用马刀砍或杀n. (Saber)人名；(法)萨贝；(阿拉伯)萨比尔1179, sacrifice [‘sækrɪfaɪs]n. 牺牲；祭品；供奉vt. 牺牲；献祭；亏本出售vi. 献祭；奉献1180, sage [seɪdʒ]n.圣人；贤人；哲人adj.明智的；贤明的；审慎的n.(Sage)人名；(日)三下(姓)；(英)塞奇；(意)萨杰；(德)扎格；(法)萨热1181, sake [seɪk]n. 目的；利益；理由；日本米酒n. (Sake)人名；(罗)萨克；(日)酒(姓)1182, salt [sɔlt]n. 盐；风趣，刺激性adj. 咸水的；含盐的，咸味的；盐腌的；猥亵的vt. 用盐腌；给…加盐；将盐撒在道路上使冰或雪融化n. (Salt)人名；(西)萨尔特；(英)索尔特1183, salty [‘sɔːltɪ; ‘sɒ-]adj. 咸的；含盐的1184, salute [sə’l(j)uːt]n. 致敬，欢迎；敬礼;vt. 行礼致敬，欢迎;vi. 致意，打招呼；行礼1185, sanitizevt. 使…无害；给…消毒；对…采取卫生措施1186, sanitized [sanitized]v. 消毒；使清洁（sanitize的过去式）1187, sanity [‘sænɪtɪ]n. 明智；头脑清楚；精神健全；通情达理1188, saturate [‘sætʃərɪt]vt. 浸透，使湿透；使饱和，使充满adj. 浸透的，饱和的；深颜色的1189, saturated [‘sætʃəreɪtɪd]adj. 饱和的；渗透的；深颜色的 v. 使渗透，使饱和（saturate的过去式）1190, saturation [‘sætʃə’reʃən]n. 饱和；色饱和度；浸透；磁化饱和1191, scalability [,skeilə’biliti]n. 可扩展性；可伸缩性；可量测性1192, scalable [‘skeləbl]adj. 可攀登的；可去鳞的；可称量的1193, scale [skeɪl]n. 规模；比例；鳞；刻度；天平；数值范围;vi. 衡量；攀登；剥落；生水垢;vt. 测量；攀登；刮鳞；依比例决定;n. (Scale)人名；(意)斯卡莱1194, scarf [skɑːf]n. 围巾；嵌接，嵌接处；头巾领巾;vt. 披嵌接；用围巾围;n. (Scarf)人名；(英)斯卡夫1195, scenario [sɪ’nɑːrɪəʊ]n. 方案；情节；剧本；设想1196, scenarios [sɪ’nɛrɪ,o]n. 情节；脚本；情景介绍（scenario的复数）1197, scope [skəʊp]n. 范围；余地；视野；眼界；导弹射程;vt. 审视1198, Scope [skəʊp]n. 范围；余地；视野；眼界；导弹射程;vt. 审视1199, SCOPE [skəʊp]n. 范围；余地；视野；眼界；导弹射程;vt. 审视1200, scrape [skrep]n. 刮掉；擦痕；困境；刮擦声vt. 刮；擦伤；挖成vi. 刮掉；刮出刺耳声1201, scratch [skrætʃ]n. 擦伤；抓痕；刮擦声；乱写;adj. 打草稿用的；凑合的；碰巧的;vt. 抓；刮；挖出；乱涂;vi. 抓；搔；发刮擦声；勉强糊口；退出比赛1202, screened [screened]adj. 筛过的，屏蔽的1203, screw [skruː]vt. 旋，拧；压榨；强迫n. 螺旋；螺丝钉；吝啬鬼vi. 转动，拧1204, scroll [skrəʊl]n. 卷轴，画卷；名册；卷形物;vi. 成卷形;vt. 使成卷形1205, scum [skʌm]n. 浮渣；泡沫；糟粕vi. 产生泡沫；被浮渣覆盖vt. 将浮渣去除掉1206, segment [‘segm(ə)nt]vi. 分割;n. 段；部分;vt. 分割1207, selfie []n. 自拍照1208, semantics [sɪ’mæntɪks]n. [语] 语义学；语义论1209, semaphore [‘seməfɔː]vi. 打旗语；发信号;n. 信号；旗语；臂板信号机;vt. 用信号联络1210, semicolon [‘sɛmɪkolən]n. 分号1211, sensation [sen’seɪʃ(ə)n]n. 感觉；轰动；感动1212, sensibly [‘sɛnsəbli]adv. 明显地；容易感知地；聪明地1213, sentinel [‘sɛntɪnl]n. 哨兵vt. 守卫，放哨1214, serial [‘sɪərɪəl]adj. 连续的；连载的；分期偿还的;n. 电视连续剧；[图情] 期刊；连载小说1215, severely [sɪ’vɪəlɪ]adv. 严重地；严格地，严厉地；纯朴地1216, severity [sɪ’verɪtɪ]n. 严重；严格；猛烈1217, shell [ʃel]n. 壳，贝壳；炮弹；外形vi. 剥落；设定命令行解释器的位置vt. 剥皮；炮轰1218, shelter [‘ʃɛltɚ]n. 庇护；避难所；遮盖物vt. 保护；使掩蔽vi. 躲避，避难n. (Shelter)人名；(英)谢尔特1219, shelve [ʃɛlv]vt. 将（书等）放置在架子上；搁置，将某事放到一旁不予考虑；将…搁在一边；装搁架于；罢免vi. （陆地）逐渐倾斜1220, shenandoah [,ʃenən’dəuə]n. 谢南多厄河（美国弗吉尼亚州河流）；谢南多厄河谷（地名）；情人渡，水手瑶（歌曲名）1221, shield [ʃild]n. 盾；防护物；保护者vt. 遮蔽；包庇；避开；保卫vi. 防御；起保护作用n. (Shield)人名；(英)希尔德1222, Shield [ʃiːld]n. 盾；防护物；保护者 vt. 遮蔽；包庇；避开；保卫 vi. 防御；起保护作用 n. (Shield)人名；(英)希尔德1223, shim [ʃɪm]vt. 用木片或夹铁填；夹铁 n. 填隙用木片；夹铁 n. (Shim)人名；(朝)沈1224, shingle [‘ʃɪŋg(ə)l]n. 墙面板；木瓦；小招牌（尤指医生或律师挂的营业招牌）；鹅卵石 vt. 用盖板覆盖1225, shipper [‘ʃɪpɚ]n. 托运人；发货人；货主1226, shoehornn. 鞋拔 vt. 硬塞进1227, shot [ʃɒt]n. 发射；炮弹；射手；镜头;adj. 用尽的；破旧的；杂色的，闪光的;v. 射击（shoot的过去式和过去分词）1228, shrimp [ʃrɪmp]n. 虾；小虾；矮小的人vi. 捕虾adj. 有虾的；虾制的1229, shrink [ʃrɪŋk]n. 收缩；畏缩；&lt;俚&gt;精神病学家 vt. 使缩小，使收缩 vi. 收缩；畏缩1230, shuffle [‘ʃʌfl]v. 洗牌；推诿，推卸；拖曳，慢吞吞地走；搅乱n. 洗牌，洗纸牌；混乱，蒙混；拖着脚走1231, sidecar [‘saɪdkɑr]n. 轻快的双轮马车；跨斗1232, sidetrack [‘saɪdtræk]vt. 将（火车）[建] 转到侧线；转变（话题）n. （铁路）侧线；次要地位vi. [建] 转到侧线；转变话题1233, sierra [sɪ’erə; sɪ’eərə]n. [地理] 锯齿山脊；呈齿状起伏的山脉n. (Sierra)人名；(意、西)谢拉；(英)西拉1234, significant [sɪɡ’nɪfɪkənt]adj. 重大的；有效的；有意义的；值得注意的；意味深长的n. 象征；有意义的事物1235, silhouette [,sɪlu’ɛt]n. 轮廓，剪影vt. 使…照出影子来；使…仅仅显出轮廓n. (Silhouette)人名；(法)西卢埃特1236, Silly [‘sɪlɪ]adj. 愚蠢的n. 傻瓜n. (Silly)人名；(匈)希伊；(法)西利1237, simulate [‘sɪmjʊleɪt]vt. 模仿；假装；冒充 adj. 模仿的；假装的1238, simultaneous [,saɪml’tenɪəs]adj. 同时的；联立的；同时发生的n. 同时译员1239, simultaneously [saɪməl’tenɪəsli]adv. 同时地1240, single-minded [‘siŋɡl,’maindid]adj. 专心的；纯真的；真诚的；率直的1241, singular [‘sɪŋgjʊlə]adj. 单数的；单一的；非凡的；异常的 n. 单数1242, singularity [sɪŋgjʊ’lærɪtɪ]n. 奇异；奇点；突出；稀有1243, sink [sɪŋk]vi. 下沉；消沉；渗透vt. 使下沉；挖掘；使低落n. 水槽；洗涤槽；污水坑n. (Sink)人名；(英、瑞典)辛克1244, slack [slæk]adj. 松弛的；疏忽的；不流畅的vi. 松懈；减弱n. 煤末；峡谷vt. 放松；使缓慢adv. 马虎地；缓慢地n. (Slack)人名；(英)斯莱克1245, slash [slæʃ]vt. 猛砍；鞭打；严厉批评；大幅度裁减或削减;vi. 猛砍；严厉批评;n. 削减；斜线；猛砍；砍痕；沼泽低地1246, slave [sleɪv]n. 奴隶；从动装置;vi. 苦干；拼命工作;n. (Slave)人名；(塞、罗)斯拉韦1247, slavishly [‘slevɪʃli]adv. 奴隶般地1248, sleuth [sluːθ]n. 侦探；警犬 vi. 做侦探；侦查1249, slice [slaɪs]n. 薄片；部分；菜刀，火铲;vt. 切下；把…分成部分；将…切成薄片;vi. 切开；割破1250, slide [slaɪd]n. 滑动；幻灯片；滑梯；雪崩;vi. 滑动；滑落；不知不觉陷入;vt. 滑动；使滑动；悄悄地迅速放置1251, sloped []adj. 倾斜的;v. 倾斜；使倾斜（slope的过去式）;1252, slot [slɒt]n. 位置；狭槽；水沟；硬币投币口;vt. 跟踪；开槽于;n. (Slot)人名；(英、荷)斯洛特1253, slurp [slɜːp]vt. 出声地吃或喝 vi. 出声地吃或喝 n. 吃的声音；啜食声1254, snack [snæk]n. 小吃，快餐；一份，部分;vi. 吃快餐，吃点心1255, snap [snæp]vt. 突然折断，拉断；猛咬；啪地关上;vi. 咬；厉声说；咯嗒一声关上;n. 猛咬；劈啪声；突然折断;adj. 突然的1256, SNAPSHOT [‘snæpʃɒt]n. 快照，快相；急射，速射；简单印象;vt. 给…拍快照;vi. 拍快照1257, sneaker [‘sniːkə]n. 运动鞋；卑鄙者；鬼鬼祟祟做事的人1258, sneaky [‘sniki]adj. 鬼鬼祟祟的；暗中的，卑鄙的1259, sniffer [snɪfə]n. 嗅探器；嗅探犬；以鼻吸毒者1260, snip [snɪp]n. 剪；便宜货 vt. 剪断 vi. 剪1261, snippet [‘snɪpɪt]n. 小片；片断；不知天高地厚的年轻人1262, soap [səʊp]n. 肥皂;vt. 将肥皂涂在……上；对……拍马屁（俚语）;vi. 用肥皂擦洗1263, sock [sɑk]n. 短袜；一击vt. 重击；给……穿袜adv. 正着地；不偏不倚地adj. 非常成功的n. (Sock)人名；(德)佐克1264, sole [sol]n. 鞋底；脚底；基础；鳎目鱼adj. 唯一的；单独的；仅有的vt. 触底；上鞋底n. (Sole)人名；(意、西、芬、塞、罗、南非)索莱1265, solid [‘sɒlɪd]adj. 固体的；可靠的；立体的；结实的；一致的;n. 固体；立方体;n. (Solid)人名；(瑞典)索利德1266, sophisticated [sə’fɪstɪkeɪtɪd]adj. 复杂的；精致的；久经世故的；富有经验的;v. 使变得世故；使迷惑；篡改（sophisticate的过去分词形式）1267, sore [sɔː]adj. 疼痛的，痛心的；剧烈的，极度的；恼火的，发怒的；厉害的，迫切的n. 溃疡，痛处；恨事，伤心事n. (Sore)人名；(法)索尔；(意)索雷1268, spawn [spɔn]n. 卵；菌丝；产物vt. 产卵；酿成，造成；大量生产vi. 产卵；大量生产1269, spec [spɛk]n. 投机；说明书；细则1270, specific [spə’sɪfɪk]adj. 特殊的，特定的；明确的；详细的；[药] 具有特效的;n. 特性；细节；特效药1271, specification [,spesɪfɪ’keɪʃ(ə)n]n. 规格；说明书；详述1272, Specification [,spesɪfɪ’keɪʃ(ə)n]n. 规格；说明书；详述1273, spike [spaɪk]n. 长钉，道钉；钉鞋；细高跟vt. 阻止；以大钉钉牢；用尖物刺穿n. (Spike)人名；(瑞典)斯皮克1274, Spinlock [Spinlock]自旋锁1275, split [splɪt]vt. 分离；使分离；劈开；离开；分解;vi. 离开；被劈开；断绝关系;n. 劈开；裂缝;adj. 劈开的1276, sponsor [‘spɒnsə]n. 赞助者；主办者；保证人 vt. 赞助；发起1277, spouse [spaʊz; -s]n. 配偶vt. 和…结婚n. (Spouse)人名；(英)斯波斯1278, spout [spaʊt]n. 喷口；水龙卷；水落管；水柱vt. 喷出；喷射；滔滔不绝地讲；把…典当掉vi. 喷出；喷射；滔滔不绝地讲1279, spurious [‘spjʊərɪəs]adj. 假的；伪造的；欺骗的1280, squash [skwɔʃ]vt. 镇压；把…压扁；使沉默vi. 受挤压；发出挤压声；挤入n. 壁球；挤压；咯吱声；南瓜属植物；（英）果汁饮料1281, stacked [stækt]adj. 妖艳的；（女人）身材丰满匀称的v. 堆放（stack的过去分词）1282, stage [steɪdʒ]n. 阶段；舞台；戏剧；驿站 vt. 举行；上演；筹划 vi. 举行；适于上演；乘驿车旅行 n. (Stage)人名；(英)斯特奇1283, stale [stel]adj. 陈腐的；不新鲜的vi. 变陈旧；撒尿；变得不新鲜vt. 使变旧；变得不新鲜n. 尿n. (Stale)人名；(塞)斯塔莱1284, Stale [steɪl]adj. 陈腐的；不新鲜的;vi. 变陈旧；撒尿；变得不新鲜;vt. 使变旧；变得不新鲜;n. 尿;n. (Stale)人名；(塞)斯塔莱1285, staleness [staleness]n. 腐败；陈腐；不新鲜；泄气1286, stamp [stæmp]n. 邮票；印记；标志；跺脚vt. 铭记；标出；盖章于…；贴邮票于…；用脚踩踏vi. 跺脚；捣碎；毁掉n. (Stamp)人名；(德)施坦普；(英)斯坦普1287, stamped [stæmpt]adj. 铭刻的；盖上邮戳的；顿足的 v. 贴上邮票（stamp的过去式）；顿足1288, stand-alone [‘stændə,lon]n. 独立；单机adj. 独立的；独立操作的1289, standardization [,stændədaɪ’zeɪʃən]n. 标准化；[数] 规格化；校准1290, starvation [stɑː’veɪʃn]n. 饿死；挨饿；绝食1291, stashn. 藏匿处；藏匿物vt. 存放；贮藏vi. 存放；藏起来1292, statement [‘steɪtm(ə)nt]n. 声明；陈述，叙述；报表，清单1293, statistic [stə’tɪstɪk]adj. 统计的，统计学的;n. 统计数值1294, statisticiann. 统计学家，统计员1295, stdoutabbr. 标准输出（standard output）1296, STEADY [‘stedɪ]adj. 稳定的；不变的；沉着的;vi. 稳固;vt. 使稳定；稳固；使坚定;adv. 稳定地；稳固地;n. 关系固定的情侣；固定支架1297, stem [stem]n. 干；茎；船首；血统;vt. 阻止；除去…的茎；给…装柄;vi. 阻止；起源于某事物；逆行1298, stereotype [‘stɛrɪətaɪp]vt. 使用铅版；套用老套，使一成不变n. 陈腔滥调，老套；铅版1299, sticker [‘stɪkə]n. 尖刀；难题；张贴物；坚持不懈的人;vt. 给…贴上标签价;adj. 汽车价目标签的；汽车标签价的;n. (Sticker)人名；(德)施蒂克1300, stitch [stɪtʃ]n. 针脚，线迹；一针 vt. 缝，缝合 vi. 缝，缝合1301, straightforward [streɪt’fɔːwəd]adj. 简单的；坦率的；明确的；径直的 adv. 直截了当地；坦率地1302, strange [strendʒ]adj. 奇怪的；陌生的；外行的adv. 奇怪地；陌生地，冷淡地n. (Strange)人名；(英)斯特兰奇；(瑞典、塞)斯特朗格1303, strategist [‘strætədʒɪst]n. 战略家；军事家1304, strategy [‘strætɪdʒɪ]n. 战略，策略1305, stratum [‘strɑːtəm; ‘streɪtəm]n. （组织的）层；[地质] 地层；社会阶层1306, strict [strɪkt]adj. 严格的；绝对的；精确的；详细的1307, stride [straɪd]n. 大步；步幅；进展vt. 跨过；大踏步走过；跨坐在…vi. 跨；跨过；大步行走过去式:strode; 过去分词:stridden; 现在分词:striding1308, strip [strɪp]vt. 剥夺；剥去；脱去衣服n. 带；条状；脱衣舞vi. 脱去衣服1309, striped [straɪpt]adj. 有条纹的；有斑纹的;v. 被剥去（strip的过去分词）1310, structural [‘strʌktʃ(ə)r(ə)l]adj. 结构的；建筑的1311, Stub [stʌb]n. 存根；烟蒂；树桩；断株;vt. 踩熄；连根拔除;n. (Stub)人名；(挪、瑞典)斯图布1312, stuff [stʌf]n. 东西；材料；填充物；素材资料vt. 塞满；填塞；让吃饱vi. 吃得过多1313, subdivide [‘sʌbdɪvaɪd]vi. 细分，再分vt. 把……再分，把……细分1314, subject [‘sʌbdʒekt; ‘sʌbdʒɪkt]n. 主题；科目；[语] 主语；国民;adj. 服从的；易患…的；受制于…的;vt. 使…隶属；使屈从于…1315, submission [səb’mɪʃ(ə)n]n. 投降；提交（物）；服从；（向法官提出的）意见；谦恭1316, subscribe [səb’skraɪb]vi. 订阅；捐款；认购；赞成；签署 vt. 签署；赞成；捐助1317, Subscription [səb’skrɪpʃ(ə)n]n. 捐献；订阅；订金；签署1318, subsequent [‘sʌbsɪkw(ə)nt]adj. 后来的，随后的1319, substitute [‘sʌbstɪtut]n. 代用品；代替者vi. 替代vt. 代替1320, suddenly [ˈsʌdnlɪ]adv. 突然地；忽然1321, suit [suːt]vt. 适合；使适应n. 诉讼；恳求；套装；一套外衣vi. 合适；相称1322, suite [sut; swit]n. （一套）家具；套房；组曲；（一批）随员，随从1323, summedv. 总计，概括，总结（sum的过去时和过去分词）1324, supercede [supercede]vt. 取代；延期；紧接着…而到来vi. 推迟行动1325, supersededadj. 作废的；被取代的v. 取代；克制；废弃（supersede的过去分词）1326, superset [superset]n. [数] 超集1327, supervise [‘suːpəvaɪz; ‘sjuː-]vt. 监督，管理；指导vi. 监督，管理；指导1328, supervisor [‘supɚvaɪzɚ]n. 监督人，[管理] 管理人；检查员1329, supplemental [,sʌplə’mɛntl]adj. 补充的（等于supplementary）；追加的1330, supplier [sə’plaɪə]n. 供应厂商，供应国；供应者1331, supreme [suː’priːm]adj.最高的；至高的；最重要的n.至高；霸权1332, survey [ˈsɜːveɪ]n. 调查；测量；审视；纵览;vt. 调查；勘测；俯瞰;vi. 测量土地1333, survivor [sə’vaɪvə]n. 幸存者；生还者；残存物1334, susceptibility [sə,septɪ’bɪlɪtɪ]n. 敏感性；感情；磁化系数1335, susceptible [sə’septɪb(ə)l]adj. 易受影响的；易感动的；容许…的 n. 易得病的人1336, symmetric [sɪ’metrɪk]adj. 对称的；匀称的1337, synonyms [ˈsɪnənɪm]n. [语] 同义词，同义字；同一性（synonym的复数）1338, synopsis [sɪ’nɑpsɪs]n. 概要，大纲1339, syntactically [sɪn’tæktɪkli]adv. 依照句法地；在语句构成上1340, synthetic [sɪn’θetɪk]adj.综合的；合成的，人造的n.合成物1341, systematic [sɪstə’mætɪk]adj. 系统的；体系的；有系统的；[图情] 分类的；一贯的，惯常的1342, tactic [‘tæktɪk]n. 策略，战略adj. 按顺序的，依次排列的1343, tail [teɪl]n. 尾巴；踪迹；辫子；燕尾服vt. 尾随；装上尾巴vi. 跟踪；变少或缩小adj. 从后面而来的；尾部的1344, taint [tent]vt. 污染；腐蚀；使感染n. 污点；感染vi. 败坏；被污染1345, tamper [‘tæmpə]vi. 篡改；干预；损害；削弱；玩弄；贿赂;vt. 篡改;n. 填塞者；捣棒1346, Tarball [Tarball]n. 原始码；（Linux下最方便的）打包工具1347, tasty [‘teɪstɪ]adj. 美味的；高雅的；有趣的;n. 可口的东西；引人入胜的东西1348, technique [tek’niːk]n. 技巧，技术；手法1349, telemetry [tə’lɛmətri]n. [自] 遥测技术；遥感勘测；自动测量记录传导1350, tempo [‘tempəʊ]n. 速度，发展速度；拍子n. (Tempo)人名；(意)滕波1351, temptation [tem(p)’teɪʃ(ə)n]n. 引诱；诱惑物1352, tenant [‘tɛnənt]n. 承租人；房客；佃户；居住者vt. 租借（常用于被动语态）n. (Tenant)人名；(法)特南1353, tenure [‘tenjə]n. 任期；占有vt. 授予…终身职位1354, tenuredadj. （美）享有终身职位的 v. 授予…终身职位（tenure的过去分词）1355, term [tɝm]n. 术语；学期；期限；条款；(代数式等的)项vt. 把…叫做n. (Term)人名；(泰)丁1356, terminology [,tɜːmɪ’nɒlədʒɪ]n. 术语，术语学；用辞1357, terminus [‘tɝmɪnəs]n. 终点；终点站；界标；界石1358, territory [‘terɪt(ə)rɪ]n. 领土，领域；范围；地域；版图1359, testimonial [,testɪ’məʊnɪəl]n.证明书；推荐书adj.证明的；褒奖的；表扬的1360, theta [‘θitə]n. 希腊字母的第八字；时间递耗值1361, thin [θɪn]adj. 薄的；瘦的；稀薄的；微弱的;vt. 使瘦；使淡；使稀疏;vi. 变薄；变瘦；变淡;adv. 稀疏地；微弱地;n. 细小部分;n. (Thin)人名；(越)辰；(柬)廷1362, threshold [‘θreʃəʊld; ‘θreʃ,həʊld]n. 入口；门槛；开始；极限；临界值1363, thresholds [‘θrɛʃhold]n. 阈值；[建] 门槛；临界值值（threshold的复数）1364, thrift [θrɪft]n. 节俭；节约；[植] 海石竹n. (Thrift)人名；(英)思里夫特1365, thrilled [θrɪld]adj. 非常兴奋的；极为激动的;v. 激动（thrill的过去式）；[医] 震颤1366, throttle [‘θrɑtl]n. 节流阀；[车辆] 风门；喉咙vt. 压制，扼杀；使……窒息；使……节流vi. 节流，减速；窒息1367, throughput [‘θrʊ’pʊt]n. 生产量，生产能力1368, thumbnail [‘θʌmneɪl]n. 拇指指甲；极小的东西；短文;adj. 极小的，极短的1369, thwartedadj. 挫败的 v. 挫败（thwart的过去分词）；反对1370, tick [tɪk]vt. 标记号于；滴答地记录n. 滴答声；扁虱；记号；赊欠vi. 发出滴答声；标以记号n. (Tick)人名；(匈、芬)蒂克1371, tie [taɪ]vt. 系；约束；打结；与…成平局vi. 打结；不分胜负；被用带（或绳子等）系住n. 领带；平局；鞋带；领结；不分胜负n. (Tie)人名；(东南亚国家华语)治；(英)泰伊；(柬)狄1372, tie-breaking [tie-breaking]adj. 平局决胜的；打破僵局的1373, tier [tɪr]n. 层，排；行，列；等级vt. 使层叠vi. 成递升徘列n. (Tier)人名；(英)蒂尔1374, tiesn. 结v. 绑；连结1375, toggle [‘tɒg(ə)l]vt. 拴牢，系紧;n. 开关，触发器；拴扣；[船] 套索钉1376, Tokenizern. 分词器；编译器1377, tolerable [‘tɒl(ə)rəb(ə)l]adj. 可以的；可容忍的1378, tolerate [‘tɒləreɪt]vt. 忍受；默许；宽恕1379, topology [təˈpɑlədʒi]n. 拓扑学1380, tracker [‘trækə]n. 拉纤者，纤夫；追踪系统，[自] 跟踪装置；追踪者1381, trade off权衡；卖掉；交替使用；交替换位1382, tradeoff [‘tred,ɔf]n. 权衡；折衷；（公平）交易（等于trade-off）1383, tradeoffs [‘tred,ɔf]n. 权衡；折衷；（公平）交易（等于trade-off）1384, trampoline [‘træmpəliːn]n. 蹦床；弹簧垫1385, transform [træns’fɔːm; trɑːns-; -nz-]vt. 改变，使…变形；转换;vi. 变换，改变；转化1386, transformation [trænsfə’meɪʃ(ə)n; trɑːns-; -nz-]n. [遗] 转化；转换；改革；变形1387, transient [‘trænzɪənt]adj. 短暂的；路过的;n. 瞬变现象；过往旅客；候鸟1388, transition [træn’zɪʃ(ə)n; trɑːn-; -‘sɪʃ-]n. 过渡；转变；[分子生物] 转换；变调1389, transitive [‘trænsɪtɪv; ‘trɑːns-; -nz-]adj. 及物的；过渡的；可迁的;n. 传递；及物动词1390, transparently [‘træns’pɛrəntli]adv. 显然地，易觉察地；明亮地1391, traversal [trəˈvərs(ə)l]n. [计] 遍历；横越；横断物1392, traverse [‘trævəs; trə’vɜːs]n. 穿过；横贯；横木 vt. 穿过；反对；详细研究；在…来回移动 vi. 横越；旋转；来回移动 adj. 横贯的 n. (Traverse)人名；(英)特拉弗斯；(法)特拉韦尔斯1393, traversing [‘trævɝs]n. 穿越，通过;v. 穿过（traverse的ing形式）；横越1394, Tray [treɪ]n. 托盘；文件盒；隔底匣；（无线电的）发射箱1395, tremendously [trɪ’mɛndəsli]adv. 非常地；可怕地；惊人地1396, trends [trendz]n. 动态，[统计] 趋势1397, trial [‘traɪəl]n. 试验；审讯；努力；磨炼;adj. 试验的；审讯的;n. (Trial)人名；(法)特里亚尔1398, tribe [traɪb]n. 部落；族；宗族；一伙n. (Tribe)人名；(英)特赖布1399, tricky [‘trɪkɪ]adj. 狡猾的；机警的1400, trident [‘traɪdnt]n. 三叉戟；[数] 三叉线；三齿鱼叉1401, trillion [‘trɪljən]n. [数] 万亿 adj. 万亿的 num. [数] 万亿1402, trivia [‘trɪvɪə]n. 琐事1403, trivial [‘trɪvɪəl]adj. 不重要的，琐碎的；琐细的1404, tropic [‘trɒpɪk]n. 热带；回归线adj. 热带的1405, troubleshootingn. 解决纷争；发现并修理故障v. 检修（troubleshoot的ing形式）；当调解人1406, trunk [trʌŋk]n. 树干；躯干；象鼻；汽车车尾的行李箱;vt. 把…放入旅行箱内;adj. 干线的；躯干的；箱子的;n. (Trunk)人名；(德、匈、西)特伦克1407, tunable [‘tjʊnəbl]adj. 可调谐的；可调音的；和谐的；音调美的1408, tune [tun]n. 曲调；和谐；心情vt. 调整；使一致；为…调音vi. [电子][通信] 调谐；协调n. (Tune)人名；(英)图恩1409, tunnel [‘tʌnl]n. 隧道；坑道；洞穴通道vt. 挖；在…打开通道；在…挖掘隧道vi. 挖掘隧道；打开通道1410, tutorial [tjuː’tɔːrɪəl]adj. 辅导的；家庭教师的，个别指导的;n. 个别指导1411, tweak [twik]n. 扭；拧；焦急vt. 扭；用力拉；开足马力1412, tweet [twiːt]n. 小鸟叫声；自录音再现装置发出的高音；推特;vi. 吱吱地叫；啾鸣1413, ubiquitous [juː’bɪkwɪtəs]adj. 普遍存在的；无所不在的1414, unary [‘juːnərɪ]adj. [数] 一元的1415, unassignedadj. 未派职务的；未赋值的1416, uncle [‘ʌŋk(ə)l]n. 叔叔；伯父；伯伯；舅父；姨丈；姑父1417, undelivered [,ʌndɪ’lɪvɚd]adj. 未被释放的；未送达的1418, undergo [,ʌndɚ’ɡo]vt. 经历，经受；忍受1419, underlying [ʌndə’laɪɪŋ]adj. 潜在的；根本的；在下面的；优先的;v. 放在…的下面；为…的基础；优先于（underlie的ing形式）1420, underpin [ʌndə’pɪn]vt. 巩固；支持；从下面支撑；加强…的基础1421, underwear [‘ʌndəweə]n. 内衣物1422, unfeasible [ʌn’fiːzɪb(ə)l]adj. 难实施的，不能实行的1423, unifiedadj. 统一的；一致标准的v. 统一；使一致（unify的过去分词）1424, Unity [ˈjuːnəti]n. 团结；一致；联合；个体;n. (Unity)人名；(英)尤妮蒂1425, Until [ən’tɪl]conj. 在…以前；直到…时;prep. 在…以前；到…为止1426, unusual [ʌn’juːʒʊəl]adj. 不寻常的；与众不同的；不平常的1427, upon [ə’pɑn]prep. 根据；接近；在…之上1428, usual [‘juːʒʊəl]adj. 通常的，惯例的；平常的1429, Utility [juːˈtɪləti]n. 实用；效用；公共设施；功用;adj. 实用的；通用的；有多种用途的1430, vacant [‘veɪk(ə)nt]adj.空虚的；空的；空缺的；空闲的；茫然的n.(Vacant)人名；(法)瓦康1431, Validation [,vælɪ’deɪʃən]n. 确认；批准；生效1432, vanilla [və’nɪlə]n. 香子兰，香草adj. 香草味的1433, variant [‘vɛrɪənt]adj. 不同的；多样的n. 变体；转化1434, variantsn. [计] 变体；变异型（variant的复数）1435, Variation [veərɪ’eɪʃ(ə)n]n. 变化；[生物] 变异，变种1436, various [‘veərɪəs]adj. 各种各样的；多方面的1437, vary [‘veərɪ]vi. 变化；变异；违反;vt. 改变；使多样化；变奏;n. (Vary)人名；(英、法、罗、柬)瓦里1438, vast [vɑːst]adj. 广阔的；巨大的；大量的；巨额的;n. 浩瀚；广阔无垠的空间;n. (Vast)人名；(法)瓦斯特1439, vastly [ˈvɑ:stli]adv. 极大地；广大地；深远地1440, vector [‘vɛktɚ]n. 矢量；带菌者；航线vt. 用无线电导航1441, vehicle [‘viːɪkl]n. [车辆] 车辆；工具；交通工具；运载工具；传播媒介；媒介物1442, velocity [vəˈlɒsəti]n. 【物】速度1443, Velocity [vəˈlɒsəti]n. [力] 速率；迅速；周转率1444, vendor [‘vɛndɚ]n. 卖主；小贩；供应商；[贸易] 自动售货机1445, Venus [‘viːnəs]n. [天] 金星；维纳斯（爱与美的女神）1446, verbose [vɜː’bəʊs]adj. 冗长的；啰嗦的1447, verify [‘verɪfaɪ]vt. 核实；查证1448, versatile [‘vɜːsətaɪl]adj. 多才多艺的；通用的，万能的；多面手的1449, versus [‘vɝsəs]prep. 对；与…相对；对抗1450, vetting [vetting]n. 审查，审核1451, via [ˈvaɪə，ˈviːə]prep. 渠道，通过；经由1452, vigorously [‘vɪgərəsli]adv. 精神旺盛地，活泼地1453, violation [vaɪə’leɪʃn]n. 违反；妨碍，侵害；违背；强奸1454, virtue [‘vɜːtjuː; -tʃuː]n. 美德；优点；贞操；功效;n. (Virtue)人名；(英)弗丘1455, visual [‘vɪʒjʊəl; -zj-]adj. 视觉的，视力的；栩栩如生的1456, Visual [‘vɪʒjʊəl; -zj-]adj. 视觉的，视力的；栩栩如生的1457, volatile [‘vɒlətaɪl]adj. [化学] 挥发性的；不稳定的；爆炸性的；反复无常的;n. 挥发物；有翅的动物;n. (Volatile)人名；(意)沃拉蒂莱1458, volatility [,vɑlə’tɪləti]n. [化学] 挥发性；易变；活泼1459, voltage [‘voltɪdʒ]n. [电] 电压1460, vulnerabilitiesn. 缺陷（vulnerability的复数形式）；脆弱点1461, walkthroughn. 预排，预排工作1462, wall-mounted [,wɔ:l’mauntid]adj. 固定在墙上的；安装在墙上的1463, warranty [‘wɔrənti]n. 保证；担保；授权；（正当）理由1464, webcasts []n. 网络广播（webcast的复数）1465, webinar [‘webɪnɑː]n. 网络研讨会；在线会议1466, weighted [‘wetɪd]adj. [数] 加权的；加重的；衡量过的v. 加重量于…；重压（weight的过去分词）1467, well-distributed均匀1468, whence [wens]n. 根源adv. 从何处pron. 何处conj. 由此1469, whoop [wuːp]n. 大叫；哮喘声；呐喊；一点点vt. 高声说；唤起vi. 叫喊；喘息1470, whore [hɔː]vi.卖淫，娼妓n.娼妓，淫妇1471, wicked [‘wɪkɪd]adj. 邪恶的；恶劣的；不道德的；顽皮的1472, widgets []n. 小工具（widget的复数）；窗体小部件1473, wiggle [‘wɪɡl]vt. 使……摆动，使……扭动vi. 摆动n. 扭动1474, wildcard [‘waɪldkɑrd]n. 通配符1475, wipe [waɪp]vt. 擦；消除；涂上;vi. 擦；打;n. 擦拭；用力打1476, wizard [‘wɪzəd]n. 男巫；术士；奇才；向导程序 adj. 男巫的；巫术的1477, Wizard [‘wɪzəd]n. 男巫；术士；奇才;adj. 男巫的；巫术的1478, workbench [‘wɝkbɛntʃ]n. 工作台；手工台1479, worthwhile [wɜːθ’waɪl]adj. 值得做的，值得花时间的1480, wrap [ræp]vt. 包；缠绕；隐藏；掩护;vi. 包起来；缠绕；穿外衣;n. 外套；围巾1481, WRT [WRT]abbr. 关于（With Regard To）1482, xenial [xenial]adj. 异花受粉的；主客关系的；款待的1483, zap [zæp]n. 活力；意志；杀死；震击 vt. 攻击；打败；快速推动 vi. 快速移动 int. 咝！；糟了！1484, 演示 [yǎn shì]demonstration;show-how;reproduction]]></content>
      <categories>
        <category>Words</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>Words</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[译]ZGC: 一个可伸缩的低延迟垃圾收集器]]></title>
    <url>%2Fzgc-a-scalable-low-latency-garbage-collector.html</url>
    <content type="text"><![CDATA[翻译自：JEP 333 地址：https://openjdk.java.net/jeps/333 一、摘要Z垃圾收集器，也称为ZGC，是一个可伸缩的低延迟垃圾收集器。 二、目标 GC暂停时间不超过10ms 能处理大小从相对较小(几百MB)到非常大(TB级)的堆 与使用G1相比，应用程序吞吐量减少不超过15% 方便日后在此基础上利用彩色指针和内存屏障进一步优化收集器及实现新特性。【原文：Lay a foundation for future GC features and optimizations leveraging colored pointers and load barriers】 支持平台:Linux/x64 注：此处及下文中的内存屏障即load barrier，在ZGC中用的是读屏障。 三、动机垃圾收集是Java的主要优势之一。但是，当垃圾收集暂停太长时，就会对应用程序的响应时间产生负面影响。通过大幅度缩短停顿时间，我们可以让Java适用于更多类型的应用程序。 此外，现代系统中可用的内存数量还在继续增长。用户和应用程序开发人员希望JVM能够以一种有效的方式充分利用这种内存，并且不会出现很长的GC暂停时间。 四、 描述ZGC是一个并发的、单代（不再区分新生代和老年代）的、基于region的、支持numa的压缩收集器。Stop-the-world阶段仅限于根扫描，所以GC暂停时间不会随着堆或存活对象的多少而增加。 ZGC的一个核心设计原则是结合使用内存屏障和彩色对象指针。这使得ZGC能够在运行Java应用程序线程时执行并发操作，比如对象重定位。从Java线程的角度来看，在Java对象中加载引用字段的行为受到内存屏障的限制。除了对象地址之外，有色对象指针还包含内存屏障需要的信息，用于确定在允许Java线程使用该指针之前是否需要采取某些操作。例如，对象可能已经被重新定位，在这种情况下，内存屏障将检测情况并采取适当的操作。 与其他技术相比，我们认为颜色指针方案提供了一些非常吸引人的特性。特别是: 这允许我们在移动对象/整理内存阶段，在指向可回收/重用区域的指针确定之前回收/重用这部分内存【原文：It allows us to reclaim and reuse memory during the relocation/compaction phase, before pointers pointing into the reclaimed/reused regions have been fixed. 】。这有助于降低堆开销。这还意味着不需要实现单独的标记压缩算法来处理完整的GC。 这允许我们使用相对较少且简单的GC屏障。这有助于降低运行时开销。这还意味着在解释器和JIT编译器中更容易实现、优化和维护GC barrier代码。 我们目前将标记和重新定位相关信息存储在彩色指针中。然而，此方案的通用性允许我们存储任何类型的信息(只要我们能将其放入指针中)，并允许内存屏障根据该信息采取它想要采取的任何操作。我们相信这将为将来的许多特性打下基础。举一个例子，在异构内存环境中，这可以用来跟踪堆访问模式，以指导GC重新定位决策，将很少使用的对象移动到冷存储(不常访问的内存区域)中【原文：To pick one example, in a heterogeneous memory environment, this could be used to track heap access patterns to guide GC relocation decisions to move rarely used objects to cold storage.】。 五、性能我们已经使用SPECjbb 2015[1]做了常规性能测试。从吞吐量和延迟角度来看，性能都很好。下面是使用128G堆在复合模式下比较ZGC和G1的典型基准分数(以百分比为单位，根据ZGC的max-jOPS进行标准化)【原文：Below are typical benchmark scores (in percent, normalized against ZGC’s max-jOPS), comparing ZGC and G1, in composite mode using a 128G heap.】： 越高越好 1234567ZGC max-jOPS: 100% critical-jOPS: 76.1%G1 max-jOPS: 91.2% critical-jOPS: 54.7% 下面是来自相同基准测试的GC暂停时间。ZGC设法保持远低于10ms的目标。注意，确切的数字可能会根据使用的机器和设置而变化(上下都有，但不是很明显)。 越低越好 123456789101112131415ZGC avg: 1.091ms (+/-0.215ms) 95th percentile: 1.380ms 99th percentile: 1.512ms 99.9th percentile: 1.663ms 99.99th percentile: 1.681ms max: 1.681msG1 avg: 156.806ms (+/-71.126ms) 95th percentile: 316.672ms 99th percentile: 428.095ms 99.9th percentile: 543.846ms 99.99th percentile: 543.846ms max: 543.846ms 我们还对其他各种SPEC®基准测试和内部工作负载进行了特别的性能测量。一般情况下，ZGC能够维护个位数的毫秒暂停时间。 六、 局限性ZGC的初始实验版本将不支持类卸载。默认情况下，classunload和ClassUnloadingWithConcurrentMark选项将被禁用。即便你启用也是不生效的。 此外，ZGC最初不支持JVMCI(即Graal)。如果启用EnableJVMCI选项，将打印一条错误消息。 这些限制将在本项目的后期解决。 七、 构建和使用按照惯例，构建系统默认禁用JVM中的实验性特性。ZGC是一个实验性特性，因此不会出现在JDK构建中，除非在编译时使用configure选项: 1--with-jvm-features=zgc 显式地启用它。 (ZGC将出现在Oracle发布的所有Linux/x64 JDK版本中) JVM中的实验特性还需要在运行时显式地解锁。因此，要启用/使用ZGC，需要以下JVM选项: 1-XX:+ unlockexperimental alvmoptions -XX:+UseZGC 有关如何设置和调优ZGC的更多信息，请参阅ZGC项目Wiki（wiki地址：https://wiki.openjdk.java.net/display/zgc/Main）。 ZGC paper可以参考Azul Pauseless GC Algorithm： https://github.com/jiankunking/books-recommendation/blob/master/GC/ZGC/Azul_Pauseless_GC_Algorithm.pdf ZGC 简介PPT: https://github.com/jiankunking/books-recommendation/blob/master/GC/ZGC/ZGC-FOSDEM-2018.pdf]]></content>
      <categories>
        <category>GC</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>Java</tag>
        <tag>GC</tag>
        <tag>ZGC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 查询的秘密]]></title>
    <url>%2Felasticsearch-query-secret.html</url>
    <content type="text"><![CDATA[最近在参与一个基于Elasticsearch作为底层数据框架提供大数据量(亿级)的实时统计查询的方案设计工作，花了些时间学习Elasticsearch的基础理论知识，整理了一下，希望能对Elasticsearch感兴趣/想了解的同学有所帮助。同时也希望有发现内容不正确或者有疑问的地方，望指明，一起探讨，学习，进步。 介绍 Elasticsearch 是一个分布式可扩展的实时搜索和分析引擎. Elasticsearch 是一个建立在全文搜索引擎 Apache Lucene(TM) 基础上的搜索引擎.当然 Elasticsearch 并不仅仅是 Lucene 那么简单，它不仅包括了全文搜索功能，还可以进行以下工作: 分布式实时文件存储，并将每一个字段都编入索引，使其可以被搜索。 实时分析的分布式搜索引擎。 可以扩展到上百台服务器，处理PB级别的结构化或非结构化数据。 基本概念先说Elasticsearch的文件存储，Elasticsearch是面向文档型数据库，一条数据在这里就是一个文档，用JSON作为文档序列化的格式，比如下面这条用户数据： 12345678&#123; "name" : "John", "sex" : "Male", "age" : 25, "birthDate": "1990/05/01", "about" : "I love to go rock climbing", "interests": [ "sports", "music" ]&#125; 用Mysql这样的数据库存储就会容易想到建立一张User表，有balabala的字段等，在Elasticsearch里这就是一个文档，当然这个文档会属于一个User的类型，各种各样的类型存在于一个索引当中。这里有一份简易的将Elasticsearch和关系型数据术语对照表: 关系数据库 ⇒ 数据库 ⇒ 表 ⇒ 行 ⇒ 列(Columns) Elasticsearch ⇒ 索引 ⇒ 类型 ⇒ 文档 ⇒ 字段(Fields) 一个 Elasticsearch 集群可以包含多个索引(数据库)，也就是说其中包含了很多类型(表)。这些类型中包含了很多的文档(行)，然后每个文档中又包含了很多的字段(列)。 Elasticsearch的交互，可以使用Java API，也可以直接使用HTTP的Restful API方式，比如我们打算插入一条记录，可以简单发送一个HTTP的请求： 12345678PUT /megacorp/employee/1&#123; "name" : "John", "sex" : "Male", "age" : 25, "about" : "I love to go rock climbing", "interests": [ "sports", "music" ]&#125; 更新，查询也是类似这样的操作，具体操作手册可以参见Elasticsearch权威指南 索引Elasticsearch最关键的就是提供强大的索引能力了，其实InfoQ的这篇时间序列数据库的秘密(2)——索引写的非常好，我这里也是围绕这篇结合自己的理解进一步梳理下，也希望可以帮助大家更好的理解这篇文章。 Elasticsearch索引的精髓： 一切设计都是为了提高搜索的性能 另一层意思：为了提高搜索的性能，难免会牺牲某些其他方面，比如插入/更新，否则其他数据库不用混了:) 前面看到往Elasticsearch里插入一条记录，其实就是直接PUT一个json的对象，这个对象有多个fields，比如上面例子中的name, sex, age, about, interests，那么在插入这些数据到Elasticsearch的同时，Elasticsearch还默默^1的为这些字段建立索引–倒排索引，因为Elasticsearch最核心功能是搜索。 Elasticsearch是如何做到快速索引的InfoQ那篇文章里说Elasticsearch使用的倒排索引比关系型数据库的B-Tree索引快，为什么呢？ 什么是B-Tree索引?上大学读书时老师教过我们，二叉树查找效率是logN，同时插入新的节点不必移动全部节点，所以用树型结构存储索引，能同时兼顾插入和查询的性能。 因此在这个基础上，再结合磁盘的读取特性(顺序读/随机读)，传统关系型数据库采用了B-Tree/B+Tree这样的数据结构： 为了提高查询的效率，减少磁盘寻道次数，将多个值作为一个数组通过连续区间存放，一次寻道读取多个数据，同时也降低树的高度。 什么是倒排索引? 继续上面的例子，假设有这么几条数据(为了简单，去掉about, interests这两个field): ID Name Age Sex 1 Kate 24 Female 2 John 24 Male 3 Bill 29 Male ID是Elasticsearch自建的文档id，那么Elasticsearch建立的索引如下: Name: Term Posting List Kate 1 John 2 Bill 3 Age: Term Posting List 24 [1,2] 29 3 Sex: Term Posting List Female 1 Male [2,3] Posting ListElasticsearch分别为每个field都建立了一个倒排索引，Kate, John, 24, Female这些叫term，而[1,2]就是Posting List。Posting list就是一个int的数组，存储了所有符合某个term的文档id。 看到这里，不要认为就结束了，精彩的部分才刚开始… 通过posting list这种索引方式似乎可以很快进行查找，比如要找age=24的同学，爱回答问题的小明马上就举手回答：我知道，id是1，2的同学。但是，如果这里有上千万的记录呢？如果是想通过name来查找呢？ Term DictionaryElasticsearch为了能快速找到某个term，将所有的term排个序，二分法查找term，logN的查找效率，就像通过字典查找一样，这就是Term Dictionary。现在再看起来，似乎和传统数据库通过B-Tree的方式类似啊，为什么说比B-Tree的查询快呢？ Term IndexB-Tree通过减少磁盘寻道次数来提高查询性能，Elasticsearch也是采用同样的思路，直接通过内存查找term，不读磁盘，但是如果term太多，term dictionary也会很大，放内存不现实，于是有了Term Index，就像字典里的索引页一样，A开头的有哪些term，分别在哪页，可以理解term index是一颗树： 这棵树不会包含所有的term，它包含的是term的一些前缀。通过term index可以快速地定位到term dictionary的某个offset，然后从这个位置再往后顺序查找。 所以term index不需要存下所有的term，而仅仅是他们的一些前缀与Term Dictionary的block之间的映射关系，再结合FST(Finite State Transducers)的压缩技术，可以使term index缓存到内存中。从term index查到对应的term dictionary的block位置之后，再去磁盘上找term，大大减少了磁盘随机读的次数。 这时候爱提问的小明又举手了:”那个FST是神马东东啊?” 一看就知道小明是一个上大学读书的时候跟我一样不认真听课的孩子，数据结构老师一定讲过什么是FST。但没办法，我也忘了，这里再补下课： FSTs are finite-state machines that map a term (byte sequence) to an arbitrary output. 假设我们现在要将mop, moth, pop, star, stop and top(term index里的term前缀)映射到序号：0，1，2，3，4，5(term dictionary的block位置)。最简单的做法就是定义个Map&lt;String, Integer&gt;，大家找到自己的位置对应入座就好了，但从内存占用少的角度想想，有没有更优的办法呢？答案就是：FST(理论依据在此，但我相信99%的人不会认真看完的) ⭕️表示一种状态 –&gt;表示状态的变化过程，上面的字母/数字表示状态变化和权重 将单词分成单个字母通过⭕️和–&gt;表示出来，0权重不显示。如果⭕️后面出现分支，就标记权重，最后整条路径上的权重加起来就是这个单词对应的序号。 FSTs are finite-state machines that map a term (byte sequence) to an arbitrary output. FST以字节的方式存储所有的term，这种压缩方式可以有效的缩减存储空间，使得term index足以放进内存，但这种方式也会导致查找时需要更多的CPU资源。 后面的更精彩，看累了的同学可以喝杯咖啡…… 压缩技巧Elasticsearch里除了上面说到用FST压缩term index外，对posting list也有压缩技巧。小明喝完咖啡又举手了:”posting list不是已经只存储文档id了吗？还需要压缩？” 嗯，我们再看回最开始的例子，如果Elasticsearch需要对同学的性别进行索引(这时传统关系型数据库已经哭晕在厕所……)，会怎样？如果有上千万个同学，而世界上只有男/女这样两个性别，每个posting list都会有至少百万个文档id。Elasticsearch是如何有效的对这些文档id压缩的呢？ Frame Of Reference 增量编码压缩，将大数变小数，按字节存储 首先，Elasticsearch要求posting list是有序的(为了提高搜索的性能，再任性的要求也得满足)，这样做的一个好处是方便压缩，看下面这个图例： 如果数学不是体育老师教的话，还是比较容易看出来这种压缩技巧的。 原理就是通过增量，将原来的大数变成小数仅存储增量值，再精打细算按bit排好队，最后通过字节存储，而不是大大咧咧的尽管是2也是用int(4个字节)来存储。 Roaring bitmaps说到Roaring bitmaps，就必须先从bitmap说起。Bitmap是一种数据结构，假设有某个posting list： [1,3,4,7,10] 对应的bitmap就是： [1,0,1,1,0,0,1,0,0,1] 非常直观，用0/1表示某个值是否存在，比如10这个值就对应第10位，对应的bit值是1，这样用一个字节就可以代表8个文档id，旧版本(5.0之前)的Lucene就是用这样的方式来压缩的，但这样的压缩方式仍然不够高效，如果有1亿个文档，那么需要12.5MB的存储空间，这仅仅是对应一个索引字段(我们往往会有很多个索引字段)。于是有人想出了Roaring bitmaps这样更高效的数据结构。 Bitmap的缺点是存储空间随着文档个数线性增长，Roaring bitmaps需要打破这个魔咒就一定要用到某些指数特性： 将posting list按照65535为界限分块，比如第一块所包含的文档id范围在065535之间，第二块的id范围是65536131071，以此类推。再用&lt;商，余数&gt;的组合表示每一组id，这样每组里的id范围都在0~65535内了，剩下的就好办了，既然每组id不会变得无限大，那么我们就可以通过最有效的方式对这里的id存储。 细心的小明这时候又举手了:”为什么是以65535为界限?” 程序员的世界里除了1024外，65535也是一个经典值，因为它=2^16-1，正好是用2个字节能表示的最大数，一个short的存储单位，注意到上图里的最后一行“If a block has more than 4096 values, encode as a bit set, and otherwise as a simple array using 2 bytes per value”，如果是大块，用节省点用bitset存，小块就豪爽点，2个字节我也不计较了，用一个short[]存着方便。 那为什么用4096来区分采用数组还是bitmap的阀值呢？ 这个是从内存大小考虑的，当block块里元素超过4096后，用bitmap更剩空间：采用bitmap需要的空间是恒定的: 65536/8 = 8192bytes而如果采用short[]，所需的空间是: 2*N(N为数组元素个数)小明手指一掐N=4096刚好是边界: 联合索引上面说了半天都是单field索引，如果多个field索引的联合查询，倒排索引如何满足快速查询的要求呢？ 利用跳表(Skip list)的数据结构快速做“与”运算，或者 利用上面提到的bitset按位“与” 先看看跳表的数据结构： 将一个有序链表level0，挑出其中几个元素到level1及level2，每个level越往上，选出来的指针元素越少，查找时依次从高level往低查找，比如55，先找到level2的31，再找到level1的47，最后找到55，一共3次查找，查找效率和2叉树的效率相当，但也是用了一定的空间冗余来换取的。 假设有下面三个posting list需要联合索引： 如果使用跳表，对最短的posting list中的每个id，逐个在另外两个posting list中查找看是否存在，最后得到交集的结果。 如果使用bitset，就很直观了，直接按位与，得到的结果就是最后的交集。 总结和思考Elasticsearch的索引思路: 将磁盘里的东西尽量搬进内存，减少磁盘随机读取次数(同时也利用磁盘顺序读特性)，结合各种奇技淫巧的压缩算法，用及其苛刻的态度使用内存。 所以，对于使用Elasticsearch进行索引时需要注意: 不需要索引的字段，一定要明确定义出来，因为默认是自动建索引的 同样的道理，对于String类型的字段，不需要analysis的也需要明确定义出来，因为默认也是会analysis的 选择有规律的ID很重要，随机性太大的ID(比如java的UUID)不利于查询 关于最后一点，个人认为有多个因素: 其中一个(也许不是最重要的)因素: 上面看到的压缩算法，都是对Posting list里的大量ID进行压缩的，那如果ID是顺序的，或者是有公共前缀等具有一定规律性的ID，压缩比会比较高； 另外一个因素: 可能是最影响查询性能的，应该是最后通过Posting list里的ID到磁盘中查找Document信息的那步，因为Elasticsearch是分Segment存储的，根据ID这个大范围的Term定位到Segment的效率直接影响了最后查询的性能，如果ID是有规律的，可以快速跳过不包含该ID的Segment，从而减少不必要的磁盘读次数，具体可以参考这篇如何选择一个高效的全局ID方案(评论也很精彩) 后续再结合实际开发及调优工作分享更多内容，敬请期待！ 原文Elasticsearch 学习笔记]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>Query</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Epoll 的本质是什么？]]></title>
    <url>%2Fepoll-principle.html</url>
    <content type="text"><![CDATA[从事服务端开发，少不了要接触网络编程。epoll 作为 Linux 下高性能网络服务器的必备技术至关重要，nginx、Redis、Skynet 和大部分游戏服务器都使用到这一多路复用技术。 epoll 很重要，但是 epoll 与 select 的区别是什么呢？epoll 高效的原因是什么？ 网上虽然也有不少讲解 epoll的文章，但要么是过于浅显，或者陷入源码解析，很少能有通俗易懂的。笔者于是决定编写此文，让缺乏专业背景知识的读者也能够明白 epoll 的原理。 文章核心思想是：要让读者清晰明白 epoll 为什么性能好。 本文会从网卡接收数据的流程讲起，串联起 CPU 中断、操作系统进程调度等知识；再一步步分析阻塞接收数据、select 到 epoll 的进化过程；最后探究 epoll 的实现细节。 一、从网卡接收数据说起下边是一个典型的计算机结构图，计算机由 CPU、存储器（内存）与网络接口等部件组成，了解 epoll 本质的第一步，要从硬件的角度看计算机怎样接收网络数据。计算机结构图（图片来源：Linux内核完全注释之微型计算机组成结构） 下图展示了网卡接收数据的过程。 在 ① 阶段，网卡收到网线传来的数据； 经过 ② 阶段的硬件电路的传输； 最终 ③ 阶段将数据写入到内存中的某个地址上。 这个过程涉及到 DMA 传输、IO 通路选择等硬件有关的知识，但我们只需知道：网卡会把接收到的数据写入内存。 网卡接收数据的过程 通过硬件传输，网卡接收的数据存放到内存中，操作系统就可以去读取它们。 二、如何知道接收了数据？了解 epoll 本质的第二步，要从 CPU 的角度来看数据接收。理解这个问题，要先了解一个概念——中断。 计算机执行程序时，会有优先级的需求。比如，当计算机收到断电信号时，它应立即去保存数据，保存数据的程序具有较高的优先级（电容可以保存少许电量，供 CPU 运行很短的一小段时间）。 一般而言，由硬件产生的信号需要 CPU 立马做出回应，不然数据可能就丢失了，所以它的优先级很高。CPU 理应中断掉正在执行的程序，去做出响应；当 CPU 完成对硬件的响应后，再重新执行用户程序。中断的过程如下图，它和函数调用差不多，只不过函数调用是事先定好位置，而中断的位置由“信号”决定。中断程序调用 以键盘为例，当用户按下键盘某个按键时，键盘会给 CPU 的中断引脚发出一个高电平，CPU能够捕获这个信号，然后执行键盘中断程序。下图展示了各种硬件通过中断与 CPU 交互的过程。CPU 中断（图片来源：net.pku.edu.cn） 现在可以回答“如何知道接收了数据？”这个问题了：当网卡把数据写入到内存后，网卡向 CPU 发出一个中断信号，操作系统便能得知有新数据到来，再通过网卡中断程序去处理数据。 三、进程阻塞为什么不占用 CPU 资源？了解 epoll 本质的第三步，要从操作系统进程调度的角度来看数据接收。阻塞是进程调度的关键一环，指的是进程在等待某事件（如接收到网络数据）发生之前的等待状态，recv、select 和 epoll 都是阻塞方法。下边分析一下进程阻塞为什么不占用 CPU 资源？ 为简单起见，我们从普通的 recv 接收开始分析，先看看下面代码： 123456789101112//创建socketint s = socket(AF_INET, SOCK_STREAM, 0); //绑定bind(s, ...)//监听listen(s, ...)//接受客户端连接int c = accept(s, ...)//接收客户端数据recv(c, ...);//将数据打印出来printf(...) 这是一段最基础的网络编程代码，先新建 socket 对象，依次调用 bind、listen 与 accept，最后调用 recv 接收数据。recv 是个阻塞方法，当程序运行到 recv 时，它会一直等待，直到接收到数据才往下执行。 那么阻塞的原理是什么？ 工作队列操作系统为了支持多任务，实现了进程调度的功能，会把进程分为“运行”和“等待”等几种状态。运行状态是进程获得 CPU 使用权，正在执行代码的状态；等待状态是阻塞状态，比如上述程序运行到 recv 时，程序会从运行状态变为等待状态，接收到数据后又变回运行状态。操作系统会分时执行各个运行状态的进程，由于速度很快，看上去就像是同时执行多个任务。 下图的计算机中运行着 A、B 与 C 三个进程，其中进程 A 执行着上述基础网络程序，一开始，这 3 个进程都被操作系统的工作队列所引用，处于运行状态，会分时执行。工作队列中有 A、B 和 C 三个进程 等待队列当进程 A 执行到创建 socket 的语句时，操作系统会创建一个由文件系统管理的 socket 对象（如下图）。这个 socket 对象包含了发送缓冲区、接收缓冲区与等待队列等成员。等待队列是个非常重要的结构，它指向所有需要等待该 socket 事件的进程。创建 socket 当程序执行到 recv 时，操作系统会将进程 A 从工作队列移动到该 socket 的等待队列中（如下图）。由于工作队列只剩下了进程 B 和 C，依据进程调度，CPU 会轮流执行这两个进程的程序，不会执行进程 A 的程序。所以进程 A 被阻塞，不会往下执行代码，也不会占用 CPU 资源。socket 的等待队列 注：操作系统添加等待队列只是添加了对这个“等待中”进程的引用，以便在接收到数据时获取进程对象、将其唤醒，而非直接将进程管理纳入自己之下。上图为了方便说明，直接将进程挂到等待队列之下。 唤醒进程当 socket 接收到数据后，操作系统将该 socket 队列上的进程重新放回到工作队列，该进程变成运行状态，继续执行代码。同时由于 socket 的接收缓冲区已经有了数据，recv 可以返回接收到的数据。 四、内核接收网络数据全过程这一步，贯穿网卡、中断与进程调度的知识，叙述阻塞 recv 下，内核接收数据的全过程。 如下图所示，进程在 recv 阻塞期间，计算机收到了对端传送的数据（步骤①），数据经由网卡传送到内存（步骤②），然后网卡通过中断信号通知 CPU 有数据到达，CPU 执行中断程序（步骤③）。 此处的中断程序主要有两项功能，先将网络数据写入到对应 socket 的接收缓冲区里面（步骤④），再唤醒进程 A（步骤⑤），重新将进程 A 放入工作队列中。内核接收数据全过程 唤醒进程的过程如下图所示：唤醒进程 以上是内核接收数据全过程，这里我们可能会思考两个问题： 其一，操作系统如何知道网络数据对应于哪个 socket？ 其二，如何同时监视多个 socket 的数据？ 第一个问题：因为一个 socket 对应着一个端口号，而网络数据包中包含了 ip 和端口的信息，内核可以通过端口号找到对应的 socket。当然，为了提高处理速度，操作系统会维护端口号到 socket 的索引结构，以快速读取。 第二个问题是多路复用的重中之重，也正是本文后半部分的重点。 五、同时监视多个 socket 的简单方法服务端需要管理多个客户端连接，而 recv 只能监视单个 socket，这种矛盾下，人们开始寻找监视多个 socket 的方法。epoll的要义就是高效地监视多个 socket。 从历史发展角度看，必然先出现一种不太高效的方法，人们再加以改进，正如 select 之于 epoll。 先理解不太高效的 select，才能够更好地理解 epoll 的本质。 假如能够预先传入一个 socket 列表，如果列表中的 socket 都没有数据，挂起进程，直到有一个 socket 收到数据，唤醒进程。这种方法很直接，也是 select 的设计思想。 为方便理解，我们先复习 select 的用法。在下边的代码中，先准备一个数组 fds，让 fds 存放着所有需要监视的 socket。然后调用 select，如果 fds 中的所有 socket 都没有数据，select 会阻塞，直到有一个 socket 接收到数据，select 返回，唤醒进程。用户可以遍历 fds，通过 FD_ISSET 判断具体哪个 socket 收到数据，然后做出处理。 1234567891011int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...);listen(s, ...);int fds[] = 存放需要监听的socket;while(1)&#123; int n = select(..., fds, ...) for(int i=0; i &lt; fds.count; i++)&#123; if(FD_ISSET(fds[i], ...))&#123; //fds[i]的数据处理 &#125; &#125;&#125; select 的流程select 的实现思路很直接，假如程序同时监视如下图的 sock1、sock2 和 sock3 三个 socket，那么在调用 select 之后，操作系统把进程 A 分别加入这三个 socket 的等待队列中。操作系统把进程 A 分别加入这三个 socket 的等待队列中 当任何一个 socket 收到数据后，中断程序将唤起进程。下图展示了 sock2 接收到了数据的处理流程： 注：recv 和 select 的中断回调可以设置成不同的内容。 sock2 接收到了数据，中断程序唤起进程 A 所谓唤起进程，就是将进程从所有的等待队列中移除，加入到工作队列里面，如下图所示：将进程 A 从所有等待队列中移除，再加入到工作队列里面 经由这些步骤，当进程 A 被唤醒后，它知道至少有一个 socket 接收了数据。程序只需遍历一遍 socket 列表，就可以得到就绪的 socket。 这种简单方式行之有效，在几乎所有操作系统都有对应的实现。 但是简单的方法往往有缺点，主要是： 其一，每次调用 select 都需要将进程加入到所有监视 socket 的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个 fds 列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定 select 的最大监视数量，默认只能监视 1024 个 socket。 其二，进程被唤醒后，程序并不知道哪些 socket 收到数据，还需要遍历一次。 那么，有没有减少遍历的方法？有没有保存就绪 socket 的方法？这两个问题便是 epoll 技术要解决的。 补充说明： 本节只解释了 select 的一种情形。当程序调用 select 时，内核会先遍历一遍 socket，如果有一个以上的 socket 接收缓冲区有数据，那么 select 直接返回，不会阻塞。这也是为什么 select 的返回值有可能大于 1 的原因之一。如果没有 socket 有数据，进程才会阻塞。 六、epoll 的设计思路epoll 是在 select 出现 N 多年后才被发明的，是 select 和 poll（poll 和 select 基本一样，有少量改进）的增强版本。epoll 通过以下一些措施来改进效率： 措施一：功能分离select 低效的原因之一是将“维护等待队列”和“阻塞进程”两个步骤合二为一。如下图所示，每次调用 select 都需要这两步操作，然而大多数应用场景中，需要监视的 socket 相对固定，并不需要每次都修改。epoll 将这两个操作分开，先用 epoll_ctl 维护等待队列，再调用 epoll_wait 阻塞进程。显而易见地，效率就能得到提升。 相比 select，epoll 拆分了功能 为方便理解后续的内容，我们先了解一下 epoll 的用法。如下的代码中，先用 epoll_create 创建一个 epoll 对象 epfd，再通过 epoll_ctl 将需要监视的 socket 添加到 epfd 中，最后调用 epoll_wait 等待数据： 12345678910111213int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...)listen(s, ...)int epfd = epoll_create(...);epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中while(1)&#123; int n = epoll_wait(...) for(接收到数据的socket)&#123; //处理 &#125;&#125; 功能分离，使得 epoll 有了优化的可能。 措施二：就绪列表select 低效的另一个原因在于程序不知道哪些 socket 收到数据，只能一个个遍历。如果内核维护一个“就绪列表”，引用收到数据的 socket，就能避免遍历。如下图所示，计算机共有三个 socket，收到数据的 sock2 和 sock3 被就绪列表 rdlist 所引用。当进程被唤醒后，只要获取 rdlist 的内容，就能够知道哪些 socket 收到数据。就绪列表示意图 七、epoll 的原理与工作流程本节会以示例和图表来讲解 epoll 的原理和工作流程。 创建 epoll 对象如下图所示，当某个进程调用 epoll_create 方法时，内核会创建一个 eventpoll 对象（也就是程序中 epfd 所代表的对象）。eventpoll 对象也是文件系统中的一员，和 socket 一样，它也会有等待队列。内核创建 eventpoll 对象 创建一个代表该 epoll 的 eventpoll 对象是必须的，因为内核要维护“就绪列表”等数据，“就绪列表”可以作为 eventpoll 的成员。 维护监视列表创建 epoll 对象后，可以用 epoll_ctl 添加或删除所要监听的 socket。以添加 socket 为例，如下图，如果通过 epoll_ctl 添加 sock1、sock2 和 sock3 的监视，内核会将 eventpoll 添加到这三个 socket 的等待队列中。添加所要监听的 socket 当 socket 收到数据后，中断程序会操作 eventpoll 对象，而不是直接操作进程。 接收数据当 socket 收到数据后，中断程序会给 eventpoll 的“就绪列表”添加 socket 引用。如下图展示的是 sock2 和 sock3 收到数据后，中断程序让 rdlist 引用这两个 socket。给就绪列表添加引用 eventpoll 对象相当于 socket 和进程之间的中介，socket 的数据接收并不直接影响进程，而是通过改变 eventpoll 的就绪列表来改变进程状态。 当程序执行到 epoll_wait 时，如果 rdlist 已经引用了 socket，那么 epoll_wait 直接返回，如果 rdlist 为空，阻塞进程。 阻塞和唤醒进程假设计算机中正在运行进程 A 和进程 B，在某时刻进程 A 运行到了 epoll_wait 语句。如下图所示，内核会将进程 A 放入 eventpoll 的等待队列中，阻塞进程。epoll_wait 阻塞进程 当 socket 接收到数据，中断程序一方面修改 rdlist，另一方面唤醒 eventpoll 等待队列中的进程，进程 A 再次进入运行状态（如下图）。也因为 rdlist 的存在，进程 A 可以知道哪些 socket 发生了变化。epoll 唤醒进程 八、epoll 的实现细节至此，相信读者对 epoll 的本质已经有一定的了解。但我们还需要知道 eventpoll 的数据结构是什么样子？ 此外，就绪队列应该应使用什么数据结构？eventpoll 应使用什么数据结构来管理通过 epoll_ctl 添加或删除的 socket？ 如下图所示，eventpoll 包含了 lock、mtx、wq（等待队列）与 rdlist 等成员，其中 rdlist 和 rbr 是我们所关心的。epoll 原理示意图，图片来源：《深入理解Nginx：模块开发与架构解析(第二版)》，陶辉 就绪列表的数据结构就绪列表引用着就绪的 socket，所以它应能够快速的插入数据。 程序可能随时调用 epoll_ctl 添加监视 socket，也可能随时删除。当删除时，若该 socket 已经存放在就绪列表中，它也应该被移除。所以就绪列表应是一种能够快速插入和删除的数据结构。 双向链表就是这样一种数据结构，epoll 使用双向链表来实现就绪队列（对应上图的 rdllist）。 索引结构既然 epoll 将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的 socket，至少要方便地添加和移除，还要便于搜索，以避免重复添加。红黑树是一种自平衡二叉查找树，搜索、插入和删除时间复杂度都是O(log(N))，效率较好，epoll 使用了红黑树作为索引结构（对应上图的 rbr）。 注：因为操作系统要兼顾多种功能，以及由更多需要保存的数据，rdlist 并非直接引用 socket，而是通过 epitem 间接引用，红黑树的节点也是 epitem 对象。同样，文件系统也并非直接引用着 socket。为方便理解，本文中省略了一些间接结构。 九、小结epoll 在 select 和 poll 的基础上引入了 eventpoll 作为中间层，使用了先进的数据结构，是一种高效的多路复用技术。这里也以表格形式简单对比一下 select、poll 与 epoll，结束此文。希望读者能有所收获。 原文地址：https://my.oschina.net/editorial-story/blog/3052308]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Epoll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 原子操作的实现原理]]></title>
    <url>%2Fjava-atomic-operation-principle.html</url>
    <content type="text"><![CDATA[本文整理自《Java并发编程的艺术》第二章 作者：方腾飞 魏鹏 程晓明 原子（atomic）本意是“不能被进一步分割的最小粒子”，而原子操作（atomic operation）意为“不可被中断的一个或一系列操作”。在多处理器上实现原子操作就变得有点复杂。让我们一起来聊一聊在Intel处理器和Java里是如何实现原子操作的。 术语定义在了解原子操作的实现原理前，先要了解一下相关的术语: table th:first-of-type { width: 100px; } table th:nth-of-type(2) { width: 150px; } 术语名称 英文 解释 缓存行 Cache line 缓存的最小操作单位 比较并交换 Compare and Swap CAS操作需要输入两个数值，一个旧值(期望操作前的值)和一个新值，在操作期间先比较旧值有没有发生变化，如果没有发生变化，才交换成新值，发生了变化则不交换。 CPU流水线 CPU pipeline CPU流水线的工作方式就像工业生产上的装配流水线，在CPU中由5~6个不同功能的电路单元组成一条指令处理流水线，然后将一条X86指令分成5~6步后再由这些电路单元分别执行，这样就能实现在一个CPU时钟周期完成一条指令，因此提高CPU的运算速度 内存顺序冲突 Memory order violation 内存顺序冲突一般是由假共享引起的，假共享是指多个CPU同时修改同一个缓存行的不同部分而引起其中一个CPU的操作无效，当出现这个内存顺序冲突时，CPU必须清空流水线 处理器如何实现原子操作32位IA-32处理器使用基于对缓存加锁或总线加锁的方式来实现多处理器之间的原子操作。首先处理器会自动保证基本的内存操作的原子性。处理器保证从系统内存中读取或者写入一个字节是原子的，意思是当一个处理器读取一个字节时，其他处理器不能访问这个字节的内存地址。Pentium 6和最新的处理器能自动保证单处理器对同一个缓存行里进行16/32/64位的操作是原子的，但是复杂的内存操作处理器是不能自动保证其原子性的，比如跨总线宽度、跨多个缓存行和跨页表的访问。但是，处理器提供总线锁定和缓存锁定两个机制来保证复杂内存操作的原子性。 在Intel 2019年的文档中，该部分阐述基本不变，具体可以查考文末Intel文档的2957页 8.1 LOCKED ATOMIC OPERATIONS 使用总线锁保证原子性第一个机制是通过总线锁保证原子性。如果多个处理器同时对共享变量进行读改写操作（i++就是经典的读改写操作），那么共享变量就会被多个处理器同时进行操作，这样读改写操作就不是原子的，操作完之后共享变量的值会和期望的不一致。举个例子，如果i=1，我们进行两次i++操作，我们期望的结果是3，但是有可能结果是2，如图2-3所示。 原因可能是多个处理器同时从各自的缓存中读取变量i，分别进行加1操作，然后分别写入系统内存中。那么，想要保证读改写共享变量的操作是原子的，就必须保证CPU1读改写共享变量的时候，CPU2不能操作缓存了该共享变量内存地址的缓存。处理器使用总线锁就是来解决这个问题的。所谓总线锁就是使用处理器提供的一个LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住，那么该处理器可以独占共享内存。 Intel文档的2959页 8.1.2 Bus Locking 使用缓存锁保证原子性第二个机制是通过缓存锁定来保证原子性。在同一时刻，我们只需保证对某个内存地址的操作是原子性即可，但总线锁定把CPU和内存之间的通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，目前处理器在某些场合下使用缓存锁定代替总线锁定来进行优化。 频繁使用的内存会缓存在处理器的L1、L2和L3高速缓存里，那么原子操作就可以直接在处理器内部缓存中进行，并不需要声明总线锁，在Pentium 6和目前的处理器中可以使用“缓存锁定”的方式来实现复杂的原子性。所谓“缓存锁定”是指内存区域如果被缓存在处理器的缓存行中，并且在Lock操作期间被锁定，那么当它执行锁操作回写到内存时，处理器不在总线上声言LOCK＃信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子性，因为缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据，当其他处理器回写已被锁定的缓存行的数据时，会使缓存行无效，在如图2-3所示的例子中，当CPU1修改缓存行中的i时使用了缓存锁定，那么CPU2就不能同时缓存i的缓存行。 但是有两种情况下处理器不会使用缓存锁定： 第一种情况是：当操作的数据不能被缓存在处理器内部，或操作的数据跨多个缓存行（cache line）时，则处理器会调用总线锁定。 第二种情况是：有些处理器不支持缓存锁定。对于Intel 486和Pentium处理器，就算锁定的内存区域在处理器的缓存行中也会调用总线锁定。针对以上两个机制，我们通过Intel处理器提供了很多Lock前缀的指令来实现。例如，位测试和修改指令：BTS、BTR、BTC；交换指令XADD、CMPXCHG，以及其他一些操作数和逻辑指令（如ADD、OR）等，被这些指令操作的内存区域就会加锁，导致其他处理器不能同时访问它。 Intel文档的2961页 8.1.4 Effects of a LOCK Operation on Internal Processor Caches Java如何实现原子操作在Java中可以通过锁和循环CAS的方式来实现原子操作。 使用循环CAS实现原子操作JVM中的CAS操作正是利用了处理器提供的CMPXCHG(Compare and Exchange)指令实现的。自旋CAS实现的基本思路就是循环进行CAS操作直到成功为止，以下代码实现了一个基于CAS线程安全的计数器方法safeCount和一个非线程安全的计数器count。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354private AtomicInteger atomicI = new AtomicInteger(0);private int i = 0;public static void main(String[] args) &#123; final Counter cas = new Counter(); List&lt;Thread&gt; ts = new ArrayList&lt;Thread&gt;(600); long start = System.currentTimeMillis(); for (int j = 0; j &lt; 100; j++) &#123; Thread t = new Thread(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10000; i++) &#123; cas.count(); cas.safeCount(); &#125; &#125; &#125;); ts.add(t); &#125; for (Thread t : ts) &#123; t.start(); &#125; // 等待所有线程执行完成 for (Thread t : ts) &#123; try &#123; t.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(cas.i); System.out.println(cas.atomicI.get()); System.out.println(System.currentTimeMillis() - start);&#125;/** * 使用CAS实现线程安全计数器 */private void safeCount() &#123; for (; ; ) &#123; int i = atomicI.get(); boolean suc = atomicI.compareAndSet(i, ++i); if (suc) &#123; break; &#125; &#125;&#125;/** * 非线程安全计数器 */private void count() &#123; i++;&#125; 从Java 1.5开始，JDK的并发包里提供了一些类来支持原子操作，如AtomicBoolean（用原子方式更新的boolean值）、AtomicInteger（用原子方式更新的int值）和AtomicLong（用原子方式更新的long值）。这些原子包装类还提供了有用的工具方法，比如以原子的方式将当前值自增1和自减1。 CAS实现原子操作的三大问题在Java并发包中有一些并发框架也使用了自旋CAS的方式来实现原子操作，比如LinkedTransferQueue类的Xfer方法。CAS虽然很高效地解决了原子操作，但是CAS仍然存在三大问题。ABA问题，循环时间长开销大，以及只能保证一个共享变量的原子操作。 ABA问题。因为CAS需要在操作值的时候，检查值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加1，那么A→B→A就会变成1A→2B→3A。从Java 1.5开始，JDK的Atomic包里提供了一个类AtomicStampedReference来解决ABA问题。这个类的compareAndSet方法的作用是首先检查当前引用是否等于预期引用，并且检查当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 123456public boolean compareAndSet( V expectedReference, // 预期引用 V newReference, // 更新后的引用 int expectedStamp, // 预期标志 int newStamp // 更新后的标志) 循环时间长开销大。自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令，那么效率会有一定的提升。pause指令有两个作用：第一，它可以延迟流水线执行指令（de-pipeline），使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零；第二，它可以避免在退出循环的时候因内存顺序冲突（Memory Order Violation）而引起CPU流水线被清空（CPU Pipeline Flush），从而提高CPU的执行效率。 只能保证一个共享变量的原子操作。当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁。还有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如，有两个共享变量i＝2，j=a，合并一下ij=2a，然后用CAS来操作ij。从Java 1.5开始，JDK提供了AtomicReference类来保证引用对象之间的原子性，就可以把多个变量放在一个对象里来进行CAS操作。 使用锁机制实现原子操作锁机制保证了只有获得锁的线程才能够操作锁定的内存区域。JVM内部实现了很多种锁机制，有偏向锁、轻量级锁和互斥锁。有意思的是除了偏向锁，JVM实现锁的方式都用了循环CAS，即当一个线程想进入同步块的时候使用循环CAS的方式来获取锁，当它退出同步块的时候使用循环CAS释放锁。 推荐阅读Intel® 64 and IA-32 architectures software developer’s manual]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>Concurrent</tag>
        <tag>Atomic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里分布式事务解决方案 Fescar 解析]]></title>
    <url>%2Falibaba-seata-design.html</url>
    <content type="text"><![CDATA[通过Fescar（Seata）了解分布式事务框架设计思路 Fescar 是 阿里巴巴 开源的 分布式事务中间件，以 高效 并且对业务 0 侵入 的方式，解决 微服务 场景下面临的分布式事务问题。 什么是微服务化带来的分布式事务问题？首先，设想一个传统的单体应用（Monolithic App），通过 3 个 Module，在同一个数据源上更新数据来完成一项业务。 很自然的，整个业务过程的数据一致性由本地事务来保证。 随着业务需求和架构的变化，单体应用被拆分为微服务：原来的 3 个 Module 被拆分为 3 个独立的服务，分别使用独立的数据源（Pattern: Database per service）。业务过程将由 3 个服务的调用来完成。 此时，每一个服务内部的数据一致性仍由本地事务来保证。而整个业务层面的全局数据一致性要如何保障呢？这就是微服务架构下面临的，典型的分布式事务需求：我们需要一个分布式事务的解决方案保障业务全局的数据一致性。 Fescar 的发展历程阿里是国内最早一批进行应用分布式（微服务化）改造的企业，所以很早就遇到微服务架构下的分布式事务问题。 2014 年，阿里中间件团队发布 TXC（Taobao Transaction Constructor），为集团内应用提供分布式事务服务。 2016 年，TXC 经过产品化改造，以 GTS（Global Transaction Service） 的身份登陆阿里云，成为当时业界唯一一款云上分布式事务产品，在阿云里的公有云、专有云解决方案中，开始服务于众多外部客户。 2019 年起，基于 TXC 和 GTS 的技术积累，阿里中间件团队发起了开源项目 Fescar（Fast &amp; EaSy Commit And Rollback, FESCAR），和社区一起建设这个分布式事务解决方案。 TXC/GTS/Fescar 一脉相承，为解决微服务架构下的分布式事务问题交出了一份与众不同的答卷。 设计初衷高速增长的互联网时代，快速试错 的能力对业务来说是至关重要的： 一方面，不应该因为技术架构上的微服务化和分布式事务支持的引入，给业务层面带来额外的研发负担。 另一方面，引入分布式事务支持的业务应该基本保持在同一量级上的性能表现，不能因为事务机制显著拖慢业务。 基于这两点，我们设计之初的最重要的考量就在于： 对业务无侵入： 这里的 侵入 是指，因为分布式事务这个技术问题的制约，要求应用在业务层面进行设计和改造。这种设计和改造往往会给应用带来很高的研发和维护成本。我们希望把分布式事务问题在 中间件 这个层次解决掉，不要求应用在业务层面做额外的工作。 高性能： 引入分布式事务的保障，必然会有额外的开销，引起性能的下降。我们希望把分布式事务引入的性能损耗降到非常低的水平，让应用不因为分布式事务的引入导致业务的可用性受影响。 既有的解决方案为什么不满足？既有的分布式事务解决方案按照对业务侵入性分为两类，即：对业务无侵入的和对业务有侵入的。 业务无侵入的方案既有的主流分布式事务解决方案中，对业务无侵入的只有基于 XA 的方案，但应用 XA 方案存在 3 个方面的问题： 要求数据库提供对 XA 的支持。如果遇到不支持 XA（或支持得不好，比如 MySQL 5.7 以前的版本）的数据库，则不能使用。 受协议本身的约束，事务资源（数据记录、数据库连接）的锁定周期长。长周期的资源锁定从业务层面来看，往往是不必要的，而因为事务资源的管理器是数据库本身，应用层无法插手。这样形成的局面就是，基于 XA 的应用往往性能会比较差，而且很难优化。 已经落地的基于 XA 的分布式解决方案，都依托于重量级的应用服务器（Tuxedo/WebLogic/WebSphere 等)，这是不适用于微服务架构的。 侵入业务的方案实际上，最初分布式事务只有 XA 这个唯一方案。XA 是完备的，但在实践过程中，由于种种原因（包含但不限于上面提到的 3 点）往往不得不放弃，转而从业务层面着手来解决分布式事务问题。比如： 基于可靠消息的最终一致性方案 TCC Saga 都属于这一类。这些方案的具体机制在这里不做展开，网上这方面的论述文章非常多。总之，这些方案都要求在应用的业务层面把分布式事务技术约束考虑到设计中，通常每一个服务都需要设计实现正向和反向的幂等接口。这样的设计约束，往往会导致很高的研发和维护成本。 理想的方案应该是什么样子？不可否认，侵入业务的分布式事务方案都经过大量实践验证，能有效解决问题，在各行各业的业务应用系统中起着重要作用。但回到原点来思考，这些方案的采用实际上都是 迫于无奈。设想，如果基于 XA 的方案能够不那么 重，并且能保证业务的性能需求，相信不会有人愿意把分布式事务问题拿到业务层面来解决。 一个理想的分布式事务解决方案应该：像使用 本地事务 一样简单，业务逻辑只关注业务层面的需求，不需要考虑事务机制上的约束。 原理和设计我们要设计一个对业务无侵入的方案，所以从业务无侵入的 XA 方案来思考： 是否可以在 XA 的基础上演进，解决掉 XA 方案面临的问题呢？ 如何定义一个分布式事务？首先，很自然的，我们可以把一个分布式事务理解成一个包含了若干 分支事务 的 全局事务。全局事务 的职责是协调其下管辖的 分支事务 达成一致，要么一起成功提交，要么一起失败回滚。此外，通常 分支事务 本身就是一个满足 ACID 的 本地事务。这是我们对分布式事务结构的基本认识，与 XA 是一致的。 其次，与 XA 的模型类似，我们定义 3 个组件来协议分布式事务的处理过程。 Transaction Coordinator (TC)： 事务协调器，维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚。 Transaction Manager (TM)： 控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚的决议。 Resource Manager (RM)： 控制分支事务，负责分支注册、状态汇报，并接收事务协调器的指令，驱动分支（本地）事务的提交和回滚。 一个典型的分布式事务过程： TM 向 TC 申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的 XID。 XID 在微服务调用链路的上下文中传播。 RM 向 TC 注册分支事务，将其纳入 XID 对应全局事务的管辖。 TM 向 TC 发起针对 XID 的全局提交或回滚决议。 TC 调度 XID 下管辖的全部分支事务完成提交或回滚请求。 至此，Fescar 的协议机制总体上看与 XA 是一致的。 与 XA 的差别在什么地方？架构层次 XA 方案的 RM 实际上是在数据库层，RM 本质上就是数据库自身（通过提供支持 XA 的驱动程序来供应用使用）。 而 Fescar 的 RM 是以二方包的形式作为中间件层部署在应用程序这一侧的，不依赖与数据库本身对协议的支持，当然也不需要数据库支持 XA 协议。这点对于微服务化的架构来说是非常重要的：应用层不需要为本地事务和分布式事务两类不同场景来适配两套不同的数据库驱动。 这个设计，剥离了分布式事务方案对数据库在 协议支持 上的要求。 两阶段提交先来看一下 XA 的 2PC 过程。 无论 Phase2 的决议是 commit 还是 rollback，事务性资源的锁都要保持到 Phase2 完成才释放。 设想一个正常运行的业务，大概率是 90% 以上的事务最终应该是成功提交的，我们是否可以在 Phase1 就将本地事务提交呢？这样 90% 以上的情况下，可以省去 Phase2 持锁的时间，整体提高效率。 分支事务中数据的 本地锁 由本地事务管理，在分支事务 Phase1 结束时释放。 同时，随着本地事务结束，连接 也得以释放。 分支事务中数据的 全局锁 在事务协调器侧管理，在决议 Phase2 全局提交时，全局锁马上可以释放。只有在决议全局回滚的情况下，全局锁 才被持有至分支的 Phase2 结束。 这个设计，极大地减少了分支事务对资源（数据和连接）的锁定时间，给整体并发和吞吐的提升提供了基础。 当然，你肯定会问：Phase1 即提交的情况下，Phase2 如何回滚呢？ 分支事务如何提交和回滚？首先，应用需要使用 Fescar 的 JDBC 数据源代理，也就是 Fescar 的 RM。 Phase1： Fescar 的 JDBC 数据源代理通过对业务 SQL 的解析，把业务数据在更新前后的数据镜像组织成回滚日志，利用 本地事务 的 ACID 特性，将业务数据的更新和回滚日志的写入在同一个 本地事务 中提交。 这样，可以保证：任何提交的业务数据的更新一定有相应的回滚日志存在。 基于这样的机制，分支的本地事务便可以在全局事务的 Phase1 提交，马上释放本地事务锁定的资源。 Phase2： 如果决议是全局提交，此时分支事务此时已经完成提交，不需要同步协调处理（只需要异步清理回滚日志），Phase2 可以非常快速地完成。 如果决议是全局回滚，RM 收到协调器发来的回滚请求，通过 XID 和 Branch ID 找到相应的回滚日志记录，通过回滚记录生成反向的更新 SQL 并执行，以完成分支的回滚。 事务传播机制XID 是一个全局事务的唯一标识，事务传播机制要做的就是把 XID 在服务调用链路中传递下去，并绑定到服务的事务上下文中，这样，服务链路中的数据库更新操作，就都会向该 XID 代表的全局事务注册分支，纳入同一个全局事务的管辖。 基于这个机制，Fescar 是可以支持任何微服务 RPC 框架的。只要在特定框架中找到可以透明传播 XID 的机制即可，比如，Dubbo 的 Filter + RpcContext。 对应到 Java EE 规范和 Spring 定义的事务传播属性，Fescar 的支持如下： PROPAGATION_REQUIRED： 默认支持 PROPAGATION_SUPPORTS： 默认支持 PROPAGATION_MANDATORY：应用通过 API 来实现 PROPAGATION_REQUIRES_NEW：应用通过 API 来实现 PROPAGATION_NOT_SUPPORTED：应用通过 API 来实现 PROPAGATION_NEVER：应用通过 API 来实现 PROPAGATION_NESTED：不支持 隔离性全局事务的隔离性是建立在分支事务的本地隔离级别基础之上的。 在数据库本地隔离级别 读已提交 或以上的前提下，Fescar 设计了由事务协调器维护的 全局写排他锁，来保证事务间的 写隔离，将全局事务默认定义在 读未提交 的隔离级别上。 我们对隔离级别的共识是：微服务场景产生的分布式事务，绝大部分应用在 读已提交 的隔离级别下工作是没有问题的。而实际上，这当中又有绝大多数的应用场景，实际上工作在 读未提交 的隔离级别下同样没有问题。 在极端场景下，应用如果需要达到全局的 读已提交，Fescar 也提供了相应的机制来达到目的。默认，Fescar 是工作在 读未提交 的隔离级别下，保证绝大多数场景的高效性。 事务的 ACID 属性在 Fescar 中的体现是一个比较复杂的话题，我们会有专门的文章来深入分析，这里不做进一步展开。 适用场景分析前文所述的 Fescar 的核心原理中有一个 重要前提：分支事务中涉及的资源，必须 是支持 ACID 事务的 关系型数据库。分支的提交和回滚机制，都依赖于本地事务的保障。所以，如果应用使用的数据库是不支持事务的，或根本不是关系型数据库，就不适用。 另外，目前 Fescar 的实现还存在一些局限，比如：事务隔离级别最高支持到 读已提交 的水平，SQL 的解析还不能涵盖全部的语法等。 为了覆盖 Fescar 原生机制暂时不能支持应用场景，我们定义了另外一种工作模式。 上面介绍的 Fescar 原生工作模式称为 AT（Automatic Transaction）模式，这种模式是对业务无侵入的。与之相应的另外一种工作模式称为 MT（Manual Transaction）模式，这种模式下，分支事务需要应用自己来定义业务本身及提交和回滚的逻辑。 分支的基本行为模式作为全局事务一部分的分支事务，除本身的业务逻辑外，都包含 4 个与协调器交互的行为： 分支注册： 在分支事务的数据操作进行之前，需要向协调器注册，把即将进行的分支事务数据操作，纳入一个已经开启的全局事务的管理中去，在分支注册成功后，才可以进行数据操作。 状态上报： 在分支事务的数据操作完成后，需要向事务协调器上报其执行结果。 分支提交：响应协调器发出的分支事务提交的请求，完成分支提交。 分支回滚：响应协调器发出的分支事务回滚的请求，完成分支回滚。 AT 模式分支的行为模式业务逻辑不需要关注事务机制，分支与全局事务的交互过程自动进行。 MT 模式分支的行为模式业务逻辑需要被分解为 Prepare/Commit/Rollback 3 部分，形成一个 MT 分支，加入全局事务。 MT 模式一方面是 AT 模式的补充。另外，更重要的价值在于，通过 MT 模式可以把众多非事务性资源纳入全局事务的管理中。 混合模式因为 AT 和 MT 模式的分支从根本上行为模式是一致的，所以可以完全兼容，即，一个全局事务中，可以同时存在 AT 和 MT 的分支。这样就可以达到全面覆盖业务场景的目的：AT 模式可以支持的，使用 AT 模式；AT 模式暂时支持不了的，用 MT 模式来替代。另外，自然的，MT 模式管理的非事务性资源也可以和支持事务的关系型数据库资源一起，纳入同一个分布式事务的管理中。 应用场景的远景回到我们设计的初衷：一个理想的分布式事务解决方案是不应该侵入业务的。MT 模式是在 AT 模式暂时不能完全覆盖所有场景的情况下，一个比较自然的补充方案。我们希望通过 AT 模式的不断演进增强，逐步扩大所支持的场景，MT 模式逐步收敛。未来，我们会纳入对 XA 的原生支持，用 XA 这种无侵入的方式来覆盖 AT 模式无法触达的场景。 扩展点微服务框架的支持事务上下文在微服务间的传播需要根据微服务框架本身的机制，订制最优的，对应用层透明的解决方案。有兴趣在这方面共建的开发者可以参考内置的对 Dubbo 的支持方案，来实现对其他微服务框架的支持。 所支持的数据库类型因为 AT 涉及 SQL 的解析，所以在不同类型的数据库上工作，会有一些特定的适配。有兴趣在这方面共建的开发者可以参考内置的对 MySQL 的支持方案，来实现对其他数据库的支持。 配置和服务注册发现支持接入不同的配置和服务注册发现解决方案。比如：Nacos、Eureka、ZooKeeper 等。 MT 模式的场景拓展MT 模式的一个重要作用就是，可以把非关系型数据库的资源，通过 MT 模式分支的包装，纳入到全局事务的管辖中来。比如，Redis、HBase、RocketMQ 的事务消息等。有兴趣在这方面共建的开发者可以在这里贡献一系列相关生态的适配方案。 事务协调器的分布式高可用方案针对不同场景，支持不同的方式作为事务协调器 Server 端的高可用方案。比如，针对事务状态的持久化，可以是基于文件的实现方案，也可以是基于数据库的实现方案；集群间的状态同步，可以是基于 RPC 通信的方案，也可以是基于高可用 KV 存储的方案。 RoadmapLanscape The green part is already open sourced, the yellow part will open source by Alibaba/AntFinancial, the blue part we want co-building with out community: Developers can refer to Seata implementation of MySQL support if you want to support different databases transaction Developers can refer to Seata implementation of Dubbo support if you want to support different microservices Developers can refer to Seata implementation of TCC support if you want to support different data source(such as MQ, NoSQL) Developers can refer to Seata implementation of TCC support if you want to support different data source(such as MQ, NoSQL) Developers can easily support configuration/registry services with just a little work The blue part is warmly welcome you, join it and contribute excellent solution We will support XA which is the standard of distributed transaction in our product roadmap Roadmapv0.1.0 Microservice framework support: Dubbo Database support: MySQL Spring AOP annotation Support Transaction coordinator: Stand-alone Server v0.5.x Rename Fescar to Seata Microservice framework support: SOFA, Spring Cloud Support TCC(Try Confirm Cancel) transaction mode Dynamic configuration Services discovery v0.6.x Transaction coordinator: Cluster Server with HA v0.8.x Promethus support Management console: Monitor, Deployment, Upgrating, etc. v1.0.0 Production ready v1.5.x Database support: Oracle, PostgreSQL, OceanBase Optimization of conflict datas Independent of Spring Annotation Multiple source of data transaction support: MessageQueue(RocketMQ), HBase, Redis, etc. v2.0.0 XA transaction mode support 当然，项目迭代演进的过程，我们最重视的是社区的声音，路线图会和社区充分交流及时进行调整。 相关链接 FESCAR on GitHub GTS on Aliyun]]></content>
      <categories>
        <category>Architecture</category>
      </categories>
      <tags>
        <tag>Architecture</tag>
        <tag>Fescar</tag>
        <tag>Seata</tag>
        <tag>Distributed</tag>
        <tag>Transaction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异地多活高可用架构设计]]></title>
    <url>%2Fmulti-live-high-available-architecture-design.html</url>
    <content type="text"><![CDATA[如果构建应用的异地多活？ 概要随着业务的快速发展，对于很多公司来说，构建于单地域的技术体系架构，会面临诸如下面的多种问题：础设施的有限性限制了业务的可扩展性；机房、城市级别的故障灾害，影响服务的可持续性。 为解决遇到的这些问题，公司可以选择构建异地多活架构，在同城/异地构建多个单元(业务中心)。各个业务单元可以分布在不同的地域，从而有效解决了单地域部署带来的基础设施的扩展限制、服务可持续性。 异地多活是近几年比较热门的一个话题，那么在实际业务中什么时候需要去做这件事？如何去做？做的时候需要考虑什么？ 何时去做？个人感觉取决于以下几个方面： 业务发展 基础设施状况 技术积淀 如何做？目前在网上搜索到的异地多活方案来看，基本都是阿里、饿了么、京东、微博这些互联网大厂的实践，这些大厂的方案有一个共同点就是：大量的自研组件，来做相关的数据同步，业务切分等等，那么，对于很多传统企业或者相对小一些的企业，应该如何来做这件事？ 根据业务特性借助合适的公有云服务 做的时候，需要注意什么？ 真正需要做异地多活的业务有哪些？ 基础设施如何？ 对于不可用时间的容忍程度是多少？ 业务背景 在所有的系统中用户中心都是核心业务，因为它是进入其它很多业务前提。 我们这边IDC不是很稳定，之前发生过几次机房大规模故障，比如机房网络挂了，整个机房对外不可用了。 以上两点是我们这次要做用户中心异地容灾的出发点，以便在面对机房级别故障时，保证服务可用性。 业务梳理用户中心从整体来看，对外主要提供：注册、登陆、查询用户信息等服务。这些服务又有以下几个特点： 登陆的优先级最高 事务性要求低 涉及的公共组件主要有： MySQL：用户数据存储 Redis：Authorization Code、短信验证码、账号锁定、access token等的存储 Zookeeper：Dubbo依赖 方案用户中心是通过外包的形式进行开发的，目前已上线并交付给另一个外包商运维，所以在考虑容灾一期方案的时候，需要考虑尽量不动代码。 目标一期目标当北京机房出现故障的时候，可以一定时间内把流量切到青岛机房这边，保证用户中心核心服务的基本可用。 二期目标用户中心通过异地多活，实现高可用（需要集团智能DNS支持）。 架构设计一期架构当北京机房发生故障的时候，可以把流量快速切换到青岛这边，以保障用户中心核心服务可用。 具体方案如下： 通过otter近实时的将北京机房核心业务数据同步到青岛机房。 青岛机房部署Redis、ZooKeeper等中间件。 青岛机房部署用户中心的核心应用（实例正常部署、运行，只是平时不会有访问）。 具体架构如下： 可以达到的效果： 当北京机房出现故障的时候，可以在一定时间内把流量切到青岛机房这边，保证用户中心核心服务的基本可用，但此时已登录用户需要重新登录。 一定时间：取决于DNS修改ip时间+DNS TTL时间，目前来看TTL是10分钟，人工修改ip应该很快，所以一定时间是10~20分钟。 存在的缺点： 北京机房非故障期间，青岛机房的机器，仅做数据库同步，存在一定的资源浪费。 当北京机房出现故障，流量切换到青岛机房后，只能保证登陆这一核心服务的可用。对于注册等需要修改数据库的服务，均不支持，如果在此期间访问这类服务，会发生异常。 二期架构二期的目的就是修正一期架构的缺点，通过异地多活，实现高可用。 二期青岛机房会替换为阿里云机房。 具体方案如下： 通过阿里云DTS服务实现两地机房数据库同步，保证北京、阿里云数据的近实时一致性。 北京、阿里云两地机房均提供在线服务，提高资源利用率。 梳理服务优先级，修改应用代码，支持服务降级。 当某个机房（阿里云或者北京）出现故障的时候，通过DNS服务把流量切换到另一个机房。 如果两地部署的时候，没有冗余一定硬件资源，则需要实施服务降级。 目前集团DNS解析，无法提供自动检测服务是否可用的功能，也就无法自动进行切换。 服务可用性，可以通过我们这边的多点拨测进行监控，当多点拨测不可用的时候，发送告警通知给相关人员，以便人工介入。 多点拨测告警，应该会提供两类：1、某个拨测点不通的时候 2、所有拨测点均不可用的时候。 目前集团DNS解析，TTL生效最短时间是10分钟，无法自定义TTL时间。 具体架构如下： 可以达到的效果： 如果集团DNS可以提供，类似阿里云云解析的网站监控功能并能灵活设置TTL时间，这时当北京机房或者阿里云机房出现故障后，就可以在很短的时间（部分服务最大异常时间）内自动进行流量切换。 此处只是以阿里云云解析示例，只要能提供类似的服务均可。 如果集团DNS无法提供类似阿里云云解析的网站监控及灵活设置TTL时间的功能，则部分服务最大异常时间还是取决于DNS修改ip时间+DNS TTL时间。 名词解释什么是网站监控？HTTP/HTTPS实时探测域名解析记录，支持自定义端口，实时发现宕机立即告警；全网分布式监控，在中国各个地区模拟用户端真实请求，监控结果真实可靠；支持宕机暂停、容灾切换，最大限度的解决服务中断对您的业务带来的损失；容灾切换支持A记录、CNAME域名，满足各种场景的容灾切换需求； 什么情况会被网站监控判断为宕机并发送告警通知？监控结果中，HTTP/HTTPS的返回码大于500的服务器错误情况，才会报警通知。举例说明：如果设置了四个探测点 北京联通、深圳阿里巴巴、上海电信、重庆联通。场景一：四个探测点中50%的监控点无法收到您服务器的响应，或50%的监控点收到返回码大于等于500时，才会判断您的网站为宕机情况。场景二：四个探测点中有50%以上的探测点探测您的网站返回码是小于500的情况，则不会判断您的网站为宕机。 云解析DNS“流量管理”云解析“流量管理”可以在您设置的每条解析线路下，根据权重比例轮询返回解析结果。当线路下的IP宕机时可以通过监控自动发现，并将宕机IP从当前线路下摘除，直到监控IP正常时会恢复解析。同时，当一条解析线路下的所有IP都宕机时，可以切换至其他正常线路。最大程度保证您的网站服务高可用，减小损失。 部分服务最大异常时间比如北京机房出现异常，这时转发到阿里云机房的流量是可以正常访问，只有转发到北京机房的流量是异常的。 这时如果使用网站监控或者类似服务，进行监控，并设置拨测间隔为1分钟，TTL生效时间为1秒，那么最多有60+1秒部分服务异常时间，之后DNS会自动把北京机房的ip自动踢掉，流量全部切到阿里云。 补充 一期、二期方案的实现均强依赖于集团的DNS服务 用户中心通过ip暴露的服务，一但出现机房级别的故障，一期、二期方案均无法保证该部分服务可用。 其实除了DNS这种方案，还有一种方案就是用类似F5这种设备，作跨机房负载，但必须是gslb，而且两端必须是相同的设备。 小结对于，非一线互联网大厂的公司而言，是实现异地容灾的时候，借助公有云是很有必要的，比如： 数据跨机房同步，可以使用阿里云的DTS(Data Transmission Service) 服务，目前DTS支持关系型数据库、NoSQL、大数据(OLAP)等数据源间的数据传输。 它是一种集数据迁移、数据订阅及数据实时同步于一体的数据传输服务。 跨机房分布式数据库，可以使用OceanBase。金融环境下通常对数据可靠性有更高的要求，OceanBase每一次事务提交，对应日志总是会在多个数据中心实时同步，并持久化。即使是数据中心级别的灾难发生，总是可以在其他的数据中心恢复每一笔已经完成的交易，实现了真正金融级别的可靠性要求。 异地多活由于各个公司的业务、基础设施及要解决的问题皆不尽相同，所以选择适合自己的就好。 或者直接使用云数据库RDS MySQL 版]]></content>
      <categories>
        <category>Architecture</category>
      </categories>
      <tags>
        <tag>Architecture</tag>
        <tag>原创</tag>
        <tag>Multi-Live</tag>
        <tag>High-Available</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PaaS平台架构设计]]></title>
    <url>%2Fpass-platform-architecture-design.html</url>
    <content type="text"><![CDATA[构建一个PaaS平台 背景目前在用的PaaS平台是之前购买的一个商业产品，但没有源码，运维期也早就结束了，所以在后期使用过程中会遇到一些各种各样的问题，对于使用、运维都造成一定的困扰。 老PaaS的架构及基本功能如下： 重构为什么选择重构PaaS平台而不是全部迁移kubernates集群？kubernates集群的确提供了很多优秀的特性，比如：RC、滚动更新或回滚、资源监控和日志记录、负载均衡等等。 但在目前我们这边的环境来看，迁移kubernates集群有如下几个问题： 无法无感知迁移，即迁移到kubernates集群的过程中及迁移到kubernates集群后，不增加用户的使用、学习成本，但应用引入kubernates集群之后，很难保证这一点。因为我们这边的用户大多是我们公司的供应商，供应商其实不太关心，你平台所提供的各种新特性、功能，更不想因为这些新特性、功能增加他们的使用、学习成本。 我们这边很多项目本身是有硬负载的，比如F5，所以kubernates提供的负载均衡功能，也就显的不那么重要。 日志部分，我们已经打通各个平台的日志、监控，不再需其他的组件。 滚动更新或回滚，老PaaS平台木有，重构后新版中准备加入（二期）。 RC类似功能，目前不打算支持。 架构及用到组件梳理 新PaaS功能点梳理 迁移通过无缝迁移，在用户无感知的情况下实现迁移。 为什么要做到无感知迁移？老PaaS中目前的项目数量是：193个，应用数量是：673个总实例数：1485，其中生产环境的实例数：894如果这些项目、应用，因为你的重构都需要改动的话，那么推广难度是很大的，所以需要尽量做到，对于用户来说无感知迁移。 以下迁移部分，都需要在新PaaS上线前完成并在上线一段时间准实时同步过来。]]></content>
      <categories>
        <category>Architecture</category>
      </categories>
      <tags>
        <tag>Architecture</tag>
        <tag>原创</tag>
        <tag>PaaS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 垃圾回收算法之G1]]></title>
    <url>%2Fjava-gc-g1.html</url>
    <content type="text"><![CDATA[G1(Garbage-First)回收器是在JDK1.7中正式使用的全新垃圾回收器，G1拥有独特的垃圾回收策略，从分代上看，G1依然属于分代垃圾回收器，它会区分年代和老年代，依然有eden和survivor区，但从堆的结构上看，它并不要求整个eden区、年清代或者老年代都连续。它使用了全新的分区算法。 其特点如下： 并行性：G1在回收期间，可以由多个GC线程同时工作，有效利用多核计算能力。 并发性：G1拥有与应用程序交替执行的能力，因此一般来说，不会在整个回收期间完全阻塞应用程序。 分代GC：与之前回收器不同，其他回收器，它们要么工作在年轻代要么工作在老年代。G1可以同时兼顾年轻代与老年代。 空间整理：G1在回收过程中，会进行适当的对象移动，不像CMS，只是简单的标记清除，在若干次GC后CMS必须进行一次碎片整理，G1在每次回收时都会有效的复制对象，减少空间碎片。 可预见性：由于分区的原因，G1可以只选取部分区域进行内存回收，这样缩小了回收范围，因此对于全局停顿也能得到更好的控制。 一、G1的内存划分和主要收集过程G1收集回收器将堆进行分区，划分为一个个的区域，每次收集的时候，只收集其中几个区域，以此来控制垃圾回收产生一次停顿时间。 G1的收集过程可能有4个阶段： 新生代GC 并发标记周期 混合收集 （如果需要）进行Full GC。 二、G1的新生代GC新生代GC的主要工作是回收eden区和survivor区。 一旦eden区被占满，新生代GC就会启动。新生代GC收集前后的堆数据如下图所示，其中E表示eden区，S表示survivor区，O表示老年代。 可以看到，新生代GC只处理eden和survivor区，回收后，所有的eden区都应该被清空，而survivor区会被收集一部分数据，但是应该至少仍然存在一个survivor区，类比其他的新生代收集器，这一点似乎并没有太大变化。另一个重要的变化是老年代的区域增多，因为部分survivor区或者eden区的对象可能会晋升到老年代。 三、G1并发标记周期 G1的并发阶段和CMS有些类似，它们都是为了降低一次停顿时间，而将可以和应用程序并发执行的部分单独提取出来执行。 并发标记周期针对老年代 并发标记周期可分为以下几步： 初始标记：标记从根节点直接可达的对象。这个阶段会伴随一次新生代GC，它是会产生全局停顿的，应用程序在这个阶段必须停止执行。 根区域扫描：由于初始标记必然会伴随一次新生代GC，所以在初始化标记后，eden被清空，并且存活对象被移到survivor区。在这个阶段，将扫描由survivor区直接可达的老年代区域，并标记这些直接可达的对象。这个过程是可以和应用程序并发执行的。但是根区域扫描不能和新生代GC同时发生（因为根区域扫描依赖survivor区的对象，而新生代GC会修改这个区域），故如果恰巧此时需要新生代GC，GC就需要等待根区域扫描结束后才能进行，如果发生这种情况，这次新生代GC的时间就会延长。 并发标记：和CMS类似，并发标记将会扫描并查找整个堆的存活对象，并做好标记。这是一个并发过程，并且这个过程可以被一次新生代GC打断。 重新标记：和CMS一样，重新标记也是会使应用程序停顿，由于在并发标记过程中，应用程序依然运行，因此标记结果可能需要修正，所以在此阶段对上一次标记进行补充。在G1中，这个过程使用SATB（Snapshot-At-The-Begining）算法完成，即G1会在标记之初为存活对象创建一个快照，这个快照有助于加速重新标记的速度。 独占清理：顾名思义，这个阶段会引起停顿。它将计算各个区域的存活对象和GC回收比例并进行排序，识别可供混合回收的区域。在这个阶段，还会更新记忆集。该阶段给出了需要被混合回收的区域并进行了标记，在混合回收阶段，需要这些信息。 并发清理阶段：识别并清理完全空闲的区域。它是并发的清理，不会引起停顿。 SATB全称是Snapshot-At-The-Beginning，由字面理解，是GC开始时活着的对象的一个快照。它是通过Root Tracing得到的，作用是维持并发GC的正确性。那么它是怎么维持并发GC的正确性的呢？根据三色标记算法，我们知道对象存在三种状态：白：对象没有被标记到，标记阶段结束后，会被当做垃圾回收掉。灰：对象被标记了，但是它的field还没有被标记或标记完。黑：对象被标记了，且它的所有field也被标记完了。 SATB 利用 write barrier 将所有即将被删除的引用关系的旧引用记录下来，最后以这些旧引用为根 Stop The World 地重新扫描一遍即可避免漏标问题。 因此G1 Remark阶段 Stop The World 与 CMS了的remark有一个本质上的区别，那就是这个暂停只需要扫描有 write barrier 所追中对象为根的对象， 而 CMS 的remark 需要重新扫描整个根集合，因而CMS remark有可能会非常慢。 四、混合回收在并发标记周期中，虽有部分对象被回收，但是回收的比例是非常低的。但是在并发标记周期后，G1已经明确知道哪些区域含有比较多的垃圾对象，在混合回收阶段，就可以专门针对这些区域进行回收。当然G1会优先回收垃圾比例较高的区域（回收这些区域的性价比高），这正是G1名字的由来（Garbage First Garbage Collector：译为垃圾优先的垃圾回收器），这里的垃圾优先（Garbage First）指的是回收时优先选取垃圾比例最高的区域。 这个阶段叫做混合回收，是因为在这个阶段，即会执行正常的年轻代GC,又会选取一些被标记的老年代区域进行回收，同时处理了新生代和老年代。 混合回收会被执行多次，直到回收了足够多的内存空间，然后，它会触发一次新生代GC。新生代GC后，又可能会发生一次并发标记周期的处理，最后又会引起混合回收，因此整个过程可能是如下图： 五、必要时的Full GC和CMS类似，并发收集让应用程序和GC线程交替工作，因此在特别繁忙的情况下无可避免的会发生回收过程中内存不足的情况，当遇到这种情况，G1会转入一个Full GC 进行回收。 以下4种情况会触发这类的Full GC： 1、并发模式失效G1启动标记周期，但在Mix GC之前，老年代就被填满，这时候G1会放弃标记周期。这种情形下，需要增加堆大小，或者调整周期（例如增加线程数-XX:ConcGCThreads等）。 GC日志如下的示例： 解决办法：发生这种失败意味着堆的大小应该增加了，或者G1收集器的后台处理应该更早开始，或者需要调整周期，让它运行得更快（如，增加后台处理的线程数）。 2、晋升失败（to-space exhausted或者to-space overflow） G1收集器完成了标记阶段，开始启动混合式垃圾回收，清理老年代的分区，不过，老年代空间在垃圾回收释放出足够内存之前就会被耗尽。（G1在进行GC的时候没有足够的内存供存活对象或晋升对象使用），由此触发了Full GC。 下面日志中（可以在日志中看到(to-space exhausted)或者（to-space overflow）），反应的现象是混合式GC之后紧接着一次Full GC。 这种失败通常意味着混合式收集需要更迅速的完成垃圾收集：每次新生代垃圾收集需要处理更多老年代的分区。 解决这种问题的方式是： 增加 -XX:G1ReservePercent选项的值（并相应增加总的堆大小），为“目标空间”增加预留内存量。 通过减少 -XX:InitiatingHeapOccupancyPercent 提前启动标记周期。 也可以通过增加 -XX:ConcGCThreads 选项的值来增加并行标记线程的数目。 3、疏散失败（to-space exhausted或者to-space overflow） 进行新生代垃圾收集是，Survivor空间和老年代中没有足够的空间容纳所有的幸存对象。这种情形在GC日志中通常是： 这条日志表明堆已经几乎完全用尽或者碎片化了。G1收集器会尝试修复这一失败，但可以预期，结果会更加恶化：G1收集器会转而使用Full GC。 解决这种问题的方式是： 增加 -XX:G1ReservePercent选项的值（并相应增加总的堆大小），为“目标空间”增加预留内存量。 通过减少 -XX:InitiatingHeapOccupancyPercent 提前启动标记周期。 也可以通过增加 -XX:ConcGCThreads 选项的值来增加并行标记线程的数目。 4、Humongous Object 分配失败当Humongous Object 找不到合适的空间进行分配时，就会启动Full GC，来释放空间。这种情况下，应该避免分配大量的巨型对象，增加内存或者增大-XX:G1HeapRegionSize，使巨型对象不再是巨型对象。 对于Humongous Object 的处理还有一种方式就是切换GC算法到ZGC，因为ZGC中对于Humongous Object 的回收不会特殊处理（比如不会延迟收集）。 六、巨型对象Humongous Object：巨型对象Humongous regions：巨型区域 对于G1而言，只要超过regin大小的一半，就被认为是巨型对象。巨型对象直接被分配到老年代中的“巨型区域”。这些巨型区域是一个连续的区域集。StartsHumongous 标记该连续集的开始，ContinuesHumongous 标记它的延续。 在分配巨型对象之前先检查是否超过 initiating heap occupancy percent和the marking threshold, 如果超过的话，就启动global concurrent marking，为的是提早回收，防止 evacuation failures 和 Full GC。 对于巨型对象，有以下几个点需要注意： 没有被引用的巨型对象会在标记清理阶段或者Full GC时被释放掉。 为了减少拷贝负载，只有在Full GC的时候，才会压缩大对象region。 每一个region中都只有一个巨型对象，该region剩余的部分得不到利用，会导致堆碎片化。 如果看到由于大对象分配导致频繁的并发回收，需要把大对象变为普通的对象，建议增大Region size。（或者切换到ZGC） 对于增大Region size有一个负面影响就是：减少了可用region的数量。因此，对于这种情况，你需要进行相应的测试，以查看是否实际提高了应用程序的吞吐量或延迟。 七、常见调优参数1、-XX:MaxGCPauseMillis=N默认200毫秒 前面介绍过使用GC的最基本的参数： -XX:+UseG1GC -Xmx32g -XX:MaxGCPauseMillis=200 前面2个参数都好理解，后面这个MaxGCPauseMillis参数该怎么配置呢？这个参数从字面的意思上看，就是允许的GC最大的暂停时间。G1尽量确保每次GC暂停的时间都在设置的MaxGCPauseMillis范围内。那G1是如何做到最大暂停时间的呢？这涉及到另一个概念，CSet(collection set)。它的意思是在一次垃圾收集器中被收集的区域集合。 Young GC：选定所有新生代里的region。通过控制新生代的region个数来控制young GC的开销。 Mixed GC：选定所有新生代里的region，外加根据global concurrent marking统计得出收集收益高的若干老年代region。在用户指定的开销目标范围内尽可能选择收益高的老年代region。 在理解了这些后，我们再设置最大暂停时间就有了方向。首先，我们能容忍的最大暂停时间是有一个限度的，我们需要在这个限度范围内设置。但是应该设置的值是多少呢？我们需要在吞吐量跟MaxGCPauseMillis之间做一个平衡。如果MaxGCPauseMillis设置的过小，那么GC就会频繁，吞吐量就会下降。如果MaxGCPauseMillis设置的过大，应用程序暂停时间就会变长。G1的默认暂停时间是200毫秒，我们可以从这里入手，调整合适的时间。 2、-XX:G1HeapRegionSize=n设置的 G1 区域的大小。值是 2 的幂，范围是 1 MB 到 32 MB 之间。目标是根据最小的 Java 堆大小划分出约 2048 个区域。 -XX:ParallelGCThreads=n（调整G1垃圾收集的后台线程数） 设置 STW 工作线程数的值。将 n 的值设置为逻辑处理器的数量。n 的值与逻辑处理器的数量相同，最多为 8。 如果逻辑处理器不止八个，则将 n 的值设置为逻辑处理器数的 5/8 左右。这适用于大多数情况，除非是较大的 SPARC 系统，其中 n 的值可以是逻辑处理器数的 5/16 左右。 -XX:ConcGCThreads=n（调整G1垃圾收集的后台线程数） 设置并行标记的线程数。将 n 设置为并行垃圾回收线程数 (ParallelGCThreads) 的 1/4 左右。 3、 -XX:InitiatingHeapOccupancyPercent=45（调整G1垃圾收集运行频率）设置触发标记周期的 Java 堆占用率阈值。默认占用率是整个 Java 堆的 45%。 该值设置太高：会陷入Full GC泥潭之中，因为并发阶段没有足够的时间在剩下的堆空间被填满之前完成垃圾收集。 如果该值设置太小：应用程序又会以超过实际需要的节奏进行大量的后台处理。 避免使用以下参数：避免使用 -Xmn 选项或 -XX:NewRatio 等其他相关选项显式设置年轻代大小。固定年轻代的大小会覆盖暂停时间目标。 八、细节1、G1 mixed GC时机？mixed gc中也有一个阈值参数 -XX:InitiatingHeapOccupancyPercent，当老年代大小占整个堆大小百分比达到该阈值时，会触发一次mixed gc. 在分配humongous object之前先检查是否超过 initiating heap occupancy percent, 如果超过的话，就启动global concurrent marking，为的是提早回收，防止 evacuation failures 和 Full GC。 为了减少连续H-objs分配对GC的影响，需要把大对象变为普通的对象，建议增大Region size。 一个Region的大小可以通过参数-XX:G1HeapRegionSize设定，取值范围从1M到32M，且是2的指数。 2、XX：G1 HeapRegionSize 默认值？默认把堆内存按照2048份均分，最后得到一个合理的大小。 3、直接内存配置Q: 什么时候用直接内存？ A: 读写频繁的场合，出于性能考虑，可以考虑使用直接内存。 直接内存也是 Java 程序中非常重要的组成部分，特别是 NIO 被广泛使用之后，直接内存可以跳过 Java 堆，使 Java 程序可以直接访问原生堆空间。因此可以在一定程度上加快内存的访问速度。直接内存可以用 -XX:MaxDirectMemorySize 设置，默认值为最大堆空间，也就是 -Xmx。当直接内存达到最大值的时候，也会触发垃圾回收，如果垃圾回收不能有效释放空间，直接内存溢出依然会引起系统的 OOM。 一般而言直接内存在访问读写上直接内存有较大优势（速度较快），但是在内存空间申请的时候，直接内存毫无优势而言。 4、RSet全称是Remembered Set，是辅助GC过程的一种结构，典型的空间换时间工具，和Card Table有些类似。G1的RSet是在Card Table的基础上实现的：每个Region会记录下别的Region有指向自己的指针，并标记这些指针分别在哪些Card的范围内。这个RSet其实是一个Hash Table，Key是别的Region的起始地址，Value是一个集合，里面的元素是Card Table的Index。 RSet究竟是怎么辅助GC的呢？ 在做YGC的时候，只需要选定young generation region的RSet作为根集，这些RSet记录了old-&gt;young的跨代引用，避免了扫描整个old generation。而mixed gc的时候，old generation中记录了old-&gt;old的RSet，young-&gt;old的引用由扫描全部young generation region得到，这样也不用扫描全部old generation region。所以RSet的引入大大减少了GC的工作量。 九、JDK 12中G1的新特性1、可中断 mixed GC如果 Mixed GC 的 G1 存在超出暂停目标的可能性，则使其可被中止。 2、G1未使用分配内存即时返回增强 G1垃圾收集器，以便在空闲时自动将 Java 堆内存返回给操作系统。 十、GC 发展趋势其实可以看到Java 垃圾回收器的趋势，就是在大内存堆的前提下尽 GC 可能的降低对应用程序的影响；从 CMS 的分阶段增量标记，到 G1 通过 SATB 算法改正 remark 阶段的 Stop The World 的影响，再到 ZGC/C4甚至在标记阶段无需 Stop The World，莫不如此。 十一、结尾推荐几种学习这种GC的方式： 看JEP（JDK Enhancement Proposal）知道它的来龙去脉。 看相应算法的paper（之前看Shenandoah GC Paper的时候，就有一种收获很大的感觉，因为Shenandoah GC的处理方式，介于G1跟ZGC之间，所以看了Shenandoah GC Paper感觉对于G1、ZGC的理解也更加深入了）。 会在文章结束，补充上JEP官网地址跟我收集的一些GC资料（包含部分paper）github地址。 补一个我自己归纳的GC图： 各种GC算法都是围绕着，图中内容展开的，只是各自的处理方式不同而已。 资料推荐： 1、GC算法及paper https://github.com/jiankunking/books-recommendation/tree/master/GC 2、Java相关书籍推荐 https://github.com/jiankunking/books-recommendation/tree/master/Java 参考文献 1、实战JAVA虚拟机 JVM故障诊断与性能优化 2、jeps 3、其它 https://www.oracle.com/technetwork/articles/java/g1gc-1984535.html https://plumbr.io/handbook/gc-tuning-in-practice/other-examples/humongous-allocations]]></content>
      <categories>
        <category>GC</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>Java</tag>
        <tag>GC</tag>
        <tag>G1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SOFAMosn 如何提高 GoLang 的转发性能]]></title>
    <url>%2Fsofamosn-golang-performance.html</url>
    <content type="text"><![CDATA[通过SOFAMosn了解goroutine只能在一定并发量级上降低并发编程的难度(goroutine内存占用2kb+)高并发的场景还是NIO比较适合 GoLang 的转发性能比起 C++ 肯定是稍有逊色的，为了尽可能的提高 MOSN 的转发性能，我们在线程模型上进行优化，当前 MOSN 支持两种线程模型，用户可根据场景选择开启适用的模型。 模型一如下图所示，使用 GoLang 默认的 epoll 机制，对每个连接分配独立的读写协程进行阻塞读写操作，proxy 层做转发时，使用常驻 worker 协程池负责处理 Stream Event 此模型在 IO 上使用 GoLang 的调度机制，适用于连接数较少的场景，例如：mosn 作为 sidecar、与 client 同机部署的场景 模型二如下图所示，基于 Netpoll 重写 epoll 机制，将 IO 和 PROXY 均进行池化，downstream connection 将自身的读写事件注册到 netpoll 的 epoll/kqueue wait 协程，epoll/kqueue wait 协程接受到可读事件，触发回调，从协程池中挑选一个执行读操作。 使用自定义 Netpoll IO 池化操作带来的好处是： 当可读事件触发时，从协程池中获取一个 goroutine 来执行读处理，而不是新分配一个 goroutine，以此来控制高并发下的协程数量 当收到链接可读事件时，才真正为其分配 read buffer 以及相应的执行协程。这样 GetBytes() 可以降低因为大量空闲链接场景导致的额外协程和 read buffer 开销 此模型适用于连接数较多、可读连接数量受限的情况，例如：mosn 作为 api gateway 的场景 本文整理自SOFAMosn官方文档]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>SOFAMosn</tag>
        <tag>Performance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReentrantReadWriteLock原理解析]]></title>
    <url>%2Fjava-reentrantreadwritelock.html</url>
    <content type="text"><![CDATA[Java JDK 11 ReentrantReadWriteLock 原理分析 1、前言希望在阅读本文之前，建议先看一下以下三篇文章： 1、面试必备：Java AQS 实现原理（图文）分析 2、面试必备：Java AQS Condition的实现分析 3、面试必备：Java volatile的内存语义与AQS锁内存可见性 读完了以上三篇文章，先看一下ReentrantReadWriteLock的代码路径： 1package java.util.concurrent.locks; 来先猜一下ReentrantReadWriteLock会如何实现？ 都在java.util.concurrent包下，那么可以明确一点，那就是关于锁的实现，应该用的就是AQS，那么，读锁、写锁会不会对应的就是AQS中的共享模式与独占模式？ 2、读写锁使用场景读是多于写（比如cache） 一般情况下，读写锁的性能都会比排它锁好，因为大多数场景读是多于写的。在读多于写的情况下，读写锁能够提供比排它锁更好的并发性和吞吐量。 3、读写锁接口：ReadWriteLock代码地址：ReadWriteLock 123456789101112131415public interface ReadWriteLock &#123; /** * Returns the lock used for reading. * * @return the lock used for reading */ Lock readLock(); /** * Returns the lock used for writing. * * @return the lock used for writing */ Lock writeLock();&#125; 4、读写锁的接口与示例 ReadWriteLock仅定义了获取读锁和写锁的两个方法，即readLock()方法和writeLock()方法，而其实现：ReentrantReadWriteLock，除了接口方法之外，还提供了一些便于外界监控其内部工作状态的方法，这些方法以及描述如表所示： 接下来，通过一个缓存示例说明读写锁的使用方式，示例代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940public class Cache &#123; static Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); static ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); static Lock r = rwl.readLock(); static Lock w = rwl.writeLock(); // 获取一个key对应的value public static final Object get(String key) &#123; r.lock(); try &#123; return map.get(key); &#125; finally &#123; r.unlock(); &#125; &#125; // 设置key对应的value，并返回旧的value public static final Object put(String key, Object value) &#123; w.lock(); try &#123; return map.put(key, value); &#125; finally &#123; w.unlock(); &#125; &#125; // 清空所有的内容 public static final void clear() &#123; w.lock(); try &#123; map.clear(); &#125; finally &#123; w.unlock(); &#125; &#125;&#125; 上述示例中，Cache组合一个非线程安全的HashMap作为缓存的实现，同时使用读写锁的读锁和写锁来保证Cache是线程安全的。在读操作get(String key)方法中，需要获取读锁，这使得并发访问该方法时不会被阻塞。写操作put(String key,Object value)方法和clear()方法，在更新HashMap时必须提前获取写锁，当获取写锁后，其他线程对于读锁和写锁的获取均被阻塞，而只有写锁被释放之后，其他读写操作才能继续。Cache使用读写锁提升读操作的并发性，也保证每次写操作对所有的读写操作的可见性，同时简化了编程方式。 5、ReentrantReadWriteLock脉络梳理代码地址：ReentrantReadWriteLock 先看一下继承结构： 再看一下代码结构： 图中可以看出ReentrantReadWriteLock的实现还是比较复杂的，所以接下来主要分析ReentrantReadWriteLock实现关键点，包括： 读写状态的设计 写锁的获取与释放 读锁的获取与释放 锁降级 5.1 读写状态的设计读写锁同样依赖自定义同步器来实现同步功能，而读写状态就是其同步器的同步状态。回想ReentrantLock中自定义同步器的实现，同步状态表示锁被一个线程重复获取的次数，而读写锁的自定义同步器需要在同步状态（一个整型变量）上维护多个读线程和一个写线程的状态，使得该状态的设计成为读写锁实现的关键。 如果在一个整型变量上维护多种状态，就一定需要“按位切割使用”这个变量，读写锁将变量切分成了两个部分，高16位表示读，低16位表示写，划分方式如下图所示: 当前同步状态表示一个线程已经获取了写锁，且重进入了两次，同时也连续获取了两次读锁。读写锁是如何迅速确定读和写各自的状态呢？ 答案是通过位运算。假设当前同步状态值为S，写状态等于S&amp;0x0000FFFF（将高16位全部抹去），读状态等于S&gt;&gt;&gt;16（无符号补0右移16位）。当写状态增加1时，等于S+1，当读状态增加1时，等于S+(1&lt;&lt;16)，也就是S+0x00010000。 1、0x0000FFFF=00000000000000001111111111111111（16个0 16个1） 2、&gt;&gt;&gt;： 无符号右移，忽略符号位，空位都以0补齐 3、0x00010000=10000000000000000（1个1 16个0） 根据状态的划分能得出一个推论：S不等于0时，当写状态（S&amp;0x0000FFFF）等于0时，则读状态（S&gt;&gt;&gt;16）大于0，即读锁已被获取。 5.2 写锁的获取与释放写锁是一个支持重进入的排它锁。如果当前线程已经获取了写锁，则增加写状态。如果当前线程在获取写锁时，读锁已经被获取（读状态不为0）或者该线程不是已经获取写锁的线程，则当前线程进入等待状态，获取写锁的代码如代码如下： 12345678910111213141516171819202122232425262728293031protected final boolean tryAcquire(int acquires) &#123; /* * Walkthrough: * 1. If read count nonzero or write count nonzero * and owner is a different thread, fail. * 2. If count would saturate, fail. (This can only * happen if count is already nonzero.) * 3. Otherwise, this thread is eligible for lock if * it is either a reentrant acquire or * queue policy allows it. If so, update state * and set owner. */ Thread current = Thread.currentThread(); int c = getState(); int w = exclusiveCount(c); if (c != 0) &#123; // 存在读锁或者当前获取线程不是已经获取写锁的线程 if (w == 0 || current != getExclusiveOwnerThread()) return false; if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); // Reentrant acquire setState(c + acquires); return true; &#125; if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; setExclusiveOwnerThread(current); return true; &#125; 该方法除了重入条件（当前线程为获取了写锁的线程）之外，增加了一个读锁是否存在的判断。如果存在读锁，则写锁不能被获取，原因在于：读写锁要确保写锁的操作对读锁可见，如果允许读锁在已被获取的情况下对写锁的获取，那么正在运行的其他读线程就无法感知到当前写线程的操作。因此，只有等待其他读线程都释放了读锁，写锁才能被当前线程获取，而写锁一旦被获取，则其他读写线程的后续访问均被阻塞。 写锁的释放与ReentrantLock的释放过程基本类似，每次释放均减少写状态，当写状态为0时表示写锁已被释放，从而等待的读写线程能够继续访问读写锁，同时前次写线程的修改对后续读写线程可见。 5.3 读锁的获取与释放读锁是一个支持重进入的共享锁，它能够被多个线程同时获取，在没有其他写线程访问（或者写状态为0）时，读锁总会被成功地获取，而所做的也只是（线程安全的）增加读状态。如果当前线程已经获取了读锁，则增加读状态。如果当前线程在获取读锁时，写锁已被其他线程获取，则进入等待状态。获取读锁的实现从Java 5到Java 6变得复杂许多，主要原因是新增了一些功能，例如getReadHoldCount()方法，作用是返回当前线程获取读锁的次数。读状态是所有线程获取读锁次数的总和，而每个线程各自获取读锁的次数只能选择保存在ThreadLocal中，由线程自身维护，这使获取读锁的实现变得复杂。因此，这里将获取读锁的代码做了删减，保留必要的部分，如代码如下： 123456789101112protected final int tryAcquireShared(int unused) &#123; for (;;) &#123; int c = getState(); int nextc = c + (1 &lt;&lt; 16); if (nextc &lt; c) throw new Error(&quot;Maximum lock count exceeded&quot;); if (exclusiveCount(c) != 0 &amp;&amp; owner != Thread.currentThread()) return -1; if (compareAndSetState(c, nextc)) return 1; &#125;&#125; 在tryAcquireShared(int unused)方法中，如果其他线程已经获取了写锁，则当前线程获取读锁失败，进入等待状态。如果当前线程获取了写锁或者写锁未被获取，则当前线程（线程安全，依靠CAS保证）增加读状态，成功获取读锁。 读锁的每次释放（线程安全的，可能有多个读线程同时释放读锁）均减少读状态，减少的值是（1&lt;&lt;16）。 5.4 锁降级锁降级指的是写锁降级成为读锁。如果当前线程拥有写锁，然后将其释放，最后再获取读锁，这种分段完成的过程不能称之为锁降级。锁降级是指把持住（当前拥有的）写锁，再获取到读锁，随后释放（先前拥有的）写锁的过程。 接下来看一个锁降级的示例。因为数据不常变化，所以多个线程可以并发地进行数据处理，当数据变更后，如果当前线程感知到数据变化，则进行数据的准备工作，同时其他处理线程被阻塞，直到当前线程完成数据的准备工作，如代码如下所示： 12345678910111213141516171819202122232425public void processData() &#123; readLock.lock(); if (!update) &#123; // 必须先释放读锁 readLock.unlock(); // 锁降级从写锁获取到开始 writeLock.lock(); try &#123; if (!update) &#123; // 准备数据的流程（略） update = true; &#125; readLock.lock(); &#125; finally &#123; writeLock.unlock(); &#125; // 锁降级完成，写锁降级为读锁 &#125; try &#123; // 使用数据的流程（略） &#125; finally &#123; readLock.unlock(); &#125; &#125; 上述示例中，当数据发生变更后，update变量（布尔类型且volatile修饰）被设置为false，此时所有访问processData()方法的线程都能够感知到变化，但只有一个线程能够获取到写锁，其他线程会被阻塞在读锁和写锁的lock()方法上。当前线程获取写锁完成数据准备之后，再获取读锁，随后释放写锁，完成锁降级。 锁降级中读锁的获取是否必要呢？答案是必要的。主要是为了保证数据的可见性，如果当前线程不获取读锁而是直接释放写锁，假设此刻另一个线程（记作线程T）获取了写锁并修改了数据，那么当前线程无法感知线程T的数据更新。如果当前线程获取读锁，即遵循锁降级的步骤，则线程T将会被阻塞，直到当前线程使用数据并释放读锁之后，线程T才能获取写锁进行数据更新。 RentrantReadWriteLock不支持锁升级（把持读锁、获取写锁，最后释放读锁的过程）。目的也是保证数据可见性，如果读锁已被多个线程获取，其中任意线程成功获取了写锁并更新了数据，则其更新对其他获取到读锁的线程是不可见的。 6、小结RentrantReadWriteLock的具体流程梳理完了，回过头来想一下前言的问题，好像并没有得到答案，那么来到ReentrantReadWriteLock代码中，此处主要看一下读锁的获取、释放是否对应AQS中的共享模式。 6.1 读锁的获取、释放12345678public void lock() &#123; //看到这里是不是就明白了，我们的猜想是正确的 sync.acquireShared(1); &#125; public void unlock() &#123; //看到这里是不是就明白了，我们的猜想是正确的 sync.releaseShared(1); &#125; 先来看一下ReadLock的具体实现，在ReentrantReadWriteLock初始化的时候，会在构造函数中初始化ReadLock、WriteLock，具体代码如下： 12345678public ReentrantReadWriteLock() &#123; this(false); &#125; public ReentrantReadWriteLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync(); readerLock = new ReadLock(this); writerLock = new WriteLock(this); &#125; 从ReentrantReadWriteLock构造函数的代码中，可以看到ReadLock初始化的参数是ReentrantReadWriteLock，那么ReadLock需要ReentrantReadWriteLock来做什么呢？ 来看一下ReadLock： 1234private final Sync sync; protected ReadLock(ReentrantReadWriteLock lock) &#123; sync = lock.sync; &#125; 从ReadLock的构造函数中，可以看出，ReadLock需要获取到Sync，那么Sync是谁，又是用来做什么的？ 其实，如果看过JUC下面代码的话，看到Sync，就明白它应该就是AQS的实现类，通过它来实现相关锁的操作。 来看一下代码验证一下： 1234567/** * Synchronization implementation for ReentrantReadWriteLock. * Subclassed into fair and nonfair versions. */ abstract static class Sync extends AbstractQueuedSynchronizer &#123; //具体代码略 &#125; 看到这里可以大体得出这么一个结果：ReadLock获取锁的时候，是通过ReentrantReadWriteLock 内部Sync类来获取的共享锁，也就是读锁的获取是对应AQS中的共享模式。 点进 sync.acquireShared(1)方法，可以看到是调用Sync的父类AQS中方法： 1234public final void acquireShared(int arg) &#123; if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg); &#125; 看到这里，也就明白为啥AQS子类需要重写： tryAcquire tryRelease tryReleaseShared isHeldExclusively 等方法了。 7、参考资料本文第4、5小节整理自：《Java并发编程的艺术》]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>JUC</tag>
        <tag>AQS</tag>
        <tag>JDK</tag>
        <tag>Java</tag>
        <tag>ReentrantReadWriteLock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java AQS Condition的实现分析]]></title>
    <url>%2Fjava-aqs-condition.html</url>
    <content type="text"><![CDATA[本文整理自《Java并发编程的艺术》第五章 作者：方腾飞 魏鹏 程晓明 AQS:AbstractQueuedSynchronizer ConditionObject是同步器AQS的内部类，因为Condition的操作需要获取相关联的锁，所以作为同步器的内部类也较为合理。每个Condition对象都包含着一个队列（以下称为等待队列），该队列是Condition对象实现等待/通知功能的关键。 下面将分析Condition的实现，主要包括：等待队列、等待和通知，下面提到的Condition如果不加说明均指的是ConditionObject。 1、等待队列等待队列是一个FIFO的队列，在队列中的每个节点都包含了一个线程引用，该线程就是在Condition对象上等待的线程，如果一个线程调用了Condition.await()方法，那么该线程将会释放锁、构造成节点加入等待队列并进入等待状态。事实上，节点的定义复用了同步器中节点的定义，也就是说，同步队列和等待队列中节点类型都是同步器的静态内部类AbstractQueuedSynchronizer.Node。 一个Condition包含一个等待队列，Condition拥有首节点（firstWaiter）和尾节点（lastWaiter）。当前线程调用Condition.await()方法，将会以当前线程构造节点，并将节点从尾部加入等待队列，等待队列的基本结构如图5-9所示。 如图所示，Condition拥有首尾节点的引用，而新增节点只需要将原有的尾节点nextWaiter指向它，并且更新尾节点即可。上述节点引用更新的过程并没有使用CAS保证，原因在于调用await()方法的线程必定是获取了锁的线程，也就是说该过程是由锁来保证线程安全的。 在Object的监视器模型上，一个对象拥有一个同步队列和等待队列，而并发包中的Lock（更确切地说是同步器\）拥有一个同步队列和多个等待队列，其对应关系如图5-10所示。 2、等待调用Condition的await()方法（或者以await开头的方法），会使当前线程进入等待队列并释放锁，同时线程状态变为等待状态。当从await()方法返回时，当前线程一定获取了Condition相关联的锁。 如果从队列（同步队列和等待队列）的角度看await()方法，当调用await()方法时，相当于同步队列的首节点（获取了锁的节点）移动到Condition的等待队列中。 Condition的await()方法，如下所示： 1234567891011121314151617181920public final void await() throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); // 当前线程加入等待队列 Node node = addConditionWaiter(); // 释放同步状态，也就是释放锁 int savedState = fullyRelease(node); int interruptMode = 0; while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); &#125; 调用该方法的线程成功获取了锁的线程，也就是同步队列中的首节点，该方法会将当前线程构造成节点并加入等待队列中，然后释放同步状态，唤醒同步队列中的后继节点，然后当前线程会进入等待状态。 当等待队列中的节点被唤醒，则唤醒节点的线程开始尝试获取同步状态。如果不是通过其他线程调用Condition.signal()方法唤醒，而是对等待线程进行中断，则会抛出InterruptedException。 如果从队列的角度去看，当前线程加入Condition的等待队列，该过程如图5-11示。 如图所示，同步队列的首节点并不会直接加入等待队列，而是通过addConditionWaiter()方法把当前线程构造成一个新的节点并将其加入等待队列中。 3、通知调用Condition的signal()方法，将会唤醒在等待队列中等待时间最长的节点（首节点），在唤醒节点之前，会将节点移到同步队列中。 Condition的signal()方法，如代码清单5-23所示。代码清单5-23 ConditionObject的signal方法 12345678public final void signal() &#123; //isHeldExclusively() AQS 子类实现 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first); &#125; 调用该方法的前置条件是当前线程必须获取了锁，可以看到signal()方法进行了isHeldExclusively()检查，也就是当前线程必须是获取了锁的线程。接着获取等待队列的首节点，将其移动到同步队列并使用LockSupport唤醒节点中的线程。 节点从等待队列移动到同步队列的过程如图5-12所示。 通过调用同步器的enq(Node node)方法，等待队列中的头节点线程安全地移动到同步队列。当节点移动到同步队列后，当前线程再使用LockSupport唤醒该节点的线程。 被唤醒后的线程，将从await()方法中的while循环中退出（isOnSyncQueue(Node node)方法返回true，节点已经在同步队列中），进而调用同步器的acquireQueued()方法加入到获取同步状态的竞争中。 成功获取同步状态（或者说锁）之后，被唤醒的线程将从先前调用的await()方法返回，此时该线程已经成功地获取了锁。 Condition的signalAll()方法，相当于对等待队列中的每个节点均执行一次signal()方法，效果就是将等待队列中所有节点全部移动到同步队列中，并唤醒每个节点的线程。]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>AQS</tag>
        <tag>JDK</tag>
        <tag>Java</tag>
        <tag>Condition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL InnoDB存储引擎：外键与锁]]></title>
    <url>%2Fmysql-innodb-foreign-key-lock.html</url>
    <content type="text"><![CDATA[本文整理自：《MySQL技术内幕:InnoDB存储引擎》 第二版 作者：姜承尧 出版时间：2013-05 外键主要用于引用完整性的约束检查在InnoDB存储引擎中，对于一个外键列，如果没有显式地对这个列加索引，InnoDB存储引擎会自动对其加一个索引，因为这样可以避免表锁。 这比Oracle数据库做得好，Oracle数据库不会自动添加索引，用户必须自己手动添加，这也导致了Oracle数据库中可能产生死锁。 对于外键值的插入或更新，首先需要检查父表中的记录，既SELECT父表。但是对于父表的SELECT操作，不是使用一致性非锁定读的方式，因为这会发生数据不一致的问题，因此这时使用的是SELECT…LOCK IN SHARE MODE方式，即主动对父表加一个S锁。如果这时父表上已经这样加X锁，子表上的操作会被阻塞，如下：在上述的例子中，两个会话中的事务都没有进行COMMIT或ROLLBACK操作，而会话B的操作会被阻塞。这是因为 id为3的父表在会话 A中已经加了一个X锁，而此时在会话 B中用户又需要对父表中 id为3的行加一个 S锁，这时 INSERT的操作会被阻塞。设想如果访问父表时，使用的是一致性的非锁定读，这时Session B会读到父表有id=3的记录，可以进行插入操作。但是如果会话A对事务提交了，则父表中就不存在id为3的记录。数据在父、子表就会存在不一致的情况。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>InnoDB</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL InnoDB存储引擎：行锁的3种算法]]></title>
    <url>%2Fmysql-innodb-row-lock-algorithm.html</url>
    <content type="text"><![CDATA[行锁的三种算法InnoDB存储引擎有3种行锁的算法，其分别是： Record Lock：单个行记录上的范围 Gap Lock：间隙锁，锁定一个范围，但不包含记录本身 Next-Key Lock：Gap Lock + Record Lock，锁定一个范围，并且锁定记录本身 Record Lock总是会锁住索引记录，如果InnoDB存储引擎建立的时候没有设置任何一个索引，这时InnoDB存储引擎会使用隐式的主键来进行锁定。 Next-Key Lock是结合了Gap Lock和Record Lock的一种锁定算法，在Next-Key Lock算法下，innodb对于行的查询都是采用这种锁定算法。例如一个索引有9,11,13,20这4个值，那么该索引可能被Next-Key Locking的范围为（左开右闭 ）：(- &amp;，9](9,11](13,20](20,+ &amp;) 采用Next-Key Lock的锁定技术称为Next-Key Locking。这种设计的目的是为了解决幻读（Phantom Problem）。利用这种锁定技术，锁定的不是单个值，而是一个范围。 当查询的索引含有唯一属性时，innodb存储引擎会对Next-Key Lock进行优化，将其降级为Record Lock，即锁住索引记录本身，而不再是范围。对于唯一索引，其加上的是Record Lock，仅锁住记录本身。但也有特别情况，那就是唯一索引由多个列组成，而查询仅是查找多个唯一索引列中的其中一个，那么加锁的情况依然是Next-key lock。 12345678910DROP TABLEIF EXISTS t;CREATE TABLE t (a INT PRIMARY KEY);INSERT INTO tVALUES (1), (2), (5); 表t中共有1、2、5三个值。在上面的例子中，在会话A中首先对a=5进行X锁定。而由于a是主键且唯一，因此锁定的仅是5这个值，而不是(2,5)这个范围，这样在会话B中插入值4而不会阻塞，可以立即插入并返回。即锁定由Next-Key Lock算法降级为了Record Lock，从而提高应用的并发性。正如前面所介绍的，Next-Key降级为Record Lock仅在查询的列是唯一索引的情况下。若是辅助索引，则情况会完全不同。同样，首先根据如下代码创建测试表z： 1234567891011121314CREATE TABLE Z ( a INT, b INT, PRIMARY KEY (a), KEY (b));INSERT INTO ZVALUES (1, 1), (3, 1), (5, 3), (7, 6), (10, 8); 表z的列b是辅助索引，若在会话A中执行下面的SQL语句： 1SELECT * FROM Z WHERE b=3 FOR UPDATE; 很明显，这时SQL语句通过索引列b进行查询，因此其使用传统的Next-Key Locking技术加锁，并且由于有两个索引，其需要分别进行锁定。对于聚集索引，其仅对列a等于5的索引加上Record Lock。而对于辅助索引，其加上的是Next-Key Locking，锁定的范围是(1,3),特别需要注意的是，InnoDB存储引擎会对辅助索引下一个键值加上gap lock，即还有一个辅助索引范围为(3,6)的锁。 因此，若在新会话B中运行下面的SQL语句，都会被阻塞： 123SELECT * FROM Z WHERE a=5 LOCK IN SHARE MODE;INSERT INTO Z SELECT 4,2;INSERT INTO Z SELECT 6,5; 第一个SQL语句不能执行，因为在会话A中执行的SQL语句已经聚集索引中列a=5的值加上X锁，因此执行会被阻塞。第二个SQL语句，主键插入4，没有问题，但是插入的辅助索引值2在锁定的范围（1，3）中因此执行同样会被阻塞。第三个SQL语句，插入的主键6没有被锁定，5也不在范围（1，3）之间。但插入的值5在另一个锁定范围（3，6）中，故同样需要等待。而下面的SQL语句，不会被阻塞，可以立即执行： 123INSERT INTO Z SELECT 8,6;INSERT INTO Z SEELCT 2,0;INSERT INTO Z SELECT 6,7; 从上面的例子中可以看到，Gap Lock的作用是为了阻止多个事务将记录插入到同一个范围内，而这会导致Phantom Problem问题的产生。 例如在上面的例子中，会话A中用户已经锁定了b=3的记录。若此时没有Gap Lock锁定（3，6），那么用户可以插入索引b列为3的记录，这会导致会话A中的用户再次执行同样查询时会返回不同的记录，导致Phantom Problem问题的产生。 用户可以通过以下两种方式来显式地关闭Gap Lock： 将事务的隔离级别设置为READ COMMITTED 将参数innodb_locks_unsafe_for_binlog设置为1 在上述的配置下，除了外键约束和唯一性检查依然需要的Gap Lock，其余情况仅使用Record Lock进行锁定。但需要牢记的是，上述设置破坏了事务的隔离性，并且对于replication，可能会导致主从数据的不一致。此外，从性能上来看，READ COMMITTED也不会优于默认的事务隔离级别READ REPEATABLE。 在InnoDB存储引擎中，对于INSERT的操作，其会检查插入记录的下一条记录是否被锁定，若已被锁定，则不允许查询。对于上面的例子，会话A已经锁定了表z中b=3的记录，即已经锁定了（1，3）的范围，这时若在其他会话中进行如下的插入同样会导致阻塞： 1INSERT INTO Z SELECT 2,2; 因为在辅助索引列b上插入值为2的记录时，会监测到下一个记录3已经被索引。而将插入修改为如下的值，可以立即执行： 1INSERT INTO Z SELECT 2,0; 最后再次提醒的是，对于唯一键值的锁定,Next-Key Lock降级为Record Lock仅存在于查询所有的唯一索引一列。若唯一索引由多个列组成，而查询是查找多个唯一索引列中的其中一个，那么查询其实是range类型查询，而不是point类型查询故InnoDB存储引擎依然使用Next-Key Lock进行锁定。 解决 Phantom Problem在默认的事务隔离级别下，即REPEATABLE READ下，InnoDB存储引擎采用Next-Key Locking机制来避免Phantom Problem (幻像问题）。这点可能不同于与其他的数据库，如Oracle数据库，因为其可能需要在SERIALIZABLE的事务隔离级别下才能解决 Phantom Problem。 Phantom Problem是指在同一事务下，连续执行两次同样的SQL语句可能导致不同的结果，第二次的SQL语句可能会返回之前不存在的行。 下面将演示这个例子，使用前一小节所创建的表t。表t由1、2、5这三个值组成： 12345678910DROP TABLEIF EXISTS t;CREATE TABLE t (a INT PRIMARY KEY);INSERT INTO tVALUES (1), (2), (5); 若这时事务T1执行如下的SQL语句: 1SELECT * FROM t WHERE a&gt; 2 FOR UPDATE; 注意这时事务T1并没有进行提交操作，上述应该返回5这个结果。若与此同时,另一个事务T2插入了 4这个值，并且数据库允许该操作，那么事务T1再次执行上述SQL语句会得到结果4和5。这与第一次得到的结果不同，违反了事务的隔离性，即当前事务能够看到其他事务的结果。其过程如表6-13所示：InnoDB存储引擎采用Next-Key Locking的算法避免Phantom Problem。对于上述的SQL语句SELECT * FROM t WHERE a&gt;2 FOR UPDATE,其锁住的不是5这单个值，而是对（2, +〇〇)这个范围加了 X锁。因此任何对于这个范围的插入都是不被允许的，从而避免 Phantom Problem。 InnoDB存储引擎默认的事务隔离级别是REPEATABLE READ,在该隔离级别下,其采用Next-Key Locking的方式来加锁。而在事务隔离级别READ COMMITTED下,其仅采用Record Lock，因此在上述的示例中，会话A需要将事务的隔离级别设置为READ COMMITTED。 此外，用户可以通过InnoDB存储引擎的Next-Key Locking机制在应用层面实现唯一性的检查。 例如： 1234SELECT * FROM table WHERE col=xxx LOCK IN SHARE MODE;If not found any row:# unique for insert valueINSERT INTO table VALUES (...); 如果用户通过索引査询一个值，并对该行加上一个SLock，那么即使査询的值不在，其锁定的也是一个范围，因此若没有返回任何行，那么新插人的值一定是唯一的。也许有读者会有疑问，如果在进行第一步SELECT •••LOCK IN SHARE MODE操作时，有多个事务并发操作，那么这种唯一性检査机制是否存在问题。其实并不会，因为这时会导致死锁，只有一个事务的插人操作会成功，而其余的事务会抛出死锁的错误，如表6-14所示。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>InnoDB</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL InnoDB存储引擎：分区表]]></title>
    <url>%2Fmysql-innodb-partition-table.html</url>
    <content type="text"><![CDATA[本文整理自：《MySQL技术内幕:InnoDB存储引擎》 第二版 作者：姜承尧 出版时间：2013-05 MySQL分区表介绍分区是一种表的设计模式，正确的分区可以极大地提升数据库的查询效率，完成更高质量的SQL编程。但是如果错误地使用分区，那么分区可能带来毁灭性的的结果。 分区功能并不是在存储引擎层完成的，因此不只有InnoDB存储引擎支持分区，常见的存储引擎MyISAM、NDB等都支持分区。 但是并不是所有的存储引擎都支持，如CSV、FEDORATED、MERGE等就不支持分区。在使用此分区功能前，应该对选择的存储引擎对分区的支持有所了解。 MySQL数据库在5.1版本时添加了对分区的支持，分区的过程是将一个表或索引分解为多个更小、更可管理的部分。就访问数据库的应用而言，从逻辑上讲，只有一个表或一个索引，但是在物理上这个表或索引可能由数十个物理分区组成。每个分区都是独立的对象，可以独自处理，也可以作为一个更大对象的一部分进行处理。 MySQL数据库支持的分区类型为水平分区（指将同一个表中不同行的记录分配到不同的物理文件中），并不支持垂直分区（指将同一表中不同列的记录分配到不同的物理文件中）。此外，MySQL数据库的分区是局部分区索引，一个分区中既存放了数据又存放了索引。而全局分区是指，数据存放在各个分区中，但是所有数据的索引放在一个对象中。目前，MySQL数据库还不支持全局分区。 可以通过以下命令来查看当前数据库是否启用了分区功能： 123456789101112131415mysql&gt; show global variables like &apos;%partition%&apos;;+-------------------+-------+| Variable_name | Value |+-------------------+-------+| have_partitioning | YES |+-------------------+-------+1 row in set (0.04 sec) mysql&gt; show plugins*************************** 43. row *************************** Name: partition Status: ACTIVE Type: STORAGE ENGINELibrary: NULLLicense: GPL 有时候可能会有这么一种误区，只要启用了分区，数据库就会运行的更快。这个结论结论是存在很多问题的，就经验来看，分区可能会给某些SQL语句性能带来提高，但是分区主要用于数据库高可用性的管理。在OLTP应用中，对于分区的使用应该非常小心，总之，如果只是一味地使用分区，而不理解分区是如何工作的，也不清楚你的应用如何使用分区，那么分区极有可能会对性能产生负面的影响。 MySQL分区类型RANGE分区RANGE分区，是最常用的一种分区类型，基于属于一个给定连续区间的列值，把多行分配给分区。这些区间要连续且不能相互重叠，使用VALUES LESS THAN操作符来进行定义。 LIST分区LIST分区和RANGE分区类似，区别在于LIST分区是基于列值匹配一个离散值集合中的某个值来进行选择，而非连续的。 LIST分区通过使用“PARTITION BY LIST(expr)”来实现，其中“expr” 是某列值或一个基于某个列值、并返回一个整数值的表达式，然后通过“VALUES IN (value_list)”的方式来定义每个分区，其中“value_list”是一个通过逗号分隔的整数列表。 HASH分区HASH分区的目的是将数据均匀地分布到预先定义的各个分区中，保证各分区的数据量大致都是一样的。在RANGE和LIST分区中，必须明确指定一个给定的列值或列值集合应该保存在哪个分区中；而在HASH分区中，MySQL自动完成这些工作，用户所要做的只是基于将要进行哈希分区的列值指定一个列值或表达式，以及指定被分区的表将要被分隔成的分区数量。 要使用HASH分区来分割一个表，要在CREATE TABLE 语句上添加一个“PARTITION BY HASH (expr)”子句，其中“expr”是一个返回一个整数的表达式。它可以仅仅是字段类型为MySQL 整型的一列的名字。此外，你很可能需要在后面再添加一个“PARTITIONS num”子句，其中num是一个非负的整数，它表示表将要被分割成分区的数量，如果没有包括一个PARTITIONS子句，那么分区的数量将默认为1。 LINER HASHMySQL还支持线性哈希功能，它与常规哈希的区别在于，线性哈希功能使用的一个线性的2的幂（powers-of-two）运算法则，而常规哈希使用的是求哈希函数值的模数。线性哈希分区和常规哈希分区在语法上的唯一区别在于，在“PARTITION BY” 子句中添加“LINEAR”关键字。 KEY分区KEY分区和HASH分区相似，不同之处在于HASH分区使用用户定义的函数进行分区，支持字符串HASH分区，KEY分区使用MySQL数据库提供的函数进行分区，这些函数基于与PASSWORD()一样的运算法则。 COLUMNS在前面说了RANGE、LIST、HASH和KEY这四种分区中，分区的条件是：数据必须为整形（interger），如果不是整形，那应该需要通过函数将其转化为整形，如YEAR()，TO_DAYS()，MONTH()等函数。MySQL5.5版本开始支持COLUMNS分区，可视为RANGE分区和LIST分区的一种进化。COLUMNS分区可以直接使用非整形的数据进行分区，分区根据类型直接比较而得，不需要转化为整形。此外，RANGE COLUMNS分区可以对多个列的值进行分区。 COLUMNS分区支持以下的数据类型： 所有的整形类型，如INT、SMALLINT、TINYINT和BIGINT。而FLOAT和DECIMAL则不予支持。 日期类型，如DATE何DATETIME。其余的日期类型不予支持。 字符串类型，如CHAR、VARCHAR、BINARY和VARBINARY。而BLOB和TEXT类型不予支持。 分区中的NULL值MySQL数据库允许对NULL值做分区，但是处理的方法与其他数据库可能完全不同。MySQL数据库的分区总是视NULL值小于任何的一个非NULL值，这和MySQL数据库中处理NULL值的ORDER BY操作是一样的。 因此对于不同的分区类型，MySQL数据库对于NULL值的处理也是各不相同。 对于RANGE分区，如果向分区列插入了NULL值，则MySQL数据库会将该值放入最左边的分区。 对于LIST分区，如果向分区列插入了NULL值，则必须显示地指出哪个分区放入NULL值，否则会报错。对于LIST分区，如果向分区列插入了NULL值，则必须显示地指出哪个分区放入NULL值，否则会报错。 对于HASH和KEY分区，对于NULL值的处理方法和RANGE分区、LIST分区不一样。任何分区函数都会将含有NULL值的记录返回为0。 分区和性能分区真的会加快数据库的查询吗？实际上可能根本感觉不到查询速度的提升，甚至会发现查询速度急剧下降，因此在合理使用分区之前，必须了解分区的使用环境。 数据库的应用分为两类：一类是OLTP（在线事务处理），如Blog、电子商务、网络游戏等；另一类是OLAP（在线分析处理），如数据仓库、数据集市。对于OLAP的应用，分区的确是可以很好地提高查询的性能，因为OLAP应用大多数查询需要频繁地扫描一张很大的表。假设有一张1亿行的表，其中有一个时间戳属性列。用户的查询需要从这张表中获取一年的数据。如果按时间戳进行分区，则只需要扫描相应的分区即可。这就是前面介绍的分区修剪技术。 对于OLTP的应用，分区应该非常小心。在这种应用下，通常不可能会获取一张大表10%的数据，大部分都是通过索引返回几条记录即可。而根据B+树索引的原理可知，对于一张大表，一般的B+树需要2~3次的磁盘IO。因此B+树可以很好地完成操作，不需要分区的帮助，并且设计不好的分区会带来严重的性能问题。 如很多开发团队会认为含有1000w行的表是一张非常巨大的表，所以他们往往会选择采用分区，如对主键做10个HASH的分区，这样每个分区就只有100w的数据了，因此查询应该变得更快了。如select * from table where pk=@pk。但是有没有考虑过这样一种情况：100w和1000w行的数据本身构成的B+树的层次都是一样的，可能都是2~3层。那么上述走主键分区的索引并不会带来性能的提高。好的，如果1000w的B+树高度是3,100w的B+树高度是2，那么上述按主键分区的索引可以避免1次IO，从而提高查询的效率。这没问题，但是这张表只有主键索引，没有任何其他的列需要查询的。如果还有类似如下的SQL：select * from table where key=@key，这时对于key的查询需要扫描所有的10个分区，即使每个分区的查询开销为2次IO，则一共需要20次IO。而对于原来单表的设计，对于KEY的查询只需要2~3次IO。 由以上结论可以看出，对于在OLTP场景中使用分区一定要特别小心了。 MySQL 5.7对分区的改进在MySQL 5.6里面，分区的信息是在MySQL Server层维护的（在.par文件里面），InnoDB引擎层是不知道有分区这个概念的，InnoDB引擎层把每一个分区都当成一张普通的InnoDB表。在打开一个分区表时，会打开很多个分区，打开这些分区表就相当于打开了同等数量的InnoDB表，这需要更多内存存放InnoDB表的元数据和各种与ibd文件打开相关的各种cache与handler的信息。在MySQL 5.7里面，InnoDB引入了Native Partitioning，它把分区的信息从Server层移到了InnoDB层，打开一个分区表和打开一个InnoDB表的内存开销基本是一样的。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>InnoDB</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL InnoDB存储引擎：一致性锁定读]]></title>
    <url>%2Fmysql-innodb-consistent-locking-read.html</url>
    <content type="text"><![CDATA[本文整理自：《MySQL技术内幕:InnoDB存储引擎》 第二版 作者：姜承尧 出版时间：2013-05 在前一小节中讲到，在默认配置下，即事务的隔离级别为 REPEATABLE READ 模式下， InnoDB 存储引擎的 SELECT 操作使用一致性非锁定读。但是在某些情况下，用户需要显式地对数据库读取操作进行加锁以保证数据逻辑的一致性。而这要求数据库支持加锁语句，即使是对于SELECT的只读操作。InnoDB存储引擎对于SELECT语句支持两种一致性的锁定读（locking read)操作： 12SELECT......FOR UPDATESELECT......LOCK IN SHARE MODE SELECT…FOR UPDATE对读取的行记录加一个X锁，其他事务不能对已锁定的行加上任何锁。SELECT…LOCK IN SHARE MODE对读取的行记录加一个S锁，其他事务可以向被锁定的行加S锁，但是如果加X锁，则会被阻塞。 对于一致性非锁定读，即使读取的行已被执行了 SELECT…FOR UPDATE,也是可以进行读取的，这和之前讨论的情况一样。此外，SELECT…FOR UPDATE, SELECT…LOCK IN SHARE MODE必须在一个事务中，当事务提交了，锁也就释放了。因此在使用上述两句SELECT锁定语句时，务必加上BEGIN,START TRANSACTION 或者SET AUTOCOMMIT =0 。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>InnoDB</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL InnoDB存储引擎：一致性非锁定读]]></title>
    <url>%2Fmysql-innodb-consistent-nonlocking-read.html</url>
    <content type="text"><![CDATA[本文整理自：《MySQL技术内幕:InnoDB存储引擎》 第二版 作者：姜承尧 出版时间：2013-05 一致性的非锁定行读（consistent nonlocking read）是指InnoDB存储引擎通过行多版本控制（multi versioning）的方式来读取当前执行时间数据库中行的数据。如果读取的行正在执行DELETE、UPDATE操作，这是读取操作不会因此而会等待行上锁的释放，相反，InnoDB会去读取行的一个快照数据。 下图直观展示了一致性的非锁定行读： 之所以称其为非锁定读，因为不需要等待访问的行上X锁的释放。快照数据是指该行的之前版本的数据，该实现是通过undo段来完成。而undo段用来在此事务中回滚数据，因此快照数据本身是没有额外的开销。此外，读取快照数据是不需要上锁的，因为没有事务需要对历史的数据进行修改操作。 可以看到，非锁定读机制极大地提髙了数据库的并发性。在InnoDB存储引擎的默认设置下，这是默认的读取方式，即读取不会占用和等待表上的锁。但是在不同事务隔离级别下，读取的方式不同，并不是在每个事务隔离级别下都是采用非锁定的一致性读。此外，即使都是使用非锁定的一致性读，但是对于快照数据的定义也各不相同。 通过图6-4可以知道，快照数据其实就是当前行数据之前的历史版本，每行记录可能有多个版本。就图6-4所显示的，一个行记录可能有不止一个快照数据，一般称这种技术为行多版本技术。由此带来的并发控制，称之为多版本并发控制（MultiVersionConcurrencyControl MVCC）。 在事务隔离级别READ COMMITTED和REPEATABLE READ(InnoDB存储引擎的默认事务隔离级别）下，InnoDB存储引擎使用非锁定的一致性读。然而，对于快照数据的定义却不相同。在READ COMMITTED事务隔离级别下，对于快照数据，非一致性读总是读取被锁定行的最新一份快照数据。而在REPEATABLE READ事务隔离级别下，对于快照数据，非一致性读总是读取事务开始时的行数据版本（关键在于事务之间的隔离性）。来看下面的一个例子，首先在当前MySQL数据库的连接会话A中执行如下SQL语句： 1234567891011# Session Amysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)Sql&gt; SELECT * FROM parent WHERE id =1;+----+| id |+----+| 1 |+----+1 row in set (0.00 sec) 会话A中已通过显式地执行命令BEGIN开启了一个事务，并读取了表parent中id为1的数据，但是事务并没有结束。与此同时，用户再开启另一个会话B，这样可以模拟并发的情况，然后对会话B做如下的操作： 123456mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; UPDATE parent SET id=3 WHERE id=l;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 warnings: 0 在会话B中将事务表parent中id为1的记录修改为id=3，但是事务同样没有提交，这样id=1的行其实加了一个X锁。这时如果在会话A中再次读取id为1的记录，根据InnoDB存储引擎的特性，即在READ COMMITTED和REPEATETABLE READ的事务隔离级别下会使用非锁定的一致性读。回到之前的会话A,接着上次未提交的事务，执行SQL语句SELECT * FROM parent WHERE id=1的操作，这时不管使用READ COMMITTED还是REPEATABLE READ的事务隔离级别，显示的数据应该都是： 1234567mysql&gt; SELECT FROM parent WHERE id =l;+----+| id |+----+| 1 |+----+1 row in set (0.00 sec) 由于当前id=1的数据被修改了1次，因此只有一个行版本的记录。接着，在会话B中提交上次的事务。 123# Session Bmysql&gt; commitQuery OK, 0 rows affected (0.01 sec) 在会话B提交事务后，这时在会话A中再运行SELECT * FROM parent WHERE id=1的SQL语句，在READ COMMITTED和REPEATABLE事务隔离级别下得到结果 就不一样了。对于READ COMMITTED的事务隔离级别，它总是读取行的最新版本，如果行被锁定了，则读取该行版本的最新一个快照（fresh snapshot）。在上述例子中，因为会话B已经提交了事务，所以READ COMMITTED事务隔离级别下会得到如下结果： 1234567mysql&gt;SELECT @@tx_isolation\G;**************************** 1.row ****************************@@tx_isolation: READ-COMMITTED1 row in set (0.00 sec)mysql&gt; SELECT FROM parent WHERE id=1:Empty set (0.00 sec) 而对于REPEATETABLE 的事务隔离级别，总是读取事务开始时的行数据。因此对于REPEATETABLE READ事务隔离级别,其得到的结果如下： 123456789101112mysql&gt; SELECT @@tx_isolation\G;**************************** 1.row ****************************@@tx_isolation: REPEATABLE-READ1 row in set (0.00 sec)mysql&gt; SELECT FROM parent WHERE id=1;+----+| id |+----+| 1 |+----+1 row in set (0.00 sec) 下面将从时间的角度展现上述演示的示例过程，如表6-8所示。需要特别注意的是，对于READ COMMITTED 的事务隔离级别而言，从数据库理论的角度来看，其违反了事务ACID中的I的特性，即隔离性。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>InnoDB</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日志服务架构设计]]></title>
    <url>%2Flog-service-architecture-design.html</url>
    <content type="text"><![CDATA[在满足业务需求的前提下，代码、架构，越简单，越稳定。 最近想把之前做过的日志项目及个人的思考梳理一下，于是有了本文。 背景我们这边应用部署的环境比较复杂，主要有以下几种： 机器直接部署 通过原生docker部署 通过kubernates集群部署 部署环境不统一，导致查看应用日志很不方便。 业务需求与部署环境对应，对于日志收集需求分为以下几类： 机器上的文本日志（直接运行在物理机或者虚拟机中的应用日志） 运行在docker容器中的应用日志 运行在kubernates集群中的应用日志 具体业务需求可以拆分为： 按照项目、应用、实例维度检索日志并支持搜索关键字高亮（因为大家检索日志的时候，肯定是检索某个项目、某个应用、某个实例的日志） 支持检索出某条想要的日志后，可以查看上下文（查看该日志所在日志文件的日志上下文） 支持日志下载（目前支持两种场景：搜索结果下载、上下文下载；支持两种方式：在线下载、离线下载） 支持自动化批量部署、卸载Agent，部署、卸载过程可视化 单实例支持多elasticsearch集群 支持文本日志、docker日志、k8s日志并能与将日志与其业务意义对应上。（即不管是哪种日志形式、来源，最终都需要与业务意义上的项目、应用、实例对应起来，因为对于日志的使用者来说，查询日志的出发点肯定是查询某个项目、某个应用（可以不选）、某个实例（可以不选）、某段时间的日志。） 支持部署到业务自己的集群中 需求已经明确了，下面看一下业界方案。 业界日志系统架构 Collector的作用是： 清洗、汇聚数据，减少对于后端集群的压力。 安全，不允许Agent直连kafka等内部集群，保证一定的安全性，即使后端发生调整也能保证对于Agent连接、认证方式的稳定。 MQ的作用是削峰填谷、解耦、多次消费。 上图的架构是业界比较通用的一种架构，对于各种场景都考虑的比较全。 既然业界的架构已经这么完备，那么我们是否就直接采用呢？ 对于我们而言，有以下几个问题： 涉及的组件比较多，链路比较长，运维比较麻烦 这一整套架构，不利于单独部署（比如某个业务应用部署机房网络是隔离的，而且项目又不大，只能提供有限的几台机器，这时候如果需要部署业界这套架构的话，资源就会比较受限，如果想做到即支持业界架构组件的可插拔（比如可灵活的决定是否需要Collector、MQ），那么就需要运维几套配置或代码） 最关键的就是其中组件提供的功能，我们目前用不到。比如MQ的削峰填谷、多次消费。 组件选择选择组件，我们这边主要是从以下几个方面进行考量的： 组件对应的开源生态完整、活跃度高 对应的技术栈是我们所熟悉的，我们这边语言技术栈主要是Java、Go，如果组件语言是C、Ruby，应该就被排除了。 运维成本 易部署、性能好 Agent一提到日志收集方案，大家第一个想到的肯定是ELK(Elasticsearch、Logstash、Kibana )，但Logstash依赖于JVM不管是性能还是简洁性，都不是日志收集agent的首选。 个人感觉一个好的agent应该是资源占用少，性能好，不依赖别的组件，可以独立部署。而Logstash明显不符合这几点要求，也许正是基于这些考虑elastic推出了Filebeat。 Collector、MQElasticsearch集群在部署的时候，一般都是提前估计好容量、机器、shard等信息，因为Elasticsearch集群运行后，再水平拓展，比较麻烦，而我们这边由于业务及成本限制无法很好的预估容量，所以就结合公司实际要求：使用日志服务的业务方自带机器，也就是业务方会有独立的Elasticsearch集群。 每个业务方都使用自己的Elasticsearch集群，所以集群压力不会很大，从而Collector、MQ这两个组件对于我们的作用也就很小了。 ETL因为Elasticsearch Ingest Node完全可以满足我们的解析需求，所以就没有必要再引入Logstash等相关组件了。 到这里，基本可以看出我们的架构如下： 架构设计的几个原则： 合适优于业界领先 简单优于复杂 演化优于一步到位 具体实现基于需求及EFK套件，梳理我们场景中特有的东西： docker日志的场景比较单一，都是通过之前一个产品A发布部署的，其docker命名规则比较统一，可以通过截取docker.container.name来获取应用名字；同时在部署的时候，可以知道部署目标机器的ip，这样就可以通过应用+ip来作为实例名称。 k8s场景也比较统一，都是通过之前一个产品B发布部署的，其pod命名规则比较统一，可以通过截取kubernetes.pod.name来获取应用名字（但需要通过namespaces关联到tenant，再通过tenant与项目一一对应）；k8s中的pod.name就是唯一的，以此来作为实例名称即可。 文本日志：因为文本日志主要的场景是已经裸机部署的应用，这种场景下，不存在应用自动迁移的情况，所以文本日志的应用名称、实例名称可以在部署的时候打上标签即可。 具体规则及解析见下图（实例部分处理暂未标注）： 推荐写日志到文本文件中，使用标准输出就好。 到这里可以发现我们选择Filebeat来作为日志的收集端，Elasticsearch来存储日志并提供检索能力。 那么，日志的清洗在哪里做呢？ 日志的清洗一般有两种方式： 先把日志收集到kafka，再通过Logstash消费kafka的数据，来清洗数据 直接通过Elasticsearch的[Ingest Node]来清洗数据，因为Ingest Node也支持Grok表达式 对于，我们的场景而言，我们需要清洗数据的要求比较简单，主要是应用、实例名称的截取还有文本日志中日志时间的处理（@timestamp重置，时区处理），所以我们选择了方案2。 在我们的方案中，并没有提供Kibana 的界面直接给用户用，而是我们自己根据公司业务独立开发的。 前端界面为什么不采用Kibana，而需要自己开发？ kibana对于业务开发人员有一定的学习成本 kibana界面没有很好的将日志内容与业务意义关联起来（界面选择总比一次次的输入要好，这也是我们将日志的项目、应用、实例等业务信息解析出来的原因） log-search支持Query String，因此对于熟悉kibana的开发人员来说，在我们自己开发的前端界面检索效果是一样的。 log-search提供的功能可以参见github：https://github.com/jiankunking/log-search 如果日志需要清洗的比较多，可以采用方案1，或者先不清洗，先把数据落到Elasticsearch，然后在查询的时候，进行处理。比如在我们的场景中，可以先把日志落到Elasticsearch中，然后在需要检索应用名称的时候，通过代码来处理并获取app名字。 监控、告警其实基于日志可以做很多事情，比如： 基于日志做监控（Google Dapper） 基于日志做告警 基于日志做Machine Learning 具体思路，可以参见下图： 前提：能要求使用方，按照某种规则打印日志。监控发展：监控基本就是先打通链路trace，然后再在上报信息或者日志信息中，加强业务方面标识，即给监控添加业务维度方面的视角。 其它DaemonSet以DaemonSet方式部署Filebeat来收集日志，其实收集也是宿主机/var/lib/docker/containers目录下的日志。Running Filebeat on Kubernetes Sidecar一个POD中运行一个sidecar的日志agent容器，用于采集该POD主容器产生的日志。 莫名想起了istio。 Filebeat可以以sidecar模式来进行容器日志的收集，也就是filebeat和具体的服务容器部署在同一个pod内，指定收集日志的路径或文件，&gt; 即可将日志发送到指定位置或Elasticsearch这类的搜索引擎。每个pod内部署filebeat的模式，好处是和具体的应用服务低耦合，可扩展性强，不过需要在yaml进行额外配置。]]></content>
      <categories>
        <category>Architecture</category>
      </categories>
      <tags>
        <tag>Architecture</tag>
        <tag>原创</tag>
        <tag>Log</tag>
        <tag>Elasticsearch</tag>
        <tag>Filebeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[当你在浏览器中输入 google.com 并且按下回车之后发生了什么？]]></title>
    <url>%2Fwhat-happens-when.html</url>
    <content type="text"><![CDATA[当你在浏览器中输入 google.com 并且按下回车之后发生了什么？整理自 https://github.com/skyline75489/what-happens-when-zh_CN 当···时发生了什么？这个仓库试图回答一个古老的面试问题：当你在浏览器中输入 google.com 并且按下回车之后发生了什么？ 不过我们不再局限于平常的回答，而是想办法回答地尽可能具体，不遗漏任何细节。 目录按下”g”键接下来的内容介绍了物理键盘和系统中断的工作原理，但是有一部分内容却没有涉及。当你按下“g”键，浏览器接收到这个消息之后，会触发自动完成机制。浏览器根据自己的算法，以及你是否处于隐私浏览模式，会在浏览器的地址框下方给出输入建议。大部分算法会优先考虑根据你的搜索历史和书签等内容给出建议。你打算输入”google.com”，因此给出的建议并不匹配。但是输入过程中仍然有大量的代码在后台运行，你的每一次按键都会使得给出的建议更加准确。甚至有可能在你输入之前，浏览器就将”google.com” 建议给你。 回车键按下为了从零开始，我们选择键盘上的回车键被按到最低处作为起点。在这个时刻，一个专用于回车键的电流回路被直接地或者通过电容器间接地闭合了，使得少量的电流进入了键盘的逻辑电路系统。这个系统会扫描每个键的状态，对于按键开关的电位弹跳变化进行噪音消除(debounce)，并将其转化为键盘码值。在这里，回车的码值是13。键盘控制器在得到码值之后，将其编码，用于之后的传输。现在这个传输过程几乎都是通过通用串行总线(USB)或者蓝牙(Bluetooth)来进行的，以前是通过PS/2或者ADB连接进行。 USB键盘： 键盘的USB元件通过计算机上的USB接口与USB控制器相连接，USB接口中的第一号针为它提供了5V的电压 键码值存储在键盘内部电路一个叫做”endpoint”的寄存器内 USB控制器大概每隔10ms便查询一次”endpoint”以得到存储的键码值数据，这个最短时间间隔由键盘提供 键值码值通过USB串行接口引擎被转换成一个或者多个遵循低层USB协议的USB数据包 这些数据包通过D+针或者D-针(中间的两个针)，以最高1.5Mb/s的速度从键盘传输至计算机。速度限制是因为人机交互设备总是被声明成”低速设备”（USB 2.0 compliance） 这个串行信号在计算机的USB控制器处被解码，然后被人机交互设备通用键盘驱动进行进一步解释。之后按键的码值被传输到操作系统的硬件抽象层 虚拟键盘（触屏设备）： 在现代电容屏上，当用户把手指放在屏幕上时，一小部分电流从传导层的静电域经过手指传导，形成了一个回路，使得屏幕上触控的那一点电压下降，屏幕控制器产生一个中断，报告这次“点击”的坐标 然后移动操作系统通知当前活跃的应用，有一个点击事件发生在它的某个GUI部件上了，现在这个部件是虚拟键盘的按钮 虚拟键盘引发一个软中断，返回给OS一个“按键按下”消息 这个消息又返回来向当前活跃的应用通知一个“按键按下”事件 产生中断[非USB键盘]键盘在它的中断请求线(IRQ)上发送信号，信号会被中断控制器映射到一个中断向量，实际上就是一个整型数。CPU使用中断描述符表(IDT)把中断向量映射到对应函数，这些函数被称为中断处理器，它们由操作系统内核提供。当一个中断到达时，CPU根据IDT和中断向量索引到对应的中断处理器，然后操作系统内核出场了。 (Windows)一个 WM_KEYDOWN 消息被发往应用程序HID把键盘按下的事件传送给 KBDHID.sys驱动，把HID的信号转换成一个扫描码(Scancode)，这里回车的扫描码是VK_RETURN(0x0d)。 KBDHID.sys 驱动和 KBDCLASS.sys(键盘类驱动,keyboard class driver)进行交互，这个驱动负责安全地处理所有键盘和小键盘的输入事件。之后它又去调用Win32K.sys，在这之前有可能把消息传递给安装的第三方键盘过滤器。这些都是发生在内核模式。 Win32K.sys 通过 GetForegroundWindow()API函数找到当前哪个窗口是活跃的。这个API函数提供了当前浏览器的地址栏的句柄。Windows系统的”message pump”机制调用 SendMessage(hWnd, WM_KEYDOWN, VK_RETURN, lParam) 函数，lParam是一个用来指示这个按键的更多信息的掩码，这些信息包括按键重复次数（这里是0），实际扫描码（可能依赖于OEM厂商，不过通常不会是VK_RETURN ），功能键（alt, shift, ctrl）是否被按下（在这里没有），以及一些其他状态。 Windows的 SendMessage API直接将消息添加到特定窗口句柄 hWnd的消息队列中，之后赋给 hWnd 的主要消息处理函数 WindowProc将会被调用，用于处理队列中的消息。 当前活跃的句柄 hWnd 实际上是一个edit control控件，这种情况下，WindowProc 有一个用于处理 WM_KEYDOWN消息的处理器，这段代码会查看 SendMessage 传入的第三个参数 wParam，因为这个参数是 VK_RETURN ，于是它知道用户按下了回车键。 (Mac OS X)一个 KeyDown NSEvent被发往应用程序中断信号引发了I/O Kit Kext键盘驱动的中断处理事件，驱动把信号翻译成键码值，然后传给OS X的WindowServer 进程。然后， WindowServer将这个事件通过Mach端口分发给合适的（活跃的，或者正在监听的）应用程序，这个信号会被放到应用程序的消息队列里。队列中的消息可以被拥有足够高权限的线程使用mach_ipc_dispatch 函数读取到。这个过程通常是由 NSApplication主事件循环产生并且处理的，通过 NSEventType 为 KeyDown 的 NSEvent。 (GNU/Linux)Xorg 服务器监听键码值当使用图形化的 X Server 时，X Server会按照特定的规则把键码值再一次映射，映射成扫描码。当这个映射过程完成之后，X Server 把这个按键字符发送给窗口管理器(DWM，metacity,i3等等)，窗口管理器再把字符发送给当前窗口。当前窗口使用有关图形API把文字打印在输入框内。 解析URL 浏览器通过 URL 能够知道下面的信息： Protocol “http”使用HTTP协议 Resource “/“请求的资源是主页(index) 输入的是 URL 还是搜索的关键字？当协议或主机名不合法时，浏览器会将地址栏中输入的文字传给默认的搜索引擎。大部分情况下，在把文字传递给搜索引擎的时候，URL会带有特定的一串字符，用来告诉搜索引擎这次搜索来自这个特定浏览器。 转换非 ASCII 的 Unicode 字符 浏览器检查输入是否含有不是 a-z， A-Z，0-9， - 或者 . 的字符 这里主机名是 google.com，所以没有非ASCII的字符；如果有的话，浏览器会对主机名部分使用Punycode 编码 检查 HSTS 列表 浏览器检查自带的“预加载 HSTS（HTTP严格传输安全）”列表，这个列表里包含了那些请求浏览器只使用HTTPS进行连接的网站 如果网站在这个列表里，浏览器会使用 HTTPS 而不是 HTTP协议，否则，最初的请求会使用HTTP协议发送 注意，一个网站哪怕不在 HSTS 列表里，也可以要求浏览器对自己使用 HSTS 政策进行访问。浏览器向网站发出第一个HTTP请求之后，网站会返回浏览器一个响应，请求浏览器只使用 HTTPS 发送请求。然而，就是这第一个HTTP请求，却可能会使用户受到downgrade attack_ 的威胁，这也是为什么现代浏览器都预置了 HSTS 列表。 DNS 查询 浏览器检查域名是否在缓存当中（要查看 Chrome 当中的缓存， 打开chrome://net-internals/#dns）。 如果缓存中没有，就去调用 gethostbyname库函数（操作系统不同函数也不同）进行查询。 gethostbyname 函数在试图进行DNS解析之前首先检查域名是否在本地Hosts 里，Hosts 的位置不同的操作系统有所不同 如果 gethostbyname 没有这个域名的缓存记录，也没有在 hosts里找到，它将会向 DNS 服务器发送一条 DNS 查询请求。DNS服务器是由网络通信栈提供的，通常是本地路由器或者 ISP 的缓存 DNS 服务器。 查询本地 DNS 服务器 如果 DNS 服务器和我们的主机在同一个子网内，系统会按照下面的 ARP 过程对 DNS 服务器进行 ARP查询 如果 DNS 服务器和我们的主机在不同的子网，系统会按照下面的 ARP 过程对默认网关进行查询 ARP 过程要想发送 ARP（地址解析协议）广播，我们需要有一个目标 IP地址，同时还需要知道用于发送 ARP 广播的接口的 MAC 地址。 首先查询 ARP 缓存，如果缓存命中，我们返回结果：目标 IP = MAC 如果缓存没有命中： 查看路由表，看看目标 IP 地址是不是在本地路由表中的某个子网内。是的话，使用跟那个子网相连的接口，否则使用与默认网关相连的接口。 查询选择的网络接口的 MAC 地址 我们发送一个二层（ OSI 模型_ 中的数据链路层）ARP 请求： ARP Request: Sender MAC: interface:mac:address:here Sender IP: interface.ip.goes.here Target MAC: FF:FF:FF:FF:FF:FF (Broadcast) Target IP: target.ip.goes.here根据连接主机和路由器的硬件类型不同，可以分为以下几种情况： 直连： 如果我们和路由器是直接连接的，路由器会返回一个 ARP Reply（见下面）。 集线器： 如果我们连接到一个集线器，集线器会把 ARP 请求向所有其它端口广播，如果路由器也“连接”在其中，它会返回一个 ARP Reply 。 交换机： 如果我们连接到了一个交换机，交换机会检查本地 CAM/MAC 表，看看哪个端口有我们要找的那个 MAC地址，如果没有找到，交换机会向所有其它端口广播这个 ARP 请求。 如果交换机的 MAC/CAM 表中有对应的条目，交换机会向有我们想要查询的 MAC 地址的那个端口发送 ARP 请求 如果路由器也“连接”在其中，它会返回一个 ARP Reply ARP Reply: Sender MAC: target:mac:address:here Sender IP: target.ip.goes.here Target MAC: interface:mac:address:here Target IP: interface.ip.goes.here现在我们有了 DNS 服务器或者默认网关的 IP 地址，我们可以继续 DNS 请求了： 使用 53 端口向 DNS 服务器发送 UDP 请求包，如果响应包太大，会使用 TCP 协议 如果本地/ISP DNS服务器没有找到结果，它会发送一个递归查询请求，一层一层向高层DNS服务器做查询，直到查询到起始授权机构，如果找到会把结果返回 使用套接字当浏览器得到了目标服务器的 IP 地址，以及 URL 中给出来端口号（http 协议默认端口号是 80， https 默认端口号是 443），它会调用系统库函数socket ，请求一个 TCP流套接字，对应的参数是 AF_INET/AF_INET6 和SOCK_STREAM 。 这个请求首先被交给传输层，在传输层请求被封装成 TCP segment。目标端口会被加入头部，源端口会在系统内核的动态端口范围内选取（Linux下是ip_local_port_range) TCP segment 被送往网络层，网络层会在其中再加入一个 IP 头部，里面包含了目标服务器的IP地址以及本机的IP地址，把它封装成一个IP packet。 这个 TCP packet 接下来会进入链路层，链路层会在封包中加入 frame 头部，里面包含了本地内置网卡的MAC地址以及网关（本地路由器）的 MAC 地址。像前面说的一样，如果内核不知道网关的 MAC 地址，它必须进行 ARP 广播来查询其地址。 到了现在，TCP 封包已经准备好了，可以使用下面的方式进行传输： 以太网 WiFi 蜂窝数据网络 对于大部分家庭网络和小型企业网络来说，封包会从本地计算机出发，经过本地网络，再通过调制解调器把数字信号转换成模拟信号，使其适于在电话线路，有线电视光缆和无线电话线路上传输。在传输线路的另一端，是另外一个调制解调器，它把模拟信号转换回数字信号，交由下一个网络节点处理。节点的目标地址和源地址将在后面讨论。 大型企业和比较新的住宅通常使用光纤或直接以太网连接，这种情况下信号一直是数字的，会被直接传到下一个网络节点进行处理。 最终封包会到达管理本地子网的路由器。在那里出发，它会继续经过自治区域(autonomous system, 缩写AS)的边界路由器，其他自治区域，最终到达目标服务器。一路上经过的这些路由器会从IP数据报头部里提取出目标地址，并将封包正确地路由到下一个目的地。IP数据报头部time to live (TTL)域的值每经过一个路由器就减1，如果封包的TTL变为0，或者路由器由于网络拥堵等原因封包队列满了，那么这个包会被路由器丢弃。 上面的发送和接受过程在 TCP 连接期间会发生很多次： 客户端选择一个初始序列号(ISN)，将设置了 SYN 位的封包发送给服务器端，表明自己要建立连接并设置了初始序列号 服务器端接收到 SYN 包，如果它可以建立连接： - 服务器端选择它自己的初始序列号 - 服务器端设置 SYN 位，表明自己选择了一个初始序列号 - 服务器端把 (客户端ISN + 1) 复制到 ACK 域，并且设置 ACK 位，表明自己接收到了客户端的第一个封包 客户端通过发送下面一个封包来确认这次连接： - 自己的序列号+1 - 接收端 ACK+1 - 设置 ACK 位 数据通过下面的方式传输： - 当一方发送了N个 Bytes 的数据之后，将自己的 SEQ 序列号也增加N - 另一方确认接收到这个数据包（或者一系列数据包）之后，它发送一个 ACK包，ACK的值设置为接收到的数据包的最后一个序列号 关闭连接时： - 要关闭连接的一方发送一个 FIN 包 - 另一方确认这个 FIN 包，并且发送自己的 FIN 包 - 要关闭的一方使用 ACK 包来确认接收到了 FIN TLS 握手 客户端发送一个 ClientHello 消息到服务器端，消息中同时包含了它的Transport Layer Security (TLS) 版本，可用的加密算法和压缩算法。 服务器端向客户端返回一个 ServerHello消息，消息中包含了服务器端的TLS版本，服务器所选择的加密和压缩算法，以及数字证书认证机构（Certificate Authority，缩写CA）签发的服务器公开证书，证书中包含了公钥。客户端会使用这个公钥加密接下来的握手过程，直到协商生成一个新的对称密钥 客户端根据自己的信任CA列表，验证服务器端的证书是否可信。如果认为可信，客户端会生成一串伪随机数，使用服务器的公钥加密它。这串随机数会被用于生成新的对称密钥 服务器端使用自己的私钥解密上面提到的随机数，然后使用这串随机数生成自己的对称主密钥 客户端发送一个 Finished消息给服务器端，使用对称密钥加密这次通讯的一个散列值 服务器端生成自己的 hash 值，然后解密客户端发送来的信息，检查这两个值是否对应。如果对应，就向客户端发送一个Finished 消息，也使用协商好的对称密钥加密 从现在开始，接下来整个 TLS 会话都使用对称秘钥进行加密，传输应用层（HTTP）内容 HTTP 协议如果浏览器是 Google 出品的，它不会使用 HTTP 协议来获取页面信息，而是会与服务器端发送请求，商讨使用 SPDY 协议。 如果浏览器使用 HTTP 协议而不支持 SPDY 协议，它会向服务器发送这样的一个请求: GET / HTTP/1.1 Host: google.com Connection: close [其他头部]“其他头部”包含了一系列的由冒号分割开的键值对，它们的格式符合HTTP协议标准，它们之间由一个换行符分割开来。（这里我们假设浏览器没有违反HTTP协议标准的bug，同时假设浏览器使用HTTP/1.1 协议，不然的话头部可能不包含 Host 字段，同时 GET请求中的版本号会变成 HTTP/1.0 或者 HTTP/0.9 。） HTTP/1.1 定义了“关闭连接”的选项”close”，发送者使用这个选项指示这次连接在响应结束之后会断开。例如： Connection:close 不支持持久连接的 HTTP/1.1 应用必须在每条消息中都包含 “close” 选项。 在发送完这些请求和头部之后，浏览器发送一个换行符，表示要发送的内容已经结束了。 服务器端返回一个响应码，指示这次请求的状态，响应的形式是这样的: 200 OK [响应头部]然后是一个换行，接下来有效载荷(payload)，也就是 www.google.com的HTML内容。服务器下面可能会关闭连接，如果客户端请求保持连接的话，服务器端会保持连接打开，以供之后的请求重用。 如果浏览器发送的HTTP头部包含了足够多的信息（例如包含了 Etag 头部），以至于服务器可以判断出，浏览器缓存的文件版本自从上次获取之后没有再更改过，服务器可能会返回这样的响应: 304 Not Modified [响应头部]这个响应没有有效载荷，浏览器会从自己的缓存中取出想要的内容。 在解析完 HTML之后，浏览器和客户端会重复上面的过程，直到HTML页面引入的所有资源（图片，CSS，favicon.ico等等）全部都获取完毕，区别只是头部的GET / HTTP/1.1 会变成 GET /$(相对www.google.com的URL) HTTP/1.1 。 如果HTML引入了 www.google.com域名之外的资源，浏览器会回到上面解析域名那一步，按照下面的步骤往下一步一步执行，请求中的Host 头部会变成另外的域名。 HTTP 服务器请求处理HTTPD(HTTP Daemon)在服务器端处理请求/响应。最常见的 HTTPD 有 Linux上常用的 Apache 和 nginx，以及 Windows 上的 IIS。 HTTPD 接收请求 服务器把请求拆分为以下几个参数： - HTTP 请求方法(`GET`, `POST`, `HEAD`, `PUT`, `DELETE`, `CONNECT`, `OPTIONS`, 或者 `TRACE`)。直接在地址栏中输入 URL 这种情况下，使用的是 GET 方法 - 域名：google.com - 请求路径/页面：/ (我们没有请求google.com下的指定的页面，因此 / 是默认的路径) 服务器验证其上已经配置了 google.com 的虚拟主机 服务器验证 google.com 接受 GET 方法 服务器验证该用户可以使用 GET 方法(根据 IP 地址，身份信息等) 如果服务器安装了 URL 重写模块（例如 Apache 的 mod_rewrite 和 IIS 的URL Rewrite），服务器会尝试匹配重写规则，如果匹配上的话，服务器会按照规则重写这个请求 服务器根据请求信息获取相应的响应内容，这种情况下由于访问路径是 “/“,会访问首页文件（你可以重写这个规则，但是这个是最常用的）。 服务器会使用指定的处理程序分析处理这个文件，假如 Google 使用PHP，服务器会使用 PHP 解析 index文件，并捕获输出，把 PHP的输出结果返回给请求者 浏览器背后的故事当服务器提供了资源之后（HTML，CSS，JS，图片等），浏览器会执行下面的操作： 解析 —— HTML，CSS，JS 渲染 —— 构建 DOM 树 -&gt; 渲染 -&gt; 布局 -&gt; 绘制 浏览器浏览器的功能是从服务器上取回你想要的资源，然后展示在浏览器窗口当中。资源通常是HTML 文件，也可能是PDF，图片，或者其他类型的内容。资源的位置通过用户提供的 URI(Uniform Resource Identifier) 来确定。 浏览器解释和展示 HTML 文件的方法，在 HTML 和 CSS的标准中有详细介绍。这些标准由 Web 标准组织 W3C(World Wide Web Consortium) 维护。 不同浏览器的用户界面大都十分接近，有很多共同的 UI 元素： 一个地址栏 后退和前进按钮 书签选项 刷新和停止按钮 主页按钮 浏览器高层架构 组成浏览器的组件有： 用户界面 用户界面包含了地址栏，前进后退按钮，书签菜单等等，除了请求页面之外所有你看到的内容都是用户界面的一部分 浏览器引擎 浏览器引擎负责让 UI 和渲染引擎协调工作 渲染引擎 渲染引擎负责展示请求内容。如果请求的内容是HTML，渲染引擎会解析 HTML 和 CSS，然后将内容展示在屏幕上 网络组件 网络组件负责网络调用，例如 HTTP请求等，使用一个平台无关接口，下层是针对不同平台的具体实现 UI后端 UI 后端用于绘制基本 UI 组件，例如下拉列表框和窗口。UI后端暴露一个统一的平台无关的接口，下层使用操作系统的 UI 方法实现 Javascript 引擎 Javascript 引擎用于解析和执行 Javascript 代码 数据存储 数据存储组件是一个持久层。浏览器可能需要在本地存储各种各样的数据，例如Cookie 等。浏览器也需要支持诸如 localStorage，IndexedDB，WebSQL 和 FileSystem 之类的存储机制 HTML 解析浏览器渲染引擎从网络层取得请求的文档，一般情况下文档会分成8kB大小的分块传输。 HTML 解析器的主要工作是对 HTML 文档进行解析，生成解析树。 解析树是以 DOM 元素以及属性为节点的树。DOM是文档对象模型(Document Object Model)的缩写，它是 HTML 文档的对象表示，同时也是 HTML 元素面向外部(如Javascript)的接口。树的根部是”Document”对象。整个 DOM 和HTML 文档几乎是一对一的关系。 解析算法 HTML不能使用常见的自顶向下或自底向上方法来进行分析。主要原因有以下几点: 语言本身的“宽容”特性 HTML本身可能是残缺的，对于常见的残缺，浏览器需要有传统的容错机制来支持它们 解析过程需要反复。对于其他语言来说，源码不会在解析过程中发生变化，但是对于HTML 来说，动态代码，例如脚本元素中包含的 document.write()方法会在源码中添加内容，也就是说，解析过程实际上会改变输入的内容 由于不能使用常用的解析技术，浏览器创造了专门用于解析 HTML的解析器。解析算法在 HTML5标准规范中有详细介绍，算法主要包含了两个阶段：标记化（tokenization）和树的构建。 解析结束之后 浏览器开始加载网页的外部资源（CSS，图像，Javascript 文件等）。 此时浏览器把文档标记为可交互的（interactive），浏览器开始解析处于“推迟（deferred）”模式的脚本，也就是那些需要在文档解析完毕之后再执行的脚本。之后文档的状态会变为“完成（complete）”，浏览器会触发“加载（load）”事件。 注意解析 HTML 网页时永远不会出现“无效语法（Invalid Syntax）”错误，浏览器会修复所有错误内容，然后继续解析。 CSS 解析 根据 CSS词法和句法分析CSS文件和 &lt;style&gt; 标签包含的内容以及 style 属性的值 每个CSS文件都被解析成一个样式表对象（StyleSheet object），这个对象里包含了带有选择器的CSS规则，和对应CSS语法的对象 CSS解析器可能是自顶向下的，也可能是使用解析器生成器生成的自底向上的解析器 页面渲染 通过遍历DOM节点树创建一个“Frame树”或“渲染树”，并计算每个节点的各个CSS样式值 通过累加子节点的宽度，该节点的水平内边距(padding)、边框(border)和外边距(margin)，自底向上的计算”Frame树”中每个节点的首选(preferred)宽度 通过自顶向下的给每个节点的子节点分配可行宽度，计算每个节点的实际宽度 通过应用文字折行、累加子节点的高度和此节点的内边距(padding)、边框(border)和外边距(margin)，自底向上的计算每个节点的高度 使用上面的计算结果构建每个节点的坐标 当存在元素使用 floated，位置有 absolutely 或 relatively 属性的时候，会有更多复杂的计算，详见http://dev.w3.org/csswg/css2/ 和 http://www.w3.org/Style/CSS/current-work 创建layer(层)来表示页面中的哪些部分可以成组的被绘制，而不用被重新栅格化处理。每个帧对象都被分配给一个层 页面上的每个层都被分配了纹理(?) 每个层的帧对象都会被遍历，计算机执行绘图命令绘制各个层，此过程可能由CPU执行栅格化处理，或者直接通过D2D/SkiaGL在GPU上绘制 上面所有步骤都可能利用到最近一次页面渲染时计算出来的各个值，这样可以减少不少计算量 计算出各个层的最终位置，一组命令由Direct3D/OpenGL发出，GPU命令缓冲区清空，命令传至GPU并异步渲染，帧被送到Window Server。 GPU 渲染 在渲染过程中，图形处理层可能使用通用用途的CPU，也可能使用图形处理器 GPU 当使用 GPU用于图形渲染时，图形驱动软件会把任务分成多个部分，这样可以充分利用GPU 强大的并行计算能力，用于在渲染过程中进行大量的浮点计算。 Window Server后期渲染与用户引发的处理渲染结束后，浏览器根据某些时间机制运行JavaScript代码(比如Google Doodle动画)或与用户交互(在搜索栏输入关键字获得搜索建议)。类似Flash和Java的插件也会运行，尽管Google主页里没有。这些脚本可以触发网络请求，也可能改变网页的内容和布局，产生又一轮渲染与绘制。]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[API网关架构设计]]></title>
    <url>%2Fapi-gateway-design.html</url>
    <content type="text"><![CDATA[构建一个API网关 定义API Gateway一个比较广泛的定义如下： API网关是一个服务器，是系统的唯一入口。从面向对象设计的角度看，它与外观模式类似。API网关封装了系统内部架构，为每个客户端提供一个定制的API。API网关方式的核心要点是，所有的客户端和消费端都通过统一的网关接入微服务，在网关层处理所有的非业务功能。通常，网关也是提供REST/HTTP的访问API。服务端通过API-GW注册和管理服务。 从定义中可以归纳出一下几个核心点： 服务调用的统一入口 AuthN（Authentication is establishing the your identity.） AuthZ （Authorization is establishing your privileges.） 监控（请求延迟、异常数、审计日志、访问日志） 高可用 白名单、黑名单 限流 熔断 服务发现 协议支持 （协议转换） … 思维导图 介绍几个概念： 先说RC（Replication Controller）是什么？ RC保证在同一时间能够运行指定数量的Pod副本，保证Pod总是可用。如果实际Pod数量比指定的多就结束掉多余的，如果实际数量比指定的少就启动缺少的。当Pod失败、被删除或被终结时RC会自动创建新的Pod来保证副本数量。所以即使只有一个Pod也应该使用RC来进行管理。 HPA Horizontal Pod Autoscaling，简称HPA，是Kubernetes中实现POD水平自动伸缩的功能。 HPA是kubernetes里面pod弹性伸缩的实现,它能根据设置的监控阀值进行pod的弹性扩缩容，目前默认HPA只能支持cpu和内存的阀值检测扩缩容，但也可以通过custom metric api 调用prometheus实现自定义metric 来更加灵活的监控指标实现弹性伸缩。 取舍取舍也就是如何构建适合自己的API Gateway？ 其实，这个问题也可以拓展为如何开发适应自己业务的某系统，个人感觉应该从以下几点考虑： 自己的业务系统需要什么样的功能？ 业界中该类系统都是如何实现的？ 自己的基础设施情况（主要是PaaS及中间件）如何？ 综合1、2考虑，在满足业务需求的前提下，往远了考虑，往简单了实现（既满足目前的功能，又方便以后拓展）。回到API Gateway这个话题，那就需要考虑一下，自己的业务系统是否需要以上列出的所有功能点？如果不是或者目前不是，那我应该先实现哪一部分？ 其中，作为一个Gateway，以下几点应该是基础功能： 服务调用的统一入口 AuthN（Authentication is establishing the your identity.） AuthZ （Authorization is establishing your privileges.） 监控（请求延迟、异常数、审计日志、访问日志） 高可用 剩下的功能实现就要看业务需要及时间了。如果系统本身的访问量不大，那么限流、熔断是否就可以先不实现？ 为什么需要关注自己的基础设施情况（主要是PaaS及中间件）？比如基础设施中已提供Kubernetes集群服务，那么毫无疑问的高可用方案，应该选择RC方案。如果没有Kubernetes集群服务，那么高可用就需要考虑别的方案了。 可以发现其实手撕一个api网关，也不是多么难的事情，其中很多点，底层包都已经提供了支持，只是需要支持的功能比较多。]]></content>
      <categories>
        <category>Architecture</category>
      </categories>
      <tags>
        <tag>Architecture</tag>
        <tag>Gateway</tag>
        <tag>Design</tag>
        <tag>API</tag>
        <tag>原创</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java volatile的内存语义与AQS锁内存可见性]]></title>
    <url>%2Fjava-volatile-aqs.html</url>
    <content type="text"><![CDATA[提到volatile首先想到就是： 保证此变量对所有线程的可见性，这里的 “可见性”是指当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。 禁止指令重排序优化。 到这里大家感觉自己对volatile理解了吗？ 如果理解了，大家考虑这么一个问题：ReentrantLock（或者其它基于AQS实现的锁）是如何保证代码段中变量（变量主要是指共享变量，存在竞争问题的变量）的可见性？ 1234567891011121314private static ReentrantLock reentrantLock = new ReentrantLock();private static intcount = 0;//...// 多线程 run 如下代码reentrantLock.lock();try&#123; count++;&#125; finally&#123; reentrantLock.unlock();&#125;//... 既然提到了可见性，那就先熟悉几个概念： 1、JMMJMM：Java Memory Model 即 Java 内存模型 The Java Memory Model describes what behaviors are legal in multithreaded code, and how threads may interact through memory. It describes the relationship between variables in a program and the low-level details of storing and retrieving them to and from memory or registers in a real computer system. It does this in a way that can be implemented correctly using a wide variety of hardware and a wide variety of compiler optimizations. Java内存模型的主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量这样的底层细节。此处的变量主要是指共享变量，存在竞争问题的变量。Java内存模型规定所有的变量都存储在主内存中，而每条线程还有自己的工作内存，线程的工作内存中保存了该线程使用到的变量的主内存副本拷贝，线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，而不能直接读写主内存中的变量（根据Java虚拟机规范的规定，volatile变量依然有共享内存的拷贝，但是由于它特殊的操作顺序性规定——从工作内存中读写数据前，必须先将主内存中的数据同步到工作内存中，所有看起来如同直接在主内存中读写访问一般，因此这里的描述对于volatile也不例外）。不同线程之间也无法直接访问对方工作内存中的变量，线程间变量值得传递均需要通过主内存来完成。 2、重排序在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。重排序分3种类型： 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism，ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 从Java源代码到最终实际执行的指令序列，会分别经历下面3种重排序： 对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM的处理器重排序规则会要求Java编译器在生成指令序列时，插入特定类型的内存屏障（Memory Barriers，Intel称之为Memory Fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序。 JMM属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。 3、happens-before 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。（对一个volatile变量的读，总是能看到【任意线程】对这个volatile变量最后的写入） 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 两个操作之间具有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。 4、内存屏障 硬件层的内存屏障分为两种：Load Barrier 和 Store Barrier即读屏障和写屏障。 对于Load Barrier来说，在指令前插入Load Barrier，可以让高速缓存中的数据失效，强制从新从主内存加载数据； 对于Store Barrier来说，在指令后插入Store Barrier，能让写入缓存中的最新数据更新写入主内存，让其他线程可见。 内存屏障有两个作用： 阻止屏障两侧的指令重排序； 强制把写缓冲区/高速缓存中的数据等写回主内存，让缓存中相应的数据失效。5、volatile的内存语义 从JSR-133开始（即从JDK5开始），volatile变量的写-读可以实现线程之间的通信。 从内存语义的角度来说，volatile的写-读与锁的释放-获取有相同的内存效果： volatile写和锁的释放有相同的内存语义； volatile读与锁的获取有相同的内存语义。 volatile仅仅保证对单个volatile变量的读/写具有原子性，而锁的互斥执行的特性可以确保对整个临界区代码的执行具有原子性。在功能上，锁比volatile更强大；在可伸缩性和执行性能上，volatile更有优势。 volatile变量自身具有下列特性： 可见性。对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。 原子性：对任意单个volatile变量的读/写具有原子性，即使是64位的long型和double型变量，只要它是volatile变量，对该变量的读/写就具有原子性。如果是多个volatile操作或类似于volatile++这种复合操作，这些操作整体上不具有原子性。 volatile写和volatile读的内存语义： 线程A写一个volatile变量，实质上是线程A向接下来将要读这个volatile变量的某个线程发出了（其对共享变量所做修改的）消息。 线程B读一个volatile变量，实质上是线程B接收了之前某个线程发出的（在写这个volatile变量之前对共享变量所做修改的）消息。 线程A写一个volatile变量，随后线程B读这个volatile变量，这个过程实质上是线程A通过主内存向线程B发送消息。 JMM针对编译器制定的volatile重排序规则表 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。 当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。 为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障几乎是不可能的。为此，JMM采取保守策略。下面是基于保守策略的JMM内存屏障插入策略。 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 LoadLoad屏障：对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。 StoreStore屏障：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。 LoadStore屏障：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。 StoreLoad屏障：对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。它的开销是四种屏障中最大的。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能。上述内存屏障插入策略非常保守，但它可以保证在任意处理器平台，任意的程序中都能得到正确的volatile内存语义。 下面是保守策略下，volatile写插入内存屏障后生成的指令序列示意图. 图中的StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存。 这里比较有意思的是，volatile写后面的StoreLoad屏障。此屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。因为编译器常常无法准确判断在一个volatile写的后面是否需要插入一个StoreLoad屏障（比如，一个volatile写之后方法立即return）。为了保证能正确实现volatile的内存语义，JMM在采取了保守策略：在每个volatile写的后面，或者在每个volatile读的前面插入一个StoreLoad屏障。从整体执行效率的角度考虑，JMM最终选择了在每个volatile写的后面插入一个StoreLoad屏障。因为volatile写-读内存语义的常见使用模式是：一个写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时，选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升。从这里可以看到JMM在实现上的一个特点：首先确保正确性，然后再去追求执行效率。 下面是在保守策略下，volatile读插入内存屏障后生成的指令序列示意图:图中的LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。LoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。 上述volatile写和volatile读的内存屏障插入策略非常保守。在实际执行时，只要不改变volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。 6、AQS对于AQS需要了解这么几点： 锁的状态通过volatile int state来表示。 获取不到锁的线程会进入AQS的队列等待。 子类需要重写tryAcquire、tryRelease等方法。 AQS 详解参见：面试必备：Java AQS 实现原理（图文）分析 7、ReentrantLock以公平锁为例，看看 ReentrantLock 获取锁 &amp; 释放锁的关键代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * The synchronization state. */private volatile int state;/** * Returns the current value of synchronization state. * This operation has memory semantics of a &#123;@code volatile&#125; read. * @return current state value */protected final int getState() &#123; return state;&#125;protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c);// 释放锁的最后，写volatile变量state return free;&#125; protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState();// 获取锁的开始，首先读volatile变量state if (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false;&#125; 公平锁在释放锁的最后写volatile变量state，在获取锁时首先读这个volatile变量。根据volatile的happens-before规则，释放锁的线程在写volatile变量之前可见的共享变量，在获取锁的线程读取同一个volatile变量后将立即变得对获取锁的线程可见。从而保证了代码段中变量（变量主要是指共享变量，存在竞争问题的变量）的可见性。 8、小结 如果我们仔细分析concurrent包的源代码实现，会发现一个通用化的实现模式。 首先，声明共享变量为volatile。 然后，使用CAS的原子条件更新来实现线程之间的同步。 同时，配合以volatile的读/写和CAS所具有的volatile读和写的内存语义来实现线程之间的通信。 前文我们提到过，编译器不会对volatile读与volatile读后面的任意内存操作重排序；编译器不会对volatile写与volatile写前面的任意内存操作重排序。组合这两个条件，意味着为了同时实现volatile读和volatile写的内存语义，编译器不能对CAS与CAS前面和后面的任意内存操作重排序。 本文参考： 1、《Java并发编程的艺术》 方腾飞 魏鹏 程晓明 著 2、Java 可重入锁内存可见性分析]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>JUC</tag>
        <tag>AQS</tag>
        <tag>JDK</tag>
        <tag>Java</tag>
        <tag>Volatile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Lambda表达式 实现原理分析]]></title>
    <url>%2Fjava-lambda.html</url>
    <content type="text"><![CDATA[本文分析基于JDK 9 一、目标本文主要解决两个问题： 1、函数式接口 到底是什么？ 2、Lambda表达式是怎么实现的？ 先介绍一个jdk的bin目录下的一个字节码查看工具及反编译工具：javap 二、函数式接口1234@FunctionalInterfaceinterface IFunctionTest&lt;T&gt; &#123; public void print(T x);&#125; 通过javap 反编译IFunctionTest.class 可以看到如下信息: 12345$C:\Users\Code\Java\study&gt;javap -p IFunctionTest.classCompiled from &quot;FunctionTest.java&quot;interface IFunctionTest&lt;T&gt; &#123; public abstract void print(T);&#125; 可以看到函数式接口编译完之后依然是一个接口，这个接口具有唯一的一个抽像方法。 为什么说需要是唯一一个抽象方法？ 12345@FunctionalInterfaceinterface IFunctionTest&lt;T&gt; &#123; public void print(T x); public void print22(T x,int rr);&#125; 虽然不能在函数式接口中定义多个方法，但可以定义默认方法、静态方法、定义java.lang.Object里的public方法： 123456789101112@FunctionalInterfaceinterface Print&lt;T&gt; &#123; public void print(T x); default void doSomeMoreWork1()&#123; // Method body &#125; static void printHello()&#123; System.out.println(&quot;Hello&quot;); &#125; @Override boolean equals(Object obj);&#125; 反编译文件内容如下： 12345678$C:\Users\Code\Java\study&gt;javap -p IFunctionTest.classCompiled from &quot;FunctionTest.java&quot;interface IFunctionTest&lt;T&gt; &#123; public abstract void print(T); public void doSomeMoreWork1(); public static void printHello(); public abstract boolean equals(java.lang.Object);&#125; 三、Lambda3.1 示例代码12345678910111213public class LambdaTest &#123; public static void printString(String s, Print&lt;String&gt; print) &#123; print.print(s); &#125; public static void main(String[] args) &#123; printString(&quot;test&quot;, (x) -&gt; System.out.println(x)); &#125;&#125;@FunctionalInterfaceinterface Print&lt;T&gt; &#123; public void print(T x);&#125; 通过javac编译LambdaTest.java文件，会生成LambdaTest.class、Print.class两个class文件。 1javac LambdaTest.java 3.2 对于lambda实现的猜测那么编译器对Lambda 都做了什么？反编译一下代码如下： 12345678C:\Users\Code\Java\study&gt;javap -p LambdaTest.classCompiled from &quot;LambdaTest.java&quot;public class LambdaTest &#123; public LambdaTest(); public static void printString(java.lang.String, Print&lt;java.lang.String&gt;); public static void main(java.lang.String[]); private static void lambda$main$0(java.lang.String);&#125; 由上面的代码可以看出编译器会根据Lambda表达式生成一个私有的静态函数： 1private static void lambda$main$0(java.lang.String); 为了验证上面的转化是否正确? 我们在代码中定义一个lambda$main$0这个的函数，最终代码如下所示： 123456789101112131415public class LambdaTest &#123; public static void printString(String s, Print&lt;String&gt; print) &#123; print.print(s); &#125; public static void main(String[] args) &#123; printString(&quot;test&quot;, (x) -&gt; System.out.println(x)); &#125; private static void lambda$main$0(String s) &#123; &#125;&#125;@FunctionalInterfaceinterface Print&lt;T&gt; &#123; public void print(T x);&#125; 上面的代码在编译时会报错，错误信息如下： 12345678910C:\Users\Code\Java\study&gt;javac LambdaTest.javaLambdaTest.java:8: 错误: 符号lambda$main$0(String)与LambdaTest中的 compiler-synthesized 符号冲突 private static void lambda$main$0(String s) &#123; ^LambdaTest.java:1: 错误: 符号lambda$main$0(String)与LambdaTest中的 compiler-synthesized 符号冲突public class LambdaTest &#123;^2 个错误 有了上面的内容，可以知道的是Lambda表达式在Java 9中首先会生成一个私有的静态函数，这个私有的静态函数干的就是Lambda表达式里面的内容，那么又是如何调用的生成的私有静态函数（lambda$main$0(String s)）呢？ 3.3 反编译代码详解查看更加详细的反编译结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133$C:\Users\Code\Java\study&gt; javap -p -v -c LambdaTest.classClassfile /C:/Users/Code/Java/study/LambdaTest.class Last modified 2018-4-5; size 1184 bytes MD5 checksum b144b5a936a04a7c975eae93c7370174 Compiled from &quot;LambdaTest.java&quot;public class LambdaTest minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #9.#24 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = InterfaceMethodref #25.#26 // Print.print:(Ljava/lang/Object;)V #3 = String #27 // test #4 = InvokeDynamic #0:#33 // #0:print:()LPrint; #5 = Methodref #8.#34 // LambdaTest.printString:(Ljava/lang/String;LPrint;)V #6 = Fieldref #35.#36 // java/lang/System.out:Ljava/io/PrintStream; #7 = Methodref #37.#38 // java/io/PrintStream.println:(Ljava/lang/String;)V #8 = Class #39 // LambdaTest #9 = Class #40 // java/lang/Object #10 = Utf8 &lt;init&gt; #11 = Utf8 ()V #12 = Utf8 Code #13 = Utf8 LineNumberTable #14 = Utf8 printString #15 = Utf8 (Ljava/lang/String;LPrint;)V #16 = Utf8 Signature #17 = Utf8 (Ljava/lang/String;LPrint&lt;Ljava/lang/String;&gt;;)V #18 = Utf8 main #19 = Utf8 ([Ljava/lang/String;)V #20 = Utf8 lambda$main$0 #21 = Utf8 (Ljava/lang/String;)V #22 = Utf8 SourceFile #23 = Utf8 LambdaTest.java #24 = NameAndType #10:#11 // &quot;&lt;init&gt;&quot;:()V #25 = Class #41 // Print #26 = NameAndType #42:#43 // print:(Ljava/lang/Object;)V #27 = Utf8 test #28 = Utf8 BootstrapMethods #29 = MethodHandle #6:#44 // invokestatic java/lang/invoke/LambdaMetafactory.metafactory:(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodHandle;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite; #30 = MethodType #43 // (Ljava/lang/Object;)V #31 = MethodHandle #6:#45 // invokestatic LambdaTest.lambda$main$0:(Ljava/lang/String;)V #32 = MethodType #21 // (Ljava/lang/String;)V #33 = NameAndType #42:#46 // print:()LPrint; #34 = NameAndType #14:#15 // printString:(Ljava/lang/String;LPrint;)V #35 = Class #47 // java/lang/System #36 = NameAndType #48:#49 // out:Ljava/io/PrintStream; #37 = Class #50 // java/io/PrintStream #38 = NameAndType #51:#21 // println:(Ljava/lang/String;)V #39 = Utf8 LambdaTest #40 = Utf8 java/lang/Object #41 = Utf8 Print #42 = Utf8 print #43 = Utf8 (Ljava/lang/Object;)V #44 = Methodref #52.#53 // java/lang/invoke/LambdaMetafactory.metafactory:(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodHandle;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite; #45 = Methodref #8.#54 // LambdaTest.lambda$main$0:(Ljava/lang/String;)V #46 = Utf8 ()LPrint; #47 = Utf8 java/lang/System #48 = Utf8 out #49 = Utf8 Ljava/io/PrintStream; #50 = Utf8 java/io/PrintStream #51 = Utf8 println #52 = Class #55 // java/lang/invoke/LambdaMetafactory #53 = NameAndType #56:#60 // metafactory:(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodHandle;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite; #54 = NameAndType #20:#21 // lambda$main$0:(Ljava/lang/String;)V #55 = Utf8 java/lang/invoke/LambdaMetafactory #56 = Utf8 metafactory #57 = Class #62 // java/lang/invoke/MethodHandles$Lookup #58 = Utf8 Lookup #59 = Utf8 InnerClasses #60 = Utf8 (Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodHandle;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite; #61 = Class #63 // java/lang/invoke/MethodHandles #62 = Utf8 java/lang/invoke/MethodHandles$Lookup #63 = Utf8 java/lang/invoke/MethodHandles&#123; public LambdaTest(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 1: 0 public static void printString(java.lang.String, Print&lt;java.lang.String&gt;); descriptor: (Ljava/lang/String;LPrint;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=2, args_size=2 0: aload_1 1: aload_0 2: invokeinterface #2, 2 // InterfaceMethod Print.print:(Ljava/lang/Object;)V 7: return LineNumberTable: line 3: 0 line 4: 7 Signature: #17 // (Ljava/lang/String;LPrint&lt;Ljava/lang/String;&gt;;)V public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=1, args_size=1 0: ldc #3 // String test 2: invokedynamic #4, 0 // InvokeDynamic #0:print:()LPrint; 7: invokestatic #5 // Method printString:(Ljava/lang/String;LPrint;)V 10: return LineNumberTable: line 6: 0 line 7: 10 private static void lambda$main$0(java.lang.String); descriptor: (Ljava/lang/String;)V flags: ACC_PRIVATE, ACC_STATIC, ACC_SYNTHETIC Code: stack=2, locals=1, args_size=1 0: getstatic #6 // Field java/lang/System.out:Ljava/io/PrintStream; 3: aload_0 4: invokevirtual #7 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 7: return LineNumberTable: line 6: 0&#125;SourceFile: &quot;LambdaTest.java&quot;InnerClasses: public static final #58= #57 of #61; //Lookup=class java/lang/invoke/MethodHandles$Lookup of class java/lang/invoke/MethodHandlesBootstrapMethods: 0: #29 invokestatic java/lang/invoke/LambdaMetafactory.metafactory:(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodHandle;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite;Method arguments: #30 (Ljava/lang/Object;)V #31 invokestatic LambdaTest.lambda$main$0:(Ljava/lang/String;)V #32 (Ljava/lang/String;)V 这个 class 文件展示了三个主要部分：常量池、构造器方法和 printString、main、lambdamainmain0方法还有lambda表达式生成的内部类。 3.3.1 动态链接每个栈帧都有一个运行时常量池的引用。这个引用指向栈帧当前运行方法所在类的常量池。通过这个引用支持动态链接（dynamic linking）。 C/C++ 代码一般被编译成对象文件，然后多个对象文件被链接到一起产生可执行文件或者 dll。在链接阶段，每个对象文件的符号引用被替换成了最终执行文件的相对偏移内存地址。在 Java中，链接阶段是运行时动态完成的。 当 Java 类文件编译时，所有变量和方法的引用都被当做符号引用存储在这个类的常量池中。符号引用是一个逻辑引用，实际上并不指向物理内存地址。JVM 可以选择符号引用解析的时机，一种是当类文件加载并校验通过后，这种解析方式被称为饥饿方式。另外一种是符号引用在第一次使用的时候被解析，这种解析方式称为惰性方式。无论如何 ，JVM 必须要在第一次使用符号引用时完成解析并抛出可能发生的解析错误。绑定是将对象域、方法、类的符号引用替换为直接引用的过程。绑定只会发生一次。一旦绑定，符号引用会被完全替换。如果一个类的符号引用还没有被解析，那么就会载入这个类。每个直接引用都被存储为相对于存储结构（与运行时变量或方法的位置相关联的）偏移量。 3.3.2 常量池JVM 维护了一个按类型区分的常量池，一个类似于符号表的运行时数据结构。尽管它包含更多数据。Java 字节码需要数据。这个数据经常因为太大不能直接存储在字节码中，取而代之的是存储在常量池中，字节码包含这个常量池的引用。 常量池中可以存储多种类型的数据： 数字型 字符串型 类引用型 域引用型 方法引用 3.3.3 方法每一个方法包含四个区域： 签名和访问标签 字节码 LineNumberTable：为调试器提供源码中的每一行对应的字节码信息 LocalVariableTable：列出了所有栈帧中的局部变量 操作码 作用 aload0 这个操作码是aload格式操作码中的一个。它们用来把对象引用加载到操作码栈。表示正在被访问的局部变量数组的位置，但只能是0、1、2、3 中的一个。还有一些其它类似的操作码用来载入非对象引用的数据，如iload, lload, float 和 dload。其中 i 表示 int，l 表示 long，f 表示 float，d 表示 double。局部变量数组位置大于 3 的局部变量可以用 iload, lload, float, dload 和 aload 载入。这些操作码都只需要一个操作数，即数组中的位置。 ldc 这个操作码用来将常量从运行时常量池压栈到操作数栈。 getstatic 这个操作码用来把一个静态变量从运行时常量池的静态变量列表中压栈到操作数栈。 return 这个操作码属于ireturn、lreturn、freturn、dreturn、areturn 和 return 操作码组。每个操作码返回一种类型的返回值，其中 i 表示 int，l 表示 long，f 表示 float，d 表示 double，a 表示 对象引用。没有前缀类型字母的 return 表示返回 void。 函数调用操作码 作用 invokestatic 调用类方法（静态绑定，速度快） invokevirtual 指令调用一个对象的实例方法（动态绑定） invokespecial 指令调用实例初始化方法、私有方法、父类方法。（静态绑定，速度快） invokeinterface 调用引用类型为interface的实例方法（动态绑定） invokedynamic JDK 7引入的，主要是为了支持动态语言的方法调用 3.3.4 代码分析注意反编译后main方法部分： 123456789101112public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=1, args_size=1 // ldc 这个操作码用来将常量从运行时常量池压栈到操作数栈 0: ldc #3 // String test // 注意下面两句：通过实例调用 print 2: invokedynamic #4, 0 // InvokeDynamic #0:print:()LPrint; //调用静态方法 printString 7: invokestatic #5 // Method printString:(Ljava/lang/String;LPrint;)V 10: return 那么，既然是调用实例方法，那么实例在哪？ 12345678910InnerClasses: public static final #58= #57 of #61; //Lookup=class java/lang/invoke/MethodHandles$Lookup of class java/lang/invoke/MethodHandlesBootstrapMethods: 0: #29 invokestatic java/lang/invoke/LambdaMetafactory.metafactory:(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodHandle;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite; Method arguments: //对象类型终结符为 L 和 ; //Object V #30 (Ljava/lang/Object;)V #31 invokestatic LambdaTest.lambda$main$0:(Ljava/lang/String;)V #32 (Ljava/lang/String;)V 可以在运行时加上-Djdk.internal.lambda.dumpProxyClasses，加上这个参数后，运行时，会将生成的内部类class码输出到一个文件中。 1java -Djdk.internal.lambda.dumpProxyClasses LambdaTest 通过jad反编译LambdaTest$$Lambda$1.class文件，内容如下： 1234567891011// Decompiled by Jad v1.5.8g. Copyright 2001 Pavel Kouznetsov.// Jad home page: http://www.kpdus.com/jad.html// Decompiler options: packimports(3) final class LambdaTest$$Lambda$1 implements Print &#123; private LambdaTest$$Lambda$1() &#123; &#125; public void print(Object obj) &#123; LambdaTest.lambda$main$0((String) obj); &#125;&#125; 3.3.5 代码还原至此，我们可以推断出最终执行代码应该是这样的： 123456789101112131415161718192021222324252627public class LambdaTest &#123; public static void PrintString(String s, Print&lt;String&gt; print) &#123; print.print(s); &#125; public static void main(String[] args) &#123; PrintString(&quot;test&quot;, new LambdaTest$$Lambda$1()); &#125; private static void lambda$main$0(String x) &#123; System.out.println(x); &#125; static final class LambdaTest$$Lambda$1 implements Print &#123; public void print(Object obj) &#123; LambdaTest.lambda$main$0((String) obj); &#125; private LambdaTest$$Lambda$1() &#123; &#125; &#125;&#125;@FunctionalInterfaceinterface Print&lt;T&gt; &#123; public void print(T x);&#125; 四、总结 在类编译时，会生成一个私有静态方法+一个内部类； 在内部类中实现了函数式接口，在实现接口的方法中，会调用编译器生成的静态方法； 在使用lambda表达式的地方，通过传递内部类实例，来调用函数式接口方法。 就是传递个函数指针，在Java中搞得这么复杂。。。。。。 参考资料： https://www.cnblogs.com/WJ5888/p/4667086.html https://www.jianshu.com/p/57bffc6e7acd http://www.importnew.com/17770.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>Java</tag>
        <tag>Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java AQS 实现原理（图文）分析]]></title>
    <url>%2Fjava-aqs.html</url>
    <content type="text"><![CDATA[AQS：AbstractQueuedSynchronizer 1、AQS设计简介 AQS的实现是基于一个FIFO的等待队列。 使用单个原子变量来表示获取、释放锁状态（final int）改变该int值使用的是CAS。（思考：为什么一个int值可以保证内存可见性？） 子类应该定义一个非公开的内部类继承AQS，并实现其中方法。 AQS支持exclusive与shared两种模式。 内部类ConditionObject用于支持子类实现exclusive模式 子类需要重写： tryAcquire tryRelease tryReleaseShared isHeldExclusively等方法，并确保是线程安全的。 贯穿全文的图（核心）： 模板方法设计模式：定义一个操作中算法的骨架，而将一些步骤的实现延迟到子类中。 2、类结构 ConditionObject类 Node类 N多方法 3、FIFO队列等待队列是CLH（Craig, Landin, and Hagersten）锁队列。 通过节点中的“状态”字段来判断一个线程是否应该阻塞。当该节点的前一个节点释放锁的时候，该节点会被唤醒。 12345private transient volatile Node head;private transient volatile Node tail;//The synchronization state.//在互斥锁中它表示着线程是否已经获取了锁，0未获取，1已经获取了，大于1表示重入数。private volatile int state; AQS维护了一个volatile int state（代表共享资源）和一个FIFO线程等待队列（多线程争用资源被阻塞时会进入此队列）。 state的访问方式有三种: getState() setState() compareAndSetState() AQS定义两种资源共享方式：Exclusive（独占，只有一个线程能执行，如ReentrantLock）和Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch）。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源state的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在顶层实现好了。 自定义同步器实现时主要实现以下几种方法： isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。 tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。 再以CountDownLatch以例，任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS减1。等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后续动作。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现： tryAcquire-tryRelease tryAcquireShared-tryReleaseShared 中的一种即可。 当然AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 以下部分来自源码注释： 每次进入CLH队列时，需要对尾节点进入队列过程，是一个原子性操作。在出队列时，我们只需要更新head节点即可。在节点确定它的后继节点时， 需要花一些功夫，用于处理那些，由于等待超时时间结束或中断等原因， 而取消等待锁的线程。 节点的前驱指针，主要用于处理，取消等待锁的线程。如果一个节点取消等待锁，则此节点的前驱节点的后继指针，要指向，此节点后继节点中，非取消等待锁的线程（有效等待锁的线程节点）。 我们用next指针连接实现阻塞机制。每个节点均持有自己线程，节点通过节点的后继连接唤醒其后继节点。 CLH队列需要一个傀儡结点作为开始节点。我们不会再构造函数中创建它，因为如果没有线程竞争锁，那么，努力就白费了。取而代之的方案是，当有第一个竞争者时，我们才构造头指针和尾指针。 线程通过同一节点等待条件，但是用另外一个连接。条件只需要放在一个非并发的连接队列与节点关联，因为只有当线程独占持有锁的时候，才会去访问条件。当一个线程等待条件的时候，节点将会插入到条件队列中。当条件触发时，节点将会转移到主队列中。用一个状态值，描述节点在哪一个队列上。 4、Node123456789101112131415161718192021222324252627282930313233static final class Node &#123; //该等待节点处于共享模式 static final Node SHARED = new Node(); //该等待节点处于独占模式 static final Node EXCLUSIVE = null; //表示节点的线程是已被取消的 static final int CANCELLED = 1; //表示当前节点的后继节点的线程需要被唤醒 static final int SIGNAL = -1; //表示线程正在等待某个条件 static final int CONDITION = -2; //表示下一个共享模式的节点应该无条件的传播下去 static final int PROPAGATE = -3; //状态位 ，分别可以使CANCELLED、SINGNAL、CONDITION、PROPAGATE、0 volatile int waitStatus; volatile Node prev;//前驱节点 volatile Node next;//后继节点 volatile Thread thread;//等待锁的线程 //ConditionObject链表的后继节点或者代表共享模式的节点。 //因为Condition队列只能在独占模式下被能被访问,我们只需要简单的使用链表队列来链接正在等待条件的节点。 //然后它们会被转移到同步队列（AQS队列）再次重新获取。 //由于条件队列只能在独占模式下使用，所以我们要表示共享模式的节点的话只要使用特殊值SHARED来标明即可。 Node nextWaiter; //Returns true if node is waiting in shared mode final boolean isShared() &#123; return nextWaiter == SHARED; &#125; .......&#125; waitStatus不同值含义： SIGNAL(-1)：当前节点的后继节点已经 (或即将)被阻塞（通过park） , 所以当当前节点释放或则被取消时候，一定要unpark它的后继节点。为了避免竞争，获取方法一定要首先设置node为signal，然后再次重新调用获取方法，如果失败，则阻塞。 CANCELLED(1)：当前节点由于超时或者被中断而被取消。一旦节点被取消后，那么它的状态值不在会被改变，且当前节点的线程不会再次被阻塞。 CONDITION(-2) ：该节点的线程处于等待条件状态,不会被当作是同步队列上的节点,直到被唤醒(signal),设置其值为0,重新进入阻塞状态. PROPAGATE(-3：)共享模式下的释放操作应该被传播到其他节点。该状态值在doReleaseShared方法中被设置的。 0：以上都不是 该状态值为了简便使用，所以使用了数值类型。非负数值意味着该节点不需要被唤醒。所以，大多数代码中不需要检查该状态值的确定值。 一个正常的Node，它的waitStatus初始化值是0。如果想要修改这个值，可以使用AQS提供CAS进行修改。 5、独占模式与共享模式在锁的获取时，并不一定只有一个线程才能持有这个锁（或者称为同步状态），所以此时有了独占模式和共享模式的区别，也就是在Node节点中由nextWaiter来标识。比如ReentrantLock就是一个独占锁，只能有一个线程获得锁，而WriteAndReadLock的读锁则能由多个线程同时获取，但它的写锁则只能由一个线程持有。 5.1、独占模式5.1.1 独占模式同步状态的获取123456789//忽略中断的（即不手动抛出InterruptedException异常）独占模式下的获取方法。//该方法在成功返回前至少会调用一次tryAcquire()方法(该方法是子类重写的方法，如果返回true则代表能成功获取).//否则当前线程会进入队列排队，重复的阻塞和唤醒等待再次成功获取后返回, //该方法可以用来实现Lock.lockpublic final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; 该方法首先尝试获取锁(tryAcquire(arg)的具体实现定义在了子类中),如果获取到,则执行完毕,否则通过addWaiter(Node.EXCLUSIVE), arg)方法把当前节点添加到等待队列末尾,并设置为独占模式。 1234567891011121314151617181920212223242526272829303132private Node addWaiter(Node mode) &#123; //把当前线程包装为node,设为独占模式 Node node = new Node(Thread.currentThread(), mode); // 尝试快速入队，即无竞争条件下肯定成功。如果失败，则进入enq自旋重试入队 Node pred = tail; if (pred != null) &#123; node.prev = pred; //CAS替换当前尾部。成功则返回 if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node; &#125;//插入节点到队列中，如果队列未初始化则初始化，然后再插入。private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125; &#125; 如果tail节点为空,执行enq(node);重新尝试,最终把node插入.在把node插入队列末尾后,它并不立即挂起该节点中线程,因为在插入它的过程中,前面的线程可能已经执行完成,所以它会先进行自旋操作acquireQueued(node, arg),尝试让该线程重新获取锁!当条件满足获取到了锁则可以从自旋过程中退出，否则继续。 12345678910111213141516171819202122232425final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); //如果它的前继节点为头结点,尝试获取锁,获取成功则返回 if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; //判断当前节点的线程是否应该被挂起，如果应该被挂起则挂起。 //等待release唤醒释放 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) //在队列中取消当前节点 cancelAcquire(node); &#125; &#125; 如果没获取到锁,则判断是否应该挂起,而这个判断则得通过它的前驱节点的waitStatus来确定: 12345678910111213141516171819202122232425262728private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; //该节点如果状态如果为SIGNAL。则返回true，然后park挂起线程 if (ws == Node.SIGNAL) return true; //表明该节点已经被取消，向前循环重新调整链表节点 if (ws &gt; 0) &#123; /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; //执行到这里代表节点是0或者PROPAGATE，然后标记他们为SIGNAL，但是 //还不能park挂起线程。需要重试是否能获取，如果不能，则挂起。 compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false; &#125; //挂起当前线程，且返回线程的中断状态private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted(); &#125; 最后,我们对获取独占式锁过程对做个总结: AQS的模板方法acquire通过调用子类自定义实现的tryAcquire获取同步状态失败后-&gt;将线程构造成Node节点(addWaiter)-&gt;将Node节点添加到同步队列对尾(addWaiter)-&gt;节点以自旋的方法获取同步状态(acquirQueued)。在节点自旋获取同步状态时，只有其前驱节点是头节点的时候才会尝试获取同步状态，如果该节点的前驱不是头节点或者该节点的前驱节点是头节点单获取同步状态失败，则判断当前线程需要阻塞，如果需要阻塞则需要被唤醒过后才返回。 获取锁的过程： 当线程调用acquire()申请获取锁资源，如果成功，则进入临界区。 当获取锁失败时，则进入一个FIFO等待队列，然后被挂起等待唤醒。 当队列中的等待线程被唤醒以后就重新尝试获取锁资源，如果成功则进入临界区，否则继续挂起等待。 5.1.2 独占模式同步状态的释放既然是释放,那肯定是持有锁的该线程执行释放操作,即head节点中的线程释放锁. AQS中的release释放同步状态和acquire获取同步状态一样，都是模板方法，tryRelease释放的具体操作都有子类去实现，父类AQS只提供一个算法骨架。 12345678910111213141516171819202122232425public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125;//如果node的后继节点不为空且不是作废状态,则唤醒这个后继节点,//否则从末尾开始寻找合适的节点,如果找到,则唤醒private void unparkSuccessor(Node node) &#123; int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread); &#125; 过程：首先调用子类的tryRelease()方法释放锁，然后唤醒后继节点，在唤醒的过程中，需要判断后继节点是否满足情况，如果后继节点不为空且不是作废状态，则唤醒这个后继节点，否则从tail节点向前寻找合适的节点，如果找到，则唤醒。 释放锁过程： 当线程调用release()进行锁资源释放时，如果没有其他线程在等待锁资源，则释放完成。 如果队列中有其他等待锁资源的线程需要唤醒，则唤醒队列中的第一个等待节点（先入先出）。 5.2、共享模式5.2.1 共享模式同步状态的获取 当线程调用acquireShared()申请获取锁资源时，如果成功，则进入临界区。 当获取锁失败时，则创建一个共享类型的节点并进入一个FIFO等待队列，然后被挂起等待唤醒。 当队列中的等待线程被唤醒以后就重新尝试获取锁资源，如果成功则唤醒后面还在等待的共享节点并把该唤醒事件传递下去，即会依次唤醒在该节点后面的所有共享节点，然后进入临界区，否则继续挂起等待。 5.2.2 共享模式同步状态的释放 当线程调用releaseShared()进行锁资源释放时，如果释放成功，则唤醒队列中等待的节点，如果有的话。 6. AQS小结java.util.concurrent中的很多可阻塞类（比如ReentrantLock）都是基于AQS来实现的。AQS是一个同步框架，它提供通用机制来原子性管理同步状态、阻塞和唤醒线程，以及维护被阻塞线程的队列。 JDK中AQS被广泛使用，基于AQS实现的同步器包括： ReentrantLock Semaphore ReentrantReadWriteLock（后续会出文章讲解） CountDownLatch FutureTask 每一个基于AQS实现的同步器都会包含两种类型的操作，如下： 至少一个acquire操作。这个操作阻塞调用线程，除非/直到AQS的状态允许这个线程继续执行。 至少一个release操作。这个操作改变AQS的状态，改变后的状态可允许一个或多个阻塞线程被解除阻塞。 基于“复合优先于继承”的原则，基于AQS实现的同步器一般都是：声明一个内部私有的继承于AQS的子类Sync，对同步器所有公有方法的调用都会委托给这个内部子类。 7.后续后面会推出以下有关AQS的文章，已加深对于AQS的理解 AQS ConditionObject对象解析 AQS 应用案例 ReentrantReadWriteLock解析 Java volatile的内存语义与AQS锁内存可见性 8.感谢本文很多内容整理自网络，参考文献：https://segmentfault.com/a/1190000011376192https://segmentfault.com/a/1190000011391092https://zhuanlan.zhihu.com/p/27134110https://blog.csdn.net/wojiaolinaaa/article/details/50070031https://www.cnblogs.com/waterystone/p/4920797.html FIFO队列:https://www.cnblogs.com/waterystone/p/4920797.html]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>JUC</tag>
        <tag>AQS</tag>
        <tag>JDK</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP与UDP 笔记]]></title>
    <url>%2Ftcp-and-udp-note.html</url>
    <content type="text"><![CDATA[本文整理自：《图解TCP/IP 第5版》作者：[日] 竹下隆史，[日] 村山公保，[日] 荒井透，[日] 苅田幸雄 著译者：乌尼日其其格出版时间：2013-07 传输层的作用TCP提供可靠的通信传输，而UDP则常被用于让广播和细节控制交给应用的通信传输。 两种传输层协议TCP和UDPTCPTCP是面向连接的、可靠的流协议。流就是指不间断的数据结构，你可以把它想象成排水管道中的水流。TCP为提供可靠性传输，实行“顺序控制”或“重发控制”机制。此外还具备“流控制（流量控制）”、“拥塞控制”、提高网络利用率等众多功能。 UDPUDP是不具有可靠性的数据报协议。细微的处理它会交给上层的应用去完成。UDP情况下，虽然可以确保发送消息的大小，却不能保证消息一定会到达。因此，应用有时会根据自己的需要进行重发处理。 TCP与UDP区分TCP用于在传输层有必要实现可靠性传输的情况。由于它是面向有连接并具备顺序控制、重发控制等机制的。所以它可以为应用提供可靠传输。 UDP主要用于那些对高速传输和实时性有较高要求的通信或广播通信。举一个IP电话进行通话的例子。如果使用TCP，数据在传送途中如果丢失会被重发，但是这样无法流畅地传输通话人的声音，会导致无法进行正常交流。而采用UDP，它不会进行重发处理。从而也就不会有声音大幅度延迟到达的问题。即使有部分数据丢失，也只是影响某一小部分的通话。此外，在多播与广播通信中也使用UDP而不是TCP。RIP、DHCP等基于广播的协议也要依赖于UDP。 端口号端口号定义数据链路和IP中的地址，分别指的是MAC地址和IP地址。前者用来识别同一链路中不同的计算机，后者用来识别TCP/IP网络中互连的主机和路由器。传输层也有类似概念，就是端口号。端口号用来识别同一台计算机中进行通信的不同应用程序。因此，它也被称为程序地址。 通过IP地址、端口号、协议号进行通信识别TCP/IP或UDP/IP通信中通常采用5个信息来识别一个通信。它们是“源IP地址”、“目标IP地址”、“协议号”、“源端口号”、“目标端口号”。只要其中某一项不同，则被认为是其他通信。 端口号与协议端口号由其使用的传输层协议决定。因此，不同的传输协议可以使用相同的端口号。例如，TCP与UDP使用同一个端口号，但使用目的各不相同。 数据到达IP层后，会先检查IP首部中的协议号，再传给相应协议的模块。传给TCP或UDP去做端口号处理。即使是同一个端口号，由于传输协议是各自独立地进行处理，因此相互之间不会影响。 UDPUDP是User Datagram Protocol的缩写，即用户数据包协议。 UDP不提供复杂控制机制，利用IP提供面向无连接的通信服务。且它是将应用程序发来的数据在收到的那一刻，立即按照原样发送到网络上的一种机制。 UDP面向无连接，可以随时发送数据。它常用于几个方面： 包总量较少的通信（DNS、SNMP等） 视频、音频等多媒体通信（即时通信） 限定于LAN等特定网络中的应用通信 广播通信（广播、多播） TCPTCP是Transmission Control Protocol的缩写，传输控制协议。 TCP充分实现了数据传输时各种控制功能，可以进行丢包的重发控制，还可以对次序乱掉的分包进行顺序控制。此外，TCP作为一种面向有连接的协议，只有在确认通信对端存在时才会发送数据，从而可以控制通信流量的浪费。 连接连接是指各种设备、线路,或网络中进行通信的两个应用程序为了相互传递消息而专有的、虚拟的通信线路,也叫做虚拟电路。一旦建立了连接,进行通信的应用程序只是用这个虚拟的通信线路发送和接收数据,就可以保障信息的传输。应用程序不用顾虑IP网络上可能发生的各种问题,依然可以转发数据。TCP则负责控制链接的建立、断开、保持等管理工作。 TCP的特点及其目的为了通过数据包实现可靠性传输,需要考虑很多事情,例如数据的破坏、丢包、重复记忆分片顺序混乱等问题。如不能解决这些问题,也就无从谈起可靠传输。 TCP通过检验和、序列号、确认应答、重发控制、连接管理以及窗口控制等机制实现可靠性传输。 通过序列号与确认应答提高可靠性在TCP中，当发送端数据到达接受主机时，接收端主机会返回一个已收到的消息的通知。这个消息叫做确认应答（ACK Positive Acknowledgement）。 TCP通过肯定的确认应答（ACK）实现可靠的数据传输。当发送端将数据发出之后会等待对端的确认应答。如果有确认应答，说明数据已经成功到达对端。 在一定时间内没有等到确认应答，发送端就可以认为数据已经丢失，并进行重发。由此，即使产生了丢包，仍然能够保证数据能够到达对端，实现可靠传输。 未确认应答并不意味着数据一定丢失。也有可能是数据对方已经收到，只是返回的确认应答在途中丢失。 为了防止出现随意重发的情况，就需要引入一种机制，它能够识别是否已经接收数据，又能判断是否需要接收。 这些确认应答处理、重发控制以及重复控制等功能都可以通过序列号实现。序列号是按照顺序给发送数据的每个字节（8位字节）都标上号码的编号（序列号的初始值并非为0。而是在建立连接以后由随机数生成。而后面的计算则是对每一字节加一）。接收端查询接收数据TCP首部中序列号和数据的长度，将自己下一步应该接收的序列号作为确认应答返送回去。这样，通过序列号和确认应答号，TCP可以实现可靠传输。 重发超时如何确定重发超时是指在重发数据之前，等待确认应答到来的那个特定时间间隔。如果超过了这个时间仍未收到确认应答，发送端将进行数据重发。那么这个重发超时的具体时间长度又是如何确定的呢？ 最理想的是，找到一个最小时间，它能保证“确认应答一定能在这个时间内返回”。然而这个时间长短随着数据包途径的网络环境的不同而有所变化。例如在高速的LAN中时间相对较短，而在长距离的通信当中应该比LAN要长一些。即使是在同一个网络中，根据不同时段的网络堵塞程度时间的长短也会发生变化。TCP要求不论处在何种网络环境下都要提供高性能通信，并且无论网络拥堵情况发生何种变化，都必须保持这一特性。为此，它在每次发包时都会计算往返时间及其偏差。将这个往返时间和偏差相加重发超时的时间，就是比这个总和要稍大一点的值。 在BSD的Unix以及Windows系统中，超时都以0.5秒为单位进行控制，因此重发超时都是0.5秒的整数倍。不过，由于最初的数据包还不知道往返时间，所以其重发超时一般设置为6秒左右。数据被重发之后若还是收不到确认应答，则进行再次发送。此时，等待确认应答的时间将会以2倍、4倍的指数函数延长。此外，数据也不会被无限、反复地重发。达到一定重发次数之后，如果仍没有任何确认应答返回，就会判断为网络或对端主机发生了异常，强制关闭连接。并且通知应用通信异常强行终止。 连接管理TCP提供面向有连接的通信传输。面向有连接是指在数据通信开始之前先做好通信两端之间的准备工作。 UDP是一种面向无连接的通信协议，因此不检查对端是否可以通信，直接将UDP包发出去。TCP与此相反，它会在数据通信之前，通过TCP首部发送一个SYN包作为建立连接的请求等待确认应答（TCP中发送第一个SYN包的一方叫客户端，接收这个的一方叫服务端）。如果对端发来确认应答，则认为可以进行数据通信。如果对端的确认应答未能到达，就不会进行数据通信。此外，在通信结束时会进行断开连接的处理（FIN包）。 可以使用TCP首部用于控制的字段来管理TCP连接（也叫控制域）。一个连接的建立与断开，正常过程至少需要来回发送7个包才能完成。（建立一个TCP连接需要发送3个包，这个过程也称为3次握手） TCP以段为单位发送数据在建立TCP连接的同时，也可以确定发送数据包的单位，我们也可以称其为“最大消息长度”（MSS：Maximum Segment Size）。 最理想的情况是，最大消息长度正好是IP中不会被分片处理的最大数据长度。 TCP在传输大量数据时，是以MSS的大小将数据进行分割发送的。进行重发时也是以MSS为单位。 MSS是在三次握手的时候，在两端主机之间被计算得出。两端的主机在发出建立连接的请求时，会在TCP首部中写入MSS选项，告诉对方自己的接口能够适应的MSS的大小（为附加MSS选项，TCP首部将不再是20字节，而是4字节的整数倍）。然后会在两者之间选择一个较小的值投入使用。 SYN(synchronous建立联机) ACK(acknowledgement 确认) FIN(finish结束) 利用窗口控制提高速度TCP以1个段为单位，每发一个段进行一次确认应答的处理，如下图，这样传输的缺点是，包的往返时间越长通信性能就越低。 为解决这个问题，TCP引入了窗口这个概念。如下图，确认应答不再是以每个分段，而是以更大的单位进行确认时，转发时间将会被大幅度的缩短。就是说，发送端主机，在发送了一个段以后不必要一直等待确认应答，而是继续发送。 窗口大小就是指无需等待确认应答而可以继续发送数据的最大值。 如下图中，窗口大小为4个段。 这个机制实现了使用大量的缓冲区（Buffer 在此处标识临时保存收发数据的场所。通常是在计算机内存中开辟的一部分空间），通过对多个段同时进行确认应答的功能。 用滑动窗口方式并行处理： 下面的图中发送数据中高亮圈起的部分正是前面所提到的窗口。在这个窗口内的数据即便没有收到确认应答也可以发送出去。此外，从该窗口中能看到的数据因其某种数据已在传输中丢失，所以发送端才能收到确认应答，这种情况也需要重发。为此，发送端主机在等到确认应答返回之前，必须在缓冲区中保留这部分数据。 在滑动窗口以外的部分包括尚未发送的数据以及已经确认对端已收到的数据。当数据发出后若如期收到确认应答就可以不用再重发，此时数据皆可以从缓冲区清除。 收到确认应答，将窗口滑动到确认应答中的序列号的位置。这样可以顺序地将多个段同时发送提高通信性能。这种机制也被称为滑动窗口控制。 滑动窗口方式： 窗口控制与重发控制使用窗口控制中， 如果出现段丢失怎么办？ 首先考虑确认应答未能返回的情况。这种情况下，数据已经达到对端，是不需要进行重发的。然而，在没有使用窗口控制的时候，没有收到确认应答的数据会被重发。而使用了窗口控制，如下图，某些确认应答即便丢失也无需重发。 其次，考虑一下某个报文段丢失的情况。如下图，接收主机如果收到一个自己应该接收的序号以外的数据时，会针对当前位置收到数据返回确认应答（不过即使接收端主机收到的包序号并不连续，也不会将数据丢弃而是暂时保存至缓冲区中）。 当某一报文段丢失后，发送端会一直收到序号为1001的确认应答，这个确认应答好像在提醒发送端“我想接收的是从1001开始的数据”。因此，在窗口比较大，又出现报文段丢失的情况下，同一个序号的确认应答将会被重复不断地返回。而发送端主机如果连续3次收到同一个确认应答（之所以连续收到3次而不是两次的理由是因为，即使数据段的序号被替换两次也不会触发重发机制）。就会将其所对应的数据进行重发。这种机制比之前提到的超时管理更加高效，因此也被称作高速重发控制。 流控制发送端根据自己的实际情况发送数据。但是，接收端可能收到的是一个毫无关系的数据包有可能会在处理其他问题上花费一些时间。因此在为这个数据包做其他处理时会耗费一些时间，甚至在高负荷情况下无法接收任何数据。如此一来，如果接收端将本应该接收的数据丢弃的话，就又会触发重发机制，从而导致网络流量的浪费。 为了防止这种现象发生，TCP提供一种机制可以让发送端根据接收端的实际接收能力控制发送的数据量。这就是所谓的流控制。它的具体操作时，接收端主机向发送端主机通知自己可以接收数据的大小，于是发送端会发送不超过这个限制的数据。该大小限度就被称为窗口大小。 TCP首部中，专门有一个字段用来通知窗口大小。接收主机将自己的可以接收的缓冲区大小放入这个字段通知给发送端。这个值越大，说明网络的吞吐量越高。 不过，接收端这个缓冲区一旦面临数据溢出时，窗口大小的值也会随之被设置为一个更小的值通知给发送端，从而控制数据发送量。就是说，发送端主机会根据接收端主机的指示，对发送数据的量进行控制。这也形成了一个完整的TCP流控制（流量控制）。 根据窗口大小控制流量过程示例： 当接收端收到从3001号开始的数据段后其缓冲区即满，不得不暂时停止接收数据。之后，在收到发送窗口更新通知后通信才得以继续进行。如果这个窗口更新通知在传输途中丢失，可能会导致无法继续通信。为避免此类问题，发送端主机会时不时发送一个叫做窗口探测的数据段，此数据段仅含一个字节以获取最新的窗口大小信息。 拥塞控制有了TCP窗口控制，收发主机之间即使不再以一个数据段为单位发送确认应答，也能够连续发送大量数据包。然而，如果在通信刚开始就发送大量数据，也可能会引发其他问题。 TCP为了防止该问题的出现，在通信一开始就会通过一个叫做慢启动的算法得出的数值，对发送数据量进行控制。 首先，为了在发送端调节所要发送数据的量，定义了一个叫做“拥塞窗口”的概念。于是在慢启动的时候，将这个拥塞窗口的大小设置为1个数据段（1MSS）发送数据，之后每收到一次确认应答（ACK），拥塞窗口的值就加1。在发送数据包时，将拥塞窗口的大小与接收端主机通知的窗口大小做比较，然后按照它们当中较小的那个值，发送比其还要小的数据量。 如果重发采用超时机制,那么拥塞窗口的初始值可以设置为1以后再进行慢启动修正。有了上述这些机制,就可以有限的减少通信开始时连续发包导致的网络拥堵,还可以避免网络拥塞情况的发生。 不过,随着包的每次往返,拥塞窗口也会以1、2、4等指数函数的增长,拥堵状况激增甚至导致网络拥塞的发生。为了防止这些,引入了慢启动阀值的概念。只要拥塞窗口的值超出这个阀值,在每收到一次确认应答时,只允许以下面这种比例方法拥塞窗口: 拥塞窗口越大，确认应答的数目也会增加，不过随着每收到一个确认应答，其涨幅也会逐渐减少，甚至小过比一个数据段还要小的字节数。所以，拥塞窗口的大小会呈直线上升的趋势。 TCP通信开始时，并没有设置相应的慢启动阈值（与窗口的最大值相同)，而是在超时重发时才会设置为当时拥塞窗口一半的大小。 由重发确认应答而触发的高速重发与超时重发机制的处理多少有些不同。因为前者要求至少3次的确认应答数据段到达对方主机后才会触发，相比后者网络的拥堵要轻一些。 而由重复确认应答进行高速重发控制时，慢启动阈值的大小被设置为当时窗口大小的一半（严格来说，是设置为“实际已发送但未收到确认应答的数据量”的一半）。然后将窗口的大小设置为该慢启动阈值+3个数据段的大小。 有了这样一种控制，TCP的拥塞窗口如上图所示发生变化。由于窗口的大小会直接影响数据被转发的吞吐量，所以一般情况下，窗口越大，越会形成高吞吐量的通信。 当TCP通信开始以后,网络吞吐量会逐渐上升,但是随着网络拥堵的发生吞吐量也会急剧下降。于是会再次进入吞吐量慢慢上升的过程。因此所谓TCP的吞吐量的特点就好像是在逐步占领网络带宽的感觉。 UDP首部的格式下图展示了UDP首部的格式。除去数据的部分正式UDP的首部。UDP首部由源端口号，目标端口号，包长和校验和组成。 源端口号（Source Port）表示发送端端口号，字段长16位。该字段是可选项，有时可能不会设置源端口号。没有源端口号的时候该字段的值设置为0。可用于不需要返回的通信中。 目标端口号（Destination Port）表示接收端端口，字段长度16位。 包长度（Length）该字段保存了UDP首部的长度跟数据的长度之和。单位为字节（8位字节）。 校验和（Checksum）校验和是为了提供可靠的UDP首部和数据而设计。在计算校验和时，附加在UDP伪首部与UDP数据报之前。通过在最后一位增加一个“0”将全长增加16倍。此时将UDP首部的校验和字段设置为“0”。然后以16比特为单位进行1的补码和，并将所得到的1的补码和写入校验和字段。 接收主机在收到UDP数据报以后，从IP首部获知IP地址信息构造UDP伪首部，再进行校验和计算。校验和制度按的值是校验和字段以外余下部分的1的补码和。因此，包括校验和字段在内的所有数据之和结果为“16位全部为1”时，才会被认为所收到的数据时正确的。 另外，UDP也可有可能不用校验和。此时，校验和字段中填入0。这种情况下，由于不进行校验和计算，协议处理的开销（在处理实际数据之外，为了进行通信控制的处理而不得不付出的必要的消耗部分）就会降低，从而提高数据转发的速度。然而，如果UDP首部的端口号或是IP首部的IP地址遇到损坏，那么可能会对其他通信造成不好的影响。因此，在互联网中比较推荐使用校验和检查。 校验和计算中计算UDP伪首部的理由TCP/IP中识别一个通信的应用需要5大要素，它们分别是“源IP地址”、“目标IP地址”、“源端口”、“目标端口”、“协议号”。然而，在UDP的首部中只包含它们当中的两项（源端口和目标端口），余下的3项都包含在IP首部里。假定其他3项都被破坏？显然，这极有可能会导致应该收包的应用收不到包，不该收到包的应用却收到了包。为了避免这类问题，有必要验证一个通信中必要的5项识别码是否正确。为此，在校验和的计算中就引入和伪首部的概念。此外，IPv6中的IP首部没有检验和字段。TCP和UDP通过伪首部，得以对5项数字进行校验，从而实现即使在IP首部并不可靠的情况下仍然能够提供可靠的通信传输。 TCP首部格式 TCP中没有表示包长度和数据长度的字段。可由IP层获知TCP的包长，由TCO的包长可知数据的长度。 源端口号（Source Port）表示发送端端口号，字段长16位。 目标端口号（Destination Port）表示接收端端口号，字段长度16位。 序列号（Sequence Number）字段长32位。序列号（序号）是指发送数据的位置，每发送一次数据，就累加一次该数据字节数的大小。 序列号不会从0或1开始，而是建立连接时由计算机生成的随机数作为其初始值，通过SYN包传给接收端主机。然后再将每转发过去的字节数累加到初始值上表示数据的位置。此外，在建立连接和断开连接时发送的SYN包和FIN包虽然并不携带数据，但是也会作为一个字节增加对应的序列号。 确认应答号（Acknowledgement Number）确认应答号字段长度32位。是指下一次应该受到的数据的序列号。实际上，它是指已收到确认应答号减一为止的数据。发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。 数据偏移（Data Offset）该字段表示TCP所传输的数据部分应该从TCP包的哪个位开始计算，当然也可以把它看做TCP首部的长度。该字段长4位，单位为4字节（即32位）。 保留（Reserved）该字段主要是为了以后扩展时使用，其长度为4位。一般设置为0，但即使收到的包在该字段不为0，此包也不会被丢弃。 控制位（Control Flag）字段长为8位，每一位从左至右分别为CWR、ECE、URG、ACK、PSH、RST、SYN、FIN。这些控制标志也叫作控制位。 CWR（Congestion Window Reduced）CWR标志与后面的ECE标志都用于IP首部的ECN字段。ECE标志为1时，则通知对方已将拥塞窗口缩小。 ECE（ECN-Echo）ECE标志表示ECN-Echo。置为1会通知通信对方，从对方到这边的网络有拥塞。在收到数据包的IP首部中ECN为1时将TCP首部中的ECE设置为1。 URG（Urgent Flag）该位为1时，确认应答的字段变为有效。TCP规定除了最初建立连接时的SYN包之外该位必须设置为1。 PSH（Push Flag）该位为1时，表示需要将受到的数据立即传给上层应用协议。PSH为0时，则不需要立即传而是先进性缓存。 RST（Reset Flag）该位为1时表示TCP连接中出现异常必须强制断开连接。 SYN（Synchronize Flag）用于建立连接。SYN为1 表示希望建立连接，并在其序列号的字段进行序列号初始值的设定。 FIN（Fin Flag）该位为1时，表示今后不会再有数据发送，希望断开连接。窗口大小（Window Size） 窗口大小（Window Size）该字段长为16位。用于通知从相同TCP首部的确认应答号所指位置开始能够接收的数据大小（8位字节）。TCP不允许发送超过此处所示大小的数据。不过，如果窗口为0，则表示可以发送窗口探测，以了解最新的窗口大小。但这个数据必须是1个字节。 校验和（Checksum）TCP的校验和与UDP相似，区别在于TCP的校验和无法关闭。TCP和UDP一样在计算校验和的时候使用TCP伪首部。 接收端在收到TCP数据段以后，从IP首部获取IP地址信息构造TCP伪首部，再进行校验和计算。由于校验和字段里保存着除本字段以外洽谈部分的和的补码值，一次如果计算校验和字段在内的所有数据的16位和以后，得出的结果是“16位全部为1”说明所收到数据是正确的。 紧急指针（Urgent Pointer）略 选项（Options）略]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>读书笔记</tag>
        <tag>TCP</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 Go Channels]]></title>
    <url>%2Fgo-channels.html</url>
    <content type="text"><![CDATA[Golang Channel 一、视频信息1、视频观看地址https://www.youtube.com/watch?v=KBZlN0izeiY 2、PPT下载地址http://download.csdn.net/download/xunzaosiyecao/10212884 3、博文https://about.sourcegraph.com/go/understanding-channels-kavya-joshi/ 二、Go 的并发特性 goroutines: 独立执行每个任务，并可能并行执行 channels: 用于 goroutines 之间的通讯、同步 1、一个简单的事务处理的例子对于下面这样的非并发的程序： 1234567func main() &#123; tasks := getTasks() // 处理每个任务 for _, task := range tasks &#123; process(task) &#125;&#125; 将其转换为 Go 的并发模式很容易，使用典型的 Task Queue 的模式： 123456789101112131415161718192021func main() &#123; // 创建带缓冲的 channel ch := make(chan Task, 3) // 运行固定数量的 workers for i := 0; i &lt; numWorkers; i++ &#123; go worker(ch) &#125; // 发送任务到 workers hellaTasks := getTasks() for _, task := range hellaTasks &#123; ch &lt;- task &#125; ...&#125;func worker(ch chan Task) &#123; for &#123; // 接收任务 task := &lt;-ch process(task) &#125;&#125; 2、channels 的特性 goroutine-safe，多个 goroutine 可以同时访问一个 channel。 可以用于在 goroutine 之间存储和传递值 其语义是先入先出（FIFO） 可以导致 goroutine 的 block 和 unblock 三、解析1、构造 channel1234// 带缓冲的 channelch := make(chan Task, 3)// 无缓冲的 channelch := make(chan Task) 回顾前面提到的 channel 的特性，特别是前两个。如果忽略内置的 channel，让你设计一个具有 goroutines-safe 并且可以用来存储、传递值的东西，你会怎么做？很多人可能觉得或许可以用一个带锁的队列来做。没错，事实上，channel 内部就是一个带锁的队列。https://golang.org/src/runtime/chan.go 123456789type hchan struct &#123; ... buf unsafe.Pointer // 指向一个环形队列 ... sendx uint // 发送 index recvx uint // 接收 index ... lock mutex // 互斥量&#125; buf 的具体实现很简单，就是一个环形队列的实现。sendx 和 recvx 分别用来记录发送、接收的位置。然后用一个 lock 互斥锁来确保无竞争冒险。 对于每一个 ch := make(chan Task, 3) 这类操作，都会在堆中，分配一个空间，建立并初始化一个 hchan 结构变量，而 ch 则是指向这个 hchan 结构的指针。 因为 ch 本身就是个指针，所以我们才可以在 goroutine 函数调用的时候直接将 ch 传递过去，而不用再 &amp;ch 取指针了，所以所有使用同一个 ch 的 goroutine 都指向了同一个实际的内存空间。 2、发送、接收为了方便描述，我们用 G1 表示 main() 函数的 goroutine，而 G2 表示 worker 的 goroutine。 123456789101112131415// G1func main() &#123; ... for _, task := range tasks &#123; ch &lt;- task &#125; ...&#125;// G2func worker(ch chan Task) &#123; for &#123; task :=&lt;-ch process(task) &#125;&#125; 2.1 简单的发送、接收那么 G1 中的 ch &lt;- task0 具体是怎么做的呢？ 获取锁 enqueue(task0)（这里是内存复制 task0） 释放锁 这一步很简单，接下来看 G2 的 t := &lt;- ch 是如何读取数据的。 获取锁 t = dequeue()（同样，这里也是内存复制） 释放锁 这一步也非常简单。但是我们从这个操作中可以看到，所有 goroutine 中共享的部分只有这个 hchan 的结构体，而所有通讯的数据都是内存复制。这遵循了 Go 并发设计中很核心的一个理念： Do not communicate by sharing memory;instead, share memory by communicating 内存复制指的是： 12345678910111213141516171819// typedmemmove copies a value of type t to dst from src.// Must be nosplit, see #16026.//go:nosplitfunc typedmemmove(typ *_type, dst, src unsafe.Pointer) &#123; if typ.kind&amp;kindNoPointers == 0 &#123; bulkBarrierPreWrite(uintptr(dst), uintptr(src), typ.size) &#125; // There&apos;s a race here: if some other goroutine can write to // src, it may change some pointer in src after we&apos;ve // performed the write barrier but before we perform the // memory copy. This safe because the write performed by that // other goroutine must also be accompanied by a write // barrier, so at worst we&apos;ve unnecessarily greyed the old // pointer that was in src. memmove(dst, src, typ.size) if writeBarrier.cgo &#123; cgoCheckMemmove(typ, dst, src, 0, typ.size) &#125;&#125; 3、阻塞和恢复3.1 发送方被阻塞假设 G2 需要很长时间的处理，在此期间，G1 不断的发送任务： 123ch &lt;- task1ch &lt;- task2ch &lt;- task3 但是当再一次 ch &lt;- task4 的时候，由于 ch 的缓冲只有 3 个，所以没有地方放了，于是 G1 被 block 了，当有人从队列中取走一个 Task 的时候，G1 才会被恢复。这是我们都知道的，不过我们今天关心的不是发生了什么，而是如何做到的？ 3.2 goroutine 的运行时调度首先，goroutine 不是操作系统线程，而是 用户空间线程。因此 goroutine 是由 Go runtime 来创建并管理的，而不是 OS，所以要比操作系统线程轻量级。 当然，goroutine 最终还是要运行于某个线程中的，控制 goroutine 如何运行于线程中的是 Go runtime 中的 scheduler （调度器）。 Go 的运行时调度器是 M:N 调度模型，既 N 个 goroutine，会运行于 M 个 OS 线程中。换句话说，一个 OS 线程中，可能会运行多个 goroutine。 Go 的 M:N 调度中使用了3个结构： M: OS 线程 G: goroutine P: 调度上下文 P 拥有一个运行队列，里面是所有可以运行的 goroutine 及其上下文 3.3 goroutine 被阻塞的具体过程那么当 ch &lt;- task4 执行的时候，channel 中已经满了，需要 pause G1。这个时候： G1 会调用运行时的 gopark 然后 Go 的运行时调度器就会接管 将 G1 的状态设置为 waiting 断开 G1 和 M 之间的关系（switch out)，因此 G1 脱离 M，换句话说，M 空闲了，可以安排别的任务了。 从 P 的运行队列中，取得一个可运行的 goroutine G 建立新的 G 和 M 的关系（Switch in)，因此 G 就准备好运行了。 当调度器返回的时候，新的 G 就开始运行了，而 G1 则不会运行，也就是 block 了。 从上面的流程中可以看到，对于 goroutine 来说，G1 被阻塞了，新的 G 开始运行了；而对于操作系统线程 M 来说，则根本没有被阻塞。 我们知道 OS 线程要比 goroutine 要沉重的多，因此这里尽量避免 OS 线程阻塞，可以提高性能。 3.4 goroutine 恢复执行的具体过程前面理解了阻塞，那么接下来理解一下如何恢复运行。不过，在继续了解如何恢复之前，我们需要先进一步理解 hchan 这个结构。因为，当 channel 不在满的时候，调度器是如何知道该让哪个 goroutine 继续运行呢？而且 goroutine 又是如何知道该从哪取数据呢？ 在 hchan 中，除了之前提到的内容外，还定义有 sendq 和 recvq 两个队列，分别表示等待发送、接收的 goroutine，及其相关信息。 123456789type hchan struct &#123; ... buf unsafe.Pointer // 指向一个环形队列 ... sendq waitq // 等待发送的队列 recvq waitq // 等待接收的队列 ... lock mutex // 互斥量&#125; 其中 waitq 是一个链表结构的队列，每个元素是一个 sudog 的结构，其定义大致为： 12345type sudog struct &#123; g *g // 正在等候的 goroutine elem unsafe.Pointer // 指向需要接收、发送的元素 ...&#125; https://golang.org/src/runtime/runtime2.go?h=sudog#L270 所以在之前的阻塞 G1 的过程中，实际上： G1 会给自己创建一个 sudog 的变量 然后追加到 sendq 的等候队列中，方便将来的receiver 来使用这些信息恢复 G1。 这些都是发生在调用调度器之前。 那么现在开始看一下如何恢复。 当 G2 调用 t := &lt;- ch 的时候，channel 的状态是，缓冲是满的，而且还有一个 G1 在等候发送队列里，然后 G2 执行下面的操作： G2 先执行 dequeue() 从缓冲队列中取得 task1 给 t G2 从 sendq 中弹出一个等候发送的 sudog 将弹出的 sudog 中的 elem 的值 enqueue() 到 buf 中 将弹出的 sudog 中的 goroutine，也就是 G1，状态从 waiting 改为 runnable 然后，G2 需要通知调度器 G1 已经可以进行调度了，因此调用 goready(G1)。 调度器将 G1 的状态改为 runnable 调度器将 G1 压入 P 的运行队列，因此在将来的某个时刻调度的时候，G1 就会开始恢复运行。 返回到 G2 注意，这里是由 G2 来负责将 G1 的 elem 压入 buf 的，这是一个优化。这样将来 G1 恢复运行后，就不必再次获取锁、enqueue()、释放锁了。这样就避免了多次锁的开销。 3.5 如果接收方先阻塞呢？更酷的地方是接收方先阻塞的流程。 如果 G2 先执行了 t := &lt;- ch，此时 buf 是空的，因此 G2 会被阻塞，他的流程是这样： G2 给自己创建一个 sudog 结构变量。其中 g 是自己，也就是 G2，而 elem 则指向 t 将这个 sudog 变量压入 recvq 等候接收队列 G2 需要告诉 goroutine，自己需要 pause 了，于是调用 gopark(G2) 和之前一样，调度器将其 G2 的状态改为 waiting 断开 G2 和 M 的关系 从 P 的运行队列中取出一个 goroutine 建立新的 goroutine 和 M 的关系 返回，开始继续运行新的 goroutine 这些应该已经不陌生了，那么当 G1 开始发送数据的时候，流程是什么样子的呢？ G1 可以将 enqueue(task)，然后调用 goready(G2)。不过，我们可以更聪明一些。 我们根据 hchan 结构的状态，已经知道 task 进入 buf 后，G2 恢复运行后，会读取其值，复制到 t 中。那么 G1 可以根本不走 buf，G1 可以直接把数据给 G2。 Goroutine 通常都有自己的栈，互相之间不会访问对方的栈内数据，除了 channel。这里，由于我们已经知道了 t 的地址（通过 elem指针），而且由于 G2 不在运行，所以我们可以很安全的直接赋值。当 G2 恢复运行的时候，既不需要再次获取锁，也不需要对 buf 进行操作。从而节约了内存复制、以及锁操作的开销。 4、总结 goroutine-safe hchan 中的 lock mutex 存储、传递值，FIFO 通过 hchan 中的环形缓冲区来实现 导致 goroutine 的阻塞和恢复 hchan 中的 sendq和recvq，也就是 sudog 结构的链表队列 调用运行时调度器 (gopark(), goready()) 四、其它 channel 的操作1、无缓冲 channel无缓冲的 channel 行为就和前面说的直接发送的例子一样： 接收方阻塞 → 发送方直接写入接收方的栈 发送方阻塞 → 接受法直接从发送方的 sudog 中读取 2、selecthttps://golang.org/src/runtime/select.go 先把所有需要操作的 channel 上锁 给自己创建一个 sudog，然后添加到所有 channel 的 sendq或recvq（取决于是发送还是接收） 把所有的 channel 解锁，然后 pause 当前调用 select 的 goroutine（gopark()） 然后当有任意一个 channel 可用时，select 的这个 goroutine 就会被调度执行。 resuming mirrors the pause sequence 五、为什么 Go 会这样设计？1、Simplicity更倾向于带锁的队列，而不是无锁的实现。 性能提升不是凭空而来的，是随着复杂度增加而增加的。 dvyokov后者虽然性能可能会更好，但是这个优势，并不一定能够战胜随之而来的实现代码的复杂度所带来的劣势。 2、Performance 调用 Go 运行时调度器，这样可以保持 OS 线程不被阻塞跨 goroutine 的栈读、写。 可以让 goroutine 醒来后不必获取锁。 可以避免一些内存复制。 当然，任何优势都会有其代价。这里的代价是实现的复杂度，所以这里有更复杂的内存管理机制、垃圾回收以及栈收缩机制。 在这里性能的提高优势，要比复杂度的提高带来的劣势要大。 所以在 channel 实现的各种代码中，我们都可以见到这种simplicity vs performance 的权衡后的结果。 本文转载自：https://blog.lab99.org/post/golang-2017-10-04-video-understanding-channels.html#fa-song-jie-shou]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>Channel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go 1.9 Sync Map]]></title>
    <url>%2Fgo-sync-map.html</url>
    <content type="text"><![CDATA[基于Go 1.9 解析Sync Map 概要Package 本文主要阐述：Load、Store、Delete，更加详细的阐述可以参考源码描述（建议先大体浏览一下Map源码）。 实现思路 空间换时间。 通过冗余的两个数据结构(read、dirty),实现加锁对性能的影响。 使用只读数据(read)，避免读写冲突。 动态调整，miss次数多了之后，将dirty数据提升为read。 double-checking。 延迟删除。 删除一个键值只是打标记（会将key对应value的pointer置为nil，但read中仍然有这个key:key;value:nil的键值对），只有在提升dirty的时候才清理删除的数据。 优先从read读取、更新、删除，因为对read的读取不需要锁。 虽然read和dirty有冗余数据，但这些数据是通过指针指向同一个数据，所以尽管Map的value会很大，但是冗余的空间占用还是有限的。 数据结构Map1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// Map is a concurrent map with amortized-constant-time loads, stores, and deletes.// It is safe for multiple goroutines to call a Map&apos;s methods concurrently.//// It is optimized for use in concurrent loops with keys that are// stable over time, and either few steady-state stores, or stores// localized to one goroutine per key.//// For use cases that do not share these attributes, it will likely have// comparable or worse performance and worse type safety than an ordinary// map paired with a read-write mutex.//// The zero Map is valid and empty.//// A Map must not be copied after first use.//该 Map 是线程安全的，读取，插入，删除也都保持着常数级的时间复杂度。//多个 goroutines 协程同时调用 Map 方法也是线程安全的。该 Map 的零值是有效的，//并且零值是一个空的 Map 。线程安全的 Map 在第一次使用之后，不允许被拷贝。type Map struct &#123; mu Mutex // read contains the portion of the map&apos;s contents that are safe for // concurrent access (with or without mu held). // // The read field itself is always safe to load, but must only be stored with // mu held. // // Entries stored in read may be updated concurrently without mu, but updating // a previously-expunged entry requires that the entry be copied to the dirty // map and unexpunged with mu held. // 一个只读的数据结构，因为只读，所以不会有读写冲突。 // 所以从这个数据中读取总是安全的。 // 实际上，实际也会更新这个数据的entries,如果entry是未删除的(unexpunged), 并不需要加锁。如果entry已经被删除了，需要加锁，以便更新dirty数据。 read atomic.Value // readOnly // dirty contains the portion of the map&apos;s contents that require mu to be // held. To ensure that the dirty map can be promoted to the read map quickly, // it also includes all of the non-expunged entries in the read map. // // Expunged entries are not stored in the dirty map. An expunged entry in the // clean map must be unexpunged and added to the dirty map before a new value // can be stored to it. // // If the dirty map is nil, the next write to the map will initialize it by // making a shallow copy of the clean map, omitting stale entries. // dirty数据包含当前的map包含的entries,它包含最新的entries(包括read中未删除的数据,虽有冗余，但是提升dirty字段为read的时候非常快，不用一个一个的复制，而是直接将这个数据结构作为read字段的一部分),有些数据还可能没有移动到read字段中。 // 对于dirty的操作需要加锁，因为对它的操作可能会有读写竞争。 // 当dirty为空的时候， 比如初始化或者刚提升完，下一次的写操作会复制read字段中未删除的数据到这个数据中。 dirty map[interface&#123;&#125;]*entry // misses counts the number of loads since the read map was last updated that // needed to lock mu to determine whether the key was present. // // Once enough misses have occurred to cover the cost of copying the dirty // map, the dirty map will be promoted to the read map (in the unamended // state) and the next store to the map will make a new dirty copy. // 当从Map中读取entry的时候，如果read中不包含这个entry,会尝试从dirty中读取，这个时候会将misses加一， // 当misses累积到 dirty的长度的时候， 就会将dirty提升为read,避免从dirty中miss太多次。因为操作dirty需要加锁。 misses int&#125; readOnly1234567// readOnly is an immutable struct stored atomically in the Map.read field.type readOnly struct &#123; m map[interface&#123;&#125;]*entry // true if the dirty map contains some key not in m. // 如果Map.dirty有些数据不在中的时候，这个值为true amended bool &#125; entry123456789101112131415161718192021222324252627// An entry is a slot in the map corresponding to a particular key.type entry struct &#123; // p points to the interface&#123;&#125; value stored for the entry. // // If p == nil, the entry has been deleted and m.dirty == nil. // // If p == expunged, the entry has been deleted, m.dirty != nil, and the entry // is missing from m.dirty. // // Otherwise, the entry is valid and recorded in m.read.m[key] and, if m.dirty // != nil, in m.dirty[key]. // // An entry can be deleted by atomic replacement with nil: when m.dirty is // next created, it will atomically replace nil with expunged and leave // m.dirty[key] unset. // // An entry&apos;s associated value can be updated by atomic replacement, provided // p != expunged. If p == expunged, an entry&apos;s associated value can be updated // only after first setting m.dirty[key] = e so that lookups using the dirty // map find the entry. //p有三种值： //nil: entry已被删除了，并且m.dirty为nil //expunged: entry已被删除了，并且m.dirty不为nil，而且这个entry不存在于m.dirty中 //其它： entry是一个正常的值 p unsafe.Pointer // *interface&#123;&#125;&#125; Value1234567891011// A Value provides an atomic load and store of a consistently typed value.// Values can be created as part of other data structures.// The zero value for a Value returns nil from Load.// Once Store has been called, a Value must not be copied.//// A Value must not be copied after first use.type Value struct &#123; noCopy noCopy v interface&#123;&#125;&#125; Load据指定的key,查找对应的值value,如果不存在，通过ok反映。 123456789101112131415161718192021222324252627282930313233343536func (m *Map) Load(key interface&#123;&#125;) (value interface&#123;&#125;, ok bool) &#123; read, _ := m.read.Load().(readOnly) e, ok := read.m[key] // 如果没找到，并且m.dirty中有新数据，需要从m.dirty查找，这个时候需要加锁 if !ok &amp;&amp; read.amended &#123; m.mu.Lock() // Avoid reporting a spurious miss if m.dirty got promoted while we were // blocked on m.mu. (If further loads of the same key will not miss, it&apos;s // not worth copying the dirty map for this key.) //double check,避免加锁的时候m.dirty提升为m.read,这个时候m.read可能被替换了。 read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok &amp;&amp; read.amended &#123; e, ok = m.dirty[key] // Regardless of whether the entry was present, record a miss: this key // will take the slow path until the dirty map is promoted to the read // map. m.missLocked() &#125; m.mu.Unlock() &#125; if !ok &#123; return nil, false &#125; return e.load()&#125;func (m *Map) missLocked() &#123; m.misses++ if m.misses &lt; len(m.dirty) &#123; return &#125; m.read.Store(readOnly&#123;m: m.dirty&#125;) m.dirty = nil m.misses = 0&#125; Store更新或者新增一个entry 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091// Store sets the value for a key.func (m *Map) Store(key, value interface&#123;&#125;) &#123; read, _ := m.read.Load().(readOnly) // 从 read map 中读取 key 成功并且取出的 entry 尝试存储 value 成功，直接返回 if e, ok := read.m[key]; ok &amp;&amp; e.tryStore(&amp;value) &#123; return &#125; m.mu.Lock() read, _ = m.read.Load().(readOnly) if e, ok := read.m[key]; ok &#123; if e.unexpungeLocked() &#123;//确保未被标记成删除，即e 指向的是非 nil 的 // The entry was previously expunged, which implies that there is a // non-nil dirty map and this entry is not in it. //m.dirty中不存在这个键，所以加入m.dirty m.dirty[key] = e &#125; e.storeLocked(&amp;value) &#125; else if e, ok := m.dirty[key]; ok &#123; e.storeLocked(&amp;value) &#125; else &#123; if !read.amended &#123; // We&apos;re adding the first new key to the dirty map. // Make sure it is allocated and mark the read-only map as incomplete. m.dirtyLocked() m.read.Store(readOnly&#123;m: read.m, amended: true&#125;) &#125; m.dirty[key] = newEntry(value) &#125; m.mu.Unlock()&#125;// tryStore stores a value if the entry has not been expunged.//// If the entry is expunged, tryStore returns false and leaves the entry// unchanged.func (e *entry) tryStore(i *interface&#123;&#125;) bool &#123; p := atomic.LoadPointer(&amp;e.p) if p == expunged &#123; return false &#125; for &#123; if atomic.CompareAndSwapPointer(&amp;e.p, p, unsafe.Pointer(i)) &#123; return true &#125; p = atomic.LoadPointer(&amp;e.p) if p == expunged &#123; return false &#125; &#125;&#125;func (m *Map) dirtyLocked() &#123; if m.dirty != nil &#123; return &#125; read, _ := m.read.Load().(readOnly) m.dirty = make(map[interface&#123;&#125;]*entry, len(read.m)) for k, e := range read.m &#123; if !e.tryExpungeLocked() &#123; m.dirty[k] = e &#125; &#125;&#125;func (e *entry) tryExpungeLocked() (isExpunged bool) &#123; p := atomic.LoadPointer(&amp;e.p) for p == nil &#123; // 将已经删除标记为nil的数据标记为expunged if atomic.CompareAndSwapPointer(&amp;e.p, nil, expunged) &#123; return true &#125; p = atomic.LoadPointer(&amp;e.p) &#125; return p == expunged&#125;// unexpungeLocked ensures that the entry is not marked as expunged.// If the entry was previously expunged, it must be added to the dirty map// before m.mu is unlocked.// unexpungeLocked 函数确保了 entry 没有被标记成已被清除。// 如果 entry 先前被清除过了，那么在 mutex 解锁之前，它一定要被加入到 dirty map 中//如果 entry 的 unexpungeLocked 返回为 true，那么就说明 entry //之前被标记成了 expunged，并经过 CAS 操作成功把它置为 nil。func (e *entry) unexpungeLocked() (wasExpunged bool) &#123; return atomic.CompareAndSwapPointer(&amp;e.p, expunged, nil)&#125; Delete删除一个键值 12345678910111213141516171819202122232425262728293031// Delete deletes the value for a key.func (m *Map) Delete(key interface&#123;&#125;) &#123; read, _ := m.read.Load().(readOnly) e, ok := read.m[key] if !ok &amp;&amp; read.amended &#123; m.mu.Lock() read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok &amp;&amp; read.amended &#123; delete(m.dirty, key) &#125; m.mu.Unlock() &#125; if ok &#123; e.delete() &#125;&#125;func (e *entry) delete() (hadValue bool) &#123; for &#123; p := atomic.LoadPointer(&amp;e.p) // 已标记为删除 if p == nil || p == expunged &#123; return false &#125; // 原子操作，e.p标记为nil if atomic.CompareAndSwapPointer(&amp;e.p, p, nil) &#123; return true &#125; &#125;&#125; 疑问已经删除的key,再次Load的时候，会怎么样？1234567func (e *entry) load() (value interface&#123;&#125;, ok bool) &#123; p := atomic.LoadPointer(&amp;e.p) if p == nil || p == expunged &#123; return nil, false &#125; return *(*interface&#123;&#125;)(p), true&#125; 在Map Load方法中调用e.load()时，load方法会识别该值是否已被删除 Referencehttps://studygolang.com/articles/10511http://www.jianshu.com/p/43e66dab535b]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>Go</tag>
        <tag>Sync</tag>
        <tag>Map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Spring AOP与IOC的个人思考]]></title>
    <url>%2Fspring-aop-ioc-think.html</url>
    <content type="text"><![CDATA[在阅读本文前，强烈建议阅读：Java JDK 动态代理（AOP）使用及实现原理分析 AOP是Spring提供的关键特性之一。AOP即面向切面编程，是OOP编程的有效补充。使用AOP技术，可以将一些系统性相关的编程工作，独立提取出来，独立实现，然后通过切面切入进系统。从而避免了在业务逻辑的代码中混入很多的系统相关的逻辑——比如权限管理，事物管理，日志记录等等。这些系统性的编程工作都可以独立编码实现，然后通过AOP技术切入进系统即可。从而达到了将不同的关注点分离出来的效果。本文深入剖析Spring的AOP的原理。 一、AOP 的实现原理AOP分为静态AOP和动态AOP。 静态AOP是指AspectJ实现的AOP，他是将切面代码直接编译到Java类文件中。 动态AOP是指将切面代码进行动态织入实现的AOP。 Spring的AOP为动态AOP，实现的技术为：JDK提供的动态代理技术 和 CGLIB(动态字节码增强技术)。尽管实现技术不一样，但都是基于代理模式，都是生成一个代理对象。 1、JDK动态代理JDK部分解析参考：Java JDK 动态代理（AOP）使用及实现原理分析 2、CGLIB（code generate libary）字节码生成技术实现AOP，其实就是继承被代理对象，然后Override需要被代理的方法，在覆盖该方法时，自然是可以插入我们自己的代码的。 因为需要Override被代理对象的方法，所以自然CGLIB技术实现AOP时，就必须要求需要被代理的方法不能是final方法，因为final方法不能被子类覆盖。 12345678910111213141516171819202122232425262728package net.aazj.aop;import java.lang.reflect.Method;import net.sf.cglib.proxy.Enhancer;import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;public class CGProxy implements MethodInterceptor&#123; private Object target; // 被代理对象 public CGProxy(Object target)&#123; this.target = target; &#125; public Object intercept(Object obj, java.lang.reflect.Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; System.out.println(&quot;do sth before....&quot;); Object result = proxy.invokeSuper(obj, args); System.out.println(&quot;do sth after....&quot;); return result; &#125; public Object getProxyObject() &#123; Enhancer enhancer = new Enhancer(); // 设置父类 enhancer.setSuperclass(this.target.getClass()); // 设置回调 enhancer.setCallback(this); // 在调用父类方法时，回调 this.intercept() // 创建代理对象 return enhancer.create(); &#125;&#125; 测试： 123456789101112131415161718192021222324252627public interface UserService &#123; public void addUser(User user); public User getUser(int id);&#125;public class UserServiceImpl implements UserService &#123; public void addUser(User user) &#123; System.out.println(&quot;add user into database.&quot;); &#125; public User getUser(int id) &#123; User user = new User(); user.setId(id); System.out.println(&quot;getUser from database.&quot;); return user; &#125;&#125;public class CGProxyTest &#123; public static void main(String[] args)&#123; // 被代理的对象 Object proxyedObject = new UserServiceImpl(); CGProxy cgProxy = new CGProxy(proxyedObject); UserService proxyObject = (UserService) cgProxy.getProxyObject(); proxyObject.getUser(1); proxyObject.addUser(new User()); &#125;&#125; 输出结果： 123456do sth before....getUser from database.do sth after....do sth before....add user into database.do sth after.... 它的原理是：生成一个父类enhancer.setSuperclass(this.target.getClass())的子类enhancer.create(),然后对父类的方法进行拦截enhancer.setCallback(this). 二、思考从以上两种代理方式可以看出，实现AOP的关键是：动态代理，即将需要用的接口、类再包装一层，通过动态修改字节码文件实现各种拦截与通知。 注意，两者(JDK动态代理、CGLIB)都需要：要代理真实对象的实例。 比如：在Spring MVC的Controller层一般@Autowired是Service接口，但带有@Service标识的却是实现Service接口的实体类，这样对于JDK动态代理来说已经足以生成代理类了(其实，不过是cglib还是jdk的动态代理，你直接@Autowired Service接口实现类，也是可以注入成功的，但不如注入Service接口灵活)，大家在跟踪代码的时候可以看一下Spring注入的bean真正的类型，你就可以发现它是代理生成的实例。 比如这种： 带有注解标识的接口或者在Spring.XML中配置的bean会在Spring初始化的时候，被Spring通过反射加载实例化到Spring容器中。 做过Client/Server架构开发的朋友应该知道，在Application运行过程中一般都会有一个应用上下文Context，一般将一些系统信息放在里面，比如一些登录信息、WCF连接实例等。这些信息在系统的任何地方都可以取到（其实就是一些顶级变量集合，生命周期最长的一些家伙）。 换个角度想一下，如果我们在Application初始化的时候，用反射（获取要代理对象的实例）和动态代理获取有注解标识或者在xml中配置bean的实例，并放到应用上下文Context中，在需要的地方都能取到，这不就是一个简单版的Spring 容器吗？]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>JDK</tag>
        <tag>Spring</tag>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java HashMap]]></title>
    <url>%2Fjava-hashmap.html</url>
    <content type="text"><![CDATA[代码基于 Jdk1.8 最近在工作用到Map等一系列的集合，于是，想仔细看一下其具体实现。 结构12public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable 抽象类AbstractMap1public abstract class AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt; 该类实现了Map接口，具体结构如下：该类代码很简单，不再赘述。 序列化接口：Serializable该接口没有什么好说的，但通过该接口，就解释了为什么HashMap总一些字段是用transient来修饰。 一旦变量被transient修饰，变量将不再是对象持久化的一部分，该变量内容在序列化后无法获得访问。 阅读JDK中类注释HashMap是无序的如果希望保持元素的输入顺序应该使用LinkedHashMap 除了非同步和允许使用null之外，HashMap与Hashtable基本一致。此处的非同步指的是多线程访问，并至少一个线程修改HashMap结构。结构修改包括任何新增、删除映射，但仅仅修改HashMap中已存在项值得操作不属于结构修改。 初始容量与加载因子是影响HashMap的两个重要因素。1public HashMap(int initialCapacity, float loadFactor) 初始容量默认值： 1234/** * The default initial capacity - MUST be a power of two. */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 加载因子默认值： 1234/** * The load factor used when none specified in constructor. */static final float DEFAULT_LOAD_FACTOR = 0.75f; 容量是HashMap在创建时“桶”的数量，而初始容量是哈希表在创建时分配的空间大小。加载因子是哈希表在其容量自动增加时能达到多满的衡量尺度（比如默认为0.75，即桶中数据达到3/4就不能再放数据了）。 默认的负载因子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。当重新调整HashMap大小的时候，会存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。所以 HashMap应该避免在多线程环境下使用。 默认0.75这是时间和空间成本上一种折衷：增大负载因子可以减少 Hash 表（就是那个 Entry 数组）所占用的内存空间，但会增加查询数据的时间开销，而查询是最频繁的的操作（HashMap 的 get() 与 put() 方法都要用到查询）；减小负载因子会提高数据查询的性能，但会增加 Hash 表所占用的内存空间。 存储形式链表形式存储？树形结构？ 123456* This map usually acts as a binned (bucketed) hash table, but* when bins get too large, they are transformed into bins of* TreeNodes, each structured similarly to those in* java.util.TreeMap. Most methods try to use normal bins, but* relay to TreeNode methods when applicable (simply by checking* instanceof a node). 源码阅读添加元素12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */ public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125; /** * Implements Map.put and related methods * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don&apos;t change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //hashmap第一次添加元素，调用resize()方法初始化table if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //通过与运算判断tab[hash]位置是否有值 //从newNode这里可以看出，hashmap中key value是以Node&lt;K,V&gt;实例的形式存放的 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); //tab[i]有元素，则需要遍历结点后再添加 else &#123; Node&lt;K,V&gt; e; K k; // hash、key均等，说明待插入元素和第一个元素相等，直接更新 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode)//如果p类型为TreeNode，调用树的添加元素方法(红黑树冲突插入) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; //不是TreeNode,即为链表,遍历链表，查找给定关键字 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; //到达链表的尾端也没有找到key值相同的节点，则生成一个新的Node p.next = newNode(hash, key, value, null); //创建新节点后若超出树形化阈值，则转换为树形存储 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash);//当桶中链表的数量&gt;=9的时候，底层则改为红黑树实现 break; &#125; //如果找到关键字相同的结点 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; //更新p指向下一个节点 p = e; &#125; &#125; // e不为空，即map中存在要添加的关键字 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize();//扩容 afterNodeInsertion(evict); return null; &#125; 小注：1、回调 12afterNodeAccess(e);afterNodeInsertion(evict); 是为LinkedHashMap回调准备的。2、计算hash值 1234567891011121314151617181920 /** * Computes key.hashCode() and spreads (XORs) higher bits of hash * to lower. Because the table uses power-of-two masking, sets of * hashes that vary only in bits above the current mask will * always collide. (Among known examples are sets of Float keys * holding consecutive whole numbers in small tables.) So we * apply a transform that spreads the impact of higher bits * downward. There is a tradeoff between speed, utility, and * quality of bit-spreading. Because many common sets of hashes * are already reasonably distributed (so don&apos;t benefit from * spreading), and because we use trees to handle large sets of * collisions in bins, we just XOR some shifted bits in the * cheapest possible way to reduce systematic lossage, as well as * to incorporate impact of the highest bits that would otherwise * never be used in index calculations because of table bounds. */static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; ‘&gt;&gt;&gt;’：无符号右移，忽略符号位，空位都以0补齐 value &gt;&gt;&gt; num – num 指定要移位值value 移动的位数。 即按二进制形式把所有的数字向右移动对应位数，低位移出（舍弃），高位的空位补零。对于正数来说和带符号右移相同，对于负数来说不同。 ^异或：两个操作数的位中，相同则结果为0，不同则结果为1。 这也正好解释了为什么HashMap底层数组的长度总是 2 的 n 次方。因为这样（数组长度-1）正好相当于一个“低位掩码”。“异或”操作的结果就是散列值的高位全部归零，只保留低位值，用来做数组下标访问。以初始长度16为例，16-1=15。2进制表示是00000000 00000000 00001111。和某hash值做“异或”操作如下，结果就是截取了最低的四位值。 123410100101 11000100 0010010100000000 00000000 00001111----------------------------------00000000 00000000 00000101 //高位全部归零，只保留末四位 更详细的步骤如下： 3、存储结构 获取元素12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * Returns the value to which the specified key is mapped, * or &#123;@code null&#125; if this map contains no mapping for the key. * * &lt;p&gt;More formally, if this map contains a mapping from a key * &#123;@code k&#125; to a value &#123;@code v&#125; such that &#123;@code (key==null ? k==null : * key.equals(k))&#125;, then this method returns &#123;@code v&#125;; otherwise * it returns &#123;@code null&#125;. (There can be at most one such mapping.) * * &lt;p&gt;A return value of &#123;@code null&#125; does not &lt;i&gt;necessarily&lt;/i&gt; * indicate that the map contains no mapping for the key; it&apos;s also * possible that the map explicitly maps the key to &#123;@code null&#125;. * The &#123;@link #containsKey containsKey&#125; operation may be used to * distinguish these two cases. * * @see #put(Object, Object) */ public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; &#125; /** * Implements Map.get and related methods * * @param hash hash for key * @param key the key * @return the node, or null if none */ final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; //hash &amp; length-1 定位数组下标 (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; //第一个节点是TreeNode,则采用位桶+红黑树结构， //调用TreeNode.getTreeNode(hash,key), //遍历红黑树，得到节点的value if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; 树节点的查找： 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Calls find for root node. */final TreeNode&lt;K,V&gt; getTreeNode(int h, Object k) &#123; return ((parent != null) ? root() : this).find(h, k, null);&#125;/** * Finds the node starting at root p with the given hash and key. * The kc argument caches comparableClassFor(key) upon first use * comparing keys. *通过hash值的比较，递归的去遍历红黑树， compareableClassFor(Class k)：判断实例k对应的类是否实现了Comparable接口，如果实现了该接口并 在某些时候如果红黑树节点的元素are of the same &quot;class C implements Comparable&lt;C&gt;&quot; type *利用他们的compareTo()方法来比较大小，这里需要通过反射机制来check他们到底是不是属于同一个类,是不是具有可比较性. */final TreeNode&lt;K,V&gt; find(int h, Object k, Class&lt;?&gt; kc) &#123; TreeNode&lt;K,V&gt; p = this; do &#123; int ph, dir; K pk; TreeNode&lt;K,V&gt; pl = p.left, pr = p.right, q; if ((ph = p.hash) &gt; h) p = pl; else if (ph &lt; h) p = pr; else if ((pk = p.key) == k || (k != null &amp;&amp; k.equals(pk))) return p; else if (pl == null) p = pr; else if (pr == null) p = pl; else if ((kc != null || (kc = comparableClassFor(k)) != null) &amp;&amp; (dir = compareComparables(kc, k, pk)) != 0) p = (dir &lt; 0) ? pl : pr; else if ((q = pr.find(h, k, kc)) != null) return q; else p = pl; &#125; while (p != null); return null;&#125; 元素包含containsKey1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/** * Returns &lt;tt&gt;true&lt;/tt&gt; if this map contains a mapping for the * specified key. * * @param key The key whose presence in this map is to be tested * @return &lt;tt&gt;true&lt;/tt&gt; if this map contains a mapping for the specified * key. */public boolean containsKey(Object key) &#123; return getNode(hash(key), key) != null;&#125;/** * Implements Map.get and related methods * * @param hash hash for key * @param key the key * @return the node, or null if none */final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; //判断tab[hash]位置是否有值 (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //遍历寻找 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; /** * Calls find for root node. */ final TreeNode&lt;K,V&gt; getTreeNode(int h, Object k) &#123; return ((parent != null) ? root() : this).find(h, k, null); &#125; /** * Returns root of tree containing this node. * 获取红黑树的根 */ final TreeNode&lt;K,V&gt; root() &#123; for (TreeNode&lt;K,V&gt; r = this, p;;) &#123; if ((p = r.parent) == null) return r; r = p; &#125; &#125; /** * Finds the node starting at root p with the given hash and key. * The kc argument caches comparableClassFor(key) upon first use * comparing keys. */ final TreeNode&lt;K,V&gt; find(int h, Object k, Class&lt;?&gt; kc) &#123;// k即key，kc为null TreeNode&lt;K,V&gt; p = this; do &#123; int ph, dir; K pk; TreeNode&lt;K,V&gt; pl = p.left, pr = p.right, q; if ((ph = p.hash) &gt; h)// ph存当前节点hash p = pl; else if (ph &lt; h) // 所查hash比当前节点hash大 p = pr;// 查右子树 else if ((pk = p.key) == k || (k != null &amp;&amp; k.equals(pk))) return p;// hash、key均相同，【找到了！】返回当前节点 else if (pl == null)// hash等，key不等，且当前节点的左节点null p = pr;//查右子树 else if (pr == null) p = pl; //get-&gt;getTreeNode传递的kc为null。||逻辑或,短路运算,有真即可 // false || (false &amp;&amp; ？？) else if ((kc != null || (kc = comparableClassFor(k)) != null) &amp;&amp; (dir = compareComparables(kc, k, pk)) != 0) p = (dir &lt; 0) ? pl : pr; else if ((q = pr.find(h, k, kc)) != null) return q; else p = pl; &#125; while (p != null); return null; &#125; 移除remove123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174/** * Removes the mapping for the specified key from this map if present. * * @param key key whose mapping is to be removed from the map * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */ public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; &#125; /** * Implements Map.remove and related methods * * @param hash hash for key * @param key the key * @param value the value to match if matchValue, else ignored * @param matchValue if true only remove if value is equal * @param movable if false do not move other nodes while removing * @return the node, or null if none */ final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; //先比较内存地址，如果地址不一致，再调用equals进行比较 ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; //如果是以红黑树处理冲突，则通过getTreeNode查找 if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; //如果是以链式的方式处理冲突，则通过遍历链表来寻找节点 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; //比对找到的key的value跟要删除的是否匹配 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; //已从结构上修改 此列表的次数 ++modCount; --size; //回调 afterNodeRemoval(node); return node; &#125; &#125; return null; &#125; /** * Removes the given node, that must be present before this call. * This is messier than typical red-black deletion code because we * cannot swap the contents of an interior node with a leaf * successor that is pinned by &quot;next&quot; pointers that are accessible * independently during traversal. So instead we swap the tree * linkages. If the current tree appears to have too few nodes, * the bin is converted back to a plain bin. (The test triggers * somewhere between 2 and 6 nodes, depending on tree structure). */ final void removeTreeNode(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, boolean movable) &#123; int n; if (tab == null || (n = tab.length) == 0) return; int index = (n - 1) &amp; hash; TreeNode&lt;K,V&gt; first = (TreeNode&lt;K,V&gt;)tab[index], root = first, rl; TreeNode&lt;K,V&gt; succ = (TreeNode&lt;K,V&gt;)next, pred = prev; if (pred == null) tab[index] = first = succ; else pred.next = succ; if (succ != null) succ.prev = pred; if (first == null) return; if (root.parent != null) root = root.root(); if (root == null || root.right == null || (rl = root.left) == null || rl.left == null) &#123; tab[index] = first.untreeify(map); // too small return; &#125; TreeNode&lt;K,V&gt; p = this, pl = left, pr = right, replacement; if (pl != null &amp;&amp; pr != null) &#123; TreeNode&lt;K,V&gt; s = pr, sl; while ((sl = s.left) != null) // find successor s = sl; boolean c = s.red; s.red = p.red; p.red = c; // swap colors TreeNode&lt;K,V&gt; sr = s.right; TreeNode&lt;K,V&gt; pp = p.parent; if (s == pr) &#123; // p was s&apos;s direct parent p.parent = s; s.right = p; &#125; else &#123; TreeNode&lt;K,V&gt; sp = s.parent; if ((p.parent = sp) != null) &#123; if (s == sp.left) sp.left = p; else sp.right = p; &#125; if ((s.right = pr) != null) pr.parent = s; &#125; p.left = null; if ((p.right = sr) != null) sr.parent = p; if ((s.left = pl) != null) pl.parent = s; if ((s.parent = pp) == null) root = s; else if (p == pp.left) pp.left = s; else pp.right = s; if (sr != null) replacement = sr; else replacement = p; &#125; else if (pl != null) replacement = pl; else if (pr != null) replacement = pr; else replacement = p; if (replacement != p) &#123; TreeNode&lt;K,V&gt; pp = replacement.parent = p.parent; if (pp == null) root = replacement; else if (p == pp.left) pp.left = replacement; else pp.right = replacement; p.left = p.right = p.parent = null; &#125; TreeNode&lt;K,V&gt; r = p.red ? root : balanceDeletion(root, replacement); if (replacement == p) &#123; // detach TreeNode&lt;K,V&gt; pp = p.parent; p.parent = null; if (pp != null) &#123; if (p == pp.left) pp.left = null; else if (p == pp.right) pp.right = null; &#125; &#125; if (movable) moveRootToFront(tab, r); &#125; 小结在创建 HashMap 时根据实际需要适当地调整 load factor 的值；如果程序比较关心空间开销、内存比较紧张，可以适当地增加负载因子；如果程序比较关心时间开销，内存比较宽裕则可以适当的减少负载因子。通常情况下，程序员无需改变负载因子的值。 如果开始就知道 HashMap 会保存多个 key-value 对，可以在创建时就使用较大的初始化容量，如果 HashMap 中 Entry 的数量一直不会超过极限容量（capacity * load factor），HashMap 就无需调用 resize() 方法重新分配 table 数组，从而保证较好的性能。当然，开始就将初始容量设置太高可能会浪费空间（系统需要创建一个长度为 capacity 的 Entry 数组），因此创建 HashMap 时初始化容量设置也需要小心对待。 HashMap高性能需要以下几点： 高效的hash算法 保证hash值到内存地址（数组索引）的映射速度 根据内存地址（数组索引）可以直接得到相应的值]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>Java</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java-JDK-动态代理（AOP）使用及实现原理分析]]></title>
    <url>%2Fjava-jdk-aop.html</url>
    <content type="text"><![CDATA[一、什么是代理？代理是一种常用的设计模式，其目的就是为其他对象提供一个代理以控制对某个对象的访问。代理类负责为委托类预处理消息，过滤消息并转发消息，以及进行消息被委托类执行后的后续处理。 代理模式UML图： 简单结构示意图： 为了保持行为的一致性，代理类和委托类通常会实现相同的接口，所以在访问者看来两者没有丝毫的区别。通过代理类这中间一层，能有效控制对委托类对象的直接访问，也可以很好地隐藏和保护委托类对象，同时也为实施不同控制策略预留了空间，从而在设计上获得了更大的灵活性。Java 动态代理机制以巧妙的方式近乎完美地实践了代理模式的设计理念。 二、Java 动态代理类Java动态代理类位于java.lang.reflect包下，一般主要涉及到以下两个类： (1)Interface InvocationHandler：该接口中仅定义了一个方法 1public object invoke(Object obj,Method method, Object[] args) 在实际使用时，第一个参数obj一般是指代理类，method是被代理的方法，如上例中的request()，args为该方法的参数数组。这个抽象方法在代理类中动态实现。 (2)Proxy：该类即为动态代理类，其中主要包含以下内容： protected Proxy(InvocationHandler h)：构造函数，用于给内部的h赋值。 static Class getProxyClass( ClassLoader loader, Class[] interfaces)：获得一个代理类，其中loader是类装载器，interfaces是真实类所拥有的全部接口的数组。 static Object newProxyInstance(ClassLoaderloader, Class[] interfaces,InvocationHandler h)：返回代理类的一个实例，返回后的代理类可以当作被代理类使用(可使用被代理类的在Subject接口中声明过的方法) 所谓DynamicProxy是这样一种class：它是在运行时生成的class，在生成它时你必须提供一组interface给它，然后该class就宣称它实现了这些interface。你当然可以把该class的实例当作这些interface中的任何一个来用。当然，这个DynamicProxy其实就是一个Proxy，它不会替你作实质性的工作，在生成它的实例时你必须提供一个handler，由它接管实际的工作。 在使用动态代理类时，我们必须实现InvocationHandler接口 通过这种方式，被代理的对象(RealSubject)可以在运行时动态改变，需要控制的接口(Subject接口)可以在运行时改变，控制的方式(DynamicSubject类)也可以动态改变，从而实现了非常灵活的动态代理关系。 动态代理步骤： 创建一个实现接口InvocationHandler的类，它必须实现invoke方法 创建被代理的类以及接口 通过Proxy的静态方法 newProxyInstance(ClassLoaderloader,Class[]interfaces,InvocationHandler h)创建一个代理 通过代理调用方法 三、JDK的动态代理怎么使用？1、需要动态代理的接口： 123456789101112131415161718192021package jiankunking;/** * 需要动态代理的接口 */public interface Subject &#123; /** * 你好 * * @param name * @return */ public String SayHello(String name); /** * 再见 * * @return */ public String SayGoodBye();&#125; 2、需要代理的实际对象 1234567891011121314151617181920212223242526272829package jiankunking;/** * 实际对象 */public class RealSubject implements Subject &#123; /** * 你好 * * @param name * @return */ @Override public String SayHello(String name) &#123; return &quot;hello &quot; + name; &#125; /** * 再见 * * @return */ @Override public String SayGoodBye() &#123; return &quot; good bye &quot;; &#125;&#125; 3、调用处理器实现类（有木有感觉这里就是传说中的AOP啊） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package jiankunking;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;/** * 调用处理器实现类 * 每次生成动态代理类对象时都需要指定一个实现了该接口的调用处理器对象 */public class InvocationHandlerImpl implements InvocationHandler &#123; /** * 这个就是我们要代理的真实对象 */ private Object subject; /** * 构造方法，给我们要代理的真实对象赋初值 * * @param subject */ public InvocationHandlerImpl(Object subject) &#123; this.subject = subject; &#125; /** * 该方法负责集中处理动态代理类上的所有方法调用。 * 调用处理器根据这三个参数进行预处理或分派到委托类实例上反射执行 * * @param proxy 代理类实例 * @param method 被调用的方法对象 * @param args 调用参数 * @return * @throws Throwable */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //在代理真实对象前我们可以添加一些自己的操作 System.out.println(&quot;在调用之前，我要干点啥呢？&quot;); System.out.println(&quot;Method:&quot; + method); //当代理对象调用真实对象的方法时，其会自动的跳转到代理对象关联的handler对象的invoke方法来进行调用 Object returnValue = method.invoke(subject, args); //在代理真实对象后我们也可以添加一些自己的操作 System.out.println(&quot;在调用之后，我要干点啥呢？&quot;); return returnValue; &#125;&#125; 4、测试 12345678910111213141516171819202122232425262728293031323334package jiankunking;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Proxy;/** * 动态代理演示 */public class DynamicProxyDemonstration &#123; public static void main(String[] args) &#123; //代理的真实对象 Subject realSubject = new RealSubject(); /** * InvocationHandlerImpl 实现了 InvocationHandler 接口，并能实现方法调用从代理类到委托类的分派转发 * 其内部通常包含指向委托类实例的引用，用于真正执行分派转发过来的方法调用. * 即：要代理哪个真实对象，就将该对象传进去，最后是通过该真实对象来调用其方法 */ InvocationHandler handler = new InvocationHandlerImpl(realSubject); ClassLoader loader = handler.getClass().getClassLoader(); Class&lt;?&gt;[] interfaces = realSubject.getClass().getInterfaces(); /** * 该方法用于为指定类装载器、一组接口及调用处理器生成动态代理类实例 */ Subject subject = (Subject) Proxy.newProxyInstance(loader, interfaces, handler); System.out.println(&quot;动态代理对象的类型：&quot; + subject.getClass().getName()); String hello = subject.SayHello(&quot;jiankunking&quot;); System.out.println(hello);// String goodbye = subject.SayGoodBye();// System.out.println(goodbye); &#125;&#125; 5、输出结果如下： 四、动态代理怎么实现的？从使用代码中可以看出，关键点在： 1Subject subject = (Subject) Proxy.newProxyInstance(loader, interfaces, handler); 通过跟踪提示代码可以看出：当代理对象调用真实对象的方法时，其会自动的跳转到代理对象关联的handler对象的invoke方法来进行调用。 也就是说，当代码执行到：subject.SayHello(“jiankunking”)这句话时，会自动调用InvocationHandlerImpl的invoke方法。这是为啥呢？ 下面是代码跟分析的过程，不想看的朋友可以直接看结论 以下代码来自:JDK1.8.0_92 既然生成代理对象是用的Proxy类的静态方newProxyInstance，那么我们就去它的源码里看一下它到底都做了些什么？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596/** * Returns an instance of a proxy class for the specified interfaces * that dispatches method invocations to the specified invocation * handler. * * &lt;p&gt;&#123;@code Proxy.newProxyInstance&#125; throws * &#123;@code IllegalArgumentException&#125; for the same reasons that * &#123;@code Proxy.getProxyClass&#125; does. * * @param loader the class loader to define the proxy class * @param interfaces the list of interfaces for the proxy class * to implement * @param h the invocation handler to dispatch method invocations to * @return a proxy instance with the specified invocation handler of a * proxy class that is defined by the specified class loader * and that implements the specified interfaces * @throws IllegalArgumentException if any of the restrictions on the * parameters that may be passed to &#123;@code getProxyClass&#125; * are violated * @throws SecurityException if a security manager, &lt;em&gt;s&lt;/em&gt;, is present * and any of the following conditions is met: * &lt;ul&gt; * &lt;li&gt; the given &#123;@code loader&#125; is &#123;@code null&#125; and * the caller&apos;s class loader is not &#123;@code null&#125; and the * invocation of &#123;@link SecurityManager#checkPermission * s.checkPermission&#125; with * &#123;@code RuntimePermission(&quot;getClassLoader&quot;)&#125; permission * denies access;&lt;/li&gt; * &lt;li&gt; for each proxy interface, &#123;@code intf&#125;, * the caller&apos;s class loader is not the same as or an * ancestor of the class loader for &#123;@code intf&#125; and * invocation of &#123;@link SecurityManager#checkPackageAccess * s.checkPackageAccess()&#125; denies access to &#123;@code intf&#125;;&lt;/li&gt; * &lt;li&gt; any of the given proxy interfaces is non-public and the * caller class is not in the same &#123;@linkplain Package runtime package&#125; * as the non-public interface and the invocation of * &#123;@link SecurityManager#checkPermission s.checkPermission&#125; with * &#123;@code ReflectPermission(&quot;newProxyInPackage.&#123;package name&#125;&quot;)&#125; * permission denies access.&lt;/li&gt; * &lt;/ul&gt; * @throws NullPointerException if the &#123;@code interfaces&#125; array * argument or any of its elements are &#123;@code null&#125;, or * if the invocation handler, &#123;@code h&#125;, is * &#123;@code null&#125; */@CallerSensitive public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException &#123; //检查h 不为空，否则抛异常 Objects.requireNonNull(h); final Class&lt;?&gt;[] intfs = interfaces.clone(); final SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; checkProxyAccess(Reflection.getCallerClass(), loader, intfs); &#125; /* * 获得与指定类装载器和一组接口相关的代理类类型对象 */ Class&lt;?&gt; cl = getProxyClass0(loader, intfs); /* * 通过反射获取构造函数对象并生成代理类实例 */ try &#123; if (sm != null) &#123; checkNewProxyPermission(Reflection.getCallerClass(), cl); &#125; //获取代理对象的构造方法（也就是$Proxy0(InvocationHandler h)） final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams); final InvocationHandler ih = h; if (!Modifier.isPublic(cl.getModifiers())) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; cons.setAccessible(true); return null; &#125; &#125;); &#125; //生成代理类的实例并把InvocationHandlerImpl的实例传给它的构造方法 return cons.newInstance(new Object[]&#123;h&#125;); &#125; catch (IllegalAccessException|InstantiationException e) &#123; throw new InternalError(e.toString(), e); &#125; catch (InvocationTargetException e) &#123; Throwable t = e.getCause(); if (t instanceof RuntimeException) &#123; throw (RuntimeException) t; &#125; else &#123; throw new InternalError(t.toString(), t); &#125; &#125; catch (NoSuchMethodException e) &#123; throw new InternalError(e.toString(), e); &#125; &#125; 我们再进去getProxyClass0方法看一下： 123456789101112131415/** * Generate a proxy class. Must call the checkProxyAccess method * to perform permission checks before calling this. */private static Class&lt;?&gt; getProxyClass0(ClassLoader loader, Class&lt;?&gt;... interfaces) &#123; if (interfaces.length &gt; 65535) &#123; throw new IllegalArgumentException(&quot;interface limit exceeded&quot;); &#125; // If the proxy class defined by the given loader implementing // the given interfaces exists, this will simply return the cached copy; // otherwise, it will create the proxy class via the ProxyClassFactory return proxyClassCache.get(loader, interfaces);&#125; 真相还是没有来到，继续，看一下 proxyClassCache 1234 /** * a cache of proxy classes */private static final WeakCache&lt;ClassLoader, Class&lt;?&gt;[], Class&lt;?&gt;&gt; proxyClassCache = new WeakCache&lt;&gt;(new KeyFactory(), new ProxyClassFactory()); 奥，原来用了一下缓存啊 那么它对应的get方法啥样呢？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/** * Look-up the value through the cache. This always evaluates the * &#123;@code subKeyFactory&#125; function and optionally evaluates * &#123;@code valueFactory&#125; function if there is no entry in the cache for given * pair of (key, subKey) or the entry has already been cleared. * * @param key possibly null key * @param parameter parameter used together with key to create sub-key and * value (should not be null) * @return the cached value (never null) * @throws NullPointerException if &#123;@code parameter&#125; passed in or * &#123;@code sub-key&#125; calculated by * &#123;@code subKeyFactory&#125; or &#123;@code value&#125; * calculated by &#123;@code valueFactory&#125; is null. */public V get(K key, P parameter) &#123; Objects.requireNonNull(parameter); expungeStaleEntries(); Object cacheKey = CacheKey.valueOf(key, refQueue); // lazily install the 2nd level valuesMap for the particular cacheKey ConcurrentMap&lt;Object, Supplier&lt;V&gt;&gt; valuesMap = map.get(cacheKey); if (valuesMap == null) &#123; //putIfAbsent这个方法在key不存在的时候加入一个值,如果key存在就不放入 ConcurrentMap&lt;Object, Supplier&lt;V&gt;&gt; oldValuesMap = map.putIfAbsent(cacheKey,valuesMap = new ConcurrentHashMap&lt;&gt;()); if (oldValuesMap != null) &#123; valuesMap = oldValuesMap; &#125; &#125; // create subKey and retrieve the possible Supplier&lt;V&gt; stored by that // subKey from valuesMap Object subKey = Objects.requireNonNull(subKeyFactory.apply(key, parameter)); Supplier&lt;V&gt; supplier = valuesMap.get(subKey); Factory factory = null; while (true) &#123; if (supplier != null) &#123; // supplier might be a Factory or a CacheValue&lt;V&gt; instance V value = supplier.get(); if (value != null) &#123; return value; &#125; &#125; // else no supplier in cache // or a supplier that returned null (could be a cleared CacheValue // or a Factory that wasn&apos;t successful in installing the CacheValue) // lazily construct a Factory if (factory == null) &#123; factory = new Factory(key, parameter, subKey, valuesMap); &#125; if (supplier == null) &#123; supplier = valuesMap.putIfAbsent(subKey, factory); if (supplier == null) &#123; // successfully installed Factory supplier = factory; &#125; // else retry with winning supplier &#125; else &#123; if (valuesMap.replace(subKey, supplier, factory)) &#123; // successfully replaced // cleared CacheEntry / unsuccessful Factory // with our Factory supplier = factory; &#125; else &#123; // retry with current supplier supplier = valuesMap.get(subKey); &#125; &#125; &#125;&#125; 我们可以看到它调用了 supplier.get(); 获取动态代理类，其中supplier是Factory,这个类定义在WeakCach的内部。 来瞅瞅，get里面又做了什么？ 1234567891011121314151617181920212223242526272829303132333435363738394041public synchronized V get() &#123; // serialize access // re-check Supplier&lt;V&gt; supplier = valuesMap.get(subKey); if (supplier != this) &#123; // something changed while we were waiting: // might be that we were replaced by a CacheValue // or were removed because of failure -&gt; // return null to signal WeakCache.get() to retry // the loop return null; &#125; // else still us (supplier == this) // create new value V value = null; try &#123; value = Objects.requireNonNull(valueFactory.apply(key, parameter)); &#125; finally &#123; if (value == null) &#123; // remove us on failure valuesMap.remove(subKey, this); &#125; &#125; // the only path to reach here is with non-null value assert value != null; // wrap value with CacheValue (WeakReference) CacheValue&lt;V&gt; cacheValue = new CacheValue&lt;&gt;(value); // try replacing us with CacheValue (this should always succeed) if (valuesMap.replace(subKey, this, cacheValue)) &#123; // put also in reverseMap reverseMap.put(cacheValue, Boolean.TRUE); &#125; else &#123; throw new AssertionError(&quot;Should not reach here&quot;); &#125; // successfully replaced us with new CacheValue -&gt; return the value // wrapped by it return value; &#125; &#125; 发现重点还是木有出现，但我们可以看到它调用了valueFactory.apply(key, parameter)方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899 /** * A factory function that generates, defines and returns the proxy class given * the ClassLoader and array of interfaces. */private static final class ProxyClassFactory implements BiFunction&lt;ClassLoader, Class&lt;?&gt;[], Class&lt;?&gt;&gt; &#123; // prefix for all proxy class names private static final String proxyClassNamePrefix = &quot;$Proxy&quot;; // next number to use for generation of unique proxy class names private static final AtomicLong nextUniqueNumber = new AtomicLong(); @Override public Class&lt;?&gt; apply(ClassLoader loader, Class&lt;?&gt;[] interfaces) &#123; Map&lt;Class&lt;?&gt;, Boolean&gt; interfaceSet = new IdentityHashMap&lt;&gt;(interfaces.length); for (Class&lt;?&gt; intf : interfaces) &#123; /* * Verify that the class loader resolves the name of this * interface to the same Class object. */ Class&lt;?&gt; interfaceClass = null; try &#123; interfaceClass = Class.forName(intf.getName(), false, loader); &#125; catch (ClassNotFoundException e) &#123; &#125; if (interfaceClass != intf) &#123; throw new IllegalArgumentException( intf + &quot; is not visible from class loader&quot;); &#125; /* * Verify that the Class object actually represents an * interface. */ if (!interfaceClass.isInterface()) &#123; throw new IllegalArgumentException( interfaceClass.getName() + &quot; is not an interface&quot;); &#125; /* * Verify that this interface is not a duplicate. */ if (interfaceSet.put(interfaceClass, Boolean.TRUE) != null) &#123; throw new IllegalArgumentException( &quot;repeated interface: &quot; + interfaceClass.getName()); &#125; &#125; String proxyPkg = null; // package to define proxy class in int accessFlags = Modifier.PUBLIC | Modifier.FINAL; /* * Record the package of a non-public proxy interface so that the * proxy class will be defined in the same package. Verify that * all non-public proxy interfaces are in the same package. */ for (Class&lt;?&gt; intf : interfaces) &#123; int flags = intf.getModifiers(); if (!Modifier.isPublic(flags)) &#123; accessFlags = Modifier.FINAL; String name = intf.getName(); int n = name.lastIndexOf(&apos;.&apos;); String pkg = ((n == -1) ? &quot;&quot; : name.substring(0, n + 1)); if (proxyPkg == null) &#123; proxyPkg = pkg; &#125; else if (!pkg.equals(proxyPkg)) &#123; throw new IllegalArgumentException( &quot;non-public interfaces from different packages&quot;); &#125; &#125; &#125; if (proxyPkg == null) &#123; // if no non-public proxy interfaces, use com.sun.proxy package proxyPkg = ReflectUtil.PROXY_PACKAGE + &quot;.&quot;; &#125; /* * Choose a name for the proxy class to generate. */ long num = nextUniqueNumber.getAndIncrement(); String proxyName = proxyPkg + proxyClassNamePrefix + num; /* * Generate the specified proxy class. */ byte[] proxyClassFile = ProxyGenerator.generateProxyClass( proxyName, interfaces, accessFlags); try &#123; return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length); &#125; catch (ClassFormatError e) &#123; /* * A ClassFormatError here means that (barring bugs in the * proxy class generation code) there was some other * invalid aspect of the arguments supplied to the proxy * class creation (such as virtual machine limitations * exceeded). */ throw new IllegalArgumentException(e.toString()); &#125; &#125;&#125; 通过看代码终于找到了重点： 12//生成字节码byte[] proxyClassFile = ProxyGenerator.generateProxyClass(proxyName, interfaces, accessFlags); 那么接下来我们也使用测试一下，使用这个方法生成的字节码是个什么样子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package jiankunking; import sun.misc.ProxyGenerator; import java.io.File;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Proxy; /** * 动态代理演示 */public class DynamicProxyDemonstration &#123; public static void main(String[] args) &#123; //代理的真实对象 Subject realSubject = new RealSubject(); /** * InvocationHandlerImpl 实现了 InvocationHandler 接口，并能实现方法调用从代理类到委托类的分派转发 * 其内部通常包含指向委托类实例的引用，用于真正执行分派转发过来的方法调用. * 即：要代理哪个真实对象，就将该对象传进去，最后是通过该真实对象来调用其方法 */ InvocationHandler handler = new InvocationHandlerImpl(realSubject); ClassLoader loader = handler.getClass().getClassLoader(); Class[] interfaces = realSubject.getClass().getInterfaces(); /** * 该方法用于为指定类装载器、一组接口及调用处理器生成动态代理类实例 */ Subject subject = (Subject) Proxy.newProxyInstance(loader, interfaces, handler); System.out.println(&quot;动态代理对象的类型：&quot;+subject.getClass().getName()); String hello = subject.SayHello(&quot;jiankunking&quot;); System.out.println(hello); // 将生成的字节码保存到本地， createProxyClassFile(); &#125; private static void createProxyClassFile()&#123; String name = &quot;ProxySubject&quot;; byte[] data = ProxyGenerator.generateProxyClass(name,new Class[]&#123;Subject.class&#125;); FileOutputStream out =null; try &#123; out = new FileOutputStream(name+&quot;.class&quot;); System.out.println((new File(&quot;hello&quot;)).getAbsolutePath()); out.write(data); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if(null!=out) try &#123; out.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 可以看一下这里代理对象的类型： 我们用jd-jui 工具将生成的字节码反编译： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.lang.reflect.UndeclaredThrowableException;import jiankunking.Subject; public final class ProxySubject extends Proxy implements Subject &#123; private static Method m1; private static Method m3; private static Method m4; private static Method m2; private static Method m0; public ProxySubject(InvocationHandler paramInvocationHandler) &#123; super(paramInvocationHandler); &#125; public final boolean equals(Object paramObject)&#123; try &#123; return ((Boolean)this.h.invoke(this, m1, new Object[] &#123; paramObject &#125;)).booleanValue(); &#125; catch (Error|RuntimeException localError)&#123; throw localError; &#125; catch (Throwable localThrowable) &#123; throw new UndeclaredThrowableException(localThrowable); &#125; &#125; public final String SayGoodBye() &#123; try&#123; return (String)this.h.invoke(this, m3, null); &#125; catch (Error|RuntimeException localError) &#123; throw localError; &#125; catch (Throwable localThrowable) &#123; throw new UndeclaredThrowableException(localThrowable); &#125; &#125; public final String SayHello(String paramString) &#123; try &#123; return (String)this.h.invoke(this, m4, new Object[] &#123; paramString &#125;); &#125; catch (Error|RuntimeException localError)&#123; throw localError; &#125; catch (Throwable localThrowable)&#123; throw new UndeclaredThrowableException(localThrowable); &#125; &#125; public final String toString() &#123; try &#123; return (String)this.h.invoke(this, m2, null); &#125; catch (Error|RuntimeException localError) &#123; throw localError; &#125; catch (Throwable localThrowable) &#123; throw new UndeclaredThrowableException(localThrowable); &#125; &#125; public final int hashCode() &#123; try &#123; return ((Integer)this.h.invoke(this, m0, null)).intValue(); &#125; catch (Error|RuntimeException localError) &#123; throw localError; &#125; catch (Throwable localThrowable) &#123; throw new UndeclaredThrowableException(localThrowable); &#125; &#125; static &#123; try &#123; m1 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;equals&quot;, new Class[] &#123; Class.forName(&quot;java.lang.Object&quot;) &#125;); m3 = Class.forName(&quot;jiankunking.Subject&quot;).getMethod(&quot;SayGoodBye&quot;, new Class[0]); m4 = Class.forName(&quot;jiankunking.Subject&quot;).getMethod(&quot;SayHello&quot;, new Class[] &#123; Class.forName(&quot;java.lang.String&quot;) &#125;); m2 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;toString&quot;, new Class[0]); m0 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;hashCode&quot;, new Class[0]); return; &#125; catch (NoSuchMethodException localNoSuchMethodException) &#123; throw new NoSuchMethodError(localNoSuchMethodException.getMessage()); &#125; catch (ClassNotFoundException localClassNotFoundException) &#123; throw new NoClassDefFoundError(localClassNotFoundException.getMessage()); &#125; &#125;&#125; 这就是最终真正的代理类，它继承自Proxy并实现了我们定义的Subject接口，也就是说： 1Subject subject = (Subject) Proxy.newProxyInstance(loader, interfaces, handler); 这里的subject实际是这个类的一个实例，那么我们调用它的： 1public final String SayHello(String paramString) 就是调用我们定义的InvocationHandlerImpl的 invoke方法： 上面是代码跟分析的过程，不想看的朋友可以直接看结论 五、结论到了这里，终于解答了： subject.SayHello(“jiankunking”)这句话时，为什么会自动调用InvocationHandlerImpl的invoke方法？ 因为JDK生成的最终真正的代理类，它继承自Proxy并实现了我们定义的Subject接口，在实现Subject接口方法的内部，通过反射调用了InvocationHandlerImpl的invoke方法。 通过分析代码可以看出Java 动态代理，具体有如下四步骤： 通过实现 InvocationHandler 接口创建自己的调用处理器； 通过为 Proxy 类指定 ClassLoader 对象和一组 interface 来创建动态代理类； 通过反射机制获得动态代理类的构造函数，其唯一参数类型是调用处理器接口类型； 通过构造函数创建动态代理类实例，构造时调用处理器对象作为参数被传入。 演示代码下载：https://github.com/jiankunking/DynamicProxyDemo]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>JDK</tag>
        <tag>Spring</tag>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过IL分析C#中的委托、事件、Func、Action、Predicate之间的区别与联系]]></title>
    <url>%2Fcsharp-il-delegate-event-func-action-predicate.html</url>
    <content type="text"><![CDATA[一直以来都是对于事件与委托比较混淆，而且不太会用。找了个时间，总结了一下，感觉清晰了很多。 先说一下个人理解的结论吧： delegate是C#中的一种类型，它实际上是一个能够持有对某个方法的引用的类。 delegate声明的变量与delegate声明的事件，并没有本质的区别，事件是在delegate声明变量的基础上包装而成的，类似于变量与属性的关系（在IL代码中可以看到每一个delegate声明的事件都对应是私有的delegate声明的变量），提升了安全性。 Action 与Func：这两个其实说白了就是系统定义好的Delegate，他有很多重载的方法，便于各种应用情况下的调用。他在系统的System命名空间下，因此全局可见。 首先了解一下， ILDasm中图标含义：该图来自：http://www.cnblogs.com/zery/p/3366175.html 委托创建步骤： 用delegate关键字创建一个委托，包括声明返回值和参数类型。 使用的地方接收这个委托。 创建这个委托的实例并指定一个返回值和参数类型匹配的方法传递过去。 一、事件与委托新建一个事件委托测试项目：EventDelegateTest。 具体代码如下： 123456789namespace EventDelegateTest&#123; public class TestClass &#123; public delegate int delegateAction(); public event delegateAction OnActionEvent; public delegateAction daNew; &#125;&#125; 编译代码后，使用 Visual Studio 2010自带的ILDASM.EXE： 打开该dll，可以看到如下信息： 从上图可以看出如下几点信息： 1、delegate委托 public delegate int delegateAction();在IL中是以类（delegateAction）的形式存在的.NET将委托定义为一个密封类，派生自基类System.MulticastDelegate，并继承了基类的三个方法: 2、eventpublic event delegateAction OnActionEvent;在IL中不仅仅对应event OnActionEvent而且还对应一个field OnActionEvent;而field OnActionEvent与 public delegateAction daNew生成的field daNew是一样的.都是以字段（field ）的形式存在的。双击event OnActionEvent可以看到如下信息： 在IL中事件被封装成了包含一个add_前缀和一个remove_前缀的的代码段。其中，add_前缀的方法其实是通过调用Delegate.Combine()方法来实现的，组成了一个多播委托；remove_就是调用Delegate.Remove()方法，用于移除多播委托中的某个委托。 也就是说：事件其实就是一个特殊的多播委托。 那么对于事件进行这一次封装有什么好处呢？1、因为delegate可以支持的操作非常多，比如我们可以写onXXXChanged += aaaFunc，把某个函数指针挂载到这个委托上面，但是我们也可以简单粗暴地直接写onXXXChanged = aaaFunc，让这个委托只包含这一个函数指针。不过这样一来会产生一个安全问题：如果我们用onXXXChanged = aaaFunc这样的写法，那么会把这个委托已拥有的其他函数指针给覆盖掉，这大概不是定义onXXXChanged的程序员想要看到的结果。 小注：虽然事件不能直接=某个函数，也不可以直接=null 2、还有一个问题就是onXXXChanged这个委托应该什么时候触发（即调用它所包含的函数指针）。从面向对象的角度来说，XXX改变了这个事实（即onXXXChaned的字面含义）应该由包含它的那个对象来决定。但实际上我们可以从这个对象的外部环境调用onXXXChanged，这既产生了安全问题也不符合面向对象的初衷。说到这里对于事件与委托的管理算是说明白了，那么平时常用的Action与Func，与委托又有什么关系呢？ 二、Action 与FuncAction 委托：封装一个方法，该方法具有参数（0到16个参数）并且不返回值。具体形式如下：https://msdn.microsoft.com/zh-cn/library/system.action(v=vs.110).aspx Func&lt;T, TResult&gt; 委托：封装一个具有参数（0到16个参数）并返回 TResult 参数指定的类型值的方法。具体形式如下：https://msdn.microsoft.com/zh-cn/library/bb534960(v=vs.110).aspx 那么这Action与Func是怎么实现的呢？1、Action（以Action&lt;T1, T2&gt; 委托：封装一个方法，该方法具有两个参数并且不返回值为例）从微软公布的源码中，可以看到，如下实现： 1public Action&lt;bool,bool&gt; ac; 上面这个声明就是：该方法具有两个参数并且不返回值的委托。其余使用方式与委托变量一样。2、Func（以Func&lt;T1, T2, TResult&gt; 委托：封装一个具有两个参数并返回 TResult 参数指定的类型值的方法为例）从微软公布的源码中，可以看到，如下实现： 此处，可以看出Func与Action是类似的，唯一的区别就是，Func必须指定返回值的类型，使用方式与委托咱们自己使用委托变量是一样的，直接使用相应参数的Func或者Action声明变量，=或者+=挂载函数（方法即可）这两个其实说白了就是系统定义好的Delegate，他有很多重载的方法，便于各种应用情况下的调用。他在系统的System命名空间下，因此全局可见。 三、Predicate是返回bool型的泛型委托，Predicate有且只有一个参数，返回值固定为bool。表示定义一组条件并确定指定对象是否符合这些条件的方法。 此方法常在集合（Array 和 List）的查找中被用到，如：数组，正则拼配的结果集中被用到。 官方文档：https://docs.microsoft.com/zh-cn/dotnet/api/system.predicate-1?redirectedfrom=MSDN&amp;view=netframework-4.8 具体用法demo如下： 12345678910111213141516171819202122232425262728293031323334353637using System;using System.Collections.Generic;using System.ComponentModel;using System.Data;using System.Drawing;using System.Linq;using System.Text;using System.Windows.Forms; namespace IconTest&#123; public partial class Form2 : Form &#123; Predicate&lt;int&gt; myPredicate; int[] myNum = new int[8] &#123; 12, 33, 89, 21, 15, 29, 40, 52 &#125;; public int[] myResult; public Form2() &#123; InitializeComponent(); myPredicate = delegate(int curNum) &#123; if (curNum % 2 == 0) &#123; return true; &#125; else &#123; return false; &#125; &#125;; &#125; private void Form2_Load(object sender, EventArgs e) &#123; myResult = Array.FindAll(myNum, myPredicate); &#125; &#125;&#125; 上例中说明了Predicate的使用，FindAll方法中，参数2即是一个Predicate，在具体的执行中，每一个数组的元素都会执行指定的方法，如果满足要求返回true，并会被存放在结果集中，不符合的则被剔除，最终返回的集合，即是结果判断后想要的集合。Array.FindAll 泛型方法：https://docs.microsoft.com/zh-cn/dotnet/api/system.array.findall?redirectedfrom=MSDN&amp;view=netframework-4.8#System_Array_FindAll__1___0___System_Predicate___0__以上代码执行结果为：那么Predicate与委托又有什么关系呢？ 从微软源码中可以看出Predicate是返回bool型的泛型委托，从本质上来说与Func、Action、事件、委托变量并无本质区别。 四、资料参考文章：http://www.zhihu.com/question/28932542 关于事件部分应用注意可以参考：http://www.cnblogs.com/buptzym/archive/2013/03/15/2962300.html .NET Framework 源码：https://referencesource.microsoft.comDelegate 类:https://docs.microsoft.com/zh-cn/dotnet/api/system.delegate?redirectedfrom=MSDN&amp;view=netframework-4.8]]></content>
      <categories>
        <category>C#</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>C#</tag>
        <tag>Delegate</tag>
        <tag>Action</tag>
        <tag>Predicate</tag>
      </tags>
  </entry>
</search>
