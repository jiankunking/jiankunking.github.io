<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>衣舞晨风</title>
  
  
  <link href="https://jiankunking.com/atom.xml" rel="self"/>
  
  <link href="https://jiankunking.com/"/>
  <updated>2025-11-08T22:51:10.205Z</updated>
  <id>https://jiankunking.com/</id>
  
  <author>
    <name>jiankunking</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>多云高可用场景下云原生网关架构设计</title>
    <link href="https://jiankunking.com/cloud-native-gateway-selection-for-multi-cloud-high-availability.html"/>
    <id>https://jiankunking.com/cloud-native-gateway-selection-for-multi-cloud-high-availability.html</id>
    <published>2025-11-01T08:51:43.000Z</published>
    <updated>2025-11-08T22:51:10.205Z</updated>
    
    <content type="html"><![CDATA[<p>如何借助云的能力，实现一个跨云高可用的网关？</p><a id="more"></a><h1>背景分析</h1><p>数字化转型下，企业上云已成必然。但业务规模扩大后，单一云厂商的风险逐渐暴露，因此我们设计云原生网关架构，始终以多云高可用为核心。</p><p>网关作为微服务的流量核心入口，还集成了流量管理、安全防护与全链路可观测能力。那该如何设计这一网关，才能充分发挥这些能力，真正实现多云高可用呢？</p><p>基于这一核心定位，我们可从集群适配、流量隔离等多个关键维度设计网关，以此充分激活其各项能力并筑牢多云场景下的高可用底座。</p><ul><li>集群适配：随云环境精准选择网关类型<ul><li>依托多云技术布局，网关采用 “集群类型对应” 的选型逻辑，确保网关与应用服务底层集群基础设施无缝衔接，避免跨云。</li></ul></li><li>实例专属绑定：集群级隔离，精细化流量治理<ul><li>每个集群独立部署专属网关实例，实现资源与流量严格隔离。</li></ul></li><li>统一界面：屏蔽底层细节降低使用门槛<ul><li>网关提供一体化可视化操作平台，将底层复杂技术完全封装，贴合 APP 快速迭代的业务需求。</li></ul></li><li>云原生高可用：99.95% SLA兜底，支撑千万级访问<ul><li>高可用性全权由云厂商保障，无需额外架构投入。</li></ul></li><li>插件化安全：简化认证降低业务复杂度<ul><li>登录、权限校验等通用安全逻辑以插件化形式预置于网关，完全剥离于 APP 业务代码，大幅降低开发成本</li></ul></li></ul><h2 id="为什么一个K8S集群要部署一个网关？">为什么一个K8S集群要部署一个网关？</h2><ul><li>服务隔离与安全边界<ul><li>不同集群承载不同安全等级的业务，通过独立网关可实现网络层面的服务隔离，降低横向移动风险。</li></ul></li><li>流量就近处理<ul><li>在多云环境中，集群往往分布在不同地域或不同云厂商。为每个集群部署独立网关，可以实现流量的就近处理，减少跨地域、跨云厂商的网络延迟。</li></ul></li><li>故障隔离<ul><li>独立网关能有效隔离故障影响范围。如果一个集群的网关出现问题，不会影响其他集群的正常运行。</li></ul></li><li>部署灵活性<ul><li>为每个集群部署独立网关，可以根据集群的实际需求和业务特点，灵活配置网关参数和功能。</li></ul></li></ul><h2 id="为什么选择直接对接云网关？">为什么选择直接对接云网关？</h2><p>在确定了&quot;每集群一网关&quot;的架构模式后，我们又面临一个选择：是使用开源网关（如Kong、APISIX、Higress）自己部署，还是直接对接云厂商提供的网关服务？</p><p>经过对比分析，我们最终选择了直接对接云网关服务。而这一决策的核心依据，正是云网关服务自身具备的一系列契合我们需求的核心特性，具体体现在以下几个方面。</p><ul><li>高可用性保障<ul><li>云厂商的网关服务通常都经过了大规模的生产验证，可用性承诺一般在99.95%以上，还有专业的运维团队支持。</li></ul></li><li>运维成本降低</li><li>功能丰富</li><li>弹性扩展能力强</li><li>与云原生生态集成<ul><li>云厂商的网关服务与其他云服务深度集成，可以更好地发挥云原生的优势，提高开发效率和系统弹性。</li></ul></li></ul><h1>架构分层</h1><p><img data-src="/images/cloud-native-gateway-selection-for-multi-cloud-high-availability/%E4%BA%91%E5%8E%9F%E7%94%9F%E7%BD%91%E5%85%B3%E4%BB%A3%E7%A0%81%E5%88%86%E5%B1%82%E8%AE%BE%E8%AE%A1.jpg" alt="云原生网关代码分层设计"></p><p>这是一套<strong>多云网关统一管理架构</strong>，旨在实现跨云服务商的网关资源集中编排与适配，解决多云环境下网关管理分散、运维复杂度高的问题。以下从架构分层、组件职责和价值三个维度展开介绍：</p><h2 id="架构分层与组件解析">架构分层与组件解析</h2><p>该架构分为<strong>业务编排层、网关聚合层、适配端层</strong>三层，各层职责明确且解耦：</p><h3 id="1-业务编排层：业务逻辑的“指挥中心”">1. 业务编排层：业务逻辑的“指挥中心”</h3><ul><li><strong>用户角色</strong>：<code>网关用户</code>是操作主体，负责对<strong>逻辑网关</strong>和<strong>路由</strong>执行“增删改查”操作。</li><li><strong>核心组件</strong>：<ul><li><code>逻辑网关</code>：抽象网关的逻辑定义，依赖<code>SSL证书</code>实现安全通信，是业务侧对域名的逻辑抽象。</li><li><code>路由</code>：与逻辑网关关联，依赖<code>插件</code>和<code>服务</code>实现流量转发规则；路由的审批流程通过<code>MQ（消息队列）</code>异步处理，保障流程可靠性。</li></ul></li></ul><h3 id="2-网关聚合层：多云指令的“转换器”">2. 网关聚合层：多云指令的“转换器”</h3><ul><li>承接业务编排层的操作指令，对<code>域名</code>执行“创建/删除”，对<code>路由组</code>执行“发布/下线（Publish/Unpublish）”。</li><li>核心是<code>CloudGatewayProvider（Adapter）</code>，作为“中间桥梁”，将业务层的统一指令转换为适配多云环境的操作格式，实现<strong>多云指令的标准化</strong>。</li></ul><h3 id="3-适配端层：多云资源的“执行器”">3. 适配端层：多云资源的“执行器”</h3><ul><li>对接不同云服务商的网关服务，如<code>阿里云-MSE</code>、<code>华为云-APIG</code>、<code>腾讯云-云原生网关</code>等，实现多云网关的“一管多”。<ul><li><code>阿里云-MSE</code>：管理域名、服务、路由、策略、插件等资源，是阿里云生态下的微服务网关载体。</li><li><code>华为云-APIG</code>：通过<code>API组</code>管理域名、响应等，是华为云的API网关服务入口。</li><li><code>腾讯云-云原生网关</code>：适配腾讯云的网关产品，支持云原生场景下的流量管理。</li></ul></li></ul><h2 id="架构价值">架构价值</h2><ul><li><strong>统一编排</strong>：业务侧通过“逻辑网关+路由”的模式，无需关注底层云差异，实现网关资源的集中化、标准化编排。</li><li><strong>多云适配</strong>：通过聚合层和适配层的解耦设计，轻松对接阿里云、华为云、腾讯云等多厂商，避免“一云一套管理逻辑”的重复建设。</li><li><strong>灵活扩展</strong>：插件、策略等组件支持自定义扩展，可根据业务需求灵活增强网关能力；适配端层的“…”节点也支持快速接入新的云服务商。</li></ul><h1>核心接口设计</h1><blockquote><p>抽象比具体更重要</p></blockquote><p>在多云环境下，关键是要把不同云厂商API的差异藏起来，给业务层提供统一的网关管理能力。</p><h2 id="核心接口">核心接口</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">// CloudGateway 核心网关接口定义 - 包含域名、服务、路由的CRUD操作</span><br><span class="line">// 产品的一步操作 对应云的多步操作</span><br><span class="line">// 具体需要实现哪些接口,按需实现即可</span><br><span class="line">type CloudGateway interface &#123;</span><br><span class="line">        // ----------------- 域名管理 -----------------</span><br><span class="line"></span><br><span class="line">        // CreateDomain 域名管理--可重入</span><br><span class="line">        CreateDomain(ctx context.Context, request *model.CreateDomainRequest) (*model.CreateDomainResponse, error)</span><br><span class="line"></span><br><span class="line">        // GetDomain 查询域名</span><br><span class="line">        GetDomain(ctx context.Context, request *model.GetDomainRequest) (*model.GetDomainResponse, error)</span><br><span class="line"></span><br><span class="line">        // UpdateDomain 更新域名</span><br><span class="line">        UpdateDomain(ctx context.Context, request *model.UpdateDomainRequest) (*model.UpdateDomainResponse, error)</span><br><span class="line"></span><br><span class="line">        // DeleteDomain 删除域名</span><br><span class="line">        DeleteDomain(ctx context.Context, request *model.DeleteDomainRequest) (*model.DeleteDomainResponse, error)</span><br><span class="line"></span><br><span class="line">        // ListDomains 查询域名列表</span><br><span class="line">        ListDomains(ctx context.Context, request *model.ListDomainRequest) (*model.ListDomainResponse, error)</span><br><span class="line"></span><br><span class="line">        // ConflictCheckDomain 冲突检查域名</span><br><span class="line">        ConflictCheckDomain(ctx context.Context, request *model.ConflictCheckDomainRequest) (*model.ConflictCheckDomainResponse, error)</span><br><span class="line"></span><br><span class="line">        // ----------------- 服务管理 -----------------</span><br><span class="line"></span><br><span class="line">        // CreateService 服务管理 --可重入</span><br><span class="line">        CreateService(ctx context.Context, request *model.CreateServiceRequest) (*model.CreateServiceResponse, error)</span><br><span class="line"></span><br><span class="line">        // GetService 查询服务</span><br><span class="line">        GetService(ctx context.Context, request *model.GetServiceRequest) (*model.GetServiceResponse, error)</span><br><span class="line"></span><br><span class="line">        // UpdateService 更新服务</span><br><span class="line">        UpdateService(ctx context.Context, request *model.UpdateServiceRequest) (*model.UpdateServiceResponse, error)</span><br><span class="line"></span><br><span class="line">        // DeleteService 删除服务</span><br><span class="line">        DeleteService(ctx context.Context, request *model.DeleteServiceRequest) (*model.DeleteServiceResponse, error)</span><br><span class="line"></span><br><span class="line">        // ListServices 查询服务列表</span><br><span class="line">        ListServices(ctx context.Context, request *model.ListServiceRequest) (*model.ListServiceResponse, error)</span><br><span class="line"></span><br><span class="line">        // ServiceConflictCheck 服务冲突检查</span><br><span class="line">        ServiceConflictCheck(ctx context.Context, request *model.ConflictCheckServiceRequest) (*model.ConflictCheckServiceResponse, error)</span><br><span class="line"></span><br><span class="line">        // CreateRoute ----------------- 路由管理 -----------------</span><br><span class="line"></span><br><span class="line">        // CreateRoute 创建路由--可重入</span><br><span class="line">        CreateRoute(ctx context.Context, request *model.CreateRouteRequest) (*model.CreateRouteResponse, error)</span><br><span class="line"></span><br><span class="line">        // GetRoute 查询路由</span><br><span class="line">        GetRoute(ctx context.Context, request *model.GetRouteRequest) (*model.GetRouteResponse, error)</span><br><span class="line"></span><br><span class="line">        // UpdateRoute 更新路由</span><br><span class="line">        UpdateRoute(ctx context.Context, request *model.UpdateRouteRequest) (*model.UpdateRouteResponse, error)</span><br><span class="line"></span><br><span class="line">        // DeleteRoute 删除路由--可重入</span><br><span class="line">        DeleteRoute(ctx context.Context, request *model.DeleteRouteRequest) (*model.DeleteRouteResponse, error)</span><br><span class="line"></span><br><span class="line">        // ListRoutes 查询路由列表</span><br><span class="line">        ListRoutes(ctx context.Context, request *model.ListRouteRequest) (*model.ListRouteResponse, error)</span><br><span class="line"></span><br><span class="line">        // PatchRoute 部分更新路由</span><br><span class="line">        PatchRoute(ctx context.Context, request *model.PatchRouteRequest) (*model.PatchRouteResponse, error)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// CloudClientManager 客户端管理接口 - 用于初始化和获取云SDK客户端</span><br><span class="line">type CloudClientManager[T any] interface &#123;</span><br><span class="line">        // InitClient 初始化云SDK客户端</span><br><span class="line">        InitClient(ctx context.Context, config *model.CloudClientInitConfig) error</span><br><span class="line"></span><br><span class="line">        // GetClient 获取初始化好的客户端</span><br><span class="line">        GetClient(ctx context.Context, config model.CloudClientGetConfig) (T, error)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// CloudGatewayProvider 综合接口，组合核心功能和客户端管理</span><br><span class="line">type CloudGatewayProvider interface &#123;</span><br><span class="line">        CloudGateway</span><br><span class="line">        GetCloudGatewayProviderType() model.CloudGatewayProvider</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// NewGatewayProvider 工厂函数，用于创建特定云厂商的网关实例</span><br><span class="line">func NewGatewayProvider(provider model.CloudProvider) (CloudGatewayProvider, error) &#123;</span><br><span class="line">        switch provider &#123;</span><br><span class="line">        case model.HuaweiCloud:</span><br><span class="line">                return huawei_apig.NewHuaweiAPIGProvider(), nil</span><br><span class="line">        case model.AliCloud:</span><br><span class="line">                return ali_mse.NewAliMSEProvider(), nil</span><br><span class="line">        default:</span><br><span class="line">                return nil, &amp;bizerrors.GatewayError&#123;</span><br><span class="line">                        Code:    &quot;unsupported_provider&quot;,</span><br><span class="line">                        Message: &quot;不支持的云厂商&quot;,</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="设计思路">设计思路</h2><ul><li>统一抽象：不同云厂商的网关API差异很大，有的关注&quot;域名&quot;，有的主打&quot;服务&quot;。通过CloudGateway接口，我们把这些差异都藏在适配层下面，让业务代码不用关心具体是哪家云厂商。</li><li>工厂模式：用工厂模式创建不同云厂商的网关实例，后面要加新的云厂商时，直接加个case分支就行，不用改现有代码。实际落地时，这个设计帮我们省了不少对接新云平台的时间。</li><li>客户端管理封装：每个云厂商的SDK初始化方式都不一样，通过CloudClientManager接口，我们把这些复杂逻辑包起来，让代码更简洁，维护也更容易。</li><li>幂等性：我在接口中标注了&quot;可重入&quot;，这是从运维经验里总结的。分布式环境下网络不稳定，请求很容易重试。确保核心操作幂等，就能避免重复创建资源或状态混乱的问题。</li><li>按需实现：接口虽然覆盖了完整生命周期，但我们允许具体实现&quot;按需实现&quot;。实际落地时，确实有些云厂商的某些功能不完善，这种设计让我们能灵活应对。</li><li>错误处理要精细：专门定义了GatewayError类型，提供详细的错误码和信息。排查问题时，这个设计帮我们快速定位是哪个云厂商的哪个功能出了问题。</li></ul><h1>部署架构及高可用</h1><p><img data-src="/images/cloud-native-gateway-selection-for-multi-cloud-high-availability/%E4%BA%91%E5%8E%9F%E7%94%9F%E7%BD%91%E5%85%B3%E6%9E%B6%E6%9E%84-%E5%BA%94%E7%94%A8%E5%B1%82%E9%AB%98%E5%8F%AF%E7%94%A8.jpg" alt="云原生网关架构-应用层高可用"></p><h1>经验总结</h1><ul><li>什么是架构？<ul><li>架构的基本需求主要是在满足功能属性的前提下，关注软件质量属性，架构设计则是为满足架构需求（质量属性）寻找适当的“战术”（即架构策略）。</li><li>软件架构（及软件架构设计师）重点关注的是质量属性。因为在大量的可能结构中，可以使用不同的结构来实现同样的功能性，即功能性在很大程度上是独立于结构的，架构设计师面临着决策（对结构的选择），而功能性所关心的是它如何与其他质量属性进行交互，以及它如何限制其他质量属性。</li></ul></li><li>多为运维考虑<ul><li>设计架构不能只看开发方不方便，还要想长期怎么维护。我们选的方案让运维团队从底层维护中解放出来，能更专注于业务支撑。</li></ul></li><li>保持技术中立<ul><li>虽然用了云厂商服务，但通过抽象层保持中立，这样将来想换云厂商时，也能平滑迁移，不会被一家绑定死。</li></ul></li><li>别过度设计<ul><li>简单能落地的方案才是最好的！先解决核心问题，再慢慢完善。</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;如何借助云的能力，实现一个跨云高可用的网关？&lt;/p&gt;</summary>
    
    
    
    <category term="Architecture" scheme="https://jiankunking.com/categories/architecture/"/>
    
    
    <category term="Architecture" scheme="https://jiankunking.com/tags/architecture/"/>
    
    <category term="Gateway" scheme="https://jiankunking.com/tags/gateway/"/>
    
    <category term="Multi-Cloud" scheme="https://jiankunking.com/tags/multi-cloud/"/>
    
    <category term="High Availability" scheme="https://jiankunking.com/tags/high-availability/"/>
    
  </entry>
  
  <entry>
    <title>Apache APISIX 架构浅析</title>
    <link href="https://jiankunking.com/apache-apisix-architecture.html"/>
    <id>https://jiankunking.com/apache-apisix-architecture.html</id>
    <published>2025-03-21T22:59:22.000Z</published>
    <updated>2025-03-21T19:33:18.230Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>大家从网上肯定看到过关于Apisix性能高的文章,那么到底是如何实现的呢？<br>本文是分析也是自己学习《OpenResty从入门到实战》及Apisix官方文档的一个笔记</p></blockquote><a id="more"></a><h2 id="简单分析">简单分析</h2><p>先看一下官网的架构图<br><img data-src="/images/apache-apisix-architecture/flow-software-architecture.png" alt></p><p>从图中可以看到APISIX是基于OpenResty与Nginx实现的。</p><p>这里需要注意<font color="DeepPink"><strong>OpenResty并不是Nginx的fork，也不是在Nginx的基础上加了一些常用库重新打包，而只是把Nginx当作底层的网络库来使用。</strong></font></p><p>那具体是如何使用Nginx来作为网络库的呢？</p><p>其实很简单就是在<code>Nginx.conf</code>中做简单的配置，让所有的流量都通过网关的Lua代码来处理。</p><blockquote><p>注：Nginx默认不支持Lua的，这部分是OpenResty的能力</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 9080;</span><br><span class="line"></span><br><span class="line">    init_worker_by_lua_block &#123;</span><br><span class="line">        apisix.http_init_worker()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        access_by_lua_block &#123;</span><br><span class="line">            apisix.http_access_phase()</span><br><span class="line">        &#125;</span><br><span class="line">        header_filter_by_lua_block &#123;</span><br><span class="line">            apisix.http_header_filter_phase()</span><br><span class="line">        &#125;</span><br><span class="line">        body_filter_by_lua_block &#123;</span><br><span class="line">            apisix.http_body_filter_phase()</span><br><span class="line">        &#125;</span><br><span class="line">        log_by_lua_block &#123;</span><br><span class="line">            apisix.http_log_phase()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"># 这里只是一个简单的示意，完整的配置可以看到文末的nginx.conf文件</span><br></pre></td></tr></table></figure><p>在这个示例中，监听了 9080 端口，并通过<code>location /</code>的方式，把这个端口的所有请求都拦截下来，并依次通过<code>access</code>、<code>rewrite</code>、<code>header filter</code>、<code>body filter</code>和<code>log</code>这几个阶段进行处理，在每个阶段中都会去调用对应的插件函数。其中，<code>rewrite</code>阶段便是在<code>apisix.http_access_phase</code>函数中合并处理的。</p><p>这里是一份完整的Nginx的配置文件:<a href="/attachments/APISIX/apisix_nginx.conf" target="_blank">APISIX Nginx.conf</a></p><p>这里补充下OpenResty Phases<br><img data-src="/images/apache-apisix-architecture/openresty_phases.png" alt></p><blockquote><p>为了实现足够高的性能，Apache APISIX 使用 C 编写了基于前缀树的匹配路由算法，并在此基础上使用 LuaJIT 提供的 FFI 编写了适用于 Lua 的接口。而 Lua 的灵活性，也使得 Apache APISIX 的路由分发模块，可以轻易地支持通过特定的表达式等方法，对同一前缀的下级路由进行匹配。最终在替代 NGINX 原生路由分发功能的前提下，实现了兼具高性能、高灵活性的动态配置功能。有关这部分功能的详细实现，可以查看 <a href="https://github.com/api7/lua-resty-radixtree" target="_blank" rel="noopener">lua-resty-radixtree</a> 和 <a href="https://github.com/apache/apisix/blob/master/apisix/http/route.lua" target="_blank" rel="noopener">route.lua</a>。</p></blockquote><p>那Lua层面是如何保证高性能的呢？</p><h3 id="协程-事件">协程+事件</h3><p>在OpenResty层面，Lua的协程会与Nginx的事件机制相互配合。如果Lua代码中出现类似查询 MySQL 数据库这样的 I/O 操作，就会先调用Lua协程的 yield 把自己挂起，然后在Nginx中注册回调；在 I/O 操作完成（也可能是超时或者出错）后，再由Nginx回调 resume 来唤醒Lua协程。这样就完成了Lua协程和Nginx事件驱动的配合，避免在Lua代码中写回调。</p><p><img data-src="/images/apache-apisix-architecture/openresty_lua_%E5%8D%8F%E7%A8%8B_nginx%E4%BA%8B%E4%BB%B6%E6%9C%BA%E5%88%B6.png" alt></p><h3 id="LuaJIT">LuaJIT</h3><p>我们先来看下LuaJIT在OpenResty整体架构中的位置：<br><img data-src="/images/apache-apisix-architecture/openresty_luajit.png" alt></p><p>OpenResty的worker进程都是fork master进程而得到的，其实，master进程中的LuaJIT虚拟机也会一起fork过来。在同一个worker内的所有协程，都会共享这个LuaJIT虚拟机，Lua代码的执行也是在这个虚拟机中完成的。</p><h4 id="标准-Lua-和-LuaJIT">标准 Lua 和 LuaJIT</h4><p>其实标准Lua出于性能考虑，也内置了虚拟机，所以Lua代码并不是直接被解释执行的，而是先由Lua编译器编译为字节码（Byte Code），然后再由Lua虚拟机执行。</p><p>而LuaJIT的运行时环境，除了一个汇编实现的Lua解释器外，还有一个可以直接生成机器代码的JIT编译器。开始的时候，LuaJIT和标准Lua一样，Lua代码被编译为字节码，字节码被LuaJIT的解释器解释执行。</p><p>但不同的是，LuaJIT的解释器会在执行字节码的同时，记录一些运行时的统计信息，比如每个Lua函数调用入口的实际运行次数，还有每个Lua循环的实际执行次数。当这些次数超过某个随机的阈值时，便认为对应的Lua函数入口或者对应的Lua循环足够热，这时便会触发JIT编译器开始工作。</p><p>JIT 编译器会从热函数的入口或者热循环的某个位置开始，尝试编译对应的Lua代码路径。编译的过程，是把LuaJIT字节码先转换成LuaJIT 自己定义的中间码（IR），然后再生成针对目标体系结构的机器码。</p><p>所以，<strong>所谓LuaJIT的性能优化，本质上就是让尽可能多的Lua代码可以被JIT编译器生成机器码，而不是回退到Lua解释器的解释执行模式</strong>。</p><p>注意</p><ul><li>LuaJIT并不完备，存在<a href="https://web.archive.org/web/20220717120825/http://wiki.luajit.org/NYI" target="_blank" rel="noopener">NYI(Not Yet Implemented)</a>问题</li><li>LuaJIT 的作者目前处于半退休状态</li><li>OpenResty用的LuaJIT是LuaJIT自己维护的分支</li></ul><h3 id="语句摘录">语句摘录</h3><p>在学习《OpenResty从入门到实战》的过程中对于一下几句话深有感触,这里摘录一下与大家共勉：</p><ul><li>在我看来，明白一个技术为何存在，并弄清楚它和别的类似技术之间的差异和优势，远比你只会熟练调用它提供的 API 更为重要。这种技术视野，会给你带来一定程度的远见和洞察力，这也可以说是工程师和架构师的一个重要区别。</li><li>任何技术课程的学习，都不能代替对官方文档的仔细研读。这些耗时的笨功夫，每个人都省不掉的。</li><li>OpenResty 现在的官方文档只有英文版本，国内工程师在阅读时，难免会因为语言问题，抓不住重点，甚至误解其中的内容。但越是这样，越没有捷径可走，你更应该仔细地把文档从头到尾读完，并在有疑问时，结合测试案例集和自己的尝试，去确定出答案。这才是辅助我们学习OpenResty的正确途径。</li><li>对待技术的选择，我们可以有倾向，但还是不要一概而论绝对化，因为并没有一个可以适合所有缓存场景的银弹。根据实际场景的需要，构建一个最小化可用的方案，然后逐步地增加，是一个不错的法子。</li></ul><h2 id="OpenResty-vs-Envoy-底座比对">OpenResty vs Envoy 底座比对</h2><p>随着云原生的普及，Kubernetes技术栈的南北向流量网关也开始普及:<a href="https://jiankunking.com/south-north-entry-gateway-selection.html">南北入口网关选型</a></p><p>虽然从目前的南北向网关功能来看还不如以OpenResty为底座的Kong或者APISIX，但云原生社区明显更加活跃。<br>下面从Contributors、Star、Commits、Releases四方面来分析比对(数据取值时间:2025.03.22):</p><ul><li>Contributors<ul><li><a href="https://github.com/openresty/openresty" target="_blank" rel="noopener">OpenResty</a>:35</li><li><a href="https://github.com/envoyproxy/envoy" target="_blank" rel="noopener">Envoy</a>:1227</li></ul></li><li>Star<ul><li><a href="https://github.com/openresty/openresty" target="_blank" rel="noopener">OpenResty</a>:12.9k</li><li><a href="https://github.com/envoyproxy/envoy" target="_blank" rel="noopener">Envoy</a>:25.7k</li></ul></li><li>Commits<ul><li><a href="https://github.com/openresty/openresty" target="_blank" rel="noopener">OpenResty</a>:1746</li><li><a href="https://github.com/envoyproxy/envoy" target="_blank" rel="noopener">Envoy</a>:24125</li></ul></li><li>Releases<ul><li><a href="https://github.com/openresty/openresty" target="_blank" rel="noopener">OpenResty</a>:16</li><li><a href="https://github.com/envoyproxy/envoy" target="_blank" rel="noopener">Envoy</a>:196</li></ul></li></ul><p>从目前功能来看Kong或者APISIX能力更全，但从长远来看，个人更看好<a href="https://gateway-api.sigs.k8s.io/" target="_blank" rel="noopener">Kubernetes Gateway API</a>相关的网关。</p><p>现在感觉Cloudflare也放弃了OpenResty,转到了<a href="https://github.com/cloudflare/pingora" target="_blank" rel="noopener">Pingora</a>:<a href="https://blog.cloudflare.com/how-we-built-pingora-the-proxy-that-connects-cloudflare-to-the-internet/" target="_blank" rel="noopener">how-we-built-pingora-the-proxy-that-connects-cloudflare-to-the-internet</a></p><blockquote><p>Pingora 并不是一个工具，而是一个库。</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;大家从网上肯定看到过关于Apisix性能高的文章,那么到底是如何实现的呢？&lt;br&gt;
本文是分析也是自己学习《OpenResty从入门到实战》及Apisix官方文档的一个笔记&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="APISIX" scheme="https://jiankunking.com/categories/apisix/"/>
    
    
    <category term="Architecture" scheme="https://jiankunking.com/tags/architecture/"/>
    
    <category term="Apache" scheme="https://jiankunking.com/tags/apache/"/>
    
    <category term="APISIX" scheme="https://jiankunking.com/tags/apisix/"/>
    
  </entry>
  
  <entry>
    <title>南北入口网关选型</title>
    <link href="https://jiankunking.com/south-north-entry-gateway-selection.html"/>
    <id>https://jiankunking.com/south-north-entry-gateway-selection.html</id>
    <published>2025-03-07T22:12:47.000Z</published>
    <updated>2025-03-19T10:37:07.538Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>南北向网关的选型之旅</p></blockquote><a id="more"></a><h1>背景</h1><p>我们这边的容器云是国内外混合云，这里的混合云是指</p><ul><li><p>从提供者来分包含：</p><ul><li>阿里云</li><li>华为云</li><li>AWS</li><li>Oracle</li><li>自建</li></ul></li><li><p>从地域来分包含：</p><ul><li>国内</li><li>海外<ul><li>法兰克福</li><li>俄罗斯</li><li>新加坡</li><li>…</li></ul></li></ul></li></ul><p>所以我们希望网关方案能够</p><ul><li>涵盖住所有集群、所有地域</li><li>最好是能直接用云产品</li><li>对接各个集群的方案是统一的</li><li>能够涵盖<ul><li>Ingress Nginx的能力</li><li>业务开发对于网关的日常需求</li></ul></li></ul><blockquote><p>我们一期的目标是先弄南北向网关，二期看情况再弄服务网格(东西向)<br>所以本方案主要侧重在南北向网关的选择，但如果后期要弄服务网格也要能支持</p></blockquote><p>基于上面的背景，业界可以选择的方案，大概有这些：</p><ul><li><a href="https://istio.io/latest/docs/reference/config/networking/gateway/" target="_blank" rel="noopener">Istio Gateway</a></li><li><a href="https://help.aliyun.com/zh/asm/product-overview/what-is-asm" target="_blank" rel="noopener">阿里云ASM</a></li><li><a href="https://support.huaweicloud.com/asm/index.html" target="_blank" rel="noopener">华为云ASM</a></li><li><a href="https://gateway-api.sigs.k8s.io/" target="_blank" rel="noopener">Kubernetes Gateway API</a></li></ul><p>先捋一下业务网关的基础能力</p><ul><li>HTTP 路由</li><li>HTTP 流量拆分<br>◦ Canary traffic rollout<br>◦ Blue-green traffic rollout<br>◦ …</li><li>Timeout</li><li>Rewrite</li><li>Redirect</li><li>Header Modify</li><li>Request Mirror</li><li>WebSocket</li><li>Retry</li><li>Cors</li><li>Rate Limits</li><li>Circuit Breaking</li></ul><p>上面的这些能力也涵盖住了Ingress Nginx的能力。</p><blockquote><p>对于网关部分，大家有可能会想到为啥调研阿里云的MSE？<br>主要是MSE只能部署在阿里云的ACK中，对于非阿里云ACK及自建集群没法纳管。</p></blockquote><p>下面开始逐一分析以上网关的优劣</p><h1>网关优劣势分析</h1><h2 id="华为云ASM">华为云ASM</h2><blockquote><p>基于Istio Gateway实现,能支持服务网格但实现方式Sidecar模式。</p></blockquote><h3 id="优势">优势</h3><ul><li>ASM 完全兼容istio CRD</li></ul><h3 id="劣势">劣势</h3><ul><li>ASM基础版本(支持单集群、支持<strong>最大实例数是200</strong>、<font color="DeepPink"><strong>不保证SLA</strong></font>)</li><li>UCS 服务网格商用受限(现在还没有人用过)<ul><li>注1：<code>ASM企业版已经下线了</code></li></ul></li><li>不支持纳管别的云或者自建集群</li><li>同样存在路由优先级问题</li></ul><h2 id="阿里云ASM">阿里云ASM</h2><blockquote><p>基于Istio Gateway实现,能支持服务网格，支持：Sidecar、Ambient两种模式，不过现阶段生产不推荐Ambient模式。</p></blockquote><h3 id="优势-v2">优势</h3><ul><li>能提供SLA保证</li><li>不需要自己运维相关组件</li><li>支持纳管别的云及自建集群</li><li>针对业务集群阿里云对应地域没有regin及海外网络差的情况，有针对性方案:<a href="https://help.aliyun.com/zh/asm/user-guide/reduce-push-latency-using-asm-remote-control-plane" target="_blank" rel="noopener">ASM远程控制面</a></li><li><a href="https://help.aliyun.com/zh/asm/user-guide/overview-of-asm-gateways?spm=a2c4g.11186623.help-menu-147365.d_2_5_0.681c5bcdGTAA8B&amp;scm=20140722.H_473114._.OR_help-T_cn~zh-V_1" target="_blank" rel="noopener">功能特性全</a></li><li>同一控制面支持纳管多个kubernetes集群<br>*这里需要注意:这些集群都是类似镜像集群,集群内的ASM部署的CRD及下发的路由啥的都是一毛一样的,所以这个功能也没啥用</li></ul><h3 id="劣势-v2">劣势</h3><ul><li>因为是直接基于Istio Gateway的，所以路由的顺序有问题<ul><li><a href="https://istio.io/latest/zh/docs/concepts/traffic-management/" target="_blank" rel="noopener">同一个虚拟服务内部，按照声明顺序来匹配，匹配到一个满足的就停止了</a></li><li><a href="https://istio.io/latest/zh/docs/ops/best-practices/traffic-management/#multiple-virtual-services-and-destination-rules-for-the-same-host" target="_blank" rel="noopener">同一个域名，不同虚拟服务之间，顺序是不确定的</a></li></ul></li><li>目前支持Kubernetes Gateway API v1.1.0，不支持<a href="https://github.com/kubernetes-sigs/gateway-api/milestone/19?closed=1" target="_blank" rel="noopener">v1.2.0</a>的相关特性<ul><li>标准<ul><li>HTTPRoute 超时</li><li>后端协议支持WebSocket 和 HTTP/2</li></ul></li></ul></li></ul><h2 id="Istio-Gateway-Kubernetes-Gateway-API">Istio Gateway / Kubernetes Gateway API</h2><p>对于Istio Gateway这里分为两种方案：</p><ul><li>对接标准，也就是Kubernetes Gateway API<ul><li><strong>标准化</strong>：Kubernetes Gateway API旨在成为一种标准，用于描述在Kubernetes集群中如何暴露服务。这意味着它不仅仅限于与某个特定的服务网格或入口控制器（如Istio）一起使用。通过采用Gateway API，您可以<strong>更容易地切换不同的实现或服务提供商，而无需重写您的配置</strong>。</li><li><strong>概念一致性</strong>：跟大家之前使用的MSE、APISIX保持一致</li><li><strong>更好的集成能力</strong>：由于Gateway API是Kubernetes原生的API，因此它能够更好地与其他Kubernetes原生工具和系统集成</li><li><a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io%2fv1.HTTPRouteRule" target="_blank" rel="noopener"><strong>支持路由优先级</strong></a></li></ul></li><li>直接对接Istio Gateway<ul><li>现阶段Istio Gateway的能力比Kubernetes Gateway API涵盖的范围更广，比如：<a href="https://istio.io/latest/zh/docs/tasks/traffic-management/circuit-breaking/" target="_blank" rel="noopener">熔断</a>、<a href="https://istio.io/latest/zh/docs/tasks/policy-enforcement/rate-limit/" target="_blank" rel="noopener">限流</a></li><li>但会引入Istio Gateway虚拟服务(VirtualService)、目标规则(DestinationRule)等概念，增加用户的学习成本</li><li>路由顺序不确定</li></ul></li></ul><p>到这里可以看出来只看<code>路由优先级</code>这么一个点就过滤掉了云ASM及直接对接Istio Gateway。</p><p>但还有几个点需要注意：</p><ul><li>Kubernetes Gateway API目前不支持熔断、限流</li><li>对于跨域，也需要等到<a href="https://github.com/kubernetes-sigs/gateway-api/milestone/21" target="_blank" rel="noopener">Milestone v1.3.0</a> （当前版本v1.2.1，下一个版本应该就是v1.3.0,但具体发布时间不确定）</li></ul><h3 id="Service-Mesh"><a href="https://gateway-api.kubernetes.ac.cn/implementations/#service-mesh-implementation-status" target="_blank" rel="noopener">Service Mesh</a></h3><p>从官网上看，Istio对于Kubernetes Gateway API服务网格的支持也已经GA。</p><h3 id="Kubernetes-Gateway-API路由优先级">Kubernetes Gateway API路由优先级</h3><p>从HTTPRoutes生成的代理或负载均衡器路由配置必须根据以下标准优先匹配，并在出现平局时继续比较。在适用路由上指定的所有规则中，优先级应给予具有以下条件的匹配：</p><ul><li>“精确”路径匹配。</li><li>具有最多字符的“前缀”路径匹配。</li><li>方法匹配。</li><li>最大数量的头部匹配。</li><li>最大数量的查询参数匹配。</li></ul><p>注：正则表达式路径匹配的优先级是具体实现相关的。</p><p>如果在多个路由之间仍然存在平局，匹配优先级必须按照以下标准依次确定，并在出现平局时继续比较：</p><ul><li>基于创建时间戳的最早的路由。</li><li>按“{命名空间}/{名称}”字母顺序排列的第一个路由。</li></ul><p>如果在HTTPRoute内仍然存在平局，则应将匹配优先级授予符合上述条件的第一个匹配规则（按列表顺序）。</p><p>如果未成功附加与请求匹配的规则到请求来源的父级，则必须返回HTTP 404状态码。</p><blockquote><p>原文地址：<br><a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.HTTPRouteRule" target="_blank" rel="noopener">https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.HTTPRouteRule</a></p></blockquote><p>从文档注释的位置可以判断同一个HTTPRouteRule内的多个HTTPRouteMatch应该是会满足以上规则的，那么多个HTTPRouteRule之间呢？</p><p>下面逐一验证下以上两种情况：</p><h4 id="验证结果">验证结果</h4><p><img data-src="/images/south-north-entry-gateway-selection/%E8%B7%AF%E7%94%B1%E4%BC%98%E5%85%88%E7%BA%A7%E9%AA%8C%E8%AF%81.png" alt></p><p>针对常用的</p><ul><li>多个前缀匹配之间的优先级</li><li>前缀匹配与精确匹配的优先级<br>进行验证，通过验证结果可以看出istio路由是遵循了Kubernetes Gateway API规范的。</li></ul><h1>结果</h1><p>从文中所列举的几种方案来看，目前最可行的方案就是</p><ul><li>自己部署Istio Gateway，然后直接对接Kubernetes Gateway API的CRD。<ul><li>这个真实要用起来也要等<a href="https://github.com/kubernetes-sigs/gateway-api/milestone/21" target="_blank" rel="noopener">Milestone v1.3.0</a> 发布及istio支持之后</li></ul></li><li>或者等阿里云ASM支持<a href="https://github.com/kubernetes-sigs/gateway-api/milestone/21" target="_blank" rel="noopener">Milestone v1.3.0</a>之后</li></ul><h1>其它</h1><h2 id="云ASM支持Kubernetes版本的范围及云ASM-Istio版本">云ASM支持Kubernetes版本的范围及云ASM Istio版本</h2><ul><li><a href="https://help.aliyun.com/zh/asm/product-overview/support-for-istio-versions?spm=5176.13895322.console-base_help.dexternal.6dac5fcfo2sie4" target="_blank" rel="noopener">阿里云</a></li><li><a href="https://support.huaweicloud.com/usermanual-asm/asm_01_0020.html" target="_blank" rel="noopener">华为云</a></li></ul><h2 id="Sidecar-vs-Ambient">Sidecar vs Ambient</h2><p><a href="https://istio.io/latest/zh/docs/overview/dataplane-modes/#choosing-between-sidecar-and-ambient" target="_blank" rel="noopener">https://istio.io/latest/zh/docs/overview/dataplane-modes/#choosing-between-sidecar-and-ambient</a></p><h2 id="Istio-与-Kubernetes-Gateway-API">Istio 与 Kubernetes Gateway API</h2><p><a href="https://istio.io/latest/zh/docs/ops/configuration/traffic-management/network-topologies/" target="_blank" rel="noopener">Istio 支持 Kubernetes Gateway API， 并计划将其作为未来流量管理的默认 API。</a></p><h2 id="Istio是如何实现Kubernetes-Gateway-API路由优先级的？">Istio是如何实现Kubernetes Gateway API路由优先级的？</h2><p><a href="https://github.com/istio/istio/blob/master/pilot/pkg/config/kube/gateway/conversion.go#L823" target="_blank" rel="noopener">https://github.com/istio/istio/blob/master/pilot/pkg/config/kube/gateway/conversion.go#L823</a></p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;南北向网关的选型之旅&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://jiankunking.com/categories/kubernetes/"/>
    
    
    <category term="Gateway" scheme="https://jiankunking.com/tags/gateway/"/>
    
    <category term="Kubernetes" scheme="https://jiankunking.com/tags/kubernetes/"/>
    
    <category term="Api" scheme="https://jiankunking.com/tags/api/"/>
    
    <category term="Istio" scheme="https://jiankunking.com/tags/istio/"/>
    
    <category term="Envoy" scheme="https://jiankunking.com/tags/envoy/"/>
    
    <category term="Route" scheme="https://jiankunking.com/tags/route/"/>
    
    <category term="Priority" scheme="https://jiankunking.com/tags/priority/"/>
    
  </entry>
  
  <entry>
    <title>Go HTTP 客户端设置的调优指南</title>
    <link href="https://jiankunking.com/tuning-the-go-http-client-library.html"/>
    <id>https://jiankunking.com/tuning-the-go-http-client-library.html</id>
    <published>2025-01-18T01:57:05.000Z</published>
    <updated>2025-07-04T23:59:13.680Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>记录一次Go HTTP Client TIME_WAIT的优化</p></blockquote><a id="more"></a><h2 id="业务流程">业务流程</h2><p><img data-src="/images/tuning-the-go-http-client-library/%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B.png" alt></p><h2 id="分析">分析</h2><p>通过容器监控发现服务到事件总线的负载均衡之间有大量的短链接，回看一下代码</p><p>发送请求的代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">func SendToKEvent(ev *KEvent) error &#123;</span><br><span class="line">data, err := json.Marshal(ev.Data)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">return err</span><br><span class="line">&#125;</span><br><span class="line">log.Println(string(data))</span><br><span class="line">if !sendEvent &#123;</span><br><span class="line">log.Println(&quot;------ SEND_EVENT IS DISABLED ------&quot;)</span><br><span class="line">return nil</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">defer util.TimeCost(&quot;SendToKEvent&quot;)()</span><br><span class="line"></span><br><span class="line">body := bytes.NewReader(data)</span><br><span class="line">ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)</span><br><span class="line">defer cancel()</span><br><span class="line">req, err := http.NewRequest(http.MethodPost, ev.Url, body)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">return err</span><br><span class="line">&#125;</span><br><span class="line">req.WithContext(ctx)</span><br><span class="line">req.Header.Set(&quot;Content-Type&quot;, &quot;application/json; charset=utf-8&quot;)</span><br><span class="line">req.Header.Set(&quot;......&quot;, &quot;......&quot;)</span><br><span class="line"></span><br><span class="line">for k, v := range ev.ExtMap &#123;</span><br><span class="line">req.Header.Set(k, v)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resp, err := httpc.HttpClient.Do(req)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">return err</span><br><span class="line">&#125;</span><br><span class="line">defer resp.Body.Close()</span><br><span class="line"></span><br><span class="line">// 事件总线 2xx 均为正常</span><br><span class="line">if resp.StatusCode &gt;= 300 || resp.StatusCode &lt; 200 &#123;</span><br><span class="line">return fmt.Errorf(&quot;req failed, resp=%v&quot;, resp)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">return nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>http client的代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">var (</span><br><span class="line">HttpClient = &amp;http.Client&#123;</span><br><span class="line">Transport: &amp;http.Transport&#123;</span><br><span class="line">Proxy: http.ProxyFromEnvironment,</span><br><span class="line">DialContext: func(ctx context.Context, network, addr string) (conn net.Conn, e error) &#123;</span><br><span class="line">return (&amp;net.Dialer&#123;</span><br><span class="line">Timeout:   10 * time.Second,</span><br><span class="line">KeepAlive: 90 * time.Second,</span><br><span class="line">&#125;).DialContext(ctx, network, addr)</span><br><span class="line">&#125;,</span><br><span class="line">ForceAttemptHTTP2:     true,</span><br><span class="line">TLSHandshakeTimeout:   5 * time.Second,</span><br><span class="line">ResponseHeaderTimeout: 30 * time.Second,</span><br><span class="line">MaxIdleConnsPerHost:   10,</span><br><span class="line">IdleConnTimeout:       90 * time.Second,</span><br><span class="line">ExpectContinueTimeout: 1 * time.Second,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>代码看起来没啥问题,但想到了之前处理过Golang ES client的一个问题</p><p><a href="https://jiankunking.com/tcp-state-diagram.html">https://jiankunking.com/tcp-state-diagram.html</a></p><p>看下上文中<code>TIME_WAIT</code>部分，发现还真是<br><a href="https://pkg.go.dev/net/http#Response" target="_blank" rel="noopener">https://pkg.go.dev/net/http#Response</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// The http Client and Transport guarantee that Body is always</span><br><span class="line">// non-nil, even on responses without a body or responses with</span><br><span class="line">// a zero-length body. It is the caller&apos;s responsibility to</span><br><span class="line">// close Body. The default HTTP client&apos;s Transport may not</span><br><span class="line">// reuse HTTP/1.x &quot;keep-alive&quot; TCP connections if the Body is</span><br><span class="line">// not read to completion and closed.</span><br></pre></td></tr></table></figure><p>调整代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">func SendToKEvent(ev *KEvent) error &#123;</span><br><span class="line">......</span><br><span class="line">resp, err := httpc.HttpClient.Do(req)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">return err</span><br><span class="line">&#125;</span><br><span class="line">defer resp.Body.Close()</span><br><span class="line">io.Copy(ioutil.Discard, resp.Body)  // &lt;-- 添加这一行</span><br><span class="line">......</span><br><span class="line">return nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重新部署后,发现<code>TIME_WAIT</code>的链接少了很多，但还是有10几个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">bash-5.0# netstat -anp |grep TIME</span><br><span class="line">tcp        0      0 ::ffff:172.16.3.247:8080 ::ffff:10.200.76.64:10964 TIME_WAIT   -</span><br><span class="line">tcp        0      0 ::ffff:172.16.3.247:8080 ::ffff:10.200.76.64:45738 TIME_WAIT   -</span><br><span class="line">tcp        0      0 ::ffff:172.16.3.247:8080 ::ffff:10.200.76.64:21178 TIME_WAIT   -</span><br><span class="line">tcp        0      0 ::ffff:172.16.3.247:8080 ::ffff:10.200.76.64:37354 TIME_WAIT   -</span><br><span class="line">tcp        0      0 ::ffff:172.16.3.247:8080 ::ffff:10.200.76.64:10966 TIME_WAIT   -</span><br><span class="line">tcp        0      0 ::ffff:172.16.3.247:8080 ::ffff:10.200.76.64:37352 TIME_WAIT   -</span><br><span class="line">tcp        0      0 ::ffff:172.16.3.247:8080 ::ffff:10.200.76.64:61524 TIME_WAIT   -</span><br><span class="line">tcp        0      0 ::ffff:172.16.3.247:8080 ::ffff:10.200.76.64:61526 TIME_WAIT   -</span><br><span class="line">tcp        0      0 ::ffff:172.16.3.247:8080 ::ffff:10.200.76.64:21180 TIME_WAIT   -</span><br><span class="line">tcp        0      0 ::ffff:172.16.3.247:8080 ::ffff:10.200.76.64:33256 TIME_WAIT   -</span><br><span class="line">tcp        0      0 ::ffff:172.16.3.247:8080 ::ffff:10.200.76.64:45736 TIME_WAIT   -</span><br><span class="line">tcp        0      0 ::ffff:172.16.3.247:8080 ::ffff:10.200.76.64:33254 TIME_WAIT   -</span><br><span class="line">bash-5.0#</span><br></pre></td></tr></table></figure><p>这里需要注意一下</p><ul><li>172.16.3.247是服务POD的ip</li><li>10.200.76.64是POD所在宿主机的ip</li></ul><p>也就是说POD跟宿主机之间有短链接,那这几个短链接是在做啥呢？</p><p>抓包看下<br><img data-src="/images/tuning-the-go-http-client-library/%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E5%88%86%E6%9E%90.png" alt></p><p>着重看一下No 502这一行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Frame 502: 177 bytes on wire (1416 bits), 177 bytes captured (1416 bits)</span><br><span class="line">Ethernet II, Src: ee:ee:ee:ee:ee:ee (ee:ee:ee:ee:ee:ee), Dst: b6:cd:6a:f8:69:5e (b6:cd:6a:f8:69:5e)</span><br><span class="line">Internet Protocol Version 4, Src: 10.200.76.64, Dst: 172.16.3.247</span><br><span class="line">Transmission Control Protocol, Src Port: 29978, Dst Port: 8080, Seq: 1, Ack: 1, Len: 111</span><br><span class="line">Hypertext Transfer Protocol</span><br><span class="line">    GET /healthz HTTP/1.1\r\n &lt;-- 注意这一行,这个接口是服务配置的存活检查接口</span><br><span class="line">    Host: 172.16.3.247:8080\r\n</span><br><span class="line">    User-Agent: kube-probe/1.21\r\n</span><br><span class="line">    Accept: */*\r\n</span><br><span class="line">    Connection: close\r\n &lt;-- 注意这一行</span><br><span class="line">    \r\n</span><br><span class="line">    [Response in frame: 506]</span><br><span class="line">    [Full request URI: http://172.16.3.247:8080/healthz]</span><br></pre></td></tr></table></figure><p>Connection</p><ul><li><code>Connection: keep-alive</code> 当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接</li><li><code>Connection: close</code> 代表一个Request完成后，客户端和服务器之间用于传输HTTP数据的TCP连接会关闭， 当客户端再次发送Request，需要重新建立TCP连接。</li></ul><p>从<code>Connection</code>的注释可以看出当请求header中带有<code>Connection: keep-alive</code>表明该请求是会是一个短链接。</p><p>看下服务的Deployment的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">livenessProbe:</span><br><span class="line">  failureThreshold: 3</span><br><span class="line">  httpGet:</span><br><span class="line">    path: /healthz</span><br><span class="line">    port: 8080</span><br><span class="line">    scheme: HTTP</span><br><span class="line">  periodSeconds: 10</span><br><span class="line">  successThreshold: 1</span><br><span class="line">  timeoutSeconds: 1</span><br></pre></td></tr></table></figure><p>到这里问题都可以解释的通了,Kubernetes会每10秒请求一次服务的存活检查的接口,每一次都是短链接,而<code>TIME_WAIT</code>的默认值是120s。</p><p>那服务<code>TIME_WAIT</code>的链接应该会一直保持在11-13个左右。</p><p>到这里所有的问题都就可以解释了。</p><h2 id="结论">结论</h2><ul><li>Go HTTP Client请求完了,即使业务不关注响应的Body,还是要在代码中read一下body。</li><li>只要服务配置了存活检查就会有短链接，短链接的数据取决于检查间隔时间的配置。</li></ul><h2 id="拓展阅读">拓展阅读</h2><ul><li><a href="https://jiankunking.com/tcp-state-diagram.html">TCP 状态图</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;记录一次Go HTTP Client TIME_WAIT的优化&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="Go" scheme="https://jiankunking.com/categories/go/"/>
    
    
    <category term="Network" scheme="https://jiankunking.com/tags/network/"/>
    
    <category term="TCP" scheme="https://jiankunking.com/tags/tcp/"/>
    
    <category term="Client" scheme="https://jiankunking.com/tags/client/"/>
    
    <category term="Go" scheme="https://jiankunking.com/tags/go/"/>
    
    <category term="IP" scheme="https://jiankunking.com/tags/ip/"/>
    
    <category term="HTTP" scheme="https://jiankunking.com/tags/http/"/>
    
    <category term="Settings" scheme="https://jiankunking.com/tags/settings/"/>
    
    <category term="TIME_WAIT" scheme="https://jiankunking.com/tags/time-wait/"/>
    
  </entry>
  
  <entry>
    <title>如何优化Elasticsearch大文档查询？</title>
    <link href="https://jiankunking.com/how-to-optimize-elasticsearch-large-document-query.html"/>
    <id>https://jiankunking.com/how-to-optimize-elasticsearch-large-document-query.html</id>
    <published>2025-01-11T09:26:20.000Z</published>
    <updated>2025-02-13T10:46:23.326Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>记录一次业务复杂场景下DSL优化的过程</p></blockquote><a id="more"></a><h2 id="背景">背景</h2><p>B端商城业务有一个场景就是客户可见的产品列表是需要N多闸口及各种其它逻辑组合过滤的，各种闸口数据及产品数据都是存储在ES的(有的是独立索引，有的是作为产品属性存储在产品文档上)。</p><p>在实际使用的过程中，发现接口的毛刺比较严重，而这部分毛刺请求的耗时基本都是花费在从ES中查询产品索引的时候。</p><p>开启了一下ES慢DSL的日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PUT /jiankunking_product_prod/_settings</span><br><span class="line">&#123;</span><br><span class="line">  &quot;index.search.slowlog.threshold.query.warn&quot;: &quot;10s&quot;,</span><br><span class="line">  &quot;index.search.slowlog.threshold.query.info&quot;: &quot;5s&quot;,</span><br><span class="line">  &quot;index.search.slowlog.threshold.fetch.warn&quot;: &quot;2s&quot;,</span><br><span class="line">  &quot;index.indexing.slowlog.source&quot;: true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>经过分析慢DSL日志发现耗时长的部分都是在fetch阶段。</p><p>这里有个地方需要注意</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@jiankunking-search-01: /data/es/logs]# ls -lrth |grep -v .gz</span><br><span class="line">total 2.2G</span><br><span class="line">-rw-r--r-- 1 es es    0 Sep 30  2019 jiankunking_audit.json</span><br><span class="line">-rw-r--r-- 1 es es    0 Sep 30  2019 jiankunking_index_indexing_slowlog.log</span><br><span class="line">-rw-r--r-- 1 es es    0 Sep 30  2019 jiankunking_index_indexing_slowlog.json</span><br><span class="line">-rw-r--r-- 1 es es  53M Dec 31  2023 jiankunking_deprecation.log</span><br><span class="line">-rw-r--r-- 1 es es 108M Dec 31  2023 jiankunking_deprecation.json</span><br><span class="line">-rw-r--r-- 1 es es  55K Jul 30 10:43 jiankunking_server.json</span><br><span class="line">-rw-r--r-- 1 es es  52K Jul 30 10:43 jiankunking.log</span><br><span class="line">-rw-r--r-- 1 es es  63M Jul 30 11:32 jiankunking_index_search_slowlog.log //这里是完整的DSL</span><br><span class="line">-rw-r--r-- 1 es es 8.9M Jul 30 11:32 jiankunking_index_search_slowlog.json //这里的DSL会被截断</span><br></pre></td></tr></table></figure><h2 id="分析">分析</h2><p>已知问题点</p><ul><li>产品文档身上有4个属性会很大<ul><li>属性A(nested属性):可以到<code>几万</code>个</li><li>属性B(nested属性):可以到几百个</li><li>属性C(string数组):可以到<code>几万</code>个</li><li>属性D(大Object):可以到<code>几万</code>个</li></ul></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/_fetch_phase.html" target="_blank" rel="noopener">ES fetch阶段慢，其实就是从相关分片请求文档内容慢(这时候id其实已经知道了)</a></li></ul><p>大体就是下图这么个流程</p><p><img data-src="/images/how-to-optimize-elasticsearch-large-document-query/query_fetch.png" alt></p><p>下面简化一下请求的DSL，看下移除所有复杂的查询逻辑后，直接按照_id来terms查询效果如何？</p><h3 id="DSL">DSL</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">GET /jiankunking_product_prod/_search</span><br><span class="line">&#123;</span><br><span class="line">&quot;size&quot;: 10000,</span><br><span class="line">&quot;_source&quot;: &#123;</span><br><span class="line">&quot;includes&quot;: [</span><br><span class="line">&quot;code&quot;,</span><br><span class="line">&quot;group&quot;,</span><br><span class="line">&quot;groupBrand&quot;</span><br><span class="line">],</span><br><span class="line">&quot;excludes&quot;: []</span><br><span class="line">&#125;,</span><br><span class="line">&quot;query&quot;: &#123;</span><br><span class="line">&quot;terms&quot;: &#123;</span><br><span class="line">&quot;_id&quot;: [</span><br><span class="line">&quot;具体文档_id&quot;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="不同文档大小查询时延">不同文档大小查询时延</h3><blockquote><p>当前分析的DSL原本命中的文档数就是8306<br>下表中的文档数是直接在terms中查询的id数</p></blockquote><table><thead><tr><th>文档数</th><th>文档大小(Bytes)</th><th>文档大小(KB)</th><th>响应时延(ms)</th><th>备注</th></tr></thead><tbody><tr><td>8306</td><td>&lt;=6,766,797</td><td>&lt;=6,608</td><td>5908</td><td></td></tr><tr><td>5908</td><td>&lt;500,000</td><td>&lt;488</td><td>2327</td><td>剔除大的</td></tr><tr><td>6929</td><td>&lt;200,000</td><td>&lt;195</td><td>1507</td><td>剔除大的</td></tr><tr><td>5731</td><td>&lt;100,000</td><td>&lt;97</td><td>599</td><td>剔除大的</td></tr><tr><td>4925</td><td>&lt;50,000</td><td>&lt;49</td><td>356</td><td>剔除大的</td></tr><tr><td>4236</td><td>&lt;30,000</td><td>&lt;29</td><td>214</td><td>剔除大的(注意这里，当文档大小比较小的时候，4000+的文档查询其实是比较快的)</td></tr><tr><td>----</td><td>----</td><td>----</td><td>----</td><td>----</td></tr><tr><td>4070</td><td>&gt;30,000</td><td>&gt;29</td><td>6261</td><td>剔除小的</td></tr><tr><td>3381</td><td>&gt;50,000</td><td>&gt;49</td><td>6050</td><td>剔除小的</td></tr><tr><td>2572</td><td>&gt;100,000</td><td>&gt;97</td><td>5388</td><td>剔除小的</td></tr><tr><td>1377</td><td>&gt;200,000</td><td>&gt;195</td><td>4973</td><td>剔除小的</td></tr><tr><td>669</td><td>&gt;500,000</td><td>&gt;488</td><td>3984</td><td>剔除小的</td></tr><tr><td>381</td><td>&gt;1,000,000</td><td>&gt;976</td><td>3169</td><td>剔除小的</td></tr><tr><td>217</td><td>&gt;2,000,000</td><td>&gt;1,952</td><td>2391</td><td>剔除小的</td></tr><tr><td>88</td><td>&gt;3,000,000</td><td>&gt;2,928</td><td>1244</td><td>剔除小的</td></tr></tbody></table><h4 id="从大文档开始删除">从大文档开始删除</h4><p><img data-src="/images/how-to-optimize-elasticsearch-large-document-query/%E4%BB%8E%E5%A4%A7%E6%96%87%E6%A1%A3%E5%BC%80%E5%A7%8B%E5%88%A0%E9%99%A4.png" alt></p><h4 id="从小文档开始删除">从小文档开始删除</h4><p><img data-src="/images/how-to-optimize-elasticsearch-large-document-query/%E4%BB%8E%E5%B0%8F%E6%96%87%E6%A1%A3%E5%BC%80%E5%A7%8B%E5%88%A0%E9%99%A4.png" alt></p><h3 id="分析-v2">分析</h3><ul><li>文档数与文档大小查询分析<ul><li>剔除大文档之后，查询数据效率提升明显</li><li>剔除小文档之后，查询数据效率提升缓慢</li></ul></li></ul><p>到这里我们可以发现当文档size比较小的时候几千个文档的查询RT是很短的，但当随着请求命中的大文档越来越多，RT极速增加。</p><p>回看下我们的产品索引数据，可以发现大字段其实都是用来过滤的，并不是返回给页面需要的；那我们是不是可以：将索引拆分为两个或者ES只用来作为二级索引返回ids,然后去MySQL中查询具体的产品信息？</p><p><img data-src="/images/how-to-optimize-elasticsearch-large-document-query/query_architecture.png" alt></p><p>那我们将慢DSL中中查询的字段修改为只返回_id</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">POST /jiankunking_product_prod/_search</span><br><span class="line">&#123;</span><br><span class="line">&quot;size&quot;: 10000,</span><br><span class="line">&quot;_source&quot;: false,</span><br><span class="line">&quot;query&quot;: &#123;</span><br><span class="line">&quot;terms&quot;: &#123;</span><br><span class="line">&quot;_id&quot;: [&quot;&quot;],</span><br><span class="line">&quot;boost&quot;: 1</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这时候查询耗时只需要<code>203ms</code>,这种情况下还能不能再优化了呢？答案是可以的</p><blockquote><p>索引中文档_id就是产品的code</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">POST /jiankunking_product_prod/_search</span><br><span class="line">&#123;</span><br><span class="line">&quot;size&quot;: 10000,</span><br><span class="line">&quot;_source&quot;: false,</span><br><span class="line">&quot;stored_fields&quot;: &quot;_none_&quot;,</span><br><span class="line">&quot;docvalue_fields&quot;: [</span><br><span class="line">&quot;code&quot;</span><br><span class="line">],</span><br><span class="line">&quot;query&quot;: &#123;</span><br><span class="line">&quot;terms&quot;: &#123;</span><br><span class="line">&quot;code&quot;: [</span><br><span class="line">&quot;&quot;</span><br><span class="line">],</span><br><span class="line">&quot;boost&quot;: 1</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这时候查询只需要<code>76ms</code>。</p><h2 id="结论">结论</h2><p>到这里这次优化基本结束了，最终的方案就是</p><ul><li>通过从jiankunking_product_prod索引中通过<code>列存</code>获取ids</li><li>到MySQL或者新的产品主数据索引中查询具体的产品数据</li></ul><h2 id="思考">思考</h2><h3 id="为啥不直接从jiankunking-product-prod索引中通过列存获取前端需要的数据呢？">为啥不直接从jiankunking_product_prod索引中通过列存获取前端需要的数据呢？</h3><p>因为真实业务场景中需要返回的产品属性虽然每个不大，但总数有20多个，列存在返回字段数多且命中文档大小都不大的场景下，相比原逻辑直接从_source中取会略有下降。</p><p>更多原理性解释，可以看下这里:<a href="https://jiankunking.com/elasticsearch-source-doc-values-and-store-performance.html">https://jiankunking.com/elasticsearch-source-doc-values-and-store-performance.html</a></p><h3 id="ES适合的场景都有哪些？">ES适合的场景都有哪些？</h3><p>目前我这边遇到的场景主要有：</p><ul><li>检索加速<ul><li>数据查询的主存储<ul><li>当文档大小不是太大的时候，索引检索完直接返回需要的数据</li></ul></li><li>二级索引<ul><li>针对的就是本文这种场景</li></ul></li></ul></li><li>日志<ul><li>应用/容器日志<ul><li>这里追求的更多是高吞吐的写入</li></ul></li><li>业务日志</li></ul></li></ul><h3 id="具体索引中数据大小是什么情况呢？">具体索引中数据大小是什么情况呢？</h3><!-- | 分位数 | 大小 (KB)   ||--------|-------------|| 0.05   | 1.16    || 0.10   | 1.39    || 0.15   | 1.61    || 0.20   | 1.69    || 0.25   | 1.77    || 0.30   | 2.14    || 0.35   | 2.97    || 0.40   | 3.50    || 0.45   | 3.90    || 0.50   | 4.24    || 0.55   | 4.92    || 0.60   | 5.73    || 0.65   | 7.15    || 0.70   | 8.82    || 0.75   | 13.13   || 0.80   | 32.32   || 0.85   | 57.52   || 0.90   | 114.39  || 0.95   | 262.47  || 0.99   | 989.75  | --><table><thead><tr><th>分位数</th><th>大小 (KB)</th></tr></thead><tbody><tr><td>0.050</td><td>1.159</td></tr><tr><td>0.100</td><td>1.388</td></tr><tr><td>0.150</td><td>1.609</td></tr><tr><td>0.200</td><td>1.693</td></tr><tr><td>0.250</td><td>1.774</td></tr><tr><td>0.300</td><td>2.140</td></tr><tr><td>0.350</td><td>2.969</td></tr><tr><td>0.400</td><td>3.499</td></tr><tr><td>0.450</td><td>3.898</td></tr><tr><td>0.500</td><td>4.238</td></tr><tr><td>0.550</td><td>4.917</td></tr><tr><td>0.600</td><td>5.730</td></tr><tr><td>0.650</td><td>7.157</td></tr><tr><td>0.700</td><td>8.823</td></tr><tr><td>0.750</td><td>13.122</td></tr><tr><td>0.800</td><td>32.320</td></tr><tr><td>0.850</td><td>57.531</td></tr><tr><td>0.900</td><td>114.387</td></tr><tr><td>0.950</td><td>262.478</td></tr><tr><td>0.990</td><td>989.708</td></tr><tr><td>0.991</td><td>1,099.801</td></tr><tr><td>0.992</td><td>1,259.971</td></tr><tr><td>0.993</td><td>1,481.723</td></tr><tr><td>0.994</td><td>1,807.947</td></tr><tr><td>0.995</td><td>2,155.884</td></tr><tr><td>0.996</td><td>2,392.959</td></tr><tr><td>0.997</td><td>2,725.635</td></tr><tr><td>0.998</td><td>3,171.288</td></tr><tr><td>0.999</td><td>4,238.397</td></tr></tbody></table><p><img data-src="/images/how-to-optimize-elasticsearch-large-document-query/doc_percent_size.png" alt></p><h2 id="来自官方的点赞">来自官方的点赞</h2><p><img data-src="/images/how-to-optimize-elasticsearch-large-document-query/%E6%9D%A5%E8%87%AA%E5%AE%98%E6%96%B9%E7%9A%84%E7%82%B9%E8%B5%9E.png" alt></p><h2 id="拓展阅读">拓展阅读</h2><ul><li><a href="https://jiankunking.com/elasticsearch-source-doc-values-and-store-performance.html">https://jiankunking.com/elasticsearch-source-doc-values-and-store-performance.html</a></li><li><a href="https://jiankunking.com/elasticsearch-scroll-and-search-after.html">https://jiankunking.com/elasticsearch-scroll-and-search-after.html</a></li><li><a href="https://luis-sena.medium.com/stop-using-the-id-field-in-elasticsearch-6fb650d1fbae" target="_blank" rel="noopener">https://luis-sena.medium.com/stop-using-the-id-field-in-elasticsearch-6fb650d1fbae</a></li><li><a href="https://jiankunking.com/elasticsearch-avoid-the-fetch-phase-when-retrieving-only-id.html">https://jiankunking.com/elasticsearch-avoid-the-fetch-phase-when-retrieving-only-id.html</a></li><li><a href="https://jiankunking.com/elasticsearch-query-secret.html">https://jiankunking.com/elasticsearch-query-secret.html</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/general-recommendations.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/general-recommendations.html</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;记录一次业务复杂场景下DSL优化的过程&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="ElasticSearch" scheme="https://jiankunking.com/categories/elasticsearch/"/>
    
    
    <category term="Performance" scheme="https://jiankunking.com/tags/performance/"/>
    
    <category term="原创" scheme="https://jiankunking.com/tags/原创/"/>
    
    <category term="ElasticSearch" scheme="https://jiankunking.com/tags/elasticsearch/"/>
    
    <category term="Document" scheme="https://jiankunking.com/tags/document/"/>
    
    <category term="Size" scheme="https://jiankunking.com/tags/size/"/>
    
    <category term="Query" scheme="https://jiankunking.com/tags/query/"/>
    
    <category term="Large" scheme="https://jiankunking.com/tags/large/"/>
    
  </entry>
  
  <entry>
    <title>又一次不接受Elasticsearch官方建议导致的事故</title>
    <link href="https://jiankunking.com/another-incident-caused-by-not-accepting-elasticsearch-official-advice.html"/>
    <id>https://jiankunking.com/another-incident-caused-by-not-accepting-elasticsearch-official-advice.html</id>
    <published>2025-01-08T13:26:18.000Z</published>
    <updated>2025-01-08T08:02:00.822Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>记录一下<br>一次Elasticsearch集群事故分析、排查、处理</p></blockquote><a id="more"></a><h2 id="背景">背景</h2><p>上午同事拉群反馈他们在用的一个ES集群出现写入超时的情况，查看ES集群状态发现已经是RED了</p><ul><li>这是一个3 * 8核32G(jvm堆配置了16G)的集群</li><li>出问题的时候，集群内有5000个索引，每个索引shard数是6，副本数是1，也就是一个索引是12个shard<ul><li>shard数已经达到了<code>6w</code>个，这不包含ES自带的系统索引。</li></ul></li><li>集群的实例重启过</li><li>数据量大约480G</li></ul><p>看到这里是不是有一种似曾相识的感觉?<br>=&gt; <a href="https://jiankunking.com/an-accident-caused-by-not-accepting-the-official-advice-of-elasticsearch.html">一次不接受Elasticsearch官方建议导致的事故</a></p><p>立马联系业务删除历史数据，业务这边明确可以删除22年、23年的历史数据。</p><h2 id="处理">处理</h2><h3 id="调速">调速</h3><p>在业务执行批量删除后，我这边先调大了集群恢复相关的参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PUT /_cluster/settings</span><br><span class="line">&#123;</span><br><span class="line">  &quot;persistent&quot;: &#123;</span><br><span class="line">    &quot;cluster.routing.allocation.node_concurrent_recoveries&quot;: 200,</span><br><span class="line">    &quot;indices.recovery.max_bytes_per_sec&quot;: &quot;80mb&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>关于恢复速度相关参数解释:<a href="https://elastic.ac.cn/guide/en/elasticsearch/reference/8.17/recovery.html" target="_blank" rel="noopener">https://elastic.ac.cn/guide/en/elasticsearch/reference/8.17/recovery.html</a></p></blockquote><p>在集群恢复的期间，ES实例一直在输出WARN日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Unable to acquire permit to use snapshot files during recovery, </span><br><span class="line">this recovery will recover index files from the source node. </span><br><span class="line">Ensure snapshot files can be used during recovery by setting </span><br><span class="line">[indices.recovery.max_concurrent_snapshot_file_downloads] to be no greater than [25]</span><br></pre></td></tr></table></figure><p>看了下集群设置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">GET /_cluster/settings?include_defaults=true</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">&quot;recovery&quot;: &#123;</span><br><span class="line">&quot;recovery_activity_timeout&quot;: &quot;1800000ms&quot;,</span><br><span class="line">&quot;retry_delay_network&quot;: &quot;5s&quot;,</span><br><span class="line">&quot;internal_action_timeout&quot;: &quot;15m&quot;,</span><br><span class="line">&quot;max_concurrent_snapshot_file_downloads_per_node&quot;: &quot;25&quot;,</span><br><span class="line">&quot;retry_delay_state_sync&quot;: &quot;500ms&quot;,</span><br><span class="line">&quot;max_concurrent_snapshot_file_downloads&quot;: &quot;5&quot;,// 这里并没有比25大</span><br><span class="line">&quot;internal_action_long_timeout&quot;: &quot;1800000ms&quot;,</span><br><span class="line">&quot;max_concurrent_operations&quot;: &quot;1&quot;,</span><br><span class="line">&quot;use_snapshots&quot;: &quot;true&quot;,</span><br><span class="line">&quot;max_concurrent_file_chunks&quot;: &quot;2&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以先忽略，恢复后，该日志就不再输出了。</p><h3 id="排查">排查</h3><p>找了一个ES节点下载最近几天的日志，排查实例退出的原因</p><p>实例退出前有大量如下异常输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">this may be caused by lack of enough unfragmented virtual address space </span><br><span class="line">or too restrictive virtual memory limits enforced by the operating system, </span><br><span class="line">preventing us to map a chunk of 414627 bytes. Please review &apos;ulimit -v&apos;, &apos;ulimit -m&apos; (both should return &apos;unlimited&apos;), </span><br><span class="line">and &apos;sysctl vm.max_map_count&apos;. More information: </span><br><span class="line">http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html</span><br></pre></td></tr></table></figure><p>这里看了一下max_map_count没有问题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@prd-jiankunking-notify-es-3: /app/es/data/nodes/0]# ulimit -m</span><br><span class="line">unlimited</span><br><span class="line">[root@prd-jiankunking-notify-es-3: /app/es/data/nodes/0]# ulimit -v</span><br><span class="line">unlimited</span><br><span class="line">[root@prd-jiankunking-notify-es-3: /app/es/data/nodes/0]# </span><br><span class="line">[root@prd-jiankunking-notify-es-3: /app/es/data/nodes/0]# sysctl vm.max_map_count</span><br><span class="line">vm.max_map_count = 262144</span><br><span class="line">[root@prd-jiankunking-notify-es-3: /app/es/data/nodes/0]# docker exec -it 21871c061981 /bin/bash</span><br><span class="line">root@prd-jiankunking-notify-es-3:/usr/share/elasticsearch# ulimit -m</span><br><span class="line">unlimited</span><br><span class="line">root@prd-jiankunking-notify-es-3:/usr/share/elasticsearch# ulimit -v</span><br><span class="line">unlimited</span><br><span class="line">root@prd-jiankunking-notify-es-3:/usr/share/elasticsearch# sysctl vm.max_map_count</span><br><span class="line">vm.max_map_count = 262144</span><br></pre></td></tr></table></figure><blockquote><p>262144是elasticsearch 7.16.2启动的时候提示的最小值</p></blockquote><!-- <font color=DeepPink> **这里反应出来一个问题就是部署ES的机器vm.max_map_count值最好就是配置为unlimited**</font> --><p>找到实例退出时间点的日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fe4d805c000, 65536, 1) failed; error=&apos;Not enough space&apos; (errno=12)</span><br><span class="line"></span><br><span class="line"># There is insufficient memory for the Java Runtime Environment to continue.</span><br><span class="line"># Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.</span><br><span class="line">[thread 330 also had an error][thread 232 also had an error]</span><br><span class="line"># An error report file with more information is saved as</span><br><span class="line"># logs/hs_err_pid7.log</span><br></pre></td></tr></table></figure><p>再看下logs/hs_err_pid7.log的日志内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#</span><br><span class="line"># There is insufficient memory for the Java Runtime Environment to continue.</span><br><span class="line"># Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.</span><br><span class="line"># Possible reasons:</span><br><span class="line">#   The system is out of physical RAM or swap space</span><br><span class="line">#   The process is running with CompressedOops enabled, and the Java Heap may be blocking the growth of the native heap</span><br><span class="line"># Possible solutions:</span><br><span class="line">#   Reduce memory load on the system</span><br><span class="line">#   Increase physical memory or swap space</span><br><span class="line">#   Check if swap backing store is full</span><br><span class="line">#   Decrease Java heap size (-Xmx/-Xms)</span><br><span class="line">#   Decrease number of Java threads</span><br><span class="line">#   Decrease Java thread stack sizes (-Xss)</span><br><span class="line">#   Set larger code cache with -XX:ReservedCodeCacheSize=</span><br><span class="line">#   JVM is running with Zero Based Compressed Oops mode in which the Java heap is</span><br><span class="line">#     placed in the first 32GB address space. The Java Heap base address is the</span><br><span class="line">#     maximum limit for the native heap growth. Please use -XX:HeapBaseMinAddress</span><br><span class="line">#     to set the Java Heap base and to place the Java Heap above 32GB virtual address.</span><br><span class="line"># This output file may be truncated or incomplete.</span><br><span class="line">#</span><br><span class="line">#  Out of Memory Error (os_linux.cpp:2728), pid=7, tid=213</span><br><span class="line">#</span><br><span class="line"># JRE version: OpenJDK Runtime Environment Temurin-17.0.1+12 (17.0.1+12) (build 17.0.1+12)</span><br><span class="line"># Java VM: OpenJDK 64-Bit Server VM Temurin-17.0.1+12 (17.0.1+12, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64)</span><br><span class="line"># Core dump will be written. Default location: /usr/share/elasticsearch/core.7</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">---------------  S U M M A R Y ------------</span><br><span class="line"></span><br><span class="line">Command Line: -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -XX:+ShowCodeDetailsInExceptionMessages -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Dlog4j2.formatMsgNoLookups=true -Djava.locale.providers=SPI,COMPAT --add-opens=java.base/java.io=ALL-UNNAMED -Xms16g -Xmx16g -XX:+UseG1GC -Djava.io.tmpdir=/tmp/elasticsearch-18266345477890424435 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m -Des.cgroups.hierarchy.override=/ -XX:MaxDirectMemorySize=8589934592 -XX:InitiatingHeapOccupancyPercent=30 -XX:G1ReservePercent=25 -Des.path.home=/usr/share/elasticsearch -Des.path.conf=/usr/share/elasticsearch/config -Des.distribution.flavor=default -Des.distribution.type=docker -Des.bundled_jdk=true org.elasticsearch.bootstrap.Elasticsearch -Ebootstrap.memory_lock=true</span><br><span class="line"></span><br><span class="line">Host: Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz, 8 cores, 31G, Ubuntu 20.04.3 LTS</span><br><span class="line">Time: Wed Jan  8 00:44:25 2025 UTC elapsed time: 28895.244676 seconds (0d 8h 1m 35s)</span><br></pre></td></tr></table></figure><h2 id="结论">结论</h2><p>到这里可以看出是ES在申请Native memory allocation (mmap) 的时候内存不足,进程退出了。</p><p>数据删除后，通过</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET /_cat/health?v</span><br></pre></td></tr></table></figure><p>看到active_shards_percent恢复进度到100%后，但还有19个shard unassign</p><p>看下具体没有分配的原因</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET /_cluster/allocation/explain</span><br></pre></td></tr></table></figure><p>返回内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;reason&quot; : &quot;ALLOCATION_FAILED&quot;,</span><br><span class="line">    &quot;at&quot; : &quot;2025-01-07T16:28:22.026Z&quot;,</span><br><span class="line">    &quot;failed_allocation_attempts&quot; : 5,</span><br><span class="line">    &quot;details&quot; : &quot;&quot;&quot;failed shard on node [IYowNoBkTzq_WPjXgb_10Q]: </span><br><span class="line">    failed recovery, failure RecoveryFailedException[[notification-jiankunking-2025.01][3]: </span><br><span class="line">    Recovery failed on &#123;es-notify-203&#125;&#123;IYowNoBkTzq_WPjXgb_10Q&#125;</span><br><span class="line">    &#123;nzTGfgdBQvykqKacU_cyzA&#125;&#123;127.0.0.203&#125;&#123;127.0.0.203:9301&#125;&#123;cdfhilmrstw&#125;&#123;ml.machine_memory=33568223232, xpack.installed=true, </span><br><span class="line">    transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=17179869184&#125;]; </span><br><span class="line">    nested: IndexShardRecoveryException[failed to recover from gateway]; nested:</span><br><span class="line">     EngineCreationFailureException[failed to open reader on writer]; nested: </span><br><span class="line">     IOException[Map failed: MMapIndexInput(path=&quot;/usr/share/elasticsearch/data/nodes/0/indices/n5E4PEFpTKKDgGdNmEqSPQ/3/index/_j50_Lucene80_0.dvd&quot;) </span><br><span class="line">     [this may be caused by lack of enough unfragmented virtual address space or too restrictive virtual memory limits enforced by the operating system, </span><br><span class="line">     preventing us to map a chunk of 4321640 bytes. Please review &apos;ulimit -v&apos;, &apos;ulimit -m&apos; (both should return &apos;unlimited&apos;), </span><br><span class="line">     and &apos;sysctl vm.max_map_count&apos;. More information: </span><br><span class="line">     http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html]]; &quot;&quot;&quot;,</span><br><span class="line">    &quot;last_allocation_status&quot; : &quot;no&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>手动重新分配下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">POST /_cluster/reroute?retry_failed=true</span><br></pre></td></tr></table></figure><p>集群状态red-&gt;yellow-&gt;green</p><p>业务清理历史数据后，集群</p><ul><li>shard数降为4w</li><li>数据量降为335.3 GB</li></ul>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;记录一下&lt;br&gt;
一次Elasticsearch集群事故分析、排查、处理&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="ElasticSearch" scheme="https://jiankunking.com/categories/elasticsearch/"/>
    
    
    <category term="原创" scheme="https://jiankunking.com/tags/原创/"/>
    
    <category term="ElasticSearch" scheme="https://jiankunking.com/tags/elasticsearch/"/>
    
    <category term="Shard" scheme="https://jiankunking.com/tags/shard/"/>
    
    <category term="mmap" scheme="https://jiankunking.com/tags/mmap/"/>
    
    <category term="Native memory allocation" scheme="https://jiankunking.com/tags/native-memory-allocation/"/>
    
  </entry>
  
  <entry>
    <title>2024年终总结</title>
    <link href="https://jiankunking.com/2024-year-end-summary.html"/>
    <id>https://jiankunking.com/2024-year-end-summary.html</id>
    <published>2024-12-31T22:40:54.000Z</published>
    <updated>2025-01-01T02:23:21.240Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>又是一年</p></blockquote><a id="more"></a><h1>工作</h1><p>今年的工作内容主要集中在B端系统的稳定性治理：</p><p><img data-src="/images/2024-year-end-summary/2024_work.png" alt></p><h1>生活</h1><h2 id="个人">个人</h2><p>10月份开始了针对于腰、背、手的治疗(基本上就是每个周末去一次)，到年底手指已经不需要整天贴膏药了。</p><blockquote><p>年底29号去的时候，大夫说，后面如果没有疼痛，就逐渐拉长治疗间隔。<br>也算是一个好消息。</p></blockquote><h2 id="家庭">家庭</h2><p>同去年,还是略显艰辛。</p><h2 id="财务">财务</h2><ul><li>提前还了一些贷款。</li></ul><h2 id="读书">读书</h2><p>今年感触比较大的几段话：</p><ul><li>真没必要为三五年后的前途担忧，大家都乐观点，说不定到时候都物是人非，瞬间豁然开朗。当下便是最好的时光，及时行乐，与世界和解。我们不妨暂时不去想那些烦恼，去听喜欢的歌，拍想拍的照片，吃喜欢的甜点。给好运一点时间，只管做自己，好运一定会如约而至！</li><li>你年少时曾梦想自己将来会成为一个什么样的人，会做出多么了不起的事，但现在发现，自己其实也不过如此。你可能做了很多努力，但是只能帮助你从一个小茄子长成一个大茄子，却始终变不成一个西瓜，无法达到质变。按我们的说法是，受到了阶级的桎梏。</li></ul><h1>新一年的期望</h1><ul><li><p>家人健健康康、平平安安。</p></li><li><p>自己减肥成功。</p><ul><li>饭搭子高血压、头晕，也算是给我敲了警钟，毕竟我也没比他瘦几斤。</li></ul></li></ul><blockquote><p>23年的愿望成功拖到了24年</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;又是一年&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="年终总结" scheme="https://jiankunking.com/categories/年终总结/"/>
    
    
    <category term="年终总结" scheme="https://jiankunking.com/tags/年终总结/"/>
    
    <category term="2024" scheme="https://jiankunking.com/tags/2024/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch为什么不能在query阶段直接返回_id,从而避免fetch?</title>
    <link href="https://jiankunking.com/elasticsearch-avoid-the-fetch-phase-when-retrieving-only-id.html"/>
    <id>https://jiankunking.com/elasticsearch-avoid-the-fetch-phase-when-retrieving-only-id.html</id>
    <published>2024-11-24T09:13:48.000Z</published>
    <updated>2025-01-16T01:01:47.232Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>整理自Github的一个issue,也正好解答了我的疑惑<br><a href="https://github.com/elastic/elasticsearch/issues/17159" target="_blank" rel="noopener">https://github.com/elastic/elasticsearch/issues/17159</a></p></blockquote><a id="more"></a><h2 id="提问">提问</h2><p>是否可以避免搜索的fetch阶段并仅返回文档ID？查询阶段结束时是否有_id，这样当我只需要_id时，fetch就多余了？可以通过当前API完成此操作吗?</p><p>最终，我希望能够比目前所见的速度更快地从搜索中检索文档ID。我已经尝试了所有文档中记录的各种方法来获得更好的性能，但没有找到令人满意的结果。我所取得的最佳成果只是通过并行查询每个5个分片中的每个分片而获得了25%的速度提升。一个可接受的速度提升应该快90%。了解这是否合理以及如果不合理的原因将会很有帮助。很难理解为什么我可以快速得到a)前100个结果，b)总计数，以及c)快速排序它们，但检索结果却非常慢。</p><p>此外，通过开发插件是否有可能提高此（仅限ID）场景的性能？是否有其他选项，无论是记录在案还是未记录在案，可以减少开销？</p><p>强调一下这一点的重要性，这对我们的实施至关重要，很可能是我们决定采用Elastic以替换当前庞大的持久性层的关键因素。</p><h2 id="回答">回答</h2><p>搜索阶段获取 Lucene的文档 ID（整数），而不是 elasticsearch 的 ID（字符串）。fetch阶段使用 Lucene 的存储字段机制查找文档 ID。存储字段以压缩块的形式存储在一起。由于 _source 是一个存储字段，因此您必须解压缩大量 _source 才能获得 ID 字段。由于它是分块的，因此您还必须解压缩未命中的文档的存储字段。</p><p>聚合速度很快，因为它们使用文档值(doc values)，这是一种非分块的列式结构。它经过压缩，但使用的是数值技巧，而不是通用的压缩算法。如果能够将您的工作重新设计为一个聚合操作，通过将感兴趣的工作推送到 Elasticsearch，那么您的操作速度可以提升数个数量级。</p><h3 id="一个推荐-优化">一个推荐/优化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">GET test/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;match_all&quot;: &#123;&#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;_source&quot;: false,</span><br><span class="line">  &quot;stored_fields&quot;: &quot;_none_&quot;,</span><br><span class="line">  &quot;docvalue_fields&quot;: [&quot;my_id_field&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>“stored_fields”: “<em>none</em>”:这将禁止检索所有存储字段，如 _source 和 _id。</li><li>“docvalue_fields”: [“my_id_field”]:这样就可以只检索选定的 doc_values 字段。</li></ul><p>来源</p><ul><li><a href="https://luis-sena.medium.com/stop-using-the-id-field-in-elasticsearch-6fb650d1fbae" target="_blank" rel="noopener">https://luis-sena.medium.com/stop-using-the-id-field-in-elasticsearch-6fb650d1fbae</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;整理自Github的一个issue,也正好解答了我的疑惑&lt;br&gt;
&lt;a href=&quot;https://github.com/elastic/elasticsearch/issues/17159&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/elastic/elasticsearch/issues/17159&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="ElasticSearch" scheme="https://jiankunking.com/categories/elasticsearch/"/>
    
    
    <category term="原创" scheme="https://jiankunking.com/tags/原创/"/>
    
    <category term="ElasticSearch" scheme="https://jiankunking.com/tags/elasticsearch/"/>
    
    <category term="fetch" scheme="https://jiankunking.com/tags/fetch/"/>
    
    <category term="phase" scheme="https://jiankunking.com/tags/phase/"/>
    
    <category term="_id" scheme="https://jiankunking.com/tags/id/"/>
    
  </entry>
  
  <entry>
    <title>[译]Elasticsearch Sequence ID实现思路及用途</title>
    <link href="https://jiankunking.com/elasticsearch-sequence-ids-6-0.html"/>
    <id>https://jiankunking.com/elasticsearch-sequence-ids-6-0.html</id>
    <published>2024-11-22T22:24:54.000Z</published>
    <updated>2024-11-23T00:07:07.045Z</updated>
    
    <content type="html"><![CDATA[<p>原文地址:<a href="https://www.elastic.co/blog/elasticsearch-sequence-ids-6-0" target="_blank" rel="noopener">https://www.elastic.co/blog/elasticsearch-sequence-ids-6-0</a></p><a id="more"></a><h2 id="如果">如果</h2><p>几年前，在Elastic，我们问自己一个&quot;如果&quot;问题，我们知道这将带来有趣的见解：</p><blockquote><p>&quot;如果我们在Elasticsearch中对索引操作进行全面排序会怎样？我们可以建立什么？</p></blockquote><p>答案涵盖范围很广：</p><ul><li>我们可以构建一种称为&quot;<a href="https://github.com/elastic/elasticsearch/issues/1242" target="_blank" rel="noopener">变更API</a>&quot;的功能，它可以接受操作ID，并为您提供自那时以来数据更改的列表。很棒！</li><li>我们可以只查找发生变化的索引操作，从而<a href="https://github.com/elastic/elasticsearch/issues/20859#issuecomment-252962327" target="_blank" rel="noopener">建立增量reindex</a>！</li><li>我们可以使用增量reindex功能，通过过滤/连续reindex建立<a href="https://github.com/elastic/elasticsearch/issues/17997" target="_blank" rel="noopener">以实体为中心的索引</a>！</li><li>我们可以建立不依赖于数据按时、按顺序和全局精确时间戳到达的数据<a href="https://discuss.elastic.co/t/re-index-aggregation-elasticsearch/75325" target="_blank" rel="noopener">滚动/汇总索引</a>！</li><li>我们可以建立类似<a href="https://en.wikipedia.org/wiki/Materialized_view" target="_blank" rel="noopener">物化视图</a>的东西，在新数据/操作到达时进行更新！</li><li>如果节点之间的操作因网络断开等原因而丢失，我们可以建立一种重放操作的方法，<a href="https://github.com/elastic/elasticsearch/pull/19355" target="_blank" rel="noopener">这将大大加快恢复速度</a>！</li><li>我们可以建立一种在集群之间重放操作的方法！<a href="https://discuss.elastic.co/t/cross-data-center-replication/11670" target="_blank" rel="noopener">跨数据中心复制</a>！</li></ul><p>所有这些都需要打破一个小小的障碍：为索引操作添加序列号。很简单：我们只需要在主索引的每个操作中添加一个计数器！太简单了，我们看到社区成员和员工<a href="https://github.com/mikemccand/elasticsearch/commit/baf23885da83920e37bd87080bc2de5ef2d1a71e" target="_blank" rel="noopener">尝试</a>了好几次。但当我们<a href="https://github.com/elastic/elasticsearch/issues/10708" target="_blank" rel="noopener">层层剥开洋葱头时，我们发现它比最初看起来要复杂得多</a>。在我们开始讨论变更应用程序接口的实用性近6年后，我们仍然没有一个变更应用程序接口。原因何在？本博客的目的是分享幕后发生的事情，并就这个问题的答案提供一些见解。</p><p>在过去两年中，我们几乎从头开始重写了复制逻辑。我们从已知的<a href="https://www.microsoft.com/en-us/research/publication/pacifica-replication-in-log-based-distributed-storage-systems/" target="_blank" rel="noopener">学术算法</a>中汲取精华，同时确保我们仍能确保并行性，这正是Elasticsearch能够如此快速的原因：这是许多甚至所有传统共识算法都无法做到的。我们与分布式系统专家合作，为我们的<a href="https://github.com/elastic/elasticsearch-formal-models" target="_blank" rel="noopener">复制模型</a>建立了<a href="https://en.wikipedia.org/wiki/TLA%2B" target="_blank" rel="noopener">TLA+</a>规范。我们增加了大量<a href="https://github.com/elastic/elasticsearch/pull/17038" target="_blank" rel="noopener">测试</a>和<a href="https://github.com/elastic/elasticsearch/pull/18930" target="_blank" rel="noopener">测试基础设施</a>。</p><p>这篇博客必然是技术性的，因为我们会深入探讨Elasticsearch如何进行复制的一些核心内容。不过，我们的目的是通过解释/定义/链接一些术语（即使您可能已经理解了这些术语），让更多读者能够理解这些术语。首先，让我们深入了解Elasticsearch所面临的一些挑战。</p><h2 id="挑战">挑战</h2><p>在继续深入之前，我们必须先谈谈我们的复制模型以及它的重要性。在Elasticsearch数据索引中，数据被分割成所谓的&quot;分片&quot;，基本上就是索引的子分区。你可以有5个主分片（基本上是索引的5个子分区），每个主分片可以有任意数量的主分片副本（称为<strong>副本</strong>）。但重要的是，每个子分区只有一个&quot;<strong>主分片</strong>&quot;。主分片首先接受索引操作（索引操作包括&quot;添加文档&quot;或&quot;删除文档&quot;等操作），然后<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.0/docs-replication.html" target="_blank" rel="noopener">将索引操作复制到副本</a>。因此，在将每个操作转发给副本之前，不断递增计数器并为每个操作分配一个序列号是非常简单的。只要没有人重启服务器，网络正常运行时间达到100%，硬件不出现故障，没有长时间的Java垃圾回收事件，也没有人升级软件，这种简单易行的方法就能真正奏效。</p><p>但我们生活在现实世界中，当这些假设发生变化时，Elasticsearch就会进入&quot;故障&quot;模式和&quot;<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.0/indices-recovery.html" target="_blank" rel="noopener">恢复</a>&quot;过程。如果它们影响到运行主分片的节点，可能需要主分片停机，由另一个副本取而代之。由于变化来得突然，一些正在进行的索引操作可能尚未完全复制。如果您有2个或更多副本，其中一些操作可能只到达了其中一个副本，而没有到达另一个副本。更糟糕的是，由于Elasticsearch会并发索引文档（这也是Elasticsearch速度如此之快的原因之一！），每个副本都可能有不同的操作集，而这些操作集在另一个副本中并不存在。即使只运行一个副本（Elasticsearch的默认设置），也可能会出现问题。如果旧的主副本回来并被添加为副本，它可能包含从未复制到新主副本的操作。所有这些情况都有一个共同点：主节点失效后，分片上的操作历史可能会发生偏离，我们需要一些方法来解决这个问题。</p><h2 id="PrimaryTerms-Sequence-Numbers">PrimaryTerms &amp; Sequence Numbers</h2><p>我们采取的第一步是能够区分旧的和新的主分片。我们必须有一种方法来识别来自旧主分片的操作与来自新主分片的操作。此外，整个集群需要就此达成一致，以确保在出现问题时不会发生争执。这促使我们实现了<a href="https://github.com/elastic/elasticsearch/pull/14062" target="_blank" rel="noopener">主要术语</a>。这些主要术语是增量的，并在主分片被提升时更改。它们被持久化在<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.0/cluster-state.html" target="_blank" rel="noopener">集群状态</a>中，因此代表了集群所处的主分片的&quot;版本&quot;或&quot;生成&quot;。有了主要术语，操作历史中的任何冲突都可以通过查看操作的主要术语来解决。新术语优于旧术语。我们甚至可以<a href="https://github.com/elastic/elasticsearch/pull/17044" target="_blank" rel="noopener">开始拒绝那些太旧的操作</a>，避免混乱的情况发生。</p><p>一旦我们设置了主要术语的保护机制，我们添加了一个简单的计数器，并开始为每个操作分配一个来自该计数器的<strong>序列号</strong>。因此，这些序列号使我们能够了解在主分片上发生的索引操作的特定顺序，我们可以将其用于接下来几节中将要讨论的各种目的。您可以在响应中看到分配的序列号和主要术语：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ curl -H &apos;Content-Type: application/json&apos; -XPOST http://127.0.0.1:9200/foo/doc?pretty -d &apos;&#123; &quot;bar&quot;: &quot;baz&quot; &#125;&apos;</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">&quot;_index&quot;: &quot;foo&quot;,</span><br><span class="line">&quot;_type&quot;: &quot;doc&quot;,</span><br><span class="line">&quot;_id&quot;: &quot;MlDBm10BditXXu4kjj5E&quot;,</span><br><span class="line">&quot;_version&quot;: 1,</span><br><span class="line">&quot;result&quot;: &quot;created&quot;,</span><br><span class="line">&quot;_shards&quot;: &#123;</span><br><span class="line">&quot;total&quot;: 2,</span><br><span class="line">&quot;successful&quot;: 1,</span><br><span class="line">&quot;failed&quot;: 0</span><br><span class="line">&#125;,</span><br><span class="line">&quot;_seq_no&quot;: 19,</span><br><span class="line">&quot;_primary_term&quot;: 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意返回的响应中现在包含了_seq_no和_primary_term。</p><h3 id="Local-and-Global-Checkpoints">Local and Global Checkpoints</h3><p>有了主要术语和序列号，我们理论上有了检测分片之间差异并在主分片失败时重新对齐它们的工具。拥有主要术语为x的旧主分片可以通过删除主要术语为x且不存在于新主分片历史记录中的操作来恢复，并且具有更高主要术语的缺失操作可以索引到旧主分片中。</p><p>不幸的是，当您同时每秒索引数十万甚至数百万个事件时，比较数百万次操作的历史记录实际上是不切实际的。存储成本过高，直接比较所需的计算工作将耗时过长。为了解决这个问题，Elasticsearch维护了一个名为<strong>全局检查点</strong>的安全标记。全局检查点是一个序列号，我们知道所有活动分片的历史记录都至少与之对齐。换句话说，所有序列号低于全局检查点的操作都已经被所有活动分片处理，并在各自的历史记录中是相等的。这意味着在主分片失败后，我们只需要比较<a href="https://github.com/elastic/elasticsearch/pull/24841" target="_blank" rel="noopener">新主分片和任何剩余副本</a>之间最后已知的全局检查点以上的操作。当旧主分片恢复时，我们取其最后知道的全局检查点，并将其以上的操作与新主分片进行比较。这样，我们只需要比较需要比较的操作，而不是整个历史记录。</p><p>推进全局检查点的责任属于主分片。主分片通过跟踪副本上已完成的操作来实现这一点。一旦检测到所有副本已经超过给定的序列号，主分片将相应地更新全局检查点。分片副本不会一直跟踪所有操作，而是维护一个全局检查点的本地变体，称为<strong>本地检查点</strong>。本地检查点是一个序列号，该副本上处理了所有更低序列号的操作。每当副本确认（或ack）主节点的写操作时，它们也会向主节点提供更新的本地检查点。利用本地检查点，主节点就能更新全局检查点，然后在下一次索引操作中将全局检查点发送给所有分片副本。</p><p>下面的动画展示了在面对有损网络和突发故障等并发挑战时，随着序列号和全局/本地检查点的增加而发生的情况：</p><p><img data-src="/images/elasticsearch-sequence-ids-6-0/seqnos_animation.gif" alt></p><blockquote><p>当索引操作从主分片发送到副本时，我们会跟踪每个副本确认收到的最高序列号，并将其称为全局检查点。主分片会告诉所有副本全局检查点是多少。因此，如果主分片切换，我们只需要比较和可能重新处理自上次全局检查点以来的操作，而不是磁盘上的所有文件。</p></blockquote><p>全局检查点还具有另一个很好的特性——它代表了那些被<strong>保证会留下的操作</strong>（它们在所有活动分片的历史记录中），以及<strong>可能</strong>会被<strong>回滚</strong>的操作所在的区域，如果主分片在它们被完全复制并向用户确认之前恰好发生故障。这是一个微妙但重要的特性，对于未来的变更API或跨数据中心复制功能将是至关重要的。</p><h2 id="第一个好处：更快恢复">第一个好处：更快恢复</h2><p>在Elasticsearch6.0之前，我们跳过了实际恢复过程的工作原理。当Elasticsearch在副本处于离线状态后恢复副本时，它必须确保该副本与活动主分片完全相同。非活动分片具有<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.0/indices-synced-flush.html" target="_blank" rel="noopener">同步刷新标记</a>，以便快速进行验证，但具有活动索引的分片则没有任何保证。如果一个分片在仍有活动索引的情况下掉线，那么新的主分片将通过网络复制Lucene段（即磁盘上的文件）。如果这些分片很大，这可能是一个繁重且耗时的操作。这是因为在6.0之前，我们没有跟踪个别写操作（序列号），而在幕后，Lucene会将所有添加/更新/删除合并到更大的分段中，这样就无法恢复构成更改的单个操作…也就是说，除非你将<strong>事务日志</strong>保留一段时间。</p><p>这就是我们现在所做的：我们保留事务日志，直到它变的&quot;过大&quot;或&quot;过旧&quot;，不再有必要继续保留它。如果副本需要&quot;更新&quot;，我们会使用该副本已知的最后一个全局检查点，仅从主事务日志中回放相关更改，而不是昂贵的大文件复制。如果主事务日志&quot;过大&quot;或&quot;太旧&quot;而无法重放到副本，那么我们将回退到旧的基于文件的恢复方式。</p><p>如果您一直在运行一个大型集群，而该集群经常出现网络断开、重启、升级等情况，我们希望这将大大提高您的工作效率，因为您不必在分片恢复时长时间等待。</p><h3 id="须知">须知</h3><p>正如上一节中提到的，事务日志保留直到它&quot;过大&quot;或&quot;过旧&quot;而不再需要保留。我们如何确定什么是&quot;过大&quot;或&quot;过旧&quot;呢？当然是可配置的！在6.0中，我们引入了两个新的<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.0/index-modules-translog.html#_translog_settings" target="_blank" rel="noopener">事务日志</a>设置：</p><p>*<font color="DeepPink">index.translog.retention.size</font>:默认为512MB。如果事务日志超过这个大小，我们只保留这么多数据。<br>*<font color="DeepPink">index.translog.retention.age</font>:默认为12小时。超过这个时间段，我们将不再保留事务日志文件。</p><p><strong>这些设置很重要</strong>，因为它们影响新的、更快的恢复工作以及磁盘使用情况。较大的事务日志保留大小或较长的保留时间意味着您有更高的机会通过新的更快恢复来进行恢复，而不是依赖于旧的基于文件的恢复。然而，它们也伴随着一定的成本：这会增加磁盘利用率，而且请记住<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.0/index-modules-translog.html" target="_blank" rel="noopener">事务日志是按分片</a>进行的。举个实际的例子，如果您有20个索引，每个索引有5个主分片，并且在12小时内写入大量数据，那么可能会导致额外20<em>5</em>512mb=50GB的磁盘利用率，直到那12小时窗口过期为止。如果您在不同索引上有不同的恢复和大小需求，您可以根据需要在每个索引上进行调整。例如，如果您预计进行机器或节点维护，您可能需要考虑对事务日志保留窗口进行任何调整。</p><p>注意：在6.0之前，事务日志的大小在索引过程中也可以增长到512MB（默认值），根据<font color="DeepPink">index.translog.flush_threshold_size</font>设置。这意味着新的保留策略不会改变活动分片的存储需求。这一变化影响了停止索引的分片。现在，我们不清理事务日志，而是将其保留另外12小时。</p><h2 id="下一个优势：跨数据中心复制">下一个优势：跨数据中心复制</h2><p>正如文章开头提到的，如果我们能进行有序的索引操作，我们就能在Elasticsearch中做很多美妙的事情。虽然花了一些时间，但现在我们做到了。更快的恢复是我们决定构建的第一个用例：它允许我们测试我们添加的新功能。</p><p>但我们知道跨数据中心复制也是我们企业客户常常要求的功能，所以这是我们即将添加的另一个功能。这需要构建新的API、在复制之上增加新的监控功能，以及是的，还需要进行更多的测试和文档编写。</p><h3 id="还有更多工作要做">还有更多工作要做</h3><p>正如您在<a href="https://github.com/elastic/elasticsearch/issues/10708" target="_blank" rel="noopener">序列号GitHub问题</a>上看到的，我们在序列号功能上有了一个良好的开端，但仍有许多工作要做！我们认为迄今为止所做的工作代表了我们向前迈出的一大步，即使它还没有涵盖我们可以建立/围绕序列号的所有功能。如果您有兴趣继续关注我们的工作，请随时关注标有:<a href="https://github.com/elastic/elasticsearch/labels/%3ASequence%20IDs" target="_blank" rel="noopener">Sequence IDs</a>的ticket或PR，或直接在<a href="https://discuss.elastic.co/c/elastic-stack/elasticsearch/6" target="_blank" rel="noopener">讨论区</a>与我们联系！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;原文地址:&lt;a href=&quot;https://www.elastic.co/blog/elasticsearch-sequence-ids-6-0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.elastic.co/blog/elasticsearch-sequence-ids-6-0&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="ElasticSearch" scheme="https://jiankunking.com/categories/elasticsearch/"/>
    
    
    <category term="原创" scheme="https://jiankunking.com/tags/原创/"/>
    
    <category term="ElasticSearch" scheme="https://jiankunking.com/tags/elasticsearch/"/>
    
    <category term="翻译" scheme="https://jiankunking.com/tags/翻译/"/>
    
    <category term="Cluster" scheme="https://jiankunking.com/tags/cluster/"/>
    
    <category term="Sequence" scheme="https://jiankunking.com/tags/sequence/"/>
    
    <category term="PrimaryTerm" scheme="https://jiankunking.com/tags/primaryterm/"/>
    
    <category term="Checkpoint" scheme="https://jiankunking.com/tags/checkpoint/"/>
    
  </entry>
  
  <entry>
    <title>工具进阶：如何利用 MAT 找到问题发生的根本原因</title>
    <link href="https://jiankunking.com/how-to-use-mat-to-identify-the-root-cause-of-a-problem.html"/>
    <id>https://jiankunking.com/how-to-use-mat-to-identify-the-root-cause-of-a-problem.html</id>
    <published>2024-11-09T02:23:40.000Z</published>
    <updated>2024-11-09T02:53:13.884Z</updated>
    
    <content type="html"><![CDATA[<p>深入浅出 Java 虚拟机<br>作者： 李国</p><a id="more"></a><p>我们知道，在存储用户输入的密码时，会使用一些 hash 算法对密码进行加工，比如 SHA-1。这些信息同样不允许在日志输出里出现，必须做脱敏处理，但是对于一个拥有系统权限的攻击者来说，这些防护依然是不够的。攻击者可能会直接从内存中获取明文数据，尤其是对于 Java 来说，由于提供了 jmap 这一类非常方便的工具，可以把整个堆内存的数据 dump 下来。</p><p>比如，“我的世界”这一类使用 Java 开发的游戏，会比其他语言的游戏更加容易破解一些，所以我们在 JVM 中，如果把密码存储为 char 数组，其安全性会稍微高一些。</p><p>这是一把双刃剑，在保证安全的前提下，我们也可以借助一些外部的分析工具，帮助我们方便的找到问题根本。</p><p>有两种方式来获取内存的快照。我们前面提到过，通过配置一些参数，可以在发生 OOM 的时候，被动 dump 一份堆栈信息，这是一种；另一种，就是通过 jmap 主动去获取内存的快照。</p><p>jmap 命令在 Java 9 之后，使用 jhsdb 命令替代，它们在用法上，区别不大。注意，这些命令本身会占用操作系统的资源，在某些情况下会造成服务响应缓慢，所以不要频繁执行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">jmap -dump:format=b,file=heap.bin 37340</span><br><span class="line">jhsdb jmap  --binaryheap --pid  37340</span><br></pre></td></tr></table></figure><h2 id="1-工具介绍">1. 工具介绍</h2><p>有很多工具能够帮助我们来分析这份内存快照。在前面已多次提到 VisualVm 这个工具，它同样可以加载和分析这份 dump 数据，虽然比较“寒碜”。</p><p>专业的事情要有专业的工具来做，今天要介绍的是一款专业的开源分析工具，即 MAT。</p><p>MAT 工具是基于 Eclipse 平台开发的，本身是一个 Java 程序，所以如果你的堆快照比较大的话，则需要一台内存比较大的分析机器，并给 MAT 本身加大初始内存，这个可以修改安装目录中的 MemoryAnalyzer.ini 文件。</p><p>来看一下 MAT 工具的截图，主要的功能都体现在工具栏上了。其中，默认的启动界面，展示了占用内存最高的一些对象，并有一些常用的快捷方式。通常，发生内存泄漏的对象，会在快照中占用比较大的比重，分析这些比较大的对象，是我们切入问题的第一步。</p><p><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/MAT%E7%95%8C%E9%9D%A2.jpg" alt></p><p>点击对象，可以浏览对象的引用关系，这是一个非常有用的功能：</p><ul><li>outgoing references 对象的引出</li><li>incoming references 对象的引入</li></ul><p>path to GC Roots 这是快速分析的一个常用功能，显示和 GC Roots 之间的路径。<br><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/02.jpg" alt></p><p>另外一个比较重要的概念，就是<strong>浅堆</strong>（Shallow Heap）和<strong>深堆</strong>（Retained Heap），在 MAT 上经常看到这两个数值。</p><p><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/03.jpg" alt></p><p>浅堆代表了对象本身的内存占用，包括对象自身的内存占用，以及“为了引用”其他对象所占用的内存。</p><p>深堆是一个统计结果，会循环计算引用的具体对象所占用的内存。但是深堆和“对象大小”有一点不同，<strong>深堆指的是一个对象被垃圾回收后，能够释放的内存大小，这些被释放的对象集合，叫做保留集</strong>（Retained Set）。</p><p><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/04.png" alt></p><p>如上图所示，A 对象浅堆大小 1 KB，B 对象 2 KB，C 对象 100 KB。A 对象同时引用了 B 对象和 C 对象，但由于 C 对象也被 D 引用，所以 A 对象的深堆大小为 3 KB（1 KB + 2 KB）。</p><p>A 对象大小（1 KB + 2 KB + 100 KB）&gt; A 对象深堆 &gt; A 对象浅堆。</p><h2 id="2-代码示例">2. 代码示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.HashMap;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import java.util.stream.IntStream;</span><br><span class="line"></span><br><span class="line">public class Objects4MAT &#123;</span><br><span class="line"></span><br><span class="line">    static class A4MAT &#123;</span><br><span class="line">        B4MAT b4MAT = new B4MAT();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class B4MAT &#123;</span><br><span class="line">        C4MAT c4MAT = new C4MAT();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class C4MAT &#123;</span><br><span class="line">        List&lt;String&gt; list = new ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class DominatorTreeDemo1 &#123;</span><br><span class="line">        DominatorTreeDemo2 dominatorTreeDemo2;</span><br><span class="line"></span><br><span class="line">        public void setValue(DominatorTreeDemo2 value) &#123;</span><br><span class="line">            this.dominatorTreeDemo2 = value;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class DominatorTreeDemo2 &#123;</span><br><span class="line">        DominatorTreeDemo1 dominatorTreeDemo1;</span><br><span class="line"></span><br><span class="line">        public void setValue(DominatorTreeDemo1 value) &#123;</span><br><span class="line">            this.dominatorTreeDemo1 = value;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class Holder &#123;</span><br><span class="line">        DominatorTreeDemo1 demo1 = new DominatorTreeDemo1();</span><br><span class="line">        DominatorTreeDemo2 demo2 = new DominatorTreeDemo2();</span><br><span class="line"></span><br><span class="line">        Holder() &#123;</span><br><span class="line">            demo1.setValue(demo2);</span><br><span class="line">            demo2.setValue(demo1);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        private boolean aBoolean = false;</span><br><span class="line">        private char aChar = &apos;\0&apos;;</span><br><span class="line">        private short aShort = 1;</span><br><span class="line">        private int anInt = 1;</span><br><span class="line">        private long aLong = 1L;</span><br><span class="line">        private float aFloat = 1.0F;</span><br><span class="line">        private double aDouble = 1.0D;</span><br><span class="line">        private Double aDouble_2 = 1.0D;</span><br><span class="line">        private int[] ints = new int[2];</span><br><span class="line">        private String string = &quot;1234&quot;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Runnable runnable = () -&gt; &#123;</span><br><span class="line">        Map&lt;String, A4MAT&gt; map = new HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        IntStream.range(0, 100).forEach(i -&gt; &#123;</span><br><span class="line">            byte[] bytes = new byte[1024 * 1024];</span><br><span class="line">            String str = new String(bytes).replace(&apos;\0&apos;, (char) i);</span><br><span class="line">            A4MAT a4MAT = new A4MAT();</span><br><span class="line">            a4MAT.b4MAT.c4MAT.list.add(str);</span><br><span class="line"></span><br><span class="line">            map.put(i + &quot;&quot;, a4MAT);</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        Holder holder = new Holder();</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            //sleep forever , retain the memory</span><br><span class="line">            Thread.sleep(Integer.MAX_VALUE);</span><br><span class="line">        &#125; catch (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    void startHugeThread() throws Exception &#123;</span><br><span class="line">        new Thread(runnable, &quot;huge-thread&quot;).start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Objects4MAT objects4MAT = new Objects4MAT();</span><br><span class="line">        objects4MAT.startHugeThread();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-1-代码介绍">2.1. 代码介绍</h3><p>我们以一段代码示例 Objects4MAT，来具体看一下 MAT 工具的使用。代码创建了一个新的线程 “huge-thread”，并建立了一个引用的层级关系，总的内存大约占用 100 MB。同时，demo1 和 demo2 展示了一个循环引用的关系。最后，使用 sleep 函数，让线程永久阻塞住，此时整个堆处于一个相对“静止”的状态。</p><p><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/05.png" alt></p><p>如果你是在本地启动的示例代码，则可以使用 Accquire 的方式来获取堆快照。</p><p><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/06.jpg" alt></p><h3 id="2-2-内存泄漏检测">2.2. 内存泄漏检测</h3><p>如果问题特别突出，则可以通过 Find Leaks 菜单快速找出问题。<br><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/07.jpg" alt></p><p>如下图所示，展示了名称叫做 huge-thread 的线程，持有了超过 96% 的对象，数据被一个 HashMap 所持有。</p><p><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/08.jpg" alt></p><p>对于特别明显的内存泄漏，在这里能够帮助我们迅速定位，但通常内存泄漏问题会比较隐蔽，我们需要更加复杂的分析。</p><p>###　2.3. 支配树视图<br>支配树视图对数据进行了归类，体现了对象之间的依赖关系。如图，我们通常会根据“深堆”进行倒序排序，可以很容易的看到占用内存比较高的几个对象，点击前面的箭头，即可一层层展开支配关系。</p><p>图中显示的是其中的 1 MB 数据，从左侧的 inspector 视图，可以看到这 1 MB 的 byte 数组具体内容。<br><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/09.jpg" alt></p><p>从支配树视图同样能够找到我们创建的两个循环依赖，但它们并没有显示这个过程。<br><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/10.jpg" alt></p><p>支配树视图的概念有一点点复杂，我们只需要了解这个概念即可。</p><p><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/11.png" alt></p><p>如上图，左边是引用关系，右边是支配树视图。可以看到 A、B、C 被当作是“虚拟”的根，支配关系是可传递的，因为 C 支配 E，E 支配 G，所以 C 也支配 G。</p><p>另外，到对象 C 的路径中，可以经过 A，也可以经过 B，因此对象 C 的直接支配者也是根对象。同理，对象 E 是 H 的支配者。</p><p>我们再来看看比较特殊的 D 和 F。对象 F 与对象 D 相互引用，因为到对象 F 的所有路径必然经过对象 D，因此，对象 D 是对象 F 的直接支配者。</p><p>可以看到支配树视图并不一定总是能看到对象的真实应用关系，但对我们分析问题的影响并不是很大。</p><p>这个视图是非常好用的，甚至可以根据 package 进行归类，对目标类的查找也是非常快捷的。</p><p><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/12.jpg" alt></p><p>编译下面这段代码，可以展开视图，实际观测一下支配树，这和我们上面介绍的是一致的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">public class DorminatorTreeDemo &#123;</span><br><span class="line">    static class A &#123;</span><br><span class="line">        C c;</span><br><span class="line"></span><br><span class="line">        byte[] data = new byte[1024 * 1024 * 2];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class B &#123;</span><br><span class="line">        C c;</span><br><span class="line">        byte[] data = new byte[1024 * 1024 * 3];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class C &#123;</span><br><span class="line">        D d;</span><br><span class="line">        E e;</span><br><span class="line">        byte[] data = new byte[1024 * 1024 * 5];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class D &#123;</span><br><span class="line">        F f;</span><br><span class="line">        byte[] data = new byte[1024 * 1024 * 7];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class E &#123;</span><br><span class="line">        G g;</span><br><span class="line">        byte[] data = new byte[1024 * 1024 * 11];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class F &#123;</span><br><span class="line">        D d;</span><br><span class="line">        H h;</span><br><span class="line">        byte[] data = new byte[1024 * 1024 * 13];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class G &#123;</span><br><span class="line">        H h;</span><br><span class="line">        byte[] data = new byte[1024 * 1024 * 17];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class H &#123;</span><br><span class="line">        byte[] data = new byte[1024 * 1024 * 19];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    A makeRef(A a, B b) &#123;</span><br><span class="line">        C c = new C();</span><br><span class="line">        D d = new D();</span><br><span class="line">        E e = new E();</span><br><span class="line">        F f = new F();</span><br><span class="line">        G g = new G();</span><br><span class="line">        H h = new H();</span><br><span class="line">        a.c = c;</span><br><span class="line">        b.c = c;</span><br><span class="line">        c.e = e;</span><br><span class="line">        c.d = d;</span><br><span class="line">        d.f = f;</span><br><span class="line">        e.g = g;</span><br><span class="line">        f.d = d;</span><br><span class="line">        f.h = h;</span><br><span class="line">        g.h = h;</span><br><span class="line">        return a;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static A a = new A();</span><br><span class="line">    static B b = new B();</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        new DorminatorTreeDemo().makeRef(a, b);</span><br><span class="line"></span><br><span class="line">        Thread.sleep(Integer.MAX_VALUE);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/13.jpg" alt></p><h3 id="2-4-线程视图">2.4. 线程视图</h3><p>想要看具体的引用关系，可以通过线程视图。我们在第 5 讲，就已经了解了线程其实是可以作为 GC Roots 的。如图展示了线程内对象的引用关系，以及方法调用关系，相对比 jstack 获取的栈 dump，我们能够更加清晰地看到内存中具体的数据。</p><p>如下图，我们找到了 huge-thread，依次展开找到 holder 对象，可以看到循环依赖已经陷入了无限循环的状态。这在查看一些 Java 对象的时候，经常发生，不要感到奇怪。</p><p><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/14.jpg" alt></p><h3 id="2-5-柱状图视图">2.5. 柱状图视图</h3><p>我们返回头来再看一下柱状图视图，可以看到除了对象的大小，还有类的实例个数。结合 MAT 提供的不同显示方式，往往能够直接定位问题。也可以通过正则过滤一些信息，我们在这里输入 MAT，过滤猜测的、可能出现问题的类，可以看到，创建的这些自定义对象，不多不少正好一百个。</p><p><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/15.jpg" alt></p><p>右键点击类，然后选择 incoming，这会列出所有的引用关系。</p><p><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/16.jpg" alt></p><p>再次选择某个引用关系，然后选择菜单“Path To GC Roots”，即可显示到 GC Roots 的全路径。通常在排查内存泄漏的时候，会选择排除虚弱软等引用。</p><p><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/17.jpg" alt></p><p>使用这种方式，即可在引用之间进行跳转，方便的找到所需要的信息。<br><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/18.jpg" alt></p><p>再介绍一个比较高级的功能。</p><p>我们对于堆的快照，其实是一个**“瞬时态”**，有时候仅仅分析这个瞬时状态，并不一定能确定问题，这就需要对两个或者多个快照进行对比，来确定一个增长趋势。<br><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/19.jpg" alt></p><p>可以将代码中的 100 改成 10 或其他数字，再次 dump 一份快照进行比较。如图，通过分析某类对象的增长，即可辅助问题定位。</p><h2 id="3-高级功能—OQL">3. 高级功能—OQL</h2><p>MAT 支持一种类似于 SQL 的查询语言 OQL（Object Query Language），这个查询语言 VisualVM 工具也支持。<br><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/20.jpg" alt></p><p>以下是几个例子，你可以实际实践一下。</p><p>查询 A4MAT 对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM  Objects4MAT$A4MAT</span><br></pre></td></tr></table></figure><p>正则查询 MAT 结尾的对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM &quot;.*MAT&quot;</span><br></pre></td></tr></table></figure><p>查询 String 类的 char 数组：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SELECT OBJECTS s.value FROM java.lang.String s </span><br><span class="line">SELECT OBJECTS mat.b4MAT FROM  Objects4MAT$A4MAT mat</span><br></pre></td></tr></table></figure><p>根据内存地址查找对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from 0x55a034c8</span><br></pre></td></tr></table></figure><p>使用 INSTANCEOF 关键字，查找所有子类：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM INSTANCEOF java.util.AbstractCollection</span><br></pre></td></tr></table></figure><p>查询长度大于 1000 的 byte 数组：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM byte[] s WHERE s.@length&gt;1000</span><br></pre></td></tr></table></figure><p>查询包含 java 字样的所有字符串：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM java.lang.String s WHERE toString(s) LIKE &quot;.*java.*&quot;</span><br></pre></td></tr></table></figure><p>查找所有深堆大小大于 1 万的对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM INSTANCEOF java.lang.Object o WHERE o.@retainedHeapSize&gt;10000</span><br></pre></td></tr></table></figure><p>如果你忘记这些属性的名称的话，MAT 是可以自动补全的。<br><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/21.jpg" alt></p><p>OQL 有比较多的语法和用法，若想深入了解，<a href="https://tech.novosoft-us.com/products/oql_book.htm" target="_blank" rel="noopener">可参考这里</a>。</p><p>一般，我们使用上面这些简单的查询语句就够用了。</p><p>OQL 还有一个好处，就是可以分享。如果你和同事同时在分析一个大堆，不用告诉他先点哪一步、再点哪一步，共享给他一个 OQL 语句就可以了。</p><p>如下图，MAT 贴心的提供了复制 OQL 的功能，但是用在其他快照上，不会起作用，因为它复制的是如下的内容。<br><img data-src="/images/how-to-use-mat-to-identify-the-root-cause-of-a-problem/22.jpg" alt></p><h2 id="4-小结">4. 小结</h2><p>这一讲我们介绍了 MAT 工具的使用，其是用来分析内存快照的；在最后，简要介绍了 OQL 查询语言。</p><p>在 Java 9 以前的版本中，有一个工具 jhat，可以以 html 的方式显示堆栈信息，但和 VisualVm 一样，都太过于简陋，推荐使用 MAT 工具。</p><p>我们把问题设定为内存泄漏，但其实 OOM 或者频繁 GC 不一定就是内存泄漏，它也可能是由于某次或者某批请求频繁而创建了大量对象，所以一些严重的、频繁的 GC 问题也能在这里找到原因。有些情况下，占用内存最多的对象，并不一定是引起内存泄漏问题的元凶，但我们也有一个比较通用的分析过程。</p><p>并不是所有的堆都值得分析的，我们在做这个耗时的分析之前，需要有个依据。比如，经过初步调优之后，GC 的停顿时间还是较长，则需要找到频繁 GC 的原因；再比如，我们发现了内存泄漏，需要找到是谁在搞鬼。</p><p>首先，我们高度关注快照载入后的初始分析，占用内存高的 topN 对象，大概率是问题产生者。</p><p>对照自己的代码，首先要分析的，就是产生这些大对象的逻辑。举几个实际发生的例子。有一个 Spring Boot 应用，由于启用了 Swagger 文档生成器，但是由于它的 API 关系非常复杂，嵌套层次又非常深（每次要产生几百 M 的文档！），结果请求几次之后产生了内存溢出，这在 MAT 上就能够一眼定位到问题；而另外一个应用，在读取数据库的时候使用了分页，但是 pageSize 并没有做一些范围检查，结果在请求一个较大分页的时候，使用 fastjson 对获取的数据进行加工，直接 OOM。</p><p>如果不能通过大对象发现问题，则需要对快照进行深入分析。使用柱状图和支配树视图，配合引入引出和各种排序，能够对内存的使用进行整体的摸底。由于我们能够看到内存中的具体数据，排查一些异常数据就容易得多。</p><p>可以在程序运行的不同时间点，获取多份内存快照，对比之后问题会更加容易发现。我们还是用一个例子来看。有一个应用，使用了 Kafka 消息队列，开了一般大小的消费缓冲区，Kafka 会复用这个缓冲区，按理说不应该有内存问题，但是应用却频繁发生 GC。通过对比请求高峰和低峰期间的内存快照，我们发现有工程师把消费数据放入了另外一个 “内存队列”，写了一些画蛇添足的代码，结果在业务高峰期一股脑把数据加载到了内存中。</p><p>上面这些问题通过分析业务代码，也不难发现其关联性。问题如果非常隐蔽，则需要使用 OQL 等语言，对问题一一排查、确认。</p><p>可以看到，上手 MAT 工具是有一定门槛的，除了其操作模式，还需要对我们前面介绍的理论知识有深入的理解，比如 GC Roots、各种引用级别等。</p><p>在很多场景，MAT 并不仅仅用于内存泄漏的排查。由于我们能够看到内存上的具体数据，在排查一些难度非常高的 bug 时，MAT 也有用武之地。比如，因为某些脏数据，引起了程序的执行异常，此时，想要找到它们，不要忘了 MAT 这个老朋友。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;深入浅出 Java 虚拟机&lt;br&gt;
作者： 李国&lt;/p&gt;</summary>
    
    
    
    <category term="Java" scheme="https://jiankunking.com/categories/java/"/>
    
    
    <category term="Java" scheme="https://jiankunking.com/tags/java/"/>
    
    <category term="Heap" scheme="https://jiankunking.com/tags/heap/"/>
    
    <category term="Dump" scheme="https://jiankunking.com/tags/dump/"/>
    
    <category term="MAT" scheme="https://jiankunking.com/tags/mat/"/>
    
  </entry>
  
  <entry>
    <title>一次线程池使用错误导致的问题</title>
    <link href="https://jiankunking.com/a-problem-caused-by-a-thread-pool-usage-error.html"/>
    <id>https://jiankunking.com/a-problem-caused-by-a-thread-pool-usage-error.html</id>
    <published>2024-11-02T02:31:01.000Z</published>
    <updated>2025-01-20T00:30:54.820Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>记录一次服务线程数量异常问题的排查过程</p></blockquote><a id="more"></a><h2 id="背景">背景</h2><p>通过监控发现一个服务的线程数异常多<br><img data-src="/images/a-problem-caused-by-a-thread-pool-usage-error/%E7%BA%BF%E7%A8%8B%E6%95%B0.png" alt></p><p>同期CPU 内存 网络连接都没有什么异常。</p><h2 id="排查">排查</h2><p>第一个反应就是查看线程栈</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">&quot;pool-2493-thread-3&quot; #3718833 prio=5 os_prio=0 tid=0x00007f1610041000 nid=0x38bff6 waiting on condition [0x00007f18ba29e000]</span><br><span class="line">   java.lang.Thread.State: WAITING (parking)</span><br><span class="line">at sun.misc.Unsafe.park(Native Method)</span><br><span class="line">- parking to wait for  &lt;0x00000007a044cce0&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)</span><br><span class="line">at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)</span><br><span class="line">at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)</span><br><span class="line">at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">at java.lang.Thread.run(Thread.java:748)</span><br><span class="line"></span><br><span class="line">&quot;pool-2493-thread-2&quot; #3718832 prio=5 os_prio=0 tid=0x00007f161003f800 nid=0x38bff5 waiting on condition [0x00007f18bd502000]</span><br><span class="line">   java.lang.Thread.State: WAITING (parking)</span><br><span class="line">at sun.misc.Unsafe.park(Native Method)</span><br><span class="line">- parking to wait for  &lt;0x00000007a044cce0&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)</span><br><span class="line">at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)</span><br><span class="line">at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)</span><br><span class="line">at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">at java.lang.Thread.run(Thread.java:748)</span><br><span class="line"></span><br><span class="line">&quot;pool-2493-thread-1&quot; #3718823 prio=5 os_prio=0 tid=0x00007f161003e000 nid=0x38bff3 waiting on condition [0x00007f18c2725000]</span><br><span class="line">   java.lang.Thread.State: WAITING (parking)</span><br><span class="line">at sun.misc.Unsafe.park(Native Method)</span><br><span class="line">- parking to wait for  &lt;0x00000007a044cce0&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)</span><br><span class="line">at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)</span><br><span class="line">at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)</span><br><span class="line">at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure><p>本来想从线程栈中找到线程是从哪里创建的，但从线程栈现在只能看到线程是由ThreadPoolExecutor创建的，只能看到线程运行态的东西。</p><p>那么heap中会有吗？</p><table><thead><tr><th>名词</th><th>值</th></tr></thead><tbody><tr><td>Object/Stack Frame</td><td>java.lang.Thread @ 0x7a044af30</td></tr><tr><td>Name</td><td>pool-2493-thread-3</td></tr><tr><td>Shallow Heap</td><td>120</td></tr><tr><td>Retained Heap</td><td>1,728</td></tr><tr><td>Max. Locals’ Retained Heap</td><td></td></tr><tr><td>Context Class Loader</td><td>org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader @ 0x71d665028</td></tr><tr><td>Is Daemon</td><td>false</td></tr><tr><td>State</td><td>[alive, parked, waiting, waiting indefinitely]</td></tr><tr><td>State value</td><td>0x291</td></tr></tbody></table><p>这里只是比线程栈多了一个Context Class Loader别的没有什么有用的信息。</p><p>回看下线程栈信息，发现一个规律<code>[pool-2493-thread-1]</code> pool后面的数字的递增的,但thread后面数字基本都是1 2 3</p><p>从这篇文章<a href="https://www.baeldung.com/java-naming-executor-service-thread" target="_blank" rel="noopener">Naming Executor Service Threads and Thread Pool in Java</a> 可以推断出，应该在代码中有位置初始化了线程池，并且核心线程数是3，在代码中搜索了一下，还真找到了这么一段代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">// import cn.hutool.core.thread.ThreadUtil;</span><br><span class="line"></span><br><span class="line">    @GetMapping(value = &quot;/jiankunkingTransfer&quot;)</span><br><span class="line">    public Map&lt;String, Object&gt; jiankunkingTransfer(String date) &#123;</span><br><span class="line">        Executor executor = ThreadUtil.newExecutor(3, 6, 30000);</span><br><span class="line">        // todo 执行一些逻辑处理</span><br><span class="line">        for (Object c : json.getJSONArray(&quot;data&quot;)) &#123;</span><br><span class="line">            executor.execute(() -&gt; &#123;</span><br><span class="line">               // todo 执行一些逻辑处理</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">// 注意这里没有用CountDownLatch来await()</span><br><span class="line">// 也没有shutdown()</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>一开始以为局部创建的线程会被GC 回收掉，然后通过<a href="https://arthas.aliyun.com/doc/vmtool.html#%E5%BC%BA%E5%88%B6-gc" target="_blank" rel="noopener">Arthas vmtool</a>强制GC了一把，发现线程并没有减少。</p><p>为啥没有回收呢？</p><p>=&gt;<br><a href="https://stackoverflow.com/questions/24953978/does-an-executorservice-get-garbage-collected-when-out-of-scope" target="_blank" rel="noopener">Does an ExecutorService get garbage collected when out of scope?</a></p><p>=&gt; 这里提到了线程池的shutdown()，但此时的我还没有将重点放在这里。<br>=&gt; 导致我认为没有被回收的线程是全局变量，一顿排查操作并没有发现全局且很大的线程池<br>=&gt; 再次搜索找到查找答案：<br><a href="https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html" target="_blank" rel="noopener">https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html</a></p><p>在JDK的官方文档中找到这么一段话</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Finalization</span><br><span class="line"> A pool that is no longer referenced in a program AND has no remaining threads will be shutdown automatically.</span><br><span class="line"> If you would like to ensure that unreferenced pools are reclaimed even if users forget to call shutdown(), </span><br><span class="line"> then you must arrange that unused threads eventually die, by setting appropriate keep-alive times, using a </span><br><span class="line"> lower bound of zero core threads and/or setting allowCoreThreadTimeOut(boolean).</span><br></pre></td></tr></table></figure><p>=&gt;<br>程序中不再引用且没有剩余线程的地将自动shutdown。如果您想确保即使用户忘记调用shutdown也能回收未引用的地,那么您必须通过设置适当的保持活动时间、使用零核心线程的下限和/或设置allowCoreThreadTimeOut(boolean)来安排未使用的线程最终死亡allowCoreThreadTimeOut(boolean)</p><p>那么为什么退出作用域了，线程不会回收呢？<br>=&gt;<br><a href="https://stackoverflow.com/questions/7728102/why-doesnt-this-thread-pool-get-garbage-collected" target="_blank" rel="noopener">https://stackoverflow.com/questions/7728102/why-doesnt-this-thread-pool-get-garbage-collected</a></p><h2 id="结论">结论</h2><p>通过官方文档可以看出解决这个问题有下面几种方式</p><ol><li>线程池 设置成 成员变量 (<font color="DeepPink"><strong>推荐</strong></font>)</li><li>使用threadPoolExecutor.shutdown(); 方法关闭线程池(会等待任务执行结束)</li><li>设置核心线程超时关闭 allowCoreThreadTimeOut(true);</li><li>核心线程数设置为0, 但在使用上会带来别的麻烦(略)</li></ol><p>问题的处理方式已经清楚了，下面再来看下shutdown()跟线程池默认的命名规则</p><h2 id="拓展">拓展</h2><h3 id="shutdown-实现">shutdown()实现</h3><p>shutdown就是将线程池状态设置为SHUTDOWN，然后中断所有空闲(空闲即阻塞在队列上)的线程，最终设置线程池状态为Terminated。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Initiates an orderly shutdown in which previously submitted</span><br><span class="line"> * tasks are executed, but no new tasks will be accepted.</span><br><span class="line"> * Invocation has no additional effect if already shut down.</span><br><span class="line"> *</span><br><span class="line"> * &lt;p&gt;This method does not wait for previously submitted tasks to</span><br><span class="line"> * complete execution.  Use &#123;@link #awaitTermination awaitTermination&#125;</span><br><span class="line"> * to do that.</span><br><span class="line"> *</span><br><span class="line"> * @throws SecurityException &#123;@inheritDoc&#125;</span><br><span class="line"> */</span><br><span class="line">public void shutdown() &#123;</span><br><span class="line">    </span><br><span class="line">    final ReentrantLock mainLock = this.mainLock;</span><br><span class="line">    // 加锁(全局锁)</span><br><span class="line">    mainLock.lock();</span><br><span class="line">    try &#123;</span><br><span class="line">        // 权限校验，安全策略相关判断</span><br><span class="line">        checkShutdownAccess();</span><br><span class="line">        // 设置线程池状态为SHUTDOWN。</span><br><span class="line">        advanceRunState(SHUTDOWN);</span><br><span class="line">        // 中断所有的空闲的工作线程</span><br><span class="line">        interruptIdleWorkers();</span><br><span class="line">        // 空方法，留给子类实现</span><br><span class="line">        onShutdown(); // hook for ScheduledThreadPoolExecutor</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">        // 解锁</span><br><span class="line">        mainLock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">    tryTerminate();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a href="https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html#shutdown--" target="_blank" rel="noopener">https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html#shutdown–</a></p><h3 id="线程池默认的命名规则">线程池默认的命名规则</h3><p>java.util.concurrent.DefaultThreadFactory</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">static class DefaultThreadFactory implements ThreadFactory &#123;</span><br><span class="line">        private static final AtomicInteger poolNumber = new AtomicInteger(1);</span><br><span class="line">        private final ThreadGroup group;</span><br><span class="line">        private final AtomicInteger threadNumber = new AtomicInteger(1);</span><br><span class="line">        private final String namePrefix;</span><br><span class="line"></span><br><span class="line">        DefaultThreadFactory() &#123;</span><br><span class="line">            SecurityManager s = System.getSecurityManager();</span><br><span class="line">            group = (s != null) ? s.getThreadGroup() :</span><br><span class="line">                                  Thread.currentThread().getThreadGroup();</span><br><span class="line">            namePrefix = &quot;pool-&quot; +</span><br><span class="line">                          poolNumber.getAndIncrement() +</span><br><span class="line">                         &quot;-thread-&quot;;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        public Thread newThread(Runnable r) &#123;</span><br><span class="line">            Thread t = new Thread(group, r,</span><br><span class="line">                                  namePrefix + threadNumber.getAndIncrement(),</span><br><span class="line">                                  0);</span><br><span class="line">            if (t.isDaemon())</span><br><span class="line">                t.setDaemon(false);</span><br><span class="line">            if (t.getPriority() != Thread.NORM_PRIORITY)</span><br><span class="line">                t.setPriority(Thread.NORM_PRIORITY);</span><br><span class="line">            return t;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>从代码中可以看出默认线程的名字是按照=&gt; <font color="DeepPink"><strong>pool-线程池编号(从1开始自增)-thread-线程池中线程数量(从1开始自增)</strong></font></p><p>如何自定义线程名字可以参考下面这个<br><a href="https://stackoverflow.com/questions/6113746/naming-threads-and-thread-pools-of-executorservice" target="_blank" rel="noopener">https://stackoverflow.com/questions/6113746/naming-threads-and-thread-pools-of-executorservice</a></p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;记录一次服务线程数量异常问题的排查过程&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="Java" scheme="https://jiankunking.com/categories/java/"/>
    
    
    <category term="Executor" scheme="https://jiankunking.com/tags/executor/"/>
    
    <category term="ThreadPool" scheme="https://jiankunking.com/tags/threadpool/"/>
    
    <category term="JDK" scheme="https://jiankunking.com/tags/jdk/"/>
    
    <category term="Java" scheme="https://jiankunking.com/tags/java/"/>
    
    <category term="原创" scheme="https://jiankunking.com/tags/原创/"/>
    
  </entry>
  
  <entry>
    <title>一次Fegin CPU占用过高导致的事故</title>
    <link href="https://jiankunking.com/an-accident-caused-by-excessive-cpu-usage-of-fegin.html"/>
    <id>https://jiankunking.com/an-accident-caused-by-excessive-cpu-usage-of-fegin.html</id>
    <published>2024-10-13T01:11:01.000Z</published>
    <updated>2024-10-28T07:03:50.440Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>记录一下<br>一次应用事故分析、排查、处理</p></blockquote><a id="more"></a><h2 id="背景介绍">背景介绍</h2><p>9号上午收到CPU告警，同时业务反馈依赖该服务的上游服务接口响应耗时太长</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">应用告警-CPU使用率 告警变更</span><br><span class="line">【WARNING】项目XXX,集群qd-aliyun,分区bbbb-prod,应用customer,实例customer-6fb6448688-m47jz, POD实例CPU请求使用率 &gt;= 90.000000% 当前值138.4971051199925%</span><br><span class="line">发生时间:2024/10/09 11:17:33</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">项目XXX,集群qd-aliyun,分区bbbb-prod,应用customer,实例customer-6fb6448688-28pvs, POD实例CPU请求使用率 &gt;= 90.000000% 当前值157.7076205766934%告警已恢复</span><br><span class="line">发生时间: 2024/10/09 11:06:33</span><br><span class="line">恢复时间: 2024/10/09 12:24:33</span><br></pre></td></tr></table></figure><h3 id="服务访问量">服务访问量</h3><p>单实例峰值QPS100左右</p><blockquote><p>为啥要关注QPS，因为QPS100不应该消耗这么多CPU啊，而且请求、响应体都不大。</p></blockquote><p><img data-src="/images/an-accident-caused-by-excessive-cpu-usage-of-fegin/%E7%BD%91%E7%BB%9C%E6%B5%81%E5%85%A5%E6%83%85%E5%86%B5.png" alt><br><img data-src="/images/an-accident-caused-by-excessive-cpu-usage-of-fegin/%E7%BD%91%E7%BB%9C%E6%B5%81%E5%87%BA%E6%83%85%E5%86%B5.png" alt></p><h3 id="POD监控">POD监控</h3><p>POD配额</p><ul><li>CPU请求 2 Core CPU上限 3 Core</li><li>内存请求 7GiB 内存上限 9GiB</li></ul><p><img data-src="/images/an-accident-caused-by-excessive-cpu-usage-of-fegin/POD_CPU.png" alt><br><img data-src="/images/an-accident-caused-by-excessive-cpu-usage-of-fegin/POD_TCP%E9%93%BE%E6%8E%A5.png" alt><br><img data-src="/images/an-accident-caused-by-excessive-cpu-usage-of-fegin/POD_%E7%BA%BF%E7%A8%8B%E6%95%B0.png" alt></p><p>从图中可以看出</p><ul><li>CPU负载一直很高</li><li>TCP链接及线程数从11点40开始陡峭上升</li></ul><h3 id="Arms">Arms</h3><p>看下Trace监控发现，耗时主要是customer通过fegin调用外围接口导致的。</p><p><img data-src="/images/an-accident-caused-by-excessive-cpu-usage-of-fegin/%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83Arms%E7%9B%91%E6%8E%A7_%E6%A0%87%E8%AF%86%E5%8D%A0%E7%94%A8CPU%E8%BF%87%E9%AB%98%E6%8E%A5%E5%8F%A3.png" alt></p><h3 id="临时方案">临时方案</h3><p>临时处理方案：扩实例并增加CPU配置。</p><h2 id="根因分析">根因分析</h2><blockquote><p>此处略过排查三方接口跟开放平台网关的过程，此处的结论是：依赖的三方接口跟开放平台网关没有问题。<br>为啥会先排查三方接口跟开放平台网关是因为中Trace上来看是调用三方接口响应时间过长。</p></blockquote><p><img data-src="/images/an-accident-caused-by-excessive-cpu-usage-of-fegin/getAddressWarehouse_%E6%94%BE%E5%A4%A7.png" alt><br>从Arms图看可以看出</p><ul><li>CPU耗时集中在fegin调用的Decoder、Encoder</li><li>Decoder、Encoder耗时都集中在<ul><li>HttpMessageConverters#getDefaultConverters()=&gt;</li><li>WebMvcConfigurationSupport#addDefaultHttpMessageConverters=&gt;</li><li>…(具体调用链看下方摘要)</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">feign.ReflectiveFeign$BuildTemplateByResolvingArgs.create(Object[]) (14.37%, 1.43 minutes)</span><br><span class="line">feign.ReflectiveFeign$BuildEncodedTemplateFromArgs.reesolve(Object[], RequestTemplate, Map) (14.37%, 1.43minutes)</span><br><span class="line">org.springframework.cloud.openfeign.support.SpringEndcoder.encode(Object, Type, RequestTemplate) (14.28%,1.42 minutes)</span><br><span class="line">com.jiankunking.common.core.feign.FeignClientsConfig$$ambda$938.56729293.get0bject() (13.98%, 1.39 minutes </span><br><span class="line">com.jiankunking.common.core.feign.FeignClientsConfig.lambda$feignEncoder$2() (13.98%, 1.39 minutees)</span><br><span class="line">org.springframework.boot.autoconfigure.http.HttpmessaageConverters.&lt;init&gt;(HttpMessageConverter[]) (12.03%,1.19 minutes)</span><br><span class="line">prg.springframework.boot.autoconfigure.http.Http.HttpMessageConverters.&lt;init&gt;(Collection) (12.03%, 119 minutes)</span><br><span class="line">org.springframework.boot.autoconfigure.http.HttpmessaageConverters.&lt;init&gt;(boolean, Collection) (12.03%, 1.19 minutes)</span><br><span class="line">prg.springframework.boot.autoconfigure.http.Http.HttpMessageConverters.getDefaultConverters()(12.02%, 1.19 minutes</span><br><span class="line">org.springframework.boot.autoconfigure.http.HttpmessageConverters$1.defaultMessageConverters() (12.02%, 119 minutes)</span><br><span class="line">org.springframework.web.servlet.config.annotation.WebMvcConfigurationSupport.getMessageConverters() (12.02%, 1.19 minutes)</span><br><span class="line">org.springframework.web.servlet.config.annotation. WebMvcConfigurationSupport.addDefaultHttpMessageConverters(List) (12.02%, 1</span><br><span class="line">org.springframework.http.converter.json.Jackson2ObjectMapperBuilder.build() (5.93%, 0.59 minutes)</span><br><span class="line">org.springframework.http.converter.json.Jackson2ObjectMapperBuilder.configure(ObjectMapper)(5.91%, 0.59 minutes)</span><br><span class="line">org.springframework.http.converter.json.Jackson2Objec:tMapperBuilder.registerWellKnownModulesIfAvailable(Map)(5.89%,0.58 min</span><br><span class="line">org.springframework.util.ClassUtils.forName(String, CClassLoader)(5.84%, 0.58 minutes)</span><br><span class="line">java.lang.Class.forName(String, boolean, Classloader) (5.83%, 0.58 minutes)</span><br><span class="line">java.lang.Class.forName0(String, boolean, ClassLoader, Class) (5.83%, 0.58 minutes)</span><br><span class="line">......</span><br></pre></td></tr></table></figure><h3 id="自定义Encoder、Decoder">自定义Encoder、Decoder</h3><h4 id="Encoder">Encoder</h4><p>看下jiankunking.common.core.feign.FeignClientsConfig中的Encoder</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public Encoder feignEncoder() &#123;</span><br><span class="line">    ObjectFactory&lt;HttpMessageConverters&gt; objectFactory = () -&gt; new HttpMessageConverters(new RMappingJackson2HttpMessageConverter());</span><br><span class="line">    return new SpringEncoder(objectFactory);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public class RMappingJackson2HttpMessageConverter extends MappingJackson2HttpMessageConverter &#123;</span><br><span class="line"></span><br><span class="line">    public RMappingJackson2HttpMessageConverter(ObjectMapper objectMapper) &#123;</span><br><span class="line">        super(objectMapper);</span><br><span class="line">        List&lt;MediaType&gt; mediaTypes = new ArrayList&lt;&gt;();</span><br><span class="line">        mediaTypes.add(MediaType.valueOf(MediaType.APPLICATION_JSON_UTF8_VALUE));</span><br><span class="line">        mediaTypes.add(MediaType.valueOf(MediaType.TEXT_HTML_VALUE + &quot;;charset=UTF-8&quot;));</span><br><span class="line">        setSupportedMediaTypes(mediaTypes);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    RMappingJackson2HttpMessageConverter() &#123;</span><br><span class="line">        List&lt;MediaType&gt; mediaTypes = new ArrayList&lt;&gt;();</span><br><span class="line">        mediaTypes.add(MediaType.valueOf(MediaType.APPLICATION_JSON_UTF8_VALUE));</span><br><span class="line">        mediaTypes.add(MediaType.valueOf(MediaType.TEXT_HTML_VALUE + &quot;;charset=UTF-8&quot;));</span><br><span class="line">        setSupportedMediaTypes(mediaTypes);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Decoder">Decoder</h4><p>看下jiankunking.common.core.feign.FeignClientsConfig中的Decoder</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public Decoder feignDecoder() &#123;</span><br><span class="line">    HttpMessageConverter jacksonConverter = new MappingJackson2HttpMessageConverter(customObjectMapper());</span><br><span class="line">    ObjectFactory&lt;HttpMessageConverters&gt; objectFactory = () -&gt; new HttpMessageConverters(jacksonConverter);</span><br><span class="line">    return new ResponseEntityDecoder(new RSpringDecoder(objectFactory));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public ObjectMapper customObjectMapper() &#123;</span><br><span class="line">    ObjectMapper objectMapper = new ObjectMapper();</span><br><span class="line"></span><br><span class="line">    objectMapper.registerModule(new StringToDateModule());</span><br><span class="line">    objectMapper.configure(JsonParser.Feature.ALLOW_COMMENTS, true);</span><br><span class="line">    objectMapper.configure(JsonParser.Feature.ALLOW_UNQUOTED_FIELD_NAMES, true);</span><br><span class="line">    objectMapper.configure(JsonParser.Feature.ALLOW_SINGLE_QUOTES, true);</span><br><span class="line">    objectMapper.configure(JsonParser.Feature.ALLOW_UNQUOTED_CONTROL_CHARS, true);</span><br><span class="line">    objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);</span><br><span class="line"></span><br><span class="line">    return objectMapper;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Google了一下:<a href="https://www.google.com/search?q=spring+feign+encode+jackson+cpu+usage+high&amp;sca_esv=609f3e555a5e0eba&amp;rlz=1C1GCEU_en-GB__1000__1000&amp;ei=hlcHZ5-LEJqt4-EP6urGyQw&amp;ved=0ahUKEwif5pG6_IKJAxWa1jgGHWq1MckQ4dUDCA8&amp;uact=5&amp;oq=spring+feign+encode+jackson+cpu+usage+high&amp;gs_lp=Egxnd3Mtd2l6LXNlcnAiKnNwcmluZyBmZWlnbiBlbmNvZGUgamFja3NvbiBjcHUgdXNhZ2UgaGlnaDIIECEYoAEYwwRIiY0BUON-WPmHAXADeAGQAQCYAcwBoAGjBaoBBTAuMy4xuAEDyAEA-AEBmAIGoALrA8ICChAAGLADGNYEGEfCAggQABiABBiiBJgDAIgGAZAGCJIHAzMuM6AH9Qw&amp;sclient=gws-wiz-serp" target="_blank" rel="noopener">‘spring feign encode jackson cpu usage high’</a><br>=&gt; <a href="https://segmentfault.com/a/1190000043037032" target="_blank" rel="noopener">https://segmentfault.com/a/1190000043037032</a><br>=&gt; <a href="https://mp.weixin.qq.com/s/RuqltkN9VdVQ1K3GKuJ-Gw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/RuqltkN9VdVQ1K3GKuJ-Gw</a><br>=&gt; <a href="https://meantobe.github.io/2019/12/21/ClassLoader/" target="_blank" rel="noopener">https://meantobe.github.io/2019/12/21/ClassLoader/</a></p><h3 id="源码分析">源码分析</h3><p>查看registerWellKnownModulesIfAvailable处的代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">@SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">private void registerWellKnownModulesIfAvailable(Map&lt;Object, Module&gt; modulesToRegister) &#123;</span><br><span class="line">try &#123;</span><br><span class="line">Class&lt;? extends Module&gt; jdk8ModuleClass = (Class&lt;? extends Module&gt;)</span><br><span class="line">ClassUtils.forName(&quot;com.fasterxml.jackson.datatype.jdk8.Jdk8Module&quot;, this.moduleClassLoader);</span><br><span class="line">Module jdk8Module = BeanUtils.instantiateClass(jdk8ModuleClass);</span><br><span class="line">modulesToRegister.put(jdk8Module.getTypeId(), jdk8Module);</span><br><span class="line">&#125;</span><br><span class="line">catch (ClassNotFoundException ex) &#123;</span><br><span class="line">// jackson-datatype-jdk8 not available</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">try &#123;</span><br><span class="line">Class&lt;? extends Module&gt; javaTimeModuleClass = (Class&lt;? extends Module&gt;)</span><br><span class="line">ClassUtils.forName(&quot;com.fasterxml.jackson.datatype.jsr310.JavaTimeModule&quot;, this.moduleClassLoader);</span><br><span class="line">Module javaTimeModule = BeanUtils.instantiateClass(javaTimeModuleClass);</span><br><span class="line">modulesToRegister.put(javaTimeModule.getTypeId(), javaTimeModule);</span><br><span class="line">&#125;</span><br><span class="line">catch (ClassNotFoundException ex) &#123;</span><br><span class="line">// jackson-datatype-jsr310 not available</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Joda-Time present?</span><br><span class="line">if (ClassUtils.isPresent(&quot;org.joda.time.LocalDate&quot;, this.moduleClassLoader)) &#123;</span><br><span class="line">try &#123;</span><br><span class="line">Class&lt;? extends Module&gt; jodaModuleClass = (Class&lt;? extends Module&gt;)</span><br><span class="line">ClassUtils.forName(&quot;com.fasterxml.jackson.datatype.joda.JodaModule&quot;, this.moduleClassLoader);</span><br><span class="line">Module jodaModule = BeanUtils.instantiateClass(jodaModuleClass);</span><br><span class="line">modulesToRegister.put(jodaModule.getTypeId(), jodaModule);</span><br><span class="line">&#125;</span><br><span class="line">catch (ClassNotFoundException ex) &#123;</span><br><span class="line">// jackson-datatype-joda not available</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Kotlin present?</span><br><span class="line">if (KotlinDetector.isKotlinPresent()) &#123;</span><br><span class="line">try &#123;</span><br><span class="line">Class&lt;? extends Module&gt; kotlinModuleClass = (Class&lt;? extends Module&gt;)</span><br><span class="line">ClassUtils.forName(&quot;com.fasterxml.jackson.module.kotlin.KotlinModule&quot;, this.moduleClassLoader);</span><br><span class="line">Module kotlinModule = BeanUtils.instantiateClass(kotlinModuleClass);</span><br><span class="line">modulesToRegister.put(kotlinModule.getTypeId(), kotlinModule);</span><br><span class="line">&#125;</span><br><span class="line">catch (ClassNotFoundException ex) &#123;</span><br><span class="line">if (!kotlinWarningLogged) &#123;</span><br><span class="line">kotlinWarningLogged = true;</span><br><span class="line">logger.warn(&quot;For Jackson Kotlin classes support please add &quot; +</span><br><span class="line">&quot;\&quot;com.fasterxml.jackson.module:jackson-module-kotlin\&quot; to the classpath&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到其逻辑为若classpath中有JodaTime的LocalDate，则加载Jackson对应的JodaModule.LaunchedURLClassLoader.</p><p>为啥没有怀疑jdk8ModuleClass、javaTimeModuleClass这两个地方呢？因为common包中已经依赖了下面两个包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">compile &quot;com.fasterxml.jackson.datatype:jackson-datatype-jdk8:$&#123;v.jacksonDatatype&#125;&quot;</span><br><span class="line">compile &quot;com.fasterxml.jackson.datatype:jackson-datatype-jsr310:$&#123;v.jacksonDatatype&#125;&quot;</span><br></pre></td></tr></table></figure><p>那么解决方案就很清晰了</p><h3 id="解决方案">解决方案</h3><h4 id="避免ClassLoader反复加载">避免ClassLoader反复加载</h4><p>将这个依赖添加到工程中。加载一次后，再次调用可以通过findLoadedClass获得，减少加载类导致的资源消耗。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.fasterxml.jackson.datatype&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;jackson-datatype-joda&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;x.x.x&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h4 id="避免HttpMessageConverters重复初始化">避免HttpMessageConverters重复初始化</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public Decoder feignDecoder() &#123;</span><br><span class="line">    HttpMessageConverter jacksonConverter = new MappingJackson2HttpMessageConverter(customObjectMapper());</span><br><span class="line">    ObjectFactory&lt;HttpMessageConverters&gt; objectFactory = () -&gt; new HttpMessageConverters(false, Collections.singletonList(jacksonConverter));</span><br><span class="line">    return new ResponseEntityDecoder(new RSpringDecoder(objectFactory));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">public Encoder feignEncoder() &#123;</span><br><span class="line">    HttpMessageConverter jacksonConverter = new RMappingJackson2HttpMessageConverter(customObjectMapper());</span><br><span class="line">    ObjectFactory&lt;HttpMessageConverters&gt; objectFactory = () -&gt; new HttpMessageConverters(false, Collections.singletonList(jacksonConverter));</span><br><span class="line">    return new SpringEncoder(objectFactory);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意这种处理方式,会导致feign返回值简单类型的调用异常,比如返回是String会抛出异常</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line">feign.codec.DecodeException: Error while extracting response for type [class java.lang.String] and content type [application/json]; nested exception is org.springframework.http.converter.HttpMessageNotReadableException: JSON parse error: Cannot deserialize instance of `java.lang.String` out of START_OBJECT token; nested exception is com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot deserialize instance of `java.lang.String` out of START_OBJECT token</span><br><span class="line"> at [Source: (PushbackInputStream); line: 1, column: 1]</span><br><span class="line">at feign.SynchronousMethodHandler.decode(SynchronousMethodHandler.java:180) ~[feign-core-10.1.0.jar!/:na]</span><br><span class="line">at feign.SynchronousMethodHandler.executeAndDecode(SynchronousMethodHandler.java:140) ~[feign-core-10.1.0.jar!/:na]</span><br><span class="line">at feign.SynchronousMethodHandler.invoke(SynchronousMethodHandler.java:78) ~[feign-core-10.1.0.jar!/:na]</span><br><span class="line">at feign.ReflectiveFeign$FeignInvocationHandler.invoke(ReflectiveFeign.java:103) ~[feign-core-10.1.0.jar!/:na]</span><br><span class="line">at com.sun.proxy.$Proxy219.token(Unknown Source) ~[na:na]</span><br><span class="line">at com.jiankunking.gateway.controller.AuthController.getAccessTokenByCode(AuthController.java:399) [classes!/:na]</span><br><span class="line">at com.jiankunking.gateway.controller.AuthController.getCustomerToken(AuthController.java:86) [classes!/:na]</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_242]</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_242]</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_242]</span><br><span class="line">at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_242]</span><br><span class="line">at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:189) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:102) [spring-webmvc-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:892) [spring-webmvc-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:797) [spring-webmvc-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87) [spring-webmvc-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1038) [spring-webmvc-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:942) [spring-webmvc-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1005) [spring-webmvc-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:897) [spring-webmvc-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at javax.servlet.http.HttpServlet.service(HttpServlet.java:634) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:882) [spring-webmvc-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at javax.servlet.http.HttpServlet.service(HttpServlet.java:741) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53) [tomcat-embed-websocket-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.springframework.web.filter.CorsFilter.doFilterInternal(CorsFilter.java:96) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at com.jiankunking.common.core.filter.RequestIdFilter.doFilter(RequestIdFilter.java:73) [jsh-common-1.1.35-SNAPSHOT.jar!/:na]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.springframework.boot.actuate.web.trace.servlet.HttpTraceFilter.doFilterInternal(HttpTraceFilter.java:90) [spring-boot-actuator-2.1.4.RELEASE.jar!/:2.1.4.RELEASE]</span><br><span class="line">at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:320) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:119) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.authentication.AbstractAuthenticationProcessingFilter.doFilter(AbstractAuthenticationProcessingFilter.java:200) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at com.jiankunking.common.security.JwtAuthenticationFilter.doFilterInternal(JwtAuthenticationFilter.java:71) [jsh-common-1.1.35-SNAPSHOT.jar!/:na]</span><br><span class="line">at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:74) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:215) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:178) [spring-security-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]</span><br><span class="line">at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:357) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:270) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:92) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.springframework.web.servlet.v3_1.OpenTelemetryHandlerMappingFilter.doFilter(OpenTelemetryHandlerMappingFilter.java:80) [otel-agent.jar:na]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.filterAndRecordMetrics(WebMvcMetricsFilter.java:117) [spring-boot-actuator-2.1.4.RELEASE.jar!/:2.1.4.RELEASE]</span><br><span class="line">at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:106) [spring-boot-actuator-2.1.4.RELEASE.jar!/:2.1.4.RELEASE]</span><br><span class="line">at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-5.1.6.RELEASE.jar!/:5.1.6.RELEASE]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:200) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:490) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:139) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:343) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:408) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:834) [tomcat-embed-core-9.0.17.jar!/:9.0.17]</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(Ni</span><br></pre></td></tr></table></figure><p>进一步处理方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">List&lt;HttpMessageConverter&lt;?&gt;&gt; decodeMessageConverters = new ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">List&lt;HttpMessageConverter&lt;?&gt;&gt; encodeMessageConverters = new ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">public FeignClientsConfig() &#123;</span><br><span class="line"></span><br><span class="line">    addDefaultHttpMessageConverters(decodeMessageConverters);</span><br><span class="line">    addDefaultHttpMessageConverters(encodeMessageConverters);</span><br><span class="line"></span><br><span class="line">    HttpMessageConverter mappingJackson2HttpMessageConverter = new MappingJackson2HttpMessageConverter(customObjectMapper());</span><br><span class="line">    decodeMessageConverters.add(mappingJackson2HttpMessageConverter);</span><br><span class="line"></span><br><span class="line">    HttpMessageConverter rMappingJackson2HttpMessageConverter = new RMappingJackson2HttpMessageConverter(customObjectMapper());</span><br><span class="line">    encodeMessageConverters.add(rMappingJackson2HttpMessageConverter);</span><br><span class="line"></span><br><span class="line">    log.info(&quot;==================Custom FeignClientsConfig=======================&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Bean</span><br><span class="line">@Override</span><br><span class="line">public Decoder feignDecoder() &#123;</span><br><span class="line">    log.info(&quot;==================Override feignDecoder=======================&quot;);</span><br><span class="line">    ObjectFactory&lt;HttpMessageConverters&gt; objectFactory = () -&gt; new HttpMessageConverters(false, getDecodeMessageConverters());</span><br><span class="line">    return new ResponseEntityDecoder(new RSpringDecoder(objectFactory));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Bean</span><br><span class="line">@Override</span><br><span class="line">public Encoder feignEncoder() &#123;</span><br><span class="line">    log.info(&quot;==================Override feignEncoder=======================&quot;);</span><br><span class="line">    ObjectFactory&lt;HttpMessageConverters&gt; objectFactory = () -&gt; new HttpMessageConverters(false, getEncodeMessageConverters());</span><br><span class="line">    return new SpringEncoder(objectFactory);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private List&lt;HttpMessageConverter&lt;?&gt;&gt; getDecodeMessageConverters() &#123;</span><br><span class="line">    return Collections.unmodifiableList(decodeMessageConverters);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private List&lt;HttpMessageConverter&lt;?&gt;&gt; getEncodeMessageConverters() &#123;</span><br><span class="line">    return Collections.unmodifiableList(encodeMessageConverters);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// copy from =&gt;</span><br><span class="line">// pkg:org.springframework.web.servlet.config.annotation</span><br><span class="line">// WebMvcConfigurationSupport#addDefaultHttpMessageConverters</span><br><span class="line">protected final void addDefaultHttpMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; messageConverters) &#123;</span><br><span class="line">    StringHttpMessageConverter stringHttpMessageConverter = new StringHttpMessageConverter();</span><br><span class="line">    stringHttpMessageConverter.setWriteAcceptCharset(false);  // see SPR-7316</span><br><span class="line"></span><br><span class="line">    messageConverters.add(new ByteArrayHttpMessageConverter());</span><br><span class="line">    messageConverters.add(stringHttpMessageConverter);</span><br><span class="line">    messageConverters.add(new ResourceHttpMessageConverter());</span><br><span class="line">    messageConverters.add(new ResourceRegionHttpMessageConverter());</span><br><span class="line">    try &#123;</span><br><span class="line">        messageConverters.add(new SourceHttpMessageConverter&lt;&gt;());</span><br><span class="line">    &#125; catch (Throwable ex) &#123;</span><br><span class="line">        // Ignore when no TransformerFactory implementation is available...</span><br><span class="line">    &#125;</span><br><span class="line">    messageConverters.add(new AllEncompassingFormHttpMessageConverter());</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="优化后效果">优化后效果</h4><blockquote><p>优化后CPU占用降低一半</p></blockquote><p><img data-src="/images/an-accident-caused-by-excessive-cpu-usage-of-fegin/%E4%BC%98%E5%8C%96%E5%89%8D%E5%90%8E%E6%95%88%E6%9E%9C%E6%AF%94%E5%AF%B9.png" alt></p><h2 id="总结">总结</h2><p>大家在自定义 Feign 的编解码器时，如果用到了 SpringEncoder / SpringDecoder，应避免 HttpMessageConverters 的重复初始化。如果不需要使用那些默认的 HttpMessageConverter，可以在初始化 HttpMessageConverters 时将第一个入参设置为 false，从而不初始化那些默认的 HttpMessageConverter。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;记录一下&lt;br&gt;
一次应用事故分析、排查、处理&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="Java" scheme="https://jiankunking.com/categories/java/"/>
    
    
    <category term="Java" scheme="https://jiankunking.com/tags/java/"/>
    
    <category term="原创" scheme="https://jiankunking.com/tags/原创/"/>
    
    <category term="Fegin" scheme="https://jiankunking.com/tags/fegin/"/>
    
    <category term="CPU" scheme="https://jiankunking.com/tags/cpu/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch不停机切换(上云)方案</title>
    <link href="https://jiankunking.com/elasticsearch-non-stop-cloud-solution.html"/>
    <id>https://jiankunking.com/elasticsearch-non-stop-cloud-solution.html</id>
    <published>2024-09-21T02:16:40.000Z</published>
    <updated>2024-09-22T13:34:38.587Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>如何给飞行中的飞机换引擎?</p></blockquote><a id="more"></a><h2 id="背景">背景</h2><ul><li>业务背景<ul><li>略</li></ul></li><li>技术背景<ul><li>线下集群40个索引左右,总数据量不大,不到100G</li><li>因为ES承担的业务鉴权业务,所以不能接受停机割接<ul><li>还有就是ES中数据来自各个业务方,推送的时机不定,也没有完备的重推机制,所以不能停机割接</li></ul></li><li>索引中基本都没有创建或者更新时间字段,即使部分有,也没有用起来<ul><li>也就无法使用logstash的增量同步功能。</li></ul></li><li>希望不进行业务改造,直接替换。</li><li>虽然服务分为了读写服务，但通过读服务还是可以调用写入的API，通过写服务也可以调用读的API。</li></ul></li></ul><h2 id="架构方案">架构方案</h2><ul><li>全量数据同步logstash</li><li>脚步比对出来的差异数据,脚步补数</li></ul><p><img data-src="/images/elasticsearch-non-stop-cloud-solution/ES%E4%B8%8D%E5%81%9C%E6%9C%BA%E5%88%87%E6%8D%A2%E6%96%B9%E6%A1%88.png" alt="Elasticsearch不停机切换(上云)方案"></p><p>注意：</p><ul><li>CLB及代理层的配置一定有冗余</li><li>如果个CLB支撑不了,可以考虑<ul><li>方式一：直接申请多个CLB,并将这多个CLB的地址配置到应用中</li><li>方式二：先申请一个EIP,在EIP的后面配置多个CLB,这样应用只配置一个EIP的地址就可以了</li><li>方式三：CLB直接升配到NLB</li></ul></li><li><a href="https://help.aliyun.com/zh/slb/classic-load-balancer/user-guide/clb-overview-1?spm=5176.28426678.J_HeJR_wZokYt378dwP-lLl.11.5a0a51812dWUub&amp;scm=20140722.S_help@@%E6%96%87%E6%A1%A3@@85931.S_BB1@bl+RQW@ag0+BB2@ag0+hot+os0.ID_85931-RL_slb%E8%A7%84%E6%A0%BC-LOC_search~UND~helpdoc~UND~item-OR_ser-V_3-P0_0" target="_blank" rel="noopener">CLB文档</a></li><li><a href="https://help.aliyun.com/zh/slb/network-load-balancer/product-overview/what-is-nlb?spm=a2c4g.11186623.0.0.ad495867h32P2K#concept-2223473" target="_blank" rel="noopener">NLB文档</a></li><li>准备两套CLB及代理层的原因是：代理层是个Nginx集群,手动一台一台更新配置然后reload很慢,这时候数据写入的主ES是不确定的。</li><li></li></ul><h2 id="比对核心逻辑">比对核心逻辑</h2><ul><li>获取线下集群所有索引(跳过系统所以及不需要迁移的索引)</li><li>遍历第一步获取到的索引集合<ul><li>获取线上、线下索引的文档总数,如果总数不一样,终止比对；</li><li>如果总数一样,则通过search after(需要)分页分别从线上、线下获取数据比对。</li></ul></li></ul><p>注意：search_after的排序字段集合有几个要求</p><ol><li>如果_id就是业务ID,则直接使用该字段；</li><li>如果_id是ES自动生成的ID,则需要使用业务ID字段来排序(需要保证该业务ID索引内部不重复；如果不能保证,则需要添加其他字段来保证唯一；<strong>保证唯一的目的就是比对的两个索引在相同位置的文档就应该是一样的,不一样就是有问题</strong>)；</li><li>如果无法找到能构建复合主键的字段,则需要将索引数据完整的拉到内存中,然后根据mapping将所有字段拼接构建组合ID,然后去重,再依次比对。(索引条数不一样的,也可以通过类似的方式来查找异常的原因;采取这种简单粗暴方式的原因是：1、我们这种类型索引的数据量不大 2、这个比对程序其实就是个临时的工具,不会长期使用)</li></ol><blockquote><p>模板、mapping、index setting这些都需要比对。</p></blockquote><h3 id="比对核心代码">比对核心代码</h3><p>MapFlatUtil.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">import java.util.*;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @Author jiankunking</span><br><span class="line"> * @Date 2024/9/4 17:13</span><br><span class="line"> * @Description:</span><br><span class="line"> */</span><br><span class="line">public class MapFlatUtil &#123;</span><br><span class="line"></span><br><span class="line">    static String PREFIX = &quot;.&quot;;</span><br><span class="line"></span><br><span class="line">    public static Map&lt;String, Object&gt; flat(Map&lt;String, Object&gt; map) &#123;</span><br><span class="line">        Map&lt;String, Object&gt; configMap = new LinkedHashMap&lt;&gt;();</span><br><span class="line">        map.entrySet().forEach(entry -&gt; &#123;</span><br><span class="line">            if (entry.getValue() instanceof Map) &#123;</span><br><span class="line">                Map&lt;String, Object&gt; subMap = flat(entry.getKey(), (Map&lt;String, Object&gt;) entry.getValue());</span><br><span class="line">                if (!subMap.isEmpty()) &#123;</span><br><span class="line">                    configMap.putAll(subMap);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; else if (entry.getValue() instanceof List) &#123;</span><br><span class="line">                configMap.put(entry.getKey(), entry.getValue());</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                configMap.put(entry.getKey(), entry.getValue() == null ? &quot;&quot; : String.valueOf(entry.getValue()));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        return configMap;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static Map&lt;String, Object&gt; flat(String parentNode, Map&lt;String, Object&gt; source) &#123;</span><br><span class="line">        Map&lt;String, Object&gt; flatMap = new LinkedHashMap&lt;&gt;();</span><br><span class="line">        Set&lt;Map.Entry&lt;String, Object&gt;&gt; set = source.entrySet();</span><br><span class="line">        set.forEach(entity -&gt; &#123;</span><br><span class="line">            Object value = entity.getValue();</span><br><span class="line">            String key = entity.getKey();</span><br><span class="line">            String newKey = parentNode + PREFIX + key;</span><br><span class="line">            if (value instanceof Map) &#123;</span><br><span class="line">                flatMap.putAll(flat(newKey, (Map&lt;String, Object&gt;) value));</span><br><span class="line">            &#125; else if (value instanceof List) &#123;</span><br><span class="line">                flatMap.put(newKey, value);</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                flatMap.put(newKey, value == null ? &quot;&quot; : String.valueOf(value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        return flatMap;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>MapCompareUtil.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">import com.fasterxml.jackson.core.JsonProcessingException;</span><br><span class="line">import com.fasterxml.jackson.databind.ObjectMapper;</span><br><span class="line">import lombok.extern.slf4j.Slf4j;</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.Comparator;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import java.util.stream.Collectors;</span><br><span class="line"></span><br><span class="line">import static com.jiankunking.branchcompare.es.SortUtil.mapComparator;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @Author jiankunking</span><br><span class="line"> * @Date 2024/9/14 9:48</span><br><span class="line"> * @Description:</span><br><span class="line"> */</span><br><span class="line">@Slf4j</span><br><span class="line">public class MapCompareUtil &#123;</span><br><span class="line"></span><br><span class="line">    public static boolean isMapEquals(Map&lt;String, Object&gt; offlineMap, Map&lt;String, Object&gt; onlineMap) throws JsonProcessingException &#123;</span><br><span class="line">        offlineMap = MapFlatUtil.flat(offlineMap);</span><br><span class="line">        onlineMap = MapFlatUtil.flat(onlineMap);</span><br><span class="line">        if (offlineMap.size() != onlineMap.size()) &#123;</span><br><span class="line">            return false;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        for (Map.Entry&lt;String, Object&gt; offlineEntry : offlineMap.entrySet()) &#123;</span><br><span class="line">            String offlineEntryKey = offlineEntry.getKey();</span><br><span class="line">            if (!onlineMap.containsKey(offlineEntryKey)) &#123;</span><br><span class="line">                return false;</span><br><span class="line">            &#125;</span><br><span class="line">            Object offlineEntryValue = offlineEntry.getValue();</span><br><span class="line">            Object onlineEntryValue = onlineMap.get(offlineEntryKey);</span><br><span class="line"></span><br><span class="line">            Class offlineEntryValueClass = offlineEntryValue.getClass();</span><br><span class="line">            Class onlineEntryValueClass = onlineEntryValue.getClass();</span><br><span class="line"></span><br><span class="line">            if (offlineEntryValueClass != onlineEntryValueClass) &#123;</span><br><span class="line">                log.warn(&quot;value type not equals,offlineEntryValue:&quot; + offlineEntryValueClass.getName() + &quot;,onlineEntryValue:&quot; + onlineEntryValueClass.getName());</span><br><span class="line">                return false;</span><br><span class="line">            &#125;</span><br><span class="line">            if (offlineEntryValue instanceof Map) &#123;</span><br><span class="line">                Map&lt;String, Object&gt; offlineMapValue = (Map&lt;String, Object&gt;) offlineEntryValue;</span><br><span class="line">                Map&lt;String, Object&gt; onlineMapValue = (Map&lt;String, Object&gt;) onlineEntryValue;</span><br><span class="line">                if (!isMapEquals(offlineMapValue, onlineMapValue)) &#123;</span><br><span class="line">                    return false;</span><br><span class="line">                &#125;</span><br><span class="line">                continue;</span><br><span class="line">            &#125; else if (offlineEntryValue instanceof List) &#123;</span><br><span class="line">                List&lt;Object&gt; offlineList = (List&lt;Object&gt;) offlineEntryValue;</span><br><span class="line">                List&lt;Object&gt; onlineList = (List&lt;Object&gt;) onlineEntryValue;</span><br><span class="line">                if (offlineList.size() != onlineList.size()) &#123;</span><br><span class="line">                    log.warn(&quot;list size not equals,offlineList:&quot; + offlineList.size() + &quot;,onlineList:&quot; + onlineList.size());</span><br><span class="line">                    return false;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                // List&lt;Map&gt;</span><br><span class="line">                if (!offlineList.isEmpty() &amp;&amp; offlineList.get(0) instanceof Map) &#123;</span><br><span class="line">                    List&lt;Map&lt;String, Object&gt;&gt; offlineEntryValueTmp = (List&lt;Map&lt;String, Object&gt;&gt;) offlineEntryValue;</span><br><span class="line">                    List&lt;Map&lt;String, Object&gt;&gt; onlineEntryValueTmp = (List&lt;Map&lt;String, Object&gt;&gt;) onlineEntryValue;</span><br><span class="line"></span><br><span class="line">                    List&lt;SortUtil.Sort&gt; sorts = new ArrayList&lt;&gt;();</span><br><span class="line">                    // 按照map 的key 排序</span><br><span class="line">                    for (Map.Entry&lt;String, Object&gt; entry : offlineEntryValueTmp.get(0).entrySet()) &#123;</span><br><span class="line">                        sorts.add(new SortUtil.Sort(entry.getKey(), SortUtil.Order.ASC));</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    List&lt;Map&lt;String, Object&gt;&gt; offlineEntryValueSorted = offlineEntryValueTmp.stream()</span><br><span class="line">                                                                                            .sorted(mapComparator(sorts))</span><br><span class="line">                                                                                            .collect(Collectors.toList());</span><br><span class="line">                    List&lt;Map&lt;String, Object&gt;&gt; onlineEntryValueSorted = onlineEntryValueTmp.stream()</span><br><span class="line">                                                                                          .sorted(mapComparator(sorts))</span><br><span class="line">                                                                                          .collect(Collectors.toList());</span><br><span class="line">                    for (int i = 0; i &lt; offlineEntryValueSorted.size(); i++) &#123;</span><br><span class="line">                        Object offlineListItem = offlineEntryValueSorted.get(i);</span><br><span class="line">                        Object onlineListItem = onlineEntryValueSorted.get(i);</span><br><span class="line">                        if (!isMapEquals((Map&lt;String, Object&gt;) offlineListItem, (Map&lt;String, Object&gt;) onlineListItem)) &#123;</span><br><span class="line">                            return false;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    // List&lt;简单类型&gt;</span><br><span class="line">                    offlineList.sort(Comparator.comparing(o -&gt; o.toString()));</span><br><span class="line">                    onlineList.sort(Comparator.comparing(o -&gt; o.toString()));</span><br><span class="line">                    for (int i = 0; i &lt; offlineList.size(); i++) &#123;</span><br><span class="line">                        Object offlineListItem = offlineList.get(i);</span><br><span class="line">                        Object onlineListItem = onlineList.get(i);</span><br><span class="line">                        if (!simpleObjectEquals(offlineListItem, onlineListItem)) &#123;</span><br><span class="line">                            log.warn(&quot;list item not equals,offlineListItem:&quot; + offlineListItem + &quot;,onlineListItem:&quot; + onlineListItem);</span><br><span class="line">                            return false;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                continue;</span><br><span class="line">            &#125;</span><br><span class="line">            if (!simpleObjectEquals(offlineEntryValue, onlineEntryValue)) &#123;</span><br><span class="line">                log.warn(&quot;map value not equals,offlineEntryValue:&quot; + offlineEntryValue + &quot;,onlineEntryValue:&quot; + onlineEntryValue);</span><br><span class="line">                return false;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return true;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 只能处理简单对象 不能处理Map List等复杂类型</span><br><span class="line">    private static boolean simpleObjectEquals(Object o1, Object o2) throws JsonProcessingException &#123;</span><br><span class="line">        String offlineJson = new ObjectMapper().writeValueAsString(o1);</span><br><span class="line">        String onlineJson = new ObjectMapper().writeValueAsString(o2);</span><br><span class="line">        if (offlineJson.equals(onlineJson)) &#123;</span><br><span class="line">            return true;</span><br><span class="line">        &#125;</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>SortUtil.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">import java.math.BigDecimal;</span><br><span class="line">import java.util.*;</span><br><span class="line">import java.util.stream.Collectors;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @Author jiankunking</span><br><span class="line"> * @Date 2024/9/5 14:00</span><br><span class="line"> * @Description: https://gist.github.com/IOsetting/25ca8d70c12c11390113d343f666cd6e</span><br><span class="line"> */</span><br><span class="line">public class SortUtil &#123;</span><br><span class="line">    public enum Order &#123;ASC, DESC&#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * @param sorts keys and sort direction</span><br><span class="line">     * @return sorted list</span><br><span class="line">     */</span><br><span class="line">    public static Comparator&lt;Map&lt;String, Object&gt;&gt; mapComparator(List&lt;Sort&gt; sorts) &#123;</span><br><span class="line">        return (o1, o2) -&gt; &#123;</span><br><span class="line">            int ret = 0;</span><br><span class="line">            for (Sort sort : sorts) &#123;</span><br><span class="line">                Object v1 = o1.get(sort.field);</span><br><span class="line">                Object v2 = o2.get(sort.field);</span><br><span class="line">                ret = singleCompare(v1, v2, sort.order == Order.ASC);</span><br><span class="line">                if (ret != 0) &#123;</span><br><span class="line">                    break;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            return ret;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static class Sort &#123;</span><br><span class="line">        public String field;</span><br><span class="line">        public Order order;</span><br><span class="line"></span><br><span class="line">        public Sort(String field, Order order) &#123;</span><br><span class="line">            this.field = field;</span><br><span class="line">            this.order = order;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static int singleCompare(Object ao, Object bo, boolean asc) &#123;</span><br><span class="line">        int ret;</span><br><span class="line">        if (ao == null &amp;&amp; bo == null) &#123;</span><br><span class="line">            ret = 0;</span><br><span class="line">        &#125; else if (ao == null) &#123;</span><br><span class="line">            ret = -1;</span><br><span class="line">        &#125; else if (bo == null) &#123;</span><br><span class="line">            ret = 1;</span><br><span class="line">        &#125; else if (ao instanceof BigDecimal) &#123;</span><br><span class="line">            ret = ((BigDecimal) ao).compareTo((BigDecimal) bo);</span><br><span class="line">        &#125; else if (ao instanceof Number) &#123;</span><br><span class="line">            if (((Number) ao).doubleValue() != ((Number) bo).doubleValue()) &#123;</span><br><span class="line">                ret = ((Number) ao).doubleValue() &gt; ((Number) bo).doubleValue() ? 1 : -1;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                ret = 0;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else if (ao instanceof Date) &#123;</span><br><span class="line">            ret = ((Date) ao).compareTo((Date) bo);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            ret = String.valueOf(ao).compareTo(String.valueOf(bo));</span><br><span class="line">        &#125;</span><br><span class="line">        if (!asc) &#123;</span><br><span class="line">            return -ret;</span><br><span class="line">        &#125;</span><br><span class="line">        return ret;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;&gt;();</span><br><span class="line">        List&lt;Sort&gt; sorts = new ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        List&lt;Map&lt;String, Object&gt;&gt; sorted = list.stream()</span><br><span class="line">                                               .sorted(mapComparator(sorts))</span><br><span class="line">                                               .collect(Collectors.toList());</span><br><span class="line">        for (Map&lt;String, Object&gt; map : sorted) &#123;</span><br><span class="line">            System.out.println(map.get(&quot;somekey&quot;));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>EsQueryUtil.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">public static SearchResponse searchAfterByMultiFields(RestHighLevelClient restHighLevelClient, String indexName, List&lt;String&gt; searchAfterSortFields, List&lt;Object&gt; searchAfterValues, int size) throws IOException &#123;</span><br><span class="line">        SearchSourceBuilder builder = new SearchSourceBuilder();</span><br><span class="line">        builder.size(size);</span><br><span class="line">        builder.trackTotalHits(true);</span><br><span class="line"></span><br><span class="line">        builder.query(QueryBuilders.matchAllQuery());</span><br><span class="line"></span><br><span class="line">        // USING SEARCH AFTER</span><br><span class="line">        if (searchAfterValues != null &amp;&amp; !searchAfterValues.isEmpty()) &#123;</span><br><span class="line">            builder.searchAfter(searchAfterValues.toArray());</span><br><span class="line">        &#125;</span><br><span class="line">        for (String sortField : searchAfterSortFields) &#123;</span><br><span class="line">            builder.sort(sortField, SortOrder.ASC);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        SearchRequest searchRequest = new SearchRequest();</span><br><span class="line">        searchRequest.indices(indexName);</span><br><span class="line">        searchRequest.source(builder);</span><br><span class="line">        // log.info(searchRequest.toString());</span><br><span class="line">        log.info(searchRequest.source().toString());</span><br><span class="line"></span><br><span class="line">        SearchResponse response = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);</span><br><span class="line"></span><br><span class="line">        return response;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static List&lt;Object&gt; getSearchAfterValues(List&lt;String&gt; searchAfterSortFields, SearchHit hit) &#123;</span><br><span class="line">        List&lt;Object&gt; searchAfterValues = new ArrayList&lt;&gt;(searchAfterSortFields.size());</span><br><span class="line">        Map&lt;String, Object&gt; map = hit.getSourceAsMap();</span><br><span class="line">        for (String field : searchAfterSortFields) &#123;</span><br><span class="line">            if (field.equals(&quot;_id&quot;)) &#123;</span><br><span class="line">                searchAfterValues.add(hit.getId());</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                searchAfterValues.add(map.get(field));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return searchAfterValues;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="反思">反思</h2><ul><li>要拉通全流程及相关人员,核对每个可能出现的问题及应对方案</li><li>有些东西不能因为是临时的就放松警惕性<ul><li>比如本次代理层申请的机器是有两块的盘：1、一个50G的系统盘 2、一个500G的数据盘；但最终落地的时候云厂商同学还是把nginx的访问日志落到了系统盘,导致系统盘满了,系统受到的影响。<ul><li>这个500G的盘当时还讨论过,要用来存储访问日志,防止机器磁盘写满。</li></ul></li><li>任务列表也梳理了代理层遇到问题要发送告警,但没有一一核实,导致系统盘满的时候,没有第一时间收到告警。</li><li><font color="DeepPink"><strong>只要是在核心链路上的,不管是不是临时的,必须一一测试、验证。</strong></font></li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;如何给飞行中的飞机换引擎?&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="ElasticSearch" scheme="https://jiankunking.com/categories/elasticsearch/"/>
    
    
    <category term="原创" scheme="https://jiankunking.com/tags/原创/"/>
    
    <category term="ElasticSearch" scheme="https://jiankunking.com/tags/elasticsearch/"/>
    
    <category term="停机" scheme="https://jiankunking.com/tags/停机/"/>
    
    <category term="切换" scheme="https://jiankunking.com/tags/切换/"/>
    
    <category term="上云" scheme="https://jiankunking.com/tags/上云/"/>
    
  </entry>
  
  <entry>
    <title>OAuth 2 PKCE</title>
    <link href="https://jiankunking.com/oauth-2-pkce.html"/>
    <id>https://jiankunking.com/oauth-2-pkce.html</id>
    <published>2024-06-15T08:19:06.000Z</published>
    <updated>2025-11-10T06:12:52.099Z</updated>
    
    <content type="html"><![CDATA[<p>OAuth 2<br>作者： 王新栋</p><a id="more"></a><p>PKCE 协议，全称是 Proof Key for Code Exchange by OAuth Public Clients。</p><p>下面看一下PKCE 协议的交互流程</p><blockquote><p>注意，为了突出第三方软件使用 PKCE 协议时与授权服务之间的通信过程，省略了受保护资源服务和资源拥有者的角色：</p></blockquote><p><img data-src="/images/oauth-2-pkce/%E4%BD%BF%E7%94%A8PKCE%E5%8D%8F%E8%AE%AE%E7%9A%84%E6%B5%81%E7%A8%8B%E5%9B%BE.png" alt></p><p>首先，App 自己要生成一个随机的、长度在 43~128 字符之间的、参数为 <strong>code_verifier</strong> 的字符串验证码；接着，我们再利用这个 <strong>code_verifier，来生成一个被称为&quot;挑战码&quot;的参数code_challenge</strong>。</p><p>那怎么生成这个 <code>code_challenge</code> 的值呢？OAuth 2.0 规范里面给出了两种方法，就是看<code>code_challenge_method</code> 这个参数的值：</p><ul><li>一种 <code>code_challenge_method=plain</code>，此时 <code>code_verifier</code> 的值就是 <code>code_challenge</code>的值；</li><li>另外一种 <code>code_challenge_method=S256</code>，就是将 <code>code_verifier</code> 值进行 ASCII 编码之后再进行哈希，然后再将哈希之后的值进行 BASE64-URL 编码，如下代码所示:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">code_challenge = BASE64URL-ENCODE(SHA256(ASCII(code_verifier)))</span><br></pre></td></tr></table></figure><p>那么如何使用呢？</p><p>在<strong>第一步获取授权码 <code>code</code> 的时候，我们使用 <code>code_challenge</code></strong> 参数。需要注意的是，我们要同时将 <code>code_challenge_method</code> 参数也传过去，目的是让授权服务知道生成<code>code_challenge</code> 值的方法是 <code>plain</code> 还是 <code>S256</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">https://authorization-server.com/auth?</span><br><span class="line">response_type=code&amp;</span><br><span class="line">app_id=APP_ID&amp;</span><br><span class="line">redirect_uri=REDIRECT_URI&amp;</span><br><span class="line">code_challenge=CODE_CHALLENGE&amp;</span><br><span class="line">code_challenge_method=S256</span><br></pre></td></tr></table></figure><p>在<strong>第二步获取访问令牌的时候，我们使用 <code>code_verifier</code> 参数</strong>，授权服务此时会将<code>code_verifier</code> 的值进行一次运算。那怎么运算呢？就是上面<code>code_challenge_method=S256</code> 的这种方式。</p><p>没错，第一步请求授权码的时候，已经告诉授权服务生成 <code>code_challenge</code> 的方法了。所以，在第二步的过程中，授权服务将运算的值跟第一步接收到的值做比较，如果相同就颁发访问令牌。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">POST https://api.authorization-server.com/token?</span><br><span class="line">grant_type=authorization_code&amp;</span><br><span class="line">code=AUTH_CODE_HERE&amp;</span><br><span class="line">redirect_uri=REDIRECT_URI&amp;</span><br><span class="line">app_id=APP_ID&amp;</span><br><span class="line">code_verifier=CODE_VERIFIER</span><br></pre></td></tr></table></figure><p>现在，你就知道了我们是如何使用 <code>code_verifier</code> 和 <code>code_challenge</code> 这两个参数的了吧。</p><p>总结一下就是，换取授权码 <code>code</code> 的时候，我们使用 <code>code_challenge</code> 参数值；换取访问令牌的时候，我们使用 <code>code_verifier</code> 参数值。</p><blockquote><p>由于网络不法分子这样的中间人虽然可以截获 code_challenge，但是他并不能由 code_challenge 逆推 code_verifier，只有客户端自己才知道这两个值。因此即使中间人截获了 code_challenge，Authorization Code 等，也无法换取Access Token，避免了安全问题。</p></blockquote><p><a href="/attachments/OAuth 2/07丨如何在移动App中使用OAuth2.pdf" target="_blank">如何在移动App中使用OAuth2</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;OAuth 2&lt;br&gt;
作者： 王新栋&lt;/p&gt;</summary>
    
    
    
    <category term="OAuth" scheme="https://jiankunking.com/categories/oauth/"/>
    
    
    <category term="OAuth" scheme="https://jiankunking.com/tags/oauth/"/>
    
    <category term="PKCE" scheme="https://jiankunking.com/tags/pkce/"/>
    
  </entry>
  
  <entry>
    <title>[译]Elasticsearch _source Doc_values And Store Performance</title>
    <link href="https://jiankunking.com/elasticsearch-source-doc-values-and-store-performance.html"/>
    <id>https://jiankunking.com/elasticsearch-source-doc-values-and-store-performance.html</id>
    <published>2024-05-04T01:11:55.000Z</published>
    <updated>2024-05-14T08:36:09.485Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原文地址<br><a href="https://sease.io/2021/02/field-retrieval-performance-in-elasticsearch.html" target="_blank" rel="noopener">https://sease.io/2021/02/field-retrieval-performance-in-elasticsearch.html</a></p></blockquote><a id="more"></a><p>在这篇博文中，我想从<strong>性能的角度探讨 Elasticsearch 为我们存储字段和查询时检索字段提供了哪些可能性。</strong> 事实上，Lucene（Elasticsearch 和 Solr 构建的基础库）提供了两种存储和检索字段的方法:存储字段(stored fields)和文档值(docvalues)。 此外，Elasticsearch 默认使用 _source 字段，这是一个大 JSON，其中包含在索引时作为输入给出的文档的所有字段。</p><p>为什么 Elasticsearch 使用 _source 字段作为默认值？从性能的角度来看，所有这些可能性有什么区别？ 让我们来看看吧！</p><h1>Stored And Docvalues Fields In Lucene</h1><p>当我们在 Lucene 中索引文档时，已索引的原始字段的信息会丢失。 根据模式配置对字段进行相应的分析、转换和索引。 在没有任何额外数据结构的情况下，当我们搜索文档时，我们得到的是搜索到的文档的 id，而不是原始字段。 为了获取这些信息，我们需要额外的数据结构。 Lucene 为此提供了两种可能性:存储字段和文档值。</p><h2 id="STORED-FIELDS">STORED FIELDS</h2><p>存储字段的目的是存储字段的值（不进行任何分析）以便在查询时检索它们。</p><h2 id="DOCVALUES">DOCVALUES</h2><p>引入文档值是为了加速分面、排序和分组等操作。 文档值还可用于在查询时返回字段值。 我们唯一的限制是我们不能将它们用于文本字段。</p><p>存储字段和文档值在 Lucene 库中实现，它们可以在 Solr 和 Elasticsearch 中使用。</p><p>我写了一篇博客文章，其中比较了 Solr 中存储字段和文档值的字段检索性能:</p><p><a href="https://sease.io/2020/03/docvalues-vs-stored-fields-apache-solr-features-and-performance-smackdown.html" target="_blank" rel="noopener">DocValues VS 存储字段:Apache Solr 功能和性能 SmackDown</a></p><p>在那里您可以找到有关存储字段和文档值、其利用率和约束的更详细描述。</p><h1>Field Retrieval In Elasticsearch</h1><p>如果我们在映射中显式定义存储字段和文档值，则可以在 Elasticsearch 中使用它们:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;properties&quot; : &#123;</span><br><span class="line"> &quot;field&quot;: &#123;</span><br><span class="line">  &quot;type&quot;: &quot;keyword&quot;,</span><br><span class="line">   &quot;store&quot;: true,</span><br><span class="line">  &quot;doc_values&quot; true</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>默认情况下，每个字段的存储设置为 false。 相反，所有支持文档值的字段都默认启用它们。</p><p>独立于存储和文档值配置，在查询时返回查询命中的文档中每个字段的值。 发生这种情况是因为 Elasticsearch 使用另一个工具进行字段检索:<strong>elasticsearch _source</strong> 字段。</p><h2 id="ELASTICSEARCH-SOURCE-FIELD">ELASTICSEARCH _SOURCE FIELD</h2><p>源字段是在索引时传递到 Elasticsearch 的 JSON。 该字段在 Elasticsearch 中默认设置为 true，并且可以通过以下方式使用映射来禁用:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&quot;mappings&quot;: &#123;</span><br><span class="line">  &quot;_source&quot;: &#123;</span><br><span class="line">   &quot;enabled&quot;: false</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查询时默认返回所有字段。 您甚至可以仅指定要在响应中返回的源中的字段子集。 这应该可以加快响应在网络上的传输速度。</p><p>通过正确的配置，某些字段可以被源字段排除:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">PUT logs</span><br><span class="line">&#123;</span><br><span class="line"> &quot;mappings&quot;: &#123;</span><br><span class="line">  &quot;_source&quot;: &#123;</span><br><span class="line">   &quot;excludes&quot;: [</span><br><span class="line">    &quot;meta.description&quot;,</span><br><span class="line">     &quot;meta.other.*&quot;</span><br><span class="line">   ]</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从源中排除字段将减少磁盘空间使用量，但排除的字段永远不会在响应中返回。</p><p>禁用 elasticsearch _source 字段将导致无法在不从头开始重新索引的情况下更新文档(Disabling the elasticsearch _source field will make it impossible to update a document without reindexing that from scratch)。 事实上，为了更新文档，我们需要从旧文档中获取字段的值。 从逻辑上讲，使用存储的字段或文档值从旧文档中获取字段的值应该是可行的（这就是 Solr 中原子更新的工作方式）。 但是，由于设计决策，Elasticsearch 中不允许这样做，如果您需要更新文档，则必须在 Elasticsearch 索引配置中启用 _source 字段。</p><h2 id="RETRIEVING-FIELDS">RETRIEVING FIELDS</h2><p>在 Elasticsearch 中，您可以启用或禁用 _source 字段并使字段存储和/或文档值。 但是我们如何在查询时检索字段呢？</p><p>默认情况下，如果定义了整个源，则返回整个源。 您可以避免它并仅返回源的子集，如下所示:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;fields&quot;: [&quot;field1&quot;, &quot;field2&quot;],</span><br><span class="line">&quot;_source&quot;: false</span><br></pre></td></tr></table></figure><p>但是，如果您没有启用源字段，并且想要从存储的或文档值返回字段，则必须以其他方式告诉 Elasticsearch。 对于您使用的每个源，您必须以不同的方式指定字段列表:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;fields&quot;: [&quot;sv1&quot;, &quot;sv2&quot;,...],</span><br><span class="line">&quot;docvalue_fields&quot;: [&quot;dv1&quot;, &quot;dv2&quot;,...],</span><br><span class="line">&quot;stored_fields&quot; : [&quot;s1&quot;, &quot;s2&quot;,...],</span><br></pre></td></tr></table></figure><p>例如，如果您有一个存储字段和文档值字段，您可以选择是否要从文档值或存储字段中检索它。 从功能的角度来看，这是完全相同的，但您的选择可能会影响查询的执行时间。</p><h1>STORED FIELDS,DOCVALUES AND ELASTICSEARCH_SOURCE INTERNAL REPRESENTATION</h1><p>在本节中，我只想对存储字段、_source 字段和文档值的内部结构进行简要概述，以便有一些工具来理解使用这些方法进行字段检索的性能期望是什么。</p><h2 id="STORED-FIELDS-INTERNALS">STORED FIELDS INTERNALS</h2><p>存储的字段以行的方式放置在磁盘上:对于每个文档，我们都有一行连续包含所有存储的字段。</p><p><img data-src="/images/elasticsearch-source-doc-values-and-store-performance/storedfields.jpg" alt></p><p>以上图为例。 要访问文档 x 的 field3，我们必须访问文档 x 的行并跳过 field3 之前存储的所有字段。 跳过字段需要获取其长度。 跳过字段并不像读取字段那么昂贵，但此操作并不是免费的。</p><h2 id="DOCVALUES-INTERNALS">DOCVALUES INTERNALS</h2><p>文档值以列的方式存储。 不同文档的相同字段的值都连续地存储在内存中，并且可以&quot;几乎&quot;直接访问某个文档的某个字段。 计算所需值的地址并不是一项简单的操作，并且具有计算成本，但我们可以想象，如果我们只需要一个字段，那么使用这种访问会更有效。</p><h2 id="ELASTICSEARCH-SOURCE-FIELD-INTERNALS">ELASTICSEARCH _SOURCE FIELD INTERNALS</h2><p>那 _source 呢？ 嗯，如上所述，源是一个大字段，其中包含一个 JSON，其中包含索引时提供给 Elasticsearch 的所有输入。 但是，这个字段是如何存储的呢？ 毫不奇怪，Elasticsearch 利用了 Lucene 已经实现和提供的机制:存储字段。 特别是，_source 字段是该行中第一个存储的字段。</p><p><img data-src="/images/elasticsearch-source-doc-values-and-store-performance/sourcefield.jpg" alt></p><p>必须读取整个 _source 才能使用它包含的信息。 如果我们要返回文档的所有字段，这个过程直观上是最快的。 另一方面，如果我们只需要返回它包含的信息的一小部分，读取这个巨大的字段可能会浪费计算能力。</p><h1>Benchmarking</h1><p>为了对 3 种类型的字段进行基准测试，我在 Elasticsearch 中创建了 3 个不同的索引。 我对来自 Wikipedia 的 100 万份文档建立了索引，对于每个文档，我用三种不同的方法对 100 个包含 15 个字符的字符串字段建立了索引:在第一个索引中，我将字段设置为存储，在第二个索引中将字段设置为文档值。 在这两个索引中，我禁用了源字段。 相反，在第三个索引中，我只是启用了源字段。</p><p>文档和查询集合取自<a href="https://github.com/quickwit-oss/search-benchmark-game" target="_blank" rel="noopener">此处</a>。 我使用真实的集合来模拟现实场景。</p><p>执行详情:</p><ul><li>CPU:AMD锐龙3600</li><li>内存:32GB</li></ul><p>对于每个查询，我请求最好的 200 个文档，并重复测试，将要返回的字段数（在我创建的 100 个随机字符串字段中）从 1 更改为 100。</p><p>这是基准测试的结果:<br><img data-src="/images/elasticsearch-source-doc-values-and-store-performance/elastic-benachmarks.jpg" alt></p><p>结果完全符合我们的预期。**如果每个文档需要几个字段，则建议使用文档值。**另一方面，<strong>当我们想要返回整个文档时，_source字段是最好的字段，而存储字段的使用是其他两个字段之间的完美折衷。</strong></p><p>在我执行的基准测试场景中，<strong>如果我们只需要一个字段，则 docvalues 的速度几乎是 _source 字段的两倍</strong>，而在极端相反的情况下，如果我们想返回所有字段，则图表显示，当我们只需要一个字段时，速度几乎提高了 2 倍。 使用 _source 字段代替 docvalues。</p><p>总之，性能并不是我们必须考虑的唯一参数。 正如我们在这篇博文中简要解释的那样，使用一种或另一种方法存在一些限制。 由于您的用例的某些限制，您可能被迫使用这三种方法之一。 即使从表现来看，我们也没有明显的赢家。</p><p>如果磁盘空间不是问题，<strong>您甚至可以混合使用不同的方法并将字段设置为存储和文档值，并启用源。</strong> 在查询时，Elasticsearch 使您能够选择所需的字段列表，以及是否希望从 _source、stored 或 docvalues 返回它们。</p><h1>译者补充</h1><p>文档的_source为Lucene中的单个字段。此结构意味着，即使您只请求整个_source对象的一部分，也必须加载和解析该对象。为了避免此限制，您可以尝试加载字段的其他选项：</p><ul><li>使用docvalue_fields参数获取所选字段的值。当返回相当少的支持文档值的字段（如关键字和日期）时，这是一个很好的选择。</li><li>使用stored_fields参数获取特定存储字段（使用存储映射选项的字段）的值。<ul><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-fields.html#stored-fields" target="_blank" rel="noopener">stored_fields通常是不被推荐的</a></li></ul></li></ul><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-fields.html#field-retrieval-methods" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/search-fields.html#field-retrieval-methods</a></p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原文地址&lt;br&gt;
&lt;a href=&quot;https://sease.io/2021/02/field-retrieval-performance-in-elasticsearch.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://sease.io/2021/02/field-retrieval-performance-in-elasticsearch.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="ElasticSearch" scheme="https://jiankunking.com/categories/elasticsearch/"/>
    
    
    <category term="原创" scheme="https://jiankunking.com/tags/原创/"/>
    
    <category term="ElasticSearch" scheme="https://jiankunking.com/tags/elasticsearch/"/>
    
    <category term="翻译" scheme="https://jiankunking.com/tags/翻译/"/>
    
    <category term="_source" scheme="https://jiankunking.com/tags/source/"/>
    
    <category term="doc_values" scheme="https://jiankunking.com/tags/doc-values/"/>
    
    <category term="store" scheme="https://jiankunking.com/tags/store/"/>
    
  </entry>
  
  <entry>
    <title>手把手落地DDD 笔记</title>
    <link href="https://jiankunking.com/hand-in-hand-landing-ddd.html"/>
    <id>https://jiankunking.com/hand-in-hand-landing-ddd.html</id>
    <published>2024-03-21T07:15:11.000Z</published>
    <updated>2025-01-22T01:12:04.419Z</updated>
    
    <content type="html"><![CDATA[<p>手把手落地DDD<br>作者：钟敬</p><p>追风赶月莫停留，平芜尽处是春山。</p><p>如果想看改造旧系统相关可以直接查看【34｜落地经验：怎样在实际项目中推广DDD？】</p><a id="more"></a><h1>02｜迭代一概述：怎样开启一个麻雀虽小五脏俱全的项目？</h1><p>领域驱动设计主要的开发流程:<br><img data-src="/images/hand-in-hand-landing-ddd/02-%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1%E4%B8%BB%E8%A6%81%E7%9A%84%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B.png" alt></p><h1>03｜事件风暴（上）：怎样和业务愉快地聊需求？</h1><h2 id="事件风暴是怎么一回事？">事件风暴是怎么一回事？</h2><p>事件风暴的主要过程:<br><img data-src="/images/hand-in-hand-landing-ddd/03-%E4%BA%8B%E4%BB%B6%E9%A3%8E%E6%9A%B4%E7%9A%84%E4%B8%BB%E8%A6%81%E8%BF%87%E7%A8%8B.png" alt></p><p>实际上，领域事件表示的是，业务流程中每个步骤引发的结果。</p><p>在 DDD 中的各种命名，一般都优先使用约定俗成的业务术语。</p><p>关于领域事件，我们还要注意下面这两点。</p><ul><li>第一，<strong>不要把技术事件当成领域事件</strong>。领域事件一定要是领域专家所关注的，用的是业务术语。像数据库事务已回滚、缓存已命中之类的技术术语，不是领域事件，不在这个阶段讨论。</li><li>第二，<strong>查询功能不算领域事件</strong>。领域事件应该是对某样事物产生了影响，并被记录的事情。一般是某个事物的创建、修改和删除。还有一种情况是向其他人或者系统发消息，例如&quot;通知邮件已发送&quot;也算领域事件，因为接收方可能会通过进一步处理来影响某些事物。</li></ul><p>所谓<strong>统一语言</strong>，英文是 <strong>Ubiquitous Language</strong>，是 DDD 中的一个核心模式。指的是业务人员和开发人员使用的语言要一致。语言是知识的载体。语言一致就意味着背后对领域知识的理解一致。统一语言贯穿了 DDD 的全过程。</p><h1>04｜事件风暴（下）：事件风暴还有哪些诀窍？</h1><h2 id="事件风暴第二步：识别命令">事件风暴第二步：识别命令</h2><p>所谓命令（command），就是引发领域事件的操作，我们可以通过分析领域事件得到。除了识别出命令本身以外，我们通常还要识别出谁执行的命令，以及为了执行命令我们要查询出什么数据。</p><h2 id="事件风暴第三步：识别领域名词">事件风暴第三步：识别领域名词</h2><p>这里说的领域名词，是从命令、领域事件、执行者、查询数据里找到的名词性概念。例如，对于签订合同这个命令而言，受到影响的名词性概念是&quot;合同&quot;；类似地，对于合同已签订这个领域事件，是由于&quot;合同&quot;这个名词性概念的状态变化所导致的。</p><p>##　再谈事件风暴的作用</p><p>首先我们看看领域事件的作用。从代码实现的角度来看，领域事件一般会对应一段代码逻辑，这段逻辑可能会最终改变数据库中的数据。另外，在事件驱动的架构中，一个领域事件可能会表现为一个向外部发送的异步消息。</p><p>那命令的作用体现在哪儿呢？领域建模时，我们可以通过对命令的走查（walkthrough），细化和验证领域模型。在实现层面，一个命令可能对应前端的一个操作，例如按下按钮；对于后端而言，一个命令可能对应一个 API。</p><p>再来说命令的执行者。在领域建模时，执行者可能本身就是一个领域对象，也可能是领域对象充当的角色，或者是权限管理中的一个角色。</p><p>其实识别领域名词的最终目的是<strong>要找到领域模型中的对象</strong>。</p><h2 id="事件风暴的常见问题">事件风暴的常见问题</h2><h3 id="第一个问题是，在事件风暴里是否要列出所有的领域事件和命令？">第一个问题是，在事件风暴里是否要列出所有的领域事件和命令？</h3><p><strong>在事件风暴里只列出主要的、足以用于表达和交流领域知识的步骤</strong>，例如签订合同、生效合同等等。而像修改合同和删除合同这样的步骤是显而易见的，在讨论过程中可以提一下，但不必真的列出来，这样是为了保持简洁</p><h3 id="第二个问题是，各个领域事件需要体现严格的时间顺序吗？">第二个问题是，各个领域事件需要体现严格的时间顺序吗？</h3><p>只需要按照<strong>大致</strong>的顺序，贴出领域事件就可以了。这是因为，如果要体现严格的时间顺序，需要用到更复杂的符号，例如条件判断，还有要画更多的连线，这会使事件风暴变得非常繁琐。</p><p>因此，我们应该关注点分离。如果要体现严格的时间顺序，我们可以用流程图、用顺序图等方法，但事件风暴不必关注这一点。</p><h3 id="第三个问题是，每个步骤的颗粒度应该有多大？">第三个问题是，每个步骤的颗粒度应该有多大？</h3><p>这里说的步骤，指的是一对领域事件和命令。</p><p>比如说，&quot;签订合同&quot;这个命令，在具体操作的时候，可能分成录入合同基本信息、录入合同明细、上传附件等等更小的步骤。那么，我们需要为每一个小步骤都识别出领域事件和命令吗？</p><p>这就要考虑从业务的角度，我们是把每个小步骤都当作独立的一个事务来看待，还是把它们合起来作为同一个事务。</p><p>另外，可以设想，如果每个小步骤都向外界发出一个领域事件，对系统后续的功能是不是有意义。那么在目前的需求里，合同作为一个整体来提交就可以了，分成小的领域事件，并没有意义，所以不再分成更小的步骤了。</p><p>在实践里，有时仍然会有模棱两可的情况，这时，<strong>原则上宜粗不宜细</strong>。可以先采用比较大的颗粒度。后面必要的时候，再拆细，就可以了。</p><h3 id="第四个问题是，事件风暴适用于所有项目吗？">第四个问题是，事件风暴适用于所有项目吗？</h3><p>事件风暴主要应用在需求不清晰，或者理解不统一的情况下，通过协作的方式理清业务、达成一致，所以通常对于新项目比较适用。</p><p>至于遗留系统改造的情况，如果这个系统的知识已经流失得很严重，那么事件风暴仍然是有意义的。但如果大家对这个系统的业务知识很清楚，只是要进行架构改造，那么事件风暴的意义就不大了。</p><h2 id="总结">总结</h2><p>事件风暴是一种通过协作的方式捕获行为需求的方法，在这个过程里，业务人员和技术人员一起消化领域知识、形成统一语言、并为领域建模奠定基础。</p><p>事件风暴分为识别领域事件、识别命令、识别领域名词三个步骤。这一节课讲的是后面两个步骤。</p><p>“命令&quot;是引发领域事件的操作，可以从领域事件&quot;反推&quot;出来。此外，还可以识别命令的两个附加信息，一个是发出命令的&quot;执行者”，另一个是为了完成命令要查询出的数据。</p><p>&quot;领域名词&quot;是隐含在命令和领域事件中的名词性概念。这些名词是领域建模的素材, 而对于这些素材的深入分析可以留到领域建模进行</p><h1>05｜领域建模实践（上）：怎样既准确又深刻地理解业务知识？</h1><h2 id="领域建模中的一些基本概念">领域建模中的一些基本概念</h2><p>领域建模主要有两个目的：</p><ul><li>将知识可视化，准确、深刻地反映领域知识，并且在业务和技术人员之间达成一致；</li><li>指导系统的设计和编码，也就是说，领域模型应该能够比较容易地转化成数据库模式和代码实现。</li></ul><p>而我们建立领域模型，主要是要识别<strong>领域对象（domain object）</strong>，<strong>领域对象之间的关系</strong>，以及<strong>领域对象的关键属性</strong>，必要的时候还要将<strong>领域对象组织成模块</strong>。</p><p>那么，什么是领域对象呢？<strong>我们系统中要处理的各种&quot;事物&quot;就是领域对象</strong>。比如说项目、员工、账户等等。这些对象都反映了名词性的概念。</p><p>其中，有些名词化了的动词也是领域对象。比如说我们进行了一笔支付操作，并且想把这笔操作记录下来。这时，&quot;支付&quot;也是领域对象。支付本来是动词，但这里实际上是要把一笔支付的信息记录下来，在这里就把&quot;支付&quot;当名词用了。</p><p><strong>领域模型是用领域模型图来表达的，通常用 UML 来画。</strong></p><p>在领域建模过程中，我们说领域对象时，有时指类，有时指实例，一般可以通过上下文来区分。</p><p>此外，DDD 中将领域对象又分成实体（entity）和值对象（value object）。</p><p>在领域建模阶段，我们主要关注的是实体和它们之间的关系。如果实体的名字已经能清晰说明实体的含义，那我们就不需要加属性了。如果名字还不足以充分表达含义，我们可以写几个关键属性，来辅助说明。</p><p>在 UML 中，用大括号括起来的内容称为&quot;约束&quot;（constraint）。和一般性的注释不同，<strong>凡是约束，必须在程序中的某个地方进行实现</strong>。</p><p>“0…*” 我们也可以这样理解：一个组织最少有 0 个员工，最多可以有很多员工。</p><p>“1…1” 表示，一个员工最少要属于一个组织，最多也只能属于一个组织。</p><h1>06｜领域建模实践（下）：领域建模还有什么其他技巧？</h1><h2 id="划分模块">划分模块</h2><p>操作（operation）在 UML 里也叫方法（method）。对象的属性是静态的值，而操作是动态的逻辑。在 UML 中，操作用&quot;操作名+ 括号&quot;的方式表示，括号中可以写参数。</p><p>人的认知能力是有限的，面对这样一张复杂的对象网络，就产生了认知过载（cognitive overload）。</p><p>解决这一问题的方法就是&quot;模块化&quot;。也就是说，把模型中的业务概念组织成若干高内聚的模块（module），而模块之间尽量低耦合。</p><p>在 UML 中，可以用包来表示模块。包的符号是下面这样：<br><img data-src="/images/hand-in-hand-landing-ddd/06-UML%E5%8C%85.png" alt></p><h2 id="建立词汇表">建立词汇表</h2><p>接着我们来建立词汇表，也就是把事件风暴和领域模型中重要的词汇列成表。为什么要建立词汇表呢？主要是有两个作用。</p><p>首先，我们需要通过词汇表来<strong>规范领域模型中的词汇</strong>。同一个词，可能会在领域模型中出现多次，时间久了，就可能不一致，因此需要进一步规范。</p><p>第二，是可以<strong>用于后续编程中的命名</strong>。按照 DDD 的要求，程序中的各种命名也需要统一，并且需要与领域模型中保持一致。我们会在词汇表中列出英文全称和缩写，以达到这个目的。</p><p>词汇表是保证统一语言的重要手段。<br><img data-src="/images/hand-in-hand-landing-ddd/06-%E8%AF%8D%E6%B1%87%E8%A1%A8.png" alt></p><h1>07｜领域建模原理：DDD领域建模和传统方法有什么区别？</h1><h2 id="什么是领域模型？">什么是领域模型？</h2><p>在讨论什么是领域模型之前，咱们先说说什么是模型。</p><p>首先，<strong>模型是以解决特定问题为目的的</strong>。例如沙盘模型是为了卖房，而建筑图纸是为了盖楼。没有目的就谈不上模型。</p><p>第二，<strong>模型都是对现实世界或人们思维中的事物进行的模拟</strong>。例如沙盘模型和建筑图纸都是对建筑物的模拟，而玩具车是对真车的模拟。</p><p>第三，<strong>模型总是提取了被模拟事物中的部分信息，而忽略掉了其他大部分信息</strong>。例如，沙盘模型提取了楼盘的外观信息，但是忽略了内部结构和建筑材料信息。而建筑图纸反映了内部结构信息，但忽略了外观信息。到底提取哪些信息，忽略哪些信息，取决于模型的目的。</p><p>第四，<strong>模型可以有多种表现形式</strong>，例如图纸、影像、公式以及电脑中的文件等等。具体采用哪种形式，取决于要解决的问题和当前的技术水平。</p><p>最后，模型是一种<strong>人造物</strong>，大自然本身是不存在模型的。</p><p>DDD软件研发过程<br><img data-src="/images/hand-in-hand-landing-ddd/07-%E8%BD%AF%E4%BB%B6%E7%A0%94%E5%8F%91%E8%BF%87%E7%A8%8B.png" alt></p><p>DDD 的核心模式之一：<strong>模型驱动设计</strong>就是围绕领域模型展开的。它有两个要点：领域模型和业务需求要保持一致；系统实现和领域模型也要保持一致。最终的结果就是系统实现和业务需求保持一致。</p><p>除此之外，DDD 还有另一个核心模式：<strong>统一语言</strong>。要用好统一语言，一方面要建立好领域模型、词汇表等&quot;物质基础&quot;，另一方面要在沟通协作的过程中，不断保持模型、语言和系统实现的一致性。</p><h1>08｜数据库设计：怎样按领域模型设计数据库？</h1><p>模型中的一个一对多关联，可以映射成一个外键字段，以及一个外键约束。但基于云的应用一般不会真的建立外键约束，而外键的逻辑关系还是存在的。我们用虚线箭头表示这种逻辑上的外键关系，称为虚拟外键。对于多对多关联，我们必须增加一个关联表，其中包括了两个实体表各自的主键。另外，关联上的多重性决定了外键字段的非空约束。</p><h2 id="与-ER-图法-的区别">与&quot;ER 图法&quot;的区别</h2><p>首先，采用 UML 类图描述的领域模型图是 ER 图的超集。也就是说，ER 图能表达的，领域模型图都能表达；而领域模型图能表达的，ER 图未必能表达。因此，使用领域模型图以后，我们就不必再使用 ER 图了。</p><p>其实我们前几节课进行的领域建模，大体上相当于传统意义上的&quot;概念设计&quot;。如果把领域模型中的属性都补全，就相当于传统意义的&quot;逻辑设计&quot;了。而我们今天做的，其实就是传统上的&quot;物理设计&quot;，所以产物叫做&quot;物理数据模型&quot;。</p><p>第二个区别是，ER 图只能表达静态的数据关系，只用于数据库设计，而领域模型图则可以将静态数据和动态行为绑定，不仅可以用于数据库设计，还可以用于程序设计，这一点我们在后面的课程会看到。也就是说，基于 DDD 的方法能够保证程序设计和数据库设计的高度统一。</p><p>第三个区别是，领域模型对应的主要是传统软件工程的分析模型，而 ER 图在传统软件工程里则处于设计阶段，所以两者的层次和使用场合也是不一样的。</p><p>DDD 方法是 ER 图法的&quot;超集&quot;，并且能够将静态数据和动态逻辑整合在一起，达到业务、数据库和代码三者的统一。</p><h1>09｜分层架构：怎样逃离&quot;大泥球&quot;？</h1><p>代码中不稳定的部分，应该依赖稳定的部分。</p><h1>10｜代码实现（上）：要&quot;贫血&quot;还是要&quot;充血&quot;？</h1><h2 id="“面向对象-还是-面向过程”？">“面向对象&quot;还是&quot;面向过程”？</h2><p>贫血模型指的是领域对象中只有数据，没有行为，这种风格违背了面向对象的原则。</p><p>“富领域模型”，也就是领域对象里既包含数据，也包含行为。</p><p>DDD 强调，<strong>在代码编写阶段，如果发现模型的问题，要及时修改模型，始终保持代码和模型的一致</strong>。</p><p>领域模型图中一定不能存在只有技术人员才懂的内容。</p><h1>11｜代码实现（中）：怎样创建领域对象、实现领域逻辑？</h1><h2 id="“表意接口”（Intention-Revealing-Interfaces）模式">“表意接口”（Intention-Revealing Interfaces）模式</h2><p>DDD 强调，每个类和方法的命名都应该尽量直观地反映领域知识，与统一语言保持一致。这种做法也是 DDD 的一个模式，叫做 “Intention-Revealing Interfaces”，可以译作<strong>表意接口</strong>。</p><h2 id="“领域服务”（Domain-Service）模式">“领域服务”（Domain Service）模式</h2><p>如果一个逻辑需要和领域专家讨论才能确认的，就是领域逻辑；如果领域专家根本不感兴趣的，多半就是应用逻辑。</p><h2 id="“工厂”（Factory）模式">“工厂”（Factory）模式</h2><p>DDD 认为，领域对象的创建逻辑也是领域层的一部分。如果创建领域对象的逻辑比较简单，可以直接用对象的构造器来实现。但是如果比较复杂，就应该把创建逻辑放到一个专门的机制里，来保证领域对象的简洁和聚焦。</p><p>这里说的专门机制可以是一个方法或者一个类，可以有很多种实现方式。不论具体方式是什么，在 DDD 里统称为<strong>工厂</strong>（Factory）模式。准确地说，工厂其实是用来创建聚合的。工厂和前面说的仓库这两个模式，其实是一种隐喻（metaphor）：<strong>用工厂来创造产品，然后存到仓库</strong>。</p><p>所谓创建逻辑复杂，包括两方面：一是规则复杂，二是结构复杂。DDD 认为用于校验的领域逻辑也属于创建过程的一部分。</p><h2 id="模块划分的-打横-与-打竖">模块划分的&quot;打横&quot;与&quot;打竖&quot;</h2><p>尽管《重构》一书中没有明确说，但是一个包下有太多的类，也是一种常见的坏味道。那么多少算&quot;多&quot;呢，也没有明确的标准，不过根据心理学的认知负载理论，我建议，一个包里的类和子包的数量加在一起，最好不要超过 9 个。</p><p>事实上，<strong>一个应用服务和调用它的控制器以及被它调用的领域对象之间才具有耦合性</strong>。分层架构把本来耦合的几个对象拆到了不同的包。之所以我们在分层架构中采用按性质分包的方式，是因为，将领域逻辑与其非领域逻辑分离，以及将技术相关和无关两部分分离，这两个&quot;关注点分离&quot;的好处实在太大，大过了破坏&quot;松耦合高内聚&quot;原则的代价。</p><p><strong>但是另一方面，在每个层次内部，我比较建议尽量按照耦合性分包</strong>。这是由于排除上述两个&quot;关注点分离&quot;以后，&quot;松耦合高内聚&quot;的好处就体现出来了。如果把按层次划分叫做&quot;打横&quot;分，把按耦合性划分叫做&quot;打竖&quot;分，咱们目前采取的是就是&quot;先横后竖&quot;的分法。</p><h1>12｜代码实现（下）：怎样更加&quot;面向对象&quot;？</h1><h2 id="通过-表意接口-提高封装性">通过&quot;表意接口&quot;提高封装性</h2><p>在面向对象设计中一个常见的陷阱就是滥用继承。要防止这一倾向，要记住一个原则，<strong>不要仅仅为了复用而使用继承</strong>。你还要问自己一个问题，父类和子类的关系，在语义上，<strong>是否有分类关系，或者概念的普遍和特殊关系</strong>。只有符合这种关系的，才能采用继承，否则应该用&quot;组合&quot;来实现复用。</p><h1>13｜迭代二概述：怎样更深刻地理解领域知识？</h1><h2 id="模型的建立">模型的建立</h2><p><img data-src="/images/hand-in-hand-landing-ddd/13-%E5%BB%BA%E7%AB%8B%E6%A8%A1%E5%9E%8B.png" alt></p><h2 id="模型的实现">模型的实现</h2><p><img data-src="/images/hand-in-hand-landing-ddd/13-%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B.png" alt></p><!-- ## 迭代二的内容第一个要讲的是"聚合"。聚合是若干个具有整体部分关系的领域对象的组合。而且，需要维护有强一致的业务规则。聚合能帮助我们更简单地确定事务边界，保护业务规则。第二个是"值对象"。我们在迭代一讲过，领域对象分成实体和值对象两种。实体比较好理解，而值对象的含义就众说纷纭了。我们在这个迭代会从建模、编码、背后的认识事物的思维等方面把值对象讲清楚。理解了值对象，可以更细致地为领域知识建模，避免一些建模中常见的错误，而且也可以促使我们更深刻地理解实体。接下来，我们会讲"限定"。这个技术掌握起来并没有想象中难，而且很有用，可以起到深化领域知识和简化关联的作用。《DDD》一书中也做了强调。最后一个是"泛化"。泛化是建模技能由初级向中、高级进阶的关键技能，可以帮我们进一步提高模型的抽象程度。恰当使用的话，会使模型变得更加灵活和强大。当然，我也会介绍什么时候"不"使用泛化。 --><h1>14｜聚合的概念：怎样保护业务规则？</h1><h2 id="聚合的概念">聚合的概念</h2><p>第一，<strong>具有整体与部分的关系</strong>。也就是说，逻辑上，员工信息是整体，而技能信息是员工信息的一部分。<br>第二，<strong>具有不变规则，而且这种不变规则在并发的时候可能被破坏</strong>。要防止规则的破坏，仅仅锁住一条技能记录是不够的，必须把员工和所有技能作为一个整体锁住才能解决。或者说，员工和他的所有技能确定了一个事务边界。</p><p>具有这样特征的一组领域对象，在 DDD 里就叫做一个<strong>聚合</strong>（Aggregate）。</p><h2 id="聚合的表示法">聚合的表示法</h2><p><img data-src="/images/hand-in-hand-landing-ddd/14-%E8%81%9A%E5%90%88%E7%9A%84%E8%A1%A8%E7%A4%BA%E6%B3%95.png" alt></p><p>让我们看看这个图是怎么表达的。</p><p>首先，在员工实体名字上方，我们加了一个 &lt;&lt;aggregate root&gt;&gt; 的标识，中文是聚合根的意思。<strong>在一个聚合里，像员工这样代表整体的实体就是聚合根。一个聚合只有一个聚合根。</strong></p><p>&lt;&lt;aggregate root&gt;&gt; 外面这个像书名号一样的符号，其实不是书名号，而是两个小于号和两个大于号。这个符号在 UML 里叫做 stereotype ，中文译作 “<strong>衍型</strong>” 。这是 UML里用来扩充符号意思的一种机制。</p><p>比如说，表示员工实体的方框在 UML 中本来用来表示&quot;类&quot;。加上 &lt;&lt;aggregate root&gt;&gt;以后，就衍生出了&quot;表示聚合根的类&quot;的符号。所谓&quot;衍型&quot;就是&quot;衍生出来的符号类型&quot;。这种机制我们后面还会用到。</p><p>再看表示<strong>员工</strong>和<strong>技能</strong>一对多关联的那条实线。在员工一端，变成了一个空心棱形。这种符号专门表示整体部分关系，有菱形的一端是代表&quot;整体&quot;的对象，另一端是代表&quot;部分&quot;的对象。整体部分关系是关联关系的一种特例。</p><p>另外，原来这一端的 “1…1” 被删掉了。因为，对于这种整体部分关系而言，这一端必然是&quot;1…1&quot;。你可以思考一下为什么。虽然写上 “1…1” 也对，但由于必然是，所以出于简洁的原因，就可以不写了。</p><p>最后，我们用一个包把这个<strong>聚合</strong>中的类包起来，从而可以一眼看出这个<strong>聚合</strong>的边界。一般我们约定，<strong>聚合包的名字和聚合根的名字是一样的</strong>。</p><h2 id="识别更多的聚合">识别更多的聚合</h2><p>不过，一般来说，业务人员最关心的就是当前客户经理，历史变更信息只在少数情况下才用到。所以，领域专家希望强调<strong>当前客户经理</strong>这个概念。因此，我们保留了这个关联。</p><p>这个关联是可以由客户经理推导出来的，称为&quot;<strong>派生关联</strong>&quot;（derived association）。仔细看一下，在&quot;/ 当前客户经理&quot;前面有一个斜杠。在 UML 中，凡是前面有斜杠的，就表示是派生出来的内容。</p><p>识别出聚合以后，模型图变成下面的样子。</p><p><img data-src="/images/hand-in-hand-landing-ddd/14-%E8%AF%86%E5%88%AB%E6%9B%B4%E5%A4%9A%E8%81%9A%E5%90%88.png" alt></p><h2 id="进一步理解聚合概念">进一步理解聚合概念</h2><p>首先，作为部分的实体，只能属于一个聚合根，不可能属于多个聚合根。比如说，一条技能信息，只能属于一个员工，不能属于多个员工。又比如说，我的手只能是我一个人的手，不能同时又是其他人的手。</p><p>其次，我的手是不能&quot;跳槽&quot;的。不能今天是我的手，明天就变成了别人的手。也就是说，一个聚合的一部分，不能再变成其他聚合根的一部分。</p><p>再次，由前两条自然可以推出，<strong>聚合根被删除，那么聚合中的所有对象都会被删除</strong>。</p><p>最后，还有一个&quot;标识&quot;的问题。在业务上，为了识别每个实体，实体必然要有一个<strong>标识</strong>。例如，人的标识，可以是身份证号。如果这个人是学生，那么他的标识也可以是学号。注意，这里说的标识是一个业务概念，而不是技术概念，和数据库表中常见的没有业务概念的 ID 是不同的。</p><p>对于聚合而言，<strong>聚合根要有全局的唯一标识</strong>，而从属于聚合根的实体只需要有局部于聚合的标识。例如，员工是聚合根，员工号是全局标识。而工作经验没有必要进行全局编号，只需要在聚合内部编个号就可以了。例如，001 号员工的第 1 份工作经验、第 2 份工作经验等等。</p><p>我们再来考虑一下聚合的作用。<strong>聚合最基本的作用，是为一组具有整体部分关系的对象维护不变规则</strong>。而当我们掌握了这种建模技术以后，还可以发现其他一些层面的作用。</p><p>首先，聚合不仅是&quot;被动地&quot;实现不变规则，它还为我们提供了一个新的视角，可以更细致地和业务人员讨论业务规则。从这个视角去思考过去做过的系统，我们很可能会发现一些遗漏的业务规则。</p><p>其次，开发人员过去一般认为事务只是一个技术概念。现在我们可以看到，事务其实是来源于业务规则的，本质上是个业务问题。也就是说，聚合在业务规则和事务之间建立了起联系。</p><p>再次，我们在模型上为每个聚合建了一个包，可以认为，聚合是一种特殊的模块。这样，模型的层次就变得更清晰了。同时，我们也可以把<strong>聚合</strong>当作一个粗粒度的概念单位进行思考，降低了认知负载。</p><p>最后，不少开发人员编程时觉得事务范围的大小不好把握。聚合作为一个事务边界，给出了事务范围的下限，为开发时确定事务范围提供了参考。</p><h2 id="总结-v2">总结</h2><p>聚合是 DDD 里的一个重要模式，主要作用是<strong>维护不变规则</strong>。如果一组对象具有整体部分关系，并且需要维护整体上的不变规则，那么就可以识别为一个聚合。其中表示整体的那个实体叫做聚合根。</p><p>为了在模型中表示聚合，我们使用了叫做 &lt;&lt;aggregate root&gt;&gt; 的衍型来表示聚合根；在关联上用空心菱形符号表示整体部分关系；并用一个包把聚合包起来，包的名字一般和聚合根的名字相同。另外，在识别客户经理等聚合的时候，我们还介绍了<strong>派生关联</strong>。</p><p>通过整体部分这一特征，我们还可以推出其他几个特征，包括：表示部分的实体只能属于一个聚合，并且不能再变成其他聚合的一部分；聚合根被删除的话，整个聚合的实体都要被删除；聚合根有全局标识，非聚合根实体只有局部标识。</p><p>聚合的作用，除了确保不变规则以外，还为我们增加了一个分析业务规则的视角，将业务规则和事务联系起来，增加了模型的清晰度，并且使开发人员更容易确定事务的范围。</p><h1>18｜值对象（上）：到底什么是值对象？</h1><h2 id="值对象的概念">值对象的概念</h2><p>在 DDD 里，像<strong>员工</strong>这样有单独的标识，理论上可以改变的对象，就叫做<strong>实体</strong>（Entiy）；像<strong>员工状态</strong>和<strong>时间段</strong>这样没有单独的标识，并且不可改变的对象，就叫<strong>值对象</strong>（Value Object)。</p><p>从直观上看，实体是一个&quot;东西&quot;，而值对象是一个&quot;值&quot;，往往用来描述一个实体的属性，这也是<strong>值对象</strong>名字的由来。</p><h2 id="多种多样的值对象">多种多样的值对象</h2><h3 id="原子值对象-vs-复合值对象">原子值对象 vs 复合值对象</h3><p>首先，我们可以把值对象分成原子的和复合的。</p><p><strong>所谓原子值对象，是在概念上不能再拆分的值对象</strong>。比如说，整数、布尔值，日期、颜色以及状态等等，一般都建模成值对象。他们只有一个属性，不能再分了。</p><p><strong>而复合值对象是其他对象组合起来的值对象</strong>。</p><p>举个例子，“长度&quot;对象是由&quot;数值&quot;和&quot;长度单位&quot;两个属性组成的，比如&quot;5 米”，&quot;3毫米&quot;等等。“姓名&quot;一般也认为是值对象，由&quot;姓&quot;和&quot;名&quot;两个属性组成，如果考虑国际化，还要加上&quot;中间名”。“地址&quot;常常也认为是值对象，属性包括&quot;国家”、“省”、“市”、“区”、“街道”、&quot;门牌号&quot;等。还有，&quot;字符串&quot;也是复合值对象，它是由一系列的字符组成的，这种组合方式和前面几种不太一样。</p><p>现在你知道为什么 Java 里面 String 对象是不可变的了吧？因为它是<strong>值对象</strong>。但是你可能又发现一个问题，Java 里 Date（日期）是可变的，而我们上面说日期是值对象，不可变。这是为什么呢？</p><p>其实呀，把 Date 实现成可变的，是早期 JDK 设计的一个错误，这带来了很多问题。直到JDK8 引入了新的日期和时间库，也就是 LocalDate、 LocalDatetime 这些类型，才完美地解决了这个问题。而这些新的类型都是不可变的。</p><p>你看，哪怕是发明 Java 的牛人，有时候也没搞清楚什么是值对象。</p><p><strong>最后还有一种常见的复合值对象，就是所谓<code>快照</code></strong>。</p><p>比如修改员工的时候，可能需要把修改历史留下来，也就是我们可以看到员工信息的各个版本。一种做法就是建一个员工历史表，里面的字段和员工表差不多。每次修改，都把修改前的员工数据存一份到历史表。这些信息，就是员工在某个时刻的&quot;快照&quot;。快照是不可变的，因为它是历史信息，历史是不可改变的。多数值对象都比较小，但快照有时会很大，但仍然是值对象。</p><h2 id="独立的值对象-vs-依附于实体的值对象">独立的值对象 vs 依附于实体的值对象</h2><p>另外，<strong>值对象还可以分成独立的和依附于实体的</strong>。比如说，“时间段”、&quot;整数&quot;都是独立的，它们可以用来描述任何实体的属性，所以可以不依附于任何实体而单独存在。但是，<strong>员工状态</strong>就是依附于实体的，它只能表达员工这个实体的状态，脱离了员工，员工状态也就没有单独存在的意义了。</p><h2 id="可数值对象-vs-连续值对象">可数值对象 vs 连续值对象</h2><p><strong>值对象也可以分成可数的和连续的</strong>。可数值对象是离散的，可以一个一个列出来。比如说整数和日期、员工状态都是可数的。而实数则是连续的值对象。像颜色这样的值对象，在自然界里本来是连续的，但由于技术的限制，在计算机里一般实现为可数的，比如说，一些老式的系统只支持 256 种颜色。</p><h2 id="预定义值对象-vs-非预定义值对象">预定义值对象 vs 非预定义值对象</h2><p>最后，值对象还可以分成预定义的和非预定义的。</p><p><strong>所谓预定义的，就是需要以某种方式在系统里，把这种对象的值定义出来</strong>，常见的方式有程序里的枚举类型、数据库定义表，配置文件等。比如说，员工状态的三个对象&quot;试用期&quot; “正式工” “终止”，就是用枚举的方式定义在程序里的。而用于构造地址的&quot;省&quot; &quot;市&quot;则常常定义在数据库表里。</p><p><strong>非预定义的值对象就不必预先定义在系统里了</strong>，比如说&quot;整数&quot;，由于是无限的，根本就没有办法预定义。我们不可能用一个数据库表把所有整数都定义进去，当然，也没这个必要。</p><h1>20｜值对象（下）：值对象和实体的本质区别是什么？</h1><p>实体是靠独立于其他属性的标识来确定同一性的，而值对象以本身的值来确定同一性，没有独立于其他属性的标识；理论上，实体是可变的，而值对象是不可变的。</p><h2 id="值对象和实体的本质区别">值对象和实体的本质区别</h2><p>现实中的事物，也就是实体，总有一个产生和消亡的过程，在这个过程里，各种属性也可能发生变化，因此是可变的。</p><p>而值对象则是纯粹的概念产物，唯一的目的就是方便人的思考和沟通。所以，这样的概念本身并没有自然的产生和消亡过程，也不需要改变。5 就是 5 ， 5 如果变成 6 ，那么就已经是另一个值对象了，原来的 5 还在那里，并没有改变。换句话说，值对象并没有实体意义上的&quot;生命周期&quot;。因此，谈论值对象的改变，本身是没有意义的，这就是值对象不变性的本质原<br>因。</p><h1>21｜用&quot;限定&quot;建模：怎样简化一对多关联？</h1><p><img data-src="/images/hand-in-hand-landing-ddd/21-%E9%99%90%E5%AE%9A%E7%AC%A6.png" alt></p><p>之前，<strong>员工</strong>和<strong>工作经验</strong>之间有一个一对多关联。现在，在员工那一端加了一个小方框，里面写了&quot;: 时间段&quot;，而另一端的多重性，由原来的&quot;0…*“神奇地变成了&quot;0…1”。</p><p>这种方式所表达的意思是说，对于一个<strong>员工</strong>而言，任何一个<strong>时间段</strong>，要么没有<strong>工作经验</strong>，要么有一条<strong>工作经验</strong>，但不能有多条<strong>工作经验</strong>。换句话说，总体上看，一个<strong>员工</strong>可以有多条<strong>工作经验</strong>，但限定在一个<strong>时间段</strong>的话，那么最多就只能有一条<strong>工作经验</strong>了。</p><p>所以，这种机制就叫作&quot;限定&quot;(qualification)。而上面那个标有&quot;: 时间段&quot;的小方框，叫做&quot;限定符&quot;（qualifier）。</p><p>我们不难发现，限定机制起到了两个作用：第一，表达了更丰富的语义，把原来用注解说明的约束变成了更严格的符号；第二，简化了关联关系的多重性，把原来的一对多，在形式上，变成了一对一。</p><h1>27｜迭代三概述：怎样处理规模更大的系统？</h1><h2 id="聚合">聚合</h2><p>聚合（aggregate）是一组有整体部分关系，并且要满足一定不变规则的领域对象，其中只有一个实体表示整体，这个实体叫做聚合根。</p><p><strong>聚合的整体与部分是强关联的，也就是一旦聚合根被删除，其他部分必然也被删除。由于这种强关系，所以外界只能通过聚合根来访问非根对象，因此，只有聚合根有业务意义上的全局标识，非聚合根实体只有局部标识</strong>。</p><p>对于那些单独存在，而不属于其他聚合的实体，可以认为是只有聚合根的、退化的聚合。</p><p>在模型图里，<strong>我们可以用 &lt;&lt;aggregarte root&gt;&gt; 衍型结合菱形符号表示聚合根，并且把聚合相关的实体以及专属于聚合的值对象放在一个包里</strong>。</p><p>不变规则，指的是每时每刻都不能打破的规则。如果可以暂时打破，后面再补救，就不算不变规则了。对于聚合整体上的不变规则，需要在聚合根或者和聚合配合的领域服务中维护。</p><p>此外，我们还要考虑不变规则在并发的情况下被破坏的情况，这就要用事务把聚合的操作保护起来。所以，聚合决定了事务的最小边界。这种事务常常要用乐观锁或悲观锁来实现。</p><p>在编程上，非根实体的增、删、改，一般要由聚合根或者和聚合配合使用的工厂或领域服务来负责，外部不能直接修改非聚合根。为了实现这一点，我们可以将非聚合根的构造器和 setter设成包级私有权限。此外，聚合根返回非根实体的列表时，应该转换成不可变列表。</p><p>《领域驱动设计》原书的第 6.1 节介绍了聚合，你可以去看看。</p><h2 id="值对象">值对象</h2><p>值对象（value object）通常用来表示实体的属性值。由于值对象是纯粹的概念产物，因此并不存在从创建到消亡的生命周期，在概念上也是不可变的。而另一方面，实体则是现实中的概念，存在从产生到消亡的生命周期，理论上是可变的。</p><p>由于实体的属性变了，仍然是这个实体，所以必须具有独立于其他属性的标识，通过这个标识来判断实体的同一性。也就是只要标识一样，哪怕属性变了，这个实体还是这个实体。而值对象是不变的，所以不需要独立于其他属性的标识，而是以所有的属性值作为一个整体来判断同一性。</p><p>在模型图上，<strong>可以用 &lt;&lt;value&gt;&gt; 衍型来表示值对象。我们还要注意一点，值对象也是要封装领域逻辑的，因此不是 DTO（数据传输对象）</strong>。</p><p>值对象的主要优点是在内存和数据库布局上的灵活性，既可以采用共享的方式，也可以采用不共享的方式，这是实体所不具备的。同时，不变性也可以避免程序错误，有利于并发程序的编写和函数式编程。</p><p>《领域驱动设计》原书第 5.3 节介绍了值对象。</p><h2 id="限定">限定</h2><p>限定（qualification）可以起到简化关联的多重性，丰富模型语义的作用。如果两个实体之间本来是一对多的关系，而某个属性固定后，就可以变成一对一的关系，那么就可以使用限定。</p><p>限定在数据库里可以表现为主键和限定属性组成的唯一索引；而在程序里可以用 <strong>Map</strong> 来表示。</p><p>《领域驱动设计》原书第 5.1 节介绍关联的时候，也同时讲了限定。</p><h2 id="泛化">泛化</h2><p>泛化（generalization）表示的是分类关系，是领域建模中强大的抽象机制。当我们发现一些对象既有共性又有个性的时候，就可以考虑使用泛化。另一方面，泛化又有可能使模型复杂化，因此，是否使用泛化要经过权衡，可以从简洁性、可理解性、可维护性等方面，想一想使用泛化是否合适。</p><p>在模型中，<strong>泛化用空三角箭头来表示</strong>。</p><p>在数据库表的设计上，有三种策略：每个类一个表、每个子类一个表、整个泛化体系一个表。这三种策略的选择要考虑多种因素进行权衡。另外，还有共享主键和不共享主键两种关于主键的策略。</p><p>在编程上，通常用类的继承或接口的实现来表示泛化。由仓库进行数据库数据到内存数据的转换。仓库屏蔽了不同数据库设计策略的差别。</p><p>《领域驱动设计》原书中不少章节（例如第 3.2 节、8.1 节、8.4 节、9.1 节、9.2 节 、10.4节、10.8 节 、11 章、12.1 节、 12.2 节、14.12 节、 16.4 节、16.5 节）的例子都使用了泛化， 但书里并没有章节专门介绍泛化，算是一个小小的缺憾吧。</p><h1>28｜限界上下文（上）：怎样为更大的需求建模？</h1><h2 id="限界上下文的含义">限界上下文的含义</h2><p>限界上下文确实和划分模块、划分子系统一样，是一种分而治之的手段，可以起到分离关注点的作用。但限界上下文增加了一个要点，就是，它的目的还在于<strong>维护概念一致性</strong>。正是这一点，造成限界上下文和传统方法的本质不同。</p><p>DDD 认为，面对大规模的系统，全局概念一致性从根本上是不可能的。这是因为，人的认知能力是有限的。由于大型软件是团队协作开发的，因此这里其实是团队的认知能力。当系统规模增大时，团队规模也会相应增大，沟通难度会呈非线性增长。当系统达到一定规模，就超过了一个团队的认知能力，无法保证概念的一致性了。</p><p>这时候，就要把大系统分解成若干子系统，每个子系统对应一个领域模型。每个模型的规模都不超过一个开发小组的认知负载。<strong>在每个子系统的内部实现概念的严格一致性，而不同系统内部之间则没有必要一致。也就是说，不再追求全局一致性，而是退而求其次，只需追求局部的一致性，使概念不一致的问题得到合理管控，从而实现业务目标，这样就足够了。</strong></p><p>概念的一致性是通过语义上的一致性来表达的，所以 Evans 引用了语言学上&quot;上下文&quot;（context）的术语，来表示一个子系统、子模型或者维护这个子系统的团队。</p><p>语言学认为，一个词汇或者语句，只有在一个上下文里才有确切的含义。</p><h2 id="划分限界上下文">划分限界上下文</h2><p>通常一个敏捷的团队的大小是 7 ± 2，也就是 5 ~ 9 个人。也有人说所谓 2 pizza team，也就是两个披萨够吃的规模，大概 10 个人上下。据说这个团队规模是有心理学的实验依据的。</p><p>敏捷软件开发里常常提到的&quot;康威定理&quot;。也就是说，系统的架构总是与组织的沟通结构趋于一致。通俗地说，就是怎么划分子系统，相应就会怎么划分开发小组。</p><h2 id="总结-v3">总结</h2><p>DDD 的限界上下文，是一种解决大系统或者大模型概念不一致的手段。我们把一个大模型分成几个小模型，保持每个小模型内部概念的一致性，而不同模型之间的概念不必一致，这种小模型就叫做限界上下文。</p><p>放弃对全局一致性不切实际的追求，退而求其次，代之以局部一致性，从而使概念一致性问题得到足够的管理，达到业务目标，这种思维就是限界上下文与传统划分模块或子系统思路的本质区别。</p><p>之所以限界上文不追求全局一致性，实际上是由于全局一致性已经超过了团队的认知负载。所以限界上下文的划分在理论上应该和团队的划分保持一致。这也印证了康威定律。</p><p>另外，统一语言只有在一个限界上下文中才有意义。或者说，一个限界上下文对应一套统一语言。</p><h1>31｜CQRS（上）：实现查询功能有什么诀窍？</h1><p>事实上，前人已经意识到了查询和其他功能的不同之处，主张采用不同的方式来处理查询逻辑，并提出了所谓 CQRS 架构。</p><p>最早提出这个说法的是 Greg Young。他把增、删、改功能称为 Command（命令），把查询称为 Query，这两种功能的职责不同，应该采用不同的方式来处理，因此叫做&quot;命令查询职责分离&quot;（Command Query Responsibility Segregation ），简称 CQRS。我们可以先粗放一点来理解，一共是两条规则。</p><p><strong>第一，命令要走领域模型。</strong><br><strong>第二，查询不走领域模型，直接用 SQL 和 DTO。</strong></p><h2 id="总结-v4">总结</h2><p>今天我们学习了 CQRS （Command Query Responsibility Segregation），也就是&quot;命令和查询职责分离&quot;模式。</p><p>尽管通过 DDD 的领域模型完成增、删、改等功能是很适合的，但是通过领域模型来实现查询功能，常常是比较繁琐的，而且性能也不高。因此， CQRS 就成了 DDD 的有力补充。</p><p>根据 CQRS ，命令（也就是增、删、改功能）和查询功能的实现逻辑应该是不一样的。</p><h1>34｜落地经验：怎样在实际项目中推广DDD？</h1><h2 id="改造现有系统的步骤">改造现有系统的步骤</h2><p><img data-src="/images/hand-in-hand-landing-ddd/34-%E6%94%B9%E9%80%A0%E8%80%81%E9%A1%B9%E7%9B%AE.png" alt></p><p><strong>第一步是反推领域模型</strong>。新建系统的时候是从需求到模型，可以叫做正推。而由于现有系统已经存在了，所以我们做的第一步，反而是从系统现状中&quot;反推&quot;出当前的领域模型，目的是客观地反映出系统当前的领域知识和逻辑。这时候的模型往往有不少问题，比如不能正确反映领域知识、存在矛盾、冗余等。</p><p>反推领域模型，我们可以从数据库、用户界面、代码等方面入手。一般是先看数据库，因为数据库里常常已经凝聚了 80% 的业务知识。如果数据库看不明白，再看界面，如果还不明白，就只能翻代码了。之所以把代码放在最后，是因为阅读代码比较耗时，尤其是现有系统的代码往往很难理解。</p><p>虽然课程是按照&quot;正推&quot;来讲的，但是我们也重点强调了需求、模型和实现的一致性。如果你把这个原理吃透，那么反推领域模型也就不在话下了。</p><p>反推出的领域模型可以为进一步的改进建立&quot;基线&quot;。在反推的过程中所发现的问题，也可以作为下一步建立目标领域模型的输入。</p><p><strong>第二步是建立目标领域模型</strong>。根据当前系统的痛点、问题以及业务需求，就可以建立目标领域模型，作为改进的方向。建立目标领域模型，一定要有明确的&quot;时间点&quot;。也就是说，这个目标是 1 年的、3 个月的、还是 2 周的。脱离时间谈目标是没有意义的。越远的目标应该越宏观，越粗粒度；越近的目标应该越具体，越细致。过于长远的目标模型往往难以落地，所以要合理地设置目标时间。</p><p><strong>第三步是设计演进路线</strong>。有了当前模型和目标模型，就可以分析两者之间的差距。跨越这个差距的过程就是改进的过程。设计演进路线最大的问题就是怎么保证可行性。一般要把改进过程化整为零，迭代实施，并且还要兼顾日常的业务需求，后面我们还会提到这个问题。</p><p><strong>第四步是迭代实施</strong>。最好基于敏捷软件开发方法，小步快跑地实施。在这个过程中，必然会对之前建立的目标领域模型进行反馈，不断改进。同时还要不断评估开发现状，保证不偏离目标。</p><h3 id="选择精益切片">选择精益切片</h3><p>上面这 4 步是改造现有系统的总体思路，但真正实施的时候，不能一开始就针对整个系统进行。</p><p>就拿反推领域模型来说，由于现有系统往往规模庞大逻辑复杂，如果针对整个系统反推模型，必然旷日持久。再加上建立目标领域模型的时间，可能半年就过去了。这段时间只有模型，没有任何落地的代码，也就看不到任何实际效果。于是人们往往失去耐心， DDD 无疾而终。</p><p>另一方面，在开始引入 DDD 的时候，架构师、领域专家、开发人员其实还没有真的掌握相关技能。只有落地到代码，形成闭环，才会有真切的感受，真正学懂 DDD。所以，就算我们愿意花半年时间来建模，由于没有掌握领域建模的精髓，也很难保证模型的正确性，更加无法落地到代码了。</p><p>所以，建议的做法是，首先选择系统中一个相对独立的小模块，然后按照前面的 4 步，尽快落地到代码并上线，建立最小闭环。通过这个过程，初步掌握 DDD 落地技能并取得实际效果。同时，这么做也能培养人才，积累经验，建立必要的开发流程。完成之后，再选择下一个切片，逐步扩大范围，并深化 DDD 的技能。</p><p>这个相对独立的模块往往称为&quot;精益切片&quot;。精益切片的难度、范围、风险要适中，最好在 3 个月内形成最小闭环。</p><h2 id="低配版-的-DDD">&quot;低配版&quot;的 DDD</h2><p>再谈一个有意思的问题——怎样在推广过程中降低 DDD 的难度。</p><p>DDD 的知识点还是比较多的，而且其中有一些理解起来有一定难度。如果在推广过程中，一下子就让所有人掌握所有知识点，往往会造成很多误解，导致动作走形，影响推广效果。</p><p>所以，一开始可以聚焦在 DDD 最核心的问题上，暂时省略其他要点，推行一个&quot;低配版&quot;的 DDD。等到大家掌握了基本技能，需要更深层次的运用时，再引入其他知识点。</p><p>那么在开始的时候，哪些可以省略，哪些不能省略呢？我梳理了一张表，供你参考。<br><img data-src="/images/hand-in-hand-landing-ddd/34-DDD%E7%B2%BE%E7%AE%80%E7%89%88.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;手把手落地DDD&lt;br&gt;
作者：钟敬&lt;/p&gt;
&lt;p&gt;追风赶月莫停留，平芜尽处是春山。&lt;/p&gt;
&lt;p&gt;如果想看改造旧系统相关可以直接查看【34｜落地经验：怎样在实际项目中推广DDD？】&lt;/p&gt;</summary>
    
    
    
    <category term="DDD" scheme="https://jiankunking.com/categories/ddd/"/>
    
    
    <category term="读书笔记" scheme="https://jiankunking.com/tags/读书笔记/"/>
    
    <category term="原创" scheme="https://jiankunking.com/tags/原创/"/>
    
    <category term="DDD" scheme="https://jiankunking.com/tags/ddd/"/>
    
  </entry>
  
  <entry>
    <title>DDD实战课 笔记</title>
    <link href="https://jiankunking.com/ddd-practical-course.html"/>
    <id>https://jiankunking.com/ddd-practical-course.html</id>
    <published>2024-03-02T07:15:11.000Z</published>
    <updated>2025-06-12T06:06:32.156Z</updated>
    
    <content type="html"><![CDATA[<p>DDD实战课<br>作者：欧创新</p><a id="more"></a><h1>01 | 微服务设计为什么要选择DDD？</h1><p>微服务拆分困境产生的根本原因就是不知道业务或者微服务的边界到底在什么地方。</p><h2 id="为什么-DDD-适合微服务？">为什么 DDD 适合微服务？</h2><p><font color="DeepPink"><strong>DDD 是一种处理高度复杂领域的设计思想，它试图分离技术实现的复杂性，并围绕业务概念构建领域模型来控制业务的复杂性，以解决软件难以理解，难以演进的问题。DDD 不是架构，而是一种架构设计方法论，它通过边界划分将复杂业务领域简单化，帮我们设计出清晰的领域和应用边界，可以很容易地实现架构演进</strong>。</font></p><h3 id="DDD-包括战略设计和战术设计两部分">DDD 包括战略设计和战术设计两部分</h3><p>战略设计主要从业务视角出发，建立业务领域模型，划分领域边界，建立通用语言的限界上下文，限界上下文可以作为微服务设计的参考边界。</p><p>战术设计则从技术视角出发，侧重于领域模型的技术实现，完成软件开发和落地，包括：聚合根、实体、值对象、领域服务、应用服务和资源库等代码逻辑的设计和实现。</p><p>DDD 战略设计会建立领域模型，领域模型可以用于指导微服务的设计和拆分。<strong>事件风暴是建立领域模型的主要方法，它是一个从发散到收敛的过程。它通常采用用例分析、场景分析和用户旅程分析，尽可能全面不遗漏地分解业务领域，并梳理领域对象之间的关系，这是一个发散的过程。事件风暴过程会产生很多的实体、命令、事件等领域对象，我们将这些领域对象从不同的维度进行聚类，形成如聚合、限界上下文等边界，建立领域模型，这就是一个收敛的过程。</strong></p><p><img data-src="/images/ddd-practical-course/01-%E9%A2%86%E5%9F%9F.png" alt></p><h3 id="我们可以用三步来划定领域模型和微服务的边界。">我们可以用三步来划定领域模型和微服务的边界。</h3><p>第一步：在事件风暴中梳理业务过程中的用户操作、事件以及外部依赖关系等，根据这些要素梳理出领域实体等领域对象。</p><p>第二步：<strong>根据领域实体之间的业务关联性，将业务紧密相关的实体进行组合形成聚合，同时确定聚合中的聚合根、值对象和实体</strong>。在这个图里，聚合之间的边界是第一层边界，它们在同一个微服务实例中运行，这个边界是逻辑边界，所以用虚线表示。</p><p>第三步：根据业务及语义边界等因素，将一个或者多个聚合划定在一个限界上下文内，形成领域模型。在这个图里，限界上下文之间的边界是第二层边界，这一层边界可能就是未来微服务的边界，不同限界上下文内的领域逻辑被隔离在不同的微服务实例中运行，物理上相互隔离，所以是物理边界，边界之间用实线来表示。</p><p>有了这两层边界，微服务的设计就不是什么难事了</p><h2 id="DDD-与微服务的关系">DDD 与微服务的关系</h2><p>DDD 主要关注：<font color="red">从业务领域视角划分领域边界，构建通用语言进行高效沟通，通过业务抽象，建立领域模型，维持业务和代码的逻辑一致性。</font></p><p>微服务主要关注：运行时的进程间通信、容错和故障隔离，实现去中心化数据管理和去中心化服务治理，关注微服务的独立开发、测试、构建和部署。</p><h1>02 | 领域、子域、核心域、通用域和支撑域</h1><h2 id="如何理解领域和子域？">如何理解领域和子域？</h2><p>领域就是用来确定范围的，范围即边界，这也是DDD 在设计中不断强调边界的原因。</p><p><strong>在研究和解决业务问题时，DDD 会按照一定的规则将业务领域进行细分，当领域细分到一定的程度后，DDD 会将问题范围限定在特定的边界内，在这个边界内建立领域模型，进而用代码实现该领域模型，解决相应的业务问题</strong>。简言之，<font color="DeepPink">DDD 的领域就是这个边界内要解决的业务问题域</font>。</p><p>既然领域是用来限定业务边界和范围的，那么就会有大小之分，领域越大，业务范围就越大，反之则相反。</p><p>领域可以进一步划分为子领域。<strong>我们把划分出来的多个子领域称为子域，每个子域对应一个更小的问题域或更小的业务范围。</strong></p><p>DDD 的研究方法与自然科学的研究方法类似。当人们在自然科学研究中遇到复杂问题时，通常的做法就是将问题一步一步地细分，再针对细分出来的问题域，逐个深入研究，探索和建立所有子域的知识体系。当所有问题子域完成研究时，我们就建立了全部领域的完整知识体系了。</p><p><img data-src="/images/ddd-practical-course/02-%E6%A4%8D%E7%89%A9%E7%A0%94%E7%A9%B6%E6%97%B6%E9%A2%86%E5%9F%9F%E7%9A%84%E7%BB%86%E5%88%86.png" alt></p><p>我们来看一下上面这张图。这个例子是在讲如何给桃树建立一个完整的生物学知识体系。初中生物课其实早就告诉我们研究方法了。它的研究过程是这样的。</p><p>第一步：确定研究对象，即研究领域，这里是一棵桃树。</p><p>第二步：对研究对象进行细分，将桃树细分为器官，器官又分为营养器官和生殖器官两种。其中营养器官包括根、茎和叶，生殖器官包括花、果实和种子。桃树的知识体系是我们已经确定要研究的问题域，对应 DDD 的领域。根、茎、叶、花、果实和种子等器官则是细分后的问题子域。这个过程就是 DDD 将领域细分为多个子域的过程。</p><p>第三步：对器官进行细分，将器官细分为组织。比如，叶子器官可细分为保护组织、营养组织和输导组织等。这个过程就是 DDD 将子域进一步细分为多个子域的过程。</p><p>第四步：对组织进行细分，将组织细分为细胞，细胞成为我们研究的最小单元。细胞之间的细胞壁确定了单元的边界，也确定了研究的最小边界。</p><p>不同行业的业务模型可能会不一样，但领域建模和微服务建设的过程和方法基本类似，其核心思想就是将问题域逐步分解，降低业务理解和系统实现的复杂度。</p><h2 id="如何理解核心域、通用域和支撑域？">如何理解核心域、通用域和支撑域？</h2><p>在领域不断划分的过程中，领域会细分为不同的子域，子域可以根据自身重要性和功能属性划分为三类子域，它们分别是：核心域、通用域和支撑域。</p><p><strong>决定产品和公司核心竞争力的子域是核心域，它是业务成功的主要因素和公司的核心竞争力。没有太多个性化的诉求，同时被多个子域使用的通用功能子域是通用域。还有一种功能子域是必需的，但既不包含决定产品和公司核心竞争力的功能，也不包含通用功能的子域，它就是支撑域。</strong></p><h3 id="那为什么要划分核心域、通用域和支撑域，主要目的是什么呢？">那为什么要划分核心域、通用域和支撑域，主要目的是什么呢？</h3><p>还是拿上图的桃树来说吧。我们将桃树细分为了根、茎、叶、花、果实和种子等六个子域，那桃树是否有核心域？有的话，到底哪个是核心域呢？</p><p>不同的人对桃树的理解是不同的。如果这棵桃树生长在公园里，在园丁的眼里，他喜欢的是&quot;人面桃花相映红&quot;的阳春三月，这时花就是桃树的核心域。但如果这棵桃树生长在果园里，对果农来说，他则是希望在丰收的季节收获硕果累累的桃子，这时果实就是桃树的核心域。</p><p>在不同的场景下，不同的人对桃树核心域的理解是不同的，因此对桃树的处理方式也会不一样。园丁更关注桃树花期的营养，而果农则更关注桃树落果期的营养，有时为了保证果实的营养供给，还会裁剪掉疯长的茎和叶（通用域或支撑域）。</p><p>核心域、通用域、支撑域的划分本质是公司战略方向的体现，DDD是从战略到战术角度来进行架构设计的方法。</p><h2 id="总结">总结</h2><p><strong>领域的核心思想就是将问题域逐级细分，来降低业务理解和系统实现的复杂度。通过领域细分，逐步缩小微服务需要解决的问题域，构建合适的领域模型，而领域模型映射成系统就是微服务了</strong>。</p><h1>03 | 限界上下文：定义领域边界的利器</h1><p>通用语言定义上下文含义，限界上下文则定义领域边界，以确保每个上下文含义在它特定的边界内都具有唯一的含义，领域模型则存在于这个边界之内。</p><h2 id="什么是通用语言？">什么是通用语言？</h2><p>在事件风暴过程中，通过团队交流达成共识的，能够简单、清晰、准确描述业务涵义和规则的语言就是通用语言。</p><p>下面我带你看一张图，这张图描述了从事件风暴建立通用语言到领域对象设计和代码落地的完整过程。<br><img data-src="/images/ddd-practical-course/03-%E4%BB%8E%E4%BA%8B%E4%BB%B6%E9%A3%8E%E6%9A%B4%E5%BB%BA%E7%AB%8B%E9%80%9A%E7%94%A8%E8%AF%AD%E8%A8%80%E5%88%B0%E9%A2%86%E5%9F%9F%E5%AF%B9%E8%B1%A1%E8%AE%BE%E8%AE%A1%E5%92%8C%E4%BB%A3%E7%A0%81%E8%90%BD%E5%9C%B0%E7%9A%84%E5%AE%8C%E6%95%B4%E8%BF%87%E7%A8%8B.png" alt></p><p>设计过程中我们可以用一些表格，来记录事件风暴和微服务设计过程中产生的领域对象及其属性。比如，领域对象在 DDD 分层架构中的位置、属性、依赖关系以及与代码模型对象的映射关系等。</p><p><img data-src="/images/ddd-practical-course/03-%E8%A1%A8%E6%A0%BC.png" alt></p><p>到这里，我要再强调一次。DDD 分析和设计过程中的每一个环节都需要保证限界上下文内术语的统一，在代码模型设计的时侯就要建立领域对象和代码对象的一一映射，从而<strong>保证业务模型和代码模型的一致，实现业务语言与代码语言的统一。</strong></p><p>如果你做到了这一点，也就是建立了领域对象和代码对象的映射关系，那就可以指导软件开发人员准确无误地按照设计文档完成微服务开发了。即使是不熟悉代码的业务人员，也可以很快找到代码的位置。</p><h2 id="什么是限界上下文？">什么是限界上下文？</h2><blockquote><p>限界上下文大概是直译过来的一个晦涩的术语，理解成本较高。英文是bounded context，应该叫上下文边界更合适。</p></blockquote><p>DDD 在战略设计上提出了&quot;限界上下文&quot;这个概念，用来确定语义所在的领域边界。</p><p>我们可以将限界上下文拆解为两个词：限界和上下文。限界就是领域的边界，而上下文则是语义环境。通过领域的限界上下文，我们就可以在统一的领域边界内用统一的语言进行交流。</p><p>限界上下文的定义就是：<strong>用来封装通用语言和领域对象，提供上下文环境，保证在领域之内的一些术语、业务相关对象等（通用语言）有一个确切的含义，没有二义性</strong>。</p><p>现在我们用一个保险领域的例子来说明下术语的边界。保险业务领域有投保单、保单、批单、赔案等保险术语，它们分别应用于保险的不同业务流程。</p><ol><li>客户投保时，业务人员记录投保信息，系统对应有投保单实体对象。</li><li>缴费完成后，业务人员将投保单转为保单，系统对应有保单实体对象，保单实体与投保单实体关联。</li><li>如客户需要修改保单信息，保单变为批单，系统对应有批单实体对象，批单实体与保单实体关联。</li><li>如果客户发生理赔，生成赔案，系统对应有报案实体对象，报案实体对象与保单或者批单实体关联。</li></ol><p>投保单、保单、批单、赔案等，这些术语虽然都跟保单有关，但不能将保单这个术语作用在保险全业务领域。因为术语有它的边界，超出了边界理解上就会出现问题。</p><p>正如电商领域的商品一样，商品在不同的阶段有不同的术语，在销售阶段是商品，而在运输阶段则变成了货物。同样的一个东西，由于业务领域的不同，赋予了这些术语不同的涵义和职责边界，这个边界就可能会成为未来微服务设计的边界。看到这，我想你应该非常清楚了，领域边界就是通过限界上下文来定义的。</p><h2 id="限界上下文和微服务的关系">限界上下文和微服务的关系</h2><p>首先，领域可以拆分为多个子领域。一个领域相当于一个问题域，领域拆分为子域的过程就是大问题拆分为小问题的过程。在这个图里面保险领域被拆分为：投保、支付、保单管理和理赔四个子域。</p><p>子域还可根据需要进一步拆分为子子域，比如，支付子域可继续拆分为收款和付款子子域。拆到一定程度后，有些子子域的领域边界就可能变成限界上下文的边界了。</p><p>子域可能会包含多个限界上下文，如理赔子域就包括报案、查勘和定损等多个限界上下文（限界上下文与理赔的子子域领域边界重合）。也有可能子域本身的边界就是限界上下文边界，如投保子域。</p><p>每个领域模型都有它对应的限界上下文，团队在限界上下文内用通用语言交流。领域内所有限界上下文的领域模型构成整个领域的领域模型。</p><p>理论上限界上下文就是微服务的边界。<strong>我们将限界上下文内的领域模型映射到微服务，就完成了从问题域到软件的解决方案。</strong></p><h2 id="总结-v2">总结</h2><p>通用语言确定了项目团队内部交流的统一语言，而这个语言所在的语义环境则是由限界上下文来限定的，以确保语义的唯一性。</p><p>而领域专家、架构师和开发人员的主要工作就是通过事件风暴来划分限界上下文。限界上下文确定了微服务的设计和拆分方向，是微服务设计和拆分的主要依据。如果不考虑技术异构、团队沟通等其它外部因素，一个限界上下文理论上就可以设计为一个微服务。</p><p>可以说，限界上下文在微服务设计中具有很重要的意义，如果限界上下文的方向偏离，那微服务的设计结果也就可想而知了。因此，我们只有理解了限界上下文的真正涵义以及它在微服务设计中的作用，才能真正发挥 DDD 的价值，这是基础也是前提。</p><h1>04 | 实体和值对象：从领域模型的基础单元看系统设计</h1><h2 id="实体">实体</h2><p>在 DDD 中有这样一类对象，它们拥有唯一标识符，且标识符在历经各种状态变更后仍能保持一致。对这些对象而言，重要的不是其属性，而是其延续性和标识，对象的延续性和标识会跨越甚至超出软件的生命周期。我们把这样的对象称为实体。</p><h3 id="1-实体的业务形态">1. 实体的业务形态</h3><p>在 DDD 不同的设计过程中，实体的形态是不同的。在战略设计时，实体是领域模型的一个重要对象。领域模型中的实体是多个属性、操作或行为的载体。在事件风暴中，我们可以根据命令、操作或者事件，找出产生这些行为的业务实体对象，进而按照一定的业务规则将依存度高和业务关联紧密的多个实体对象和值对象进行聚类，形成聚合。你可以这么理解，<strong>实体和值对象是组成领域模型的基础单元。</strong></p><h3 id="2-实体的代码形态">2. 实体的代码形态</h3><p>在代码模型中，实体的表现形式是实体类，这个类包含了实体的属性和方法，通过这些方法实现实体自身的业务逻辑。在 DDD 里，<strong>这些实体类通常采用充血模型</strong>，与这个实体相关的所有业务逻辑都在实体类的方法中实现，跨多个实体的领域逻辑则在领域服务中实现。</p><h3 id="3-实体的运行形态">3. 实体的运行形态</h3><p>实体以 DO（领域对象）的形式存在，每个实体对象都有唯一的 ID。我们可以对一个实体对象进行多次修改，修改后的数据和原来的数据可能会大不相同。但是，由于它们拥有相同的ID，它们依然是同一个实体。比如商品是商品上下文的一个实体，通过唯一的商品 ID 来标识，不管这个商品的数据如何变化，商品的 ID 一直保持不变，它始终是同一个商品。</p><h3 id="4-实体的数据库形态">4. 实体的数据库形态</h3><p>与传统数据模型设计优先不同，DDD 是先构建领域模型，针对实际业务场景构建实体对象和行为，再将实体对象映射到数据持久化对象。</p><p>在领域模型映射到数据模型时，一个实体可能对应 0 个、1 个或者多个数据库持久化对象。大多数情况下实体与持久化对象是一对一。在某些场景中，有些实体只是暂驻静态内存的一个运行态实体，它不需要持久化。比如，基于多个价格配置数据计算后生成的折扣实体。</p><p>而在有些复杂场景下，实体与持久化对象则可能是一对多或者多对一的关系。比如，用户user 与角色 role 两个持久化对象可生成权限实体，一个实体对应两个持久化对象，这是一对多的场景。再比如，有些场景为了避免数据库的联表查询，提升系统性能，会将客户信息customer 和账户信息 account 两类数据保存到同一张数据库表中，客户和账户两个实体可根据需要从一个持久化对象中生成，这就是多对一的场景。</p><h2 id="值对象">值对象</h2><p>《实现领域驱动设计》一书中对值对象的定义：<strong>通过对象属性值来识别的对象，它将多个相关属性组合为一个概念整体。</strong> 在 DDD 中用来描述领域的特定方面，并且是一个没有标识符的对象，叫作值对象。</p><p>也就说，值对象描述了领域中的一件东西，这个东西是不可变的，它将不同的相关属性组合成了一个概念整体。当度量和描述改变时，可以用另外一个值对象予以替换。它可以和其它值对象进行相等性比较，且不会对协作对象造成副作用。</p><p><strong>简单来说，值对象本质上就是一个集合。</strong> 那这个集合里面有什么呢？若干个用于描述目的、具有整体概念和不可修改的属性。那这个集合存在的意义又是什么？在领域建模的过程中，值对象可以保证属性归类的清晰和概念的完整性，避免属性零碎。</p><p><img data-src="/images/ddd-practical-course/04-%E5%80%BC%E5%AF%B9%E8%B1%A1.png" alt></p><p>人员实体原本包括：姓名、年龄、性别以及人员所在的省、市、县和街道等属性。这样显示地址相关的属性就很零碎了对不对？现在，我们可以将&quot;省、市、县和街道等属性&quot;拿出来构成一个&quot;地址属性集合&quot;，这个集合就是值对象了。</p><h3 id="1-值对象的业务形态">1. 值对象的业务形态</h3><p>值对象是 DDD 领域模型中的一个基础对象，它跟实体一样都来源于事件风暴所构建的领域模型，都包含了若干个属性，它与实体一起构成聚合。</p><p>我们不妨对照实体，来看值对象的业务形态，这样更好理解。本质上，实体是看得到、摸得着的实实在在的业务对象，实体具有业务属性、业务行为和业务逻辑。而值对象只是若干个属性的集合，只有数据初始化操作和有限的不涉及修改数据的行为，基本不包含业务逻辑。值对象的属性集虽然在物理上独立出来了，但在逻辑上它仍然是实体属性的一部分，用于描述实体的特征。</p><p>在值对象中也有部分共享的标准类型的值对象，它们有自己的限界上下文，有自己的持久化对象，可以建立共享的数据类微服务，比如数据字典。</p><h3 id="2-值对象的代码形态">2. 值对象的代码形态</h3><p>值对象在代码中有这样两种形态。如果值对象是单一属性，则直接定义为实体类的属性；如果值对象是属性集合，则把它设计为 Class 类，Class 将具有整体概念的多个属性归集到属性集合，这样的值对象没有 ID，会被实体整体引用。</p><p>我们看一下下面这段代码，person 这个实体有若干个单一属性的值对象，比如 Id、name 等属性；同时它也包含多个属性的值对象，比如地址 address。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">person</span></span>&#123;<span class="comment">// 实体</span></span><br><span class="line">  <span class="keyword">public</span> String id;<span class="comment">//值对象人员唯一主键</span></span><br><span class="line">  <span class="keyword">public</span> String name;<span class="comment">//单一属性值对象</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">int</span> age;<span class="comment">//单一属性值对象</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">boolean</span> gender;<span class="comment">//单一属性值对象</span></span><br><span class="line">  <span class="keyword">public</span> Address address;<span class="comment">//属性集值对象，被实体引用</span></span><br><span class="line">  <span class="comment">//方法不列举了</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Address</span></span>&#123; <span class="comment">// 值对象无主键ID</span></span><br><span class="line">  <span class="keyword">public</span> String province;<span class="comment">//值对象</span></span><br><span class="line">  <span class="keyword">public</span> String city;<span class="comment">//值对象</span></span><br><span class="line">  <span class="keyword">public</span> String county;<span class="comment">//值对象</span></span><br><span class="line">  <span class="keyword">public</span> String street;<span class="comment">//值对象</span></span><br><span class="line">  <span class="comment">//方法不列举了</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-值对象的运行形态">3. 值对象的运行形态</h3><p>实体实例化后的 DO 对象的业务属性和业务行为非常丰富，但值对象实例化的对象则相对简单和乏味。除了值对象数据初始化和整体替换的行为外，其它业务行为就很少了。</p><p>值对象嵌入到实体的话，有这样两种不同的数据格式，也可以说是两种方式，分别是属性嵌入的方式和序列化大对象的方式。</p><p>引用单一属性的值对象或只有一条记录的多属性值对象的实体，可以采用属性嵌入的方式嵌入。引用一条或多条记录的多属性值对象的实体，可以采用序列化大对象的方式嵌入。比如，人员实体可以有多个通讯地址，多个地址序列化后可以嵌入人员的地址属性。值对象创建后就不允许修改了，只能用另外一个值对象来整体替换。</p><p>如果你对这两种方式不够了解，可以看看下面的例子。<br>案例 1：以属性嵌入的方式形成的人员实体对象，地址值对象直接以属性值嵌入人员实体中。<br><img data-src="/images/ddd-practical-course/04-02%E5%80%BC%E5%AF%B9%E8%B1%A1%E5%B1%9E%E6%80%A7%E5%B5%8C%E5%85%A5.png" alt><br>案例 2：以序列化大对象的方式形成的人员实体对象，地址值对象被序列化成大对象 Json 串后，嵌入人员实体中。<br><img data-src="/images/ddd-practical-course/04-03%E5%80%BC%E5%AF%B9%E8%B1%A1%E5%BA%8F%E5%88%97%E5%8C%96%E5%A4%A7%E5%AF%B9%E8%B1%A1%E5%BD%A2%E5%BC%8F.png" alt></p><h3 id="4-值对象的数据库形态">4. 值对象的数据库形态</h3><p>DDD 引入值对象是希望实现从&quot;数据建模为中心&quot;向&quot;领域建模为中心&quot;转变，减少数据库表的数量和表与表之间复杂的依赖关系，尽可能地简化数据库设计，提升数据库性能。</p><p>如何理解用值对象来简化数据库设计呢？</p><p>传统的数据建模大多是根据数据库范式设计的，每一个数据库表对应一个实体，每一个实体的属性值用单独的一列来存储，一个实体主表会对应 N 个实体从表。而值对象在数据库持久化方面简化了设计，它的数据库设计大多采用非数据库范式，值对象的属性值和实体对象的属性值保存在同一个数据库实体表中。</p><p>举个例子，还是基于上述人员和地址那个场景，实体和数据模型设计通常有两种解决方案：第一是把地址值对象的所有属性都放到人员实体表中，创建人员实体，创建人员数据表；第二是创建人员和地址两个实体，同时创建人员和地址两张表。</p><p>第一个方案会破坏地址的业务涵义和概念完整性，第二个方案增加了不必要的实体和表，需要处理多个实体和表的关系，从而增加了数据库设计的复杂性。</p><p>那到底应该怎样设计，才能让业务含义清楚，同时又不让数据库变得复杂呢？</p><p>我们可以综合这两个方案的优势，扬长避短。在领域建模时，我们可以把地址作为值对象，人员作为实体，这样就可以保留地址的业务涵义和概念完整性。而在数据建模时，我们可以将地址的属性值嵌入人员实体数据库表中，只创建人员数据库表。这样既可以兼顾业务含义和表达，又不增加数据库的复杂度。</p><p>值对象就是通过这种方式，简化了数据库设计，总结一下就是：在领域建模时，我们可以将部分对象设计为值对象，保留对象的业务涵义，同时又减少了实体的数量；在数据建模时，我们可以将值对象嵌入实体，减少实体表的数量，简化数据库设计。</p><p>另外，<strong>也有 DDD 专家认为，要想发挥对象的威力，就需要优先做领域建模，弱化数据库的作用，只把数据库作为一个保存数据的仓库即可。即使违反数据库设计原则，也不用大惊小怪，只要业务能够顺利运行，就没什么关系。</strong></p><h3 id="5-值对象的优势和局限">5. 值对象的优势和局限</h3><p>值对象是一把双刃剑，它的优势是可以简化数据库设计，提升数据库性能。但如果值对象使用不当，它的优势就会很快变成劣势。“知彼知己，方能百战不殆”，你需要理解值对象真正适合的场景。</p><p>值对象采用序列化大对象的方法简化了数据库设计，减少了实体表的数量，可以简单、清晰地表达业务概念。这种设计方式虽然降低了数据库设计的复杂度，但却无法满足基于值对象的快速查询，会导致搜索值对象属性值变得异常困难。</p><p>值对象采用属性嵌入的方法提升了数据库的性能，但如果实体引用的值对象过多，则会导致实体堆积一堆缺乏概念完整性的属性，这样值对象就会失去业务涵义，操作起来也不方便。</p><p>所以，你可以对照着以上这些优劣势，结合你的业务场景，好好想一想了。那如果在你的业务场景中，值对象的这些劣势都可以避免掉，那就请放心大胆地使用值对象吧。</p><h2 id="实体和值对象的关系">实体和值对象的关系</h2><p>实体和值对象是微服务底层的最基础的对象，一起实现实体最基本的核心领域逻辑。</p><p><strong>值对象和实体在某些场景下可以互换，很多 DDD 专家在这些场景下，其实也很难判断到底将领域对象设计成实体还是值对象？可以说，值对象在某些场景下有很好的价值，但是并不是所有的场景都适合值对象。</strong> 你需要根据团队的设计和开发习惯，以及上面的优势和局限分析，选择最适合的方法。</p><p>关于值对象，我还要多说几句。其实，DDD 引入值对象还有一个重要的原因，就是到底领域建模优先还是数据建模优先？</p><p><strong>DDD 提倡从领域模型设计出发，而不是先设计数据模型。</strong> 前面讲过了，传统的数据模型设计通常是一个表对应一个实体，一个主表关联多个从表，当实体表太多的时候就很容易陷入无穷无尽的复杂的数据库设计，领域模型就很容易被数据模型绑架。可以说，值对象的诞生，在一定程度上，和实体是互补的。</p><p>我们还是以前面的图示为例：<br><img data-src="/images/ddd-practical-course/04-%E5%80%BC%E5%AF%B9%E8%B1%A1.png" alt></p><p>在领域模型中人员是实体，地址是值对象，地址值对象被人员实体引用。在数据模型设计时，地址值对象可以作为一个属性集整体嵌入人员实体中，组合形成上图这样的数据模型；也可以以序列化大对象的形式加入到人员的地址属性中，前面表格有展示。</p><p>从这个例子中，我们可以看出，同样的对象在不同的场景下，可能会设计出不同的结果。有些场景中，地址会被某一实体引用，它只承担描述实体的作用，并且它的值只能整体替换，这时候你就可以将地址设计为值对象，比如收货地址。而在某些业务场景中，地址会被经常修改，地址是作为一个独立对象存在的，这时候它应该设计为实体，比如行政区划中的地址信息维护。</p><h1>05 | 聚合和聚合根：怎样设计聚合？</h1><p>聚合（Aggregate）<br>聚合根（AggregateRoot）</p><p>在事件风暴中，我们会根据一些业务操作和行为找出实体（Entity）或值对象（ValueObject），进而将业务关联紧密的实体和值对象进行组合，构成聚合，再根据业务语义将多个聚合划定到同一个限界上下文（Bounded Context）中，并在限界上下文内完成领域建模。</p><h2 id="聚合">聚合</h2><p>在 DDD 中，实体和值对象是很基础的领域对象。实体一般对应业务对象，它具有业务属性和业务行为；而值对象主要是属性集合，对实体的状态和特征进行描述。但实体和值对象都只是个体化的对象，它们的行为表现出来的是个体的能力。</p><h3 id="那聚合在其中起什么作用呢？">那聚合在其中起什么作用呢？</h3><p>举个例子。社会是由一个个的个体组成的，象征着我们每一个人。随着社会的发展，慢慢出现了社团、机构、部门等组织，我们开始从个人变成了组织的一员，大家可以协同一致的工作，朝着一个最大的目标前进，发挥出更大的力量。</p><p><strong>领域模型内的实体和值对象就好比个体，而能让实体和值对象协同工作的组织就是聚合，它用来确保这些领域对象在实现共同的业务逻辑时，能保证数据的一致性。</strong></p><p>你可以这么理解，聚合就是由业务和逻辑紧密关联的实体和值对象组合而成的，聚合是数据修改和持久化的基本单元，每一个聚合对应一个仓储，实现数据的持久化。</p><p>聚合有一个聚合根和上下文边界，这个边界根据业务单一职责和高内聚原则，定义了聚合内部应该包含哪些实体和值对象，而聚合之间的边界是松耦合的。按照这种方式设计出来的微服务很自然就是&quot;高内聚、低耦合&quot;的。</p><p><strong>聚合在 DDD 分层架构里属于领域层，领域层包含了多个聚合，共同实现核心业务逻辑。聚合内实体以充血模型实现个体业务能力，以及业务逻辑的高内聚。跨多个实体的业务逻辑通过领域服务来实现，跨多个聚合的业务逻辑通过应用服务来实现。</strong> 比如有的业务场景需要同一个聚合的 A 和 B 两个实体来共同完成，我们就可以将这段业务逻辑用领域服务来实现；而有的业务逻辑需要聚合 C 和聚合 D 中的两个服务共同完成，这时你就可以用应用服务来组合这两个服务。</p><h2 id="聚合根">聚合根</h2><p>聚合根的主要目的是为了避免由于复杂数据模型缺少统一的业务规则控制，而导致聚合、实体之间数据不一致性的问题。</p><p>传统数据模型中的每一个实体都是对等的，如果任由实体进行无控制地调用和数据修改，很可能会导致实体之间数据逻辑的不一致。而如果采用锁的方式则会增加软件的复杂度，也会降低系统的性能。</p><p><strong>如果把聚合比作组织，那聚合根就是这个组织的负责人。聚合根也称为根实体，它不仅是实体，还是聚合的管理者。</strong></p><p>首先它作为实体本身，拥有实体的属性和业务行为，实现自身的业务逻辑。</p><p>其次它作为聚合的管理者，在聚合内部负责协调实体和值对象按照固定的业务规则协同完成共同的业务逻辑。</p><p>最后在聚合之间，它还是聚合对外的接口人，以聚合根 ID 关联的方式接受外部任务和请求，在上下文内实现聚合之间的业务协同。也就是说，聚合之间通过聚合根 ID 关联引用，如果需要访问其它聚合的实体，就要先访问聚合根，再导航到聚合内部实体，外部对象不能直接访问聚合内实体。</p><h2 id="怎样设计聚合？">怎样设计聚合？</h2><p><strong>DDD 领域建模通常采用事件风暴，它通常采用用例分析、场景分析和用户旅程分析等方法，通过头脑风暴列出所有可能的业务行为和事件，然后找出产生这些行为的领域对象，并梳理领域对象之间的关系，找出聚合根，找出与聚合根业务紧密关联的实体和值对象，再将聚合根、实体和值对象组合，构建聚合。</strong></p><p>下面我们以保险的投保业务场景为例，看一下聚合的构建过程主要都包括哪些步骤。<br><img data-src="/images/ddd-practical-course/05-01-%E8%81%9A%E5%90%88%E6%9E%84%E5%BB%BA.png" alt></p><p>第 1 步：采用事件风暴，根据业务行为，梳理出在投保过程中发生这些行为的所有的实体和值对象，比如投保单、标的、客户、被保人等等。</p><p>第 2 步：从众多实体中选出适合作为对象管理者的根实体，也就是聚合根。判断一个实体是否是聚合根，你可以结合以下场景分析：是否有独立的生命周期？是否有全局唯一 ID？是否可以创建或修改其它对象？是否有专门的模块来管这个实体。图中的聚合根分别是投保单和客户实体。</p><p>第 3 步：根据业务单一职责和高内聚原则，找出与聚合根关联的所有紧密依赖的实体和值对象。构建出 1 个包含聚合根（唯一）、多个实体和值对象的对象集合，这个集合就是聚合。在图中我们构建了客户和投保这两个聚合。</p><p>第 4 步：在聚合内根据聚合根、实体和值对象的依赖关系，画出对象的引用和依赖模型。这里我需要说明一下：投保人和被保人的数据，是通过关联客户 ID 从客户聚合中获取的，在投保聚合里它们是投保单的值对象，这些值对象的数据是客户的冗余数据，即使未来客户聚合的数据发生了变更，也不会影响投保单的值对象数据。从图中我们还可以看出实体之间的引用关系，比如在投保聚合里投保单聚合根引用了报价单实体，报价单实体则引用了报价规则子实体。</p><p>第 5 步：多个聚合根据业务语义和上下文一起划分到同一个限界上下文内。</p><p>这就是一个聚合诞生的完整过程了。</p><h3 id="聚合的一些设计原则">聚合的一些设计原则</h3><ol><li><p><strong>在一致性边界内建模真正的不变条件。</strong> 聚合用来封装真正的不变性，而不是简单地将对象组合在一起。聚合内有一套不变的业务规则，各实体和值对象按照统一的业务规则运行，实现对象数据的一致性，边界之外的任何东西都与该聚合无关，这就是聚合能实现业务高内聚的原因。</p></li><li><p><strong>设计小聚合。</strong> 如果聚合设计得过大，聚合会因为包含过多的实体，导致实体之间的管理过于复杂，高频操作时会出现并发冲突或者数据库锁，最终导致系统可用性变差。而小聚合设计则可以降低由于业务过大导致聚合重构的可能性，让领域模型更能适应业务的变化。</p></li><li><p><strong>通过唯一标识引用其它聚合。</strong> 聚合之间是通过关联外部聚合根 ID 的方式引用，而不是直接对象引用的方式。外部聚合的对象放在聚合边界内管理，容易导致聚合的边界不清晰，也会增加聚合之间的耦合度。</p></li><li><p><strong>在边界之外使用最终一致性。</strong> 聚合内数据强一致性，而聚合之间数据最终一致性。在一次事务中，最多只能更改一个聚合的状态。如果一次业务操作涉及多个聚合状态的更改，应采用领域事件的方式异步修改相关的聚合，实现聚合之间的解耦。</p></li><li><p><strong>通过应用层实现跨聚合的服务调用。</strong> 为实现微服务内聚合之间的解耦，以及未来以聚合为单位的微服务组合和拆分，应避免跨聚合的领域服务调用和跨聚合的数据库表关联。</p></li></ol><p>上面的这些原则是 DDD 的一些通用的设计原则，还是那句话：&quot;适合自己的才是最好的。&quot;在系统设计过程时，你一定要考虑项目的具体情况，如果面临使用的便利性、高性能要求、技术能力缺失和全局事务管理等影响因素，这些原则也并不是不能突破的，总之一切以解决实际问题为出发点。</p><h2 id="总结-v3">总结</h2><p><strong>聚合的特点：</strong> 高内聚、低耦合，它是领域模型中最底层的边界，可以作为拆分微服务的最小单位，但我不建议你对微服务过度拆分。但在对性能有极致要求的场景中，聚合可以独立作为一个微服务，以满足版本的高频发布和极致的弹性伸缩能力。</p><p>一个微服务可以包含多个聚合，聚合之间的边界是微服务内天然的逻辑边界。有了这个逻辑边界，在微服务架构演进时就可以以聚合为单位进行拆分和组合了，微服务的架构演进也就不再是一件难事了。</p><p><strong>聚合根的特点：</strong> 聚合根是实体，有实体的特点，具有全局唯一标识，有独立的生命周期。一个聚合只有一个聚合根，聚合根在聚合内对实体和值对象采用直接对象引用的方式进行组织和协调，聚合根与聚合根之间通过 ID 关联的方式实现聚合之间的协同。</p><p><strong>实体的特点：</strong> 有 ID 标识，通过 ID 判断相等性，ID 在聚合内唯一即可。状态可变，它依附于聚合根，其生命周期由聚合根管理。实体一般会持久化，但与数据库持久化对象不一定是一对一的关系。实体可以引用聚合内的聚合根、实体和值对象。</p><p><strong>值对象的特点：</strong> 无 ID，不可变，无生命周期，用完即扔。值对象之间通过属性值判断相等性。它的核心本质是值，是一组概念完整的属性组成的集合，用于描述实体的状态和特征。值对象尽量只引用值对象。</p><h1>06 | 领域事件：解耦微服务的关键</h1><p>在事件风暴（Event Storming）时，我们发现除了命令和操作等业务行为以外，还有一种非常重要的事件，<strong>这种事件发生后通常会导致进一步的业务操作，在 DDD 中这种事件被称为领域事件。</strong></p><h2 id="领域事件">领域事件</h2><p>领域事件是领域模型中非常重要的一部分，用来表示领域中发生的事件。一个领域事件将导致进一步的业务操作，在实现业务解耦的同时，还有助于形成完整的业务闭环。</p><p>举例来说的话，领域事件可以是业务流程的一个步骤，比如投保业务缴费完成后，触发投保单转保单的动作；也可能是定时批处理过程中发生的事件，比如批处理生成季缴保费通知单，触发发送缴费邮件通知操作；或者一个事件发生后触发的后续动作，比如密码连续输错三次，触发锁定账户的动作。</p><h3 id="那如何识别领域事件呢？">那如何识别领域事件呢？</h3><p>很简单，和刚才讲的定义是强关联的。在做用户旅程或者场景分析时，我们要捕捉业务、需求人员或领域专家口中的关键词：“如果发生……，则……”“当做完……的时候，请通知……” &quot;发生……时，则……&quot;等。在这些场景中，如果发生某种事件后，会触发进一步的操作，那么这个事件很可能就是领域事件。</p><p>那领域事件为什么要用最终一致性，而不是传统 SOA 的直接调用的方式呢？</p><p>我们一起回顾一下聚合的一个设计原则：在边界之外使用最终一致性。一次事务最多只能更改一个聚合的状态。如果一次业务操作涉及多个聚合状态的更改，应采用领域事件的最终一致性。</p><p>领域事件驱动设计可以切断领域模型之间的强依赖关系，事件发布完成后，发布方不必关心后续订阅方事件处理是否成功，这样可以实现领域模型的解耦，维护领域模型的独立性和数据的一致性。在领域模型映射到微服务系统架构时，领域事件可以解耦微服务，微服务之间的数据不必要求强一致性，而是基于事件的最终一致性。</p><p>回到具体的业务场景，我们发现有的领域事件发生在微服务内的聚合之间，有的则发生在微服务之间，还有两者皆有的场景，一般来说跨微服务的领域事件处理居多。在微服务设计时不同领域事件的处理方式会不一样。</p><h3 id="1-微服务内的领域事件">1. 微服务内的领域事件</h3><p>当领域事件发生在微服务内的聚合之间，领域事件发生后完成事件实体构建和事件数据持久化，发布方聚合将事件发布到事件总线，订阅方接收事件数据完成后续业务操作。</p><p>微服务内大部分事件的集成，都发生在同一个进程内，进程自身可以很好地控制事务，因此不一定需要引入消息中间件。但一个事件如果同时更新多个聚合，按照 DDD&quot;一次事务只更新一个聚合&quot;的原则，你就要考虑是否引入事件总线。但微服务内的事件总线，可能会增加开发的复杂度，因此你需要结合应用复杂度和收益进行综合考虑。</p><p>微服务内应用服务，可以通过跨聚合的服务编排和组合，以服务调用的方式完成跨聚合的访问，这种方式通常应用于实时性和数据一致性要求高的场景。这个过程会用到分布式事务，以保证发布方和订阅方的数据同时更新成功。</p><h3 id="2-微服务之间的领域事件">2. 微服务之间的领域事件</h3><p>跨微服务的领域事件会在不同的限界上下文或领域模型之间实现业务协作，其主要目的是实现微服务解耦，减轻微服务之间实时服务访问的压力。</p><p>领域事件发生在微服务之间的场景比较多，事件处理的机制也更加复杂。跨微服务的事件可以推动业务流程或者数据在不同的子域或微服务间直接流转。</p><p>跨微服务的事件机制要总体考虑事件构建、发布和订阅、事件数据持久化、消息中间件，甚至事件数据持久化时还可能需要考虑引入分布式事务机制等。</p><p>微服务之间的访问也可以采用应用服务直接调用的方式，实现数据和服务的实时访问，弊端就是跨微服务的数据同时变更需要引入分布式事务，以确保数据的一致性。分布式事务机制会影响系统性能，增加微服务之间的耦合，所以我们还是要尽量避免使用分布式事务。</p><h2 id="领域事件相关案例">领域事件相关案例</h2><p>我来给你介绍一个保险承保业务过程中有关领域事件的案例。</p><p>一个保单的生成，经历了很多子域、业务状态变更和跨微服务业务数据的传递。这个过程会产生很多的领域事件，这些领域事件促成了保险业务数据、对象在不同的微服务和子域之间的流转和角色转换。</p><p>在下面这张图中，我列出了几个关键流程，用来说明如何用领域事件驱动设计来驱动承保业务流程。<br><img data-src="/images/ddd-practical-course/06-01-%E4%BF%9D%E5%8D%95%E7%94%9F%E6%88%90.png" alt></p><p><strong>事件起点：客户购买保险 - 业务人员完成保单录入 - 生成投保单 - 启动缴费动作。</strong></p><ol><li>投保微服务生成缴费通知单，发布第一个事件：缴费通知单已生成，将缴费通知单数据发布到消息中间件。收款微服务订阅缴费通知单事件，完成缴费操作。缴费通知单已生成，领域事件结束。</li><li>收款微服务缴费完成后，发布第二个领域事件：缴费已完成，将缴费数据发布到消息中间件。原来的订阅方收款微服务这时则变成了发布方。原来的事件发布方投保微服务转换为订阅方。投保微服务在收到缴费信息并确认缴费完成后，完成投保单转成保单的操作。缴费已完成，领域事件结束。</li><li>投保微服务在投保单转保单完成后，发布第三个领域事件：保单已生成，将保单数据发布到消息中间件。保单微服务接收到保单数据后，完成保单数据保存操作。保单已生成，领域事件结束。</li><li>保单微服务完成保单数据保存后，后面还会发生一系列的领域事件，以并发的方式将保单数据通过消息中间件发送到佣金、收付费和再保等微服务，一直到财务，完后保单后续所有业务流程。这里就不详细说了。</li></ol><p>总之，通过领域事件驱动的异步化机制，可以推动业务流程和数据在各个不同微服务之间的流转，实现微服务的解耦，减轻微服务之间服务调用的压力，提升用户体验。</p><h2 id="领域事件总体架构">领域事件总体架构</h2><p>领域事件的执行需要一系列的组件和技术来支撑。我们来看一下这个领域事件总体技术架构图，<strong>领域事件处理包括：事件构建和发布、事件数据持久化、事件总线、消息中间件、事件接收和处理等。</strong> 下面我们逐一讲一下。</p><p><img data-src="/images/ddd-practical-course/06-%E9%A2%86%E5%9F%9F%E6%80%BB%E4%BD%93%E6%9E%B6%E6%9E%84.png" alt></p><h3 id="事件数据持久化">事件数据持久化</h3><p>事件数据持久化有两种方案，在实施过程中你可以根据自己的业务场景进行选择。</p><ul><li>持久化到本地业务数据库的事件表中，利用本地事务保证业务和事件数据的一致性。</li><li>持久化到共享的事件数据库中。这里需要注意的是：业务数据库和事件数据库不在一个数据库中，它们的数据持久化操作会跨数据库，因此需要分布式事务机制来保证业务和事件数据的强一致性，结果就是会对系统性能造成一定的影响。</li></ul><h3 id="事件总线-EventBus">事件总线 (EventBus)</h3><p>事件总线是实现<strong>微服务</strong>内聚合之间领域事件的重要组件，它提供事件分发和接收等服务。事件总线是进程内模型，它会在微服务内聚合之间遍历订阅者列表，采取同步或异步的模式传递数据。事件分发流程大致如下：</p><ul><li>如果是微服务内的订阅者（其它聚合），则直接分发到指定订阅者；</li><li>如果是微服务外的订阅者，将事件数据保存到事件库（表）并异步发送到消息中间件；</li><li>如果同时存在微服务内和外订阅者，则先分发到内部订阅者，将事件消息保存到事件库（表），再异步发送到消息中间件。</li></ul><h3 id="消息中间件">消息中间件</h3><p><strong>跨微服务</strong>的领域事件大多会用到消息中间件，实现跨微服务的事件发布和订阅。</p><h2 id="领域事件运行机制相关案例">领域事件运行机制相关案例</h2><p>这里我用承保业务流程的缴费通知单事件，来给你解释一下领域事件的运行机制。这个领域事件发生在投保和收款微服务之间。发生的领域事件是：缴费通知单已生成。下一步的业务操作是：缴费。</p><p><img data-src="/images/ddd-practical-course/06-%E9%A2%86%E5%9F%9F%E4%BA%8B%E4%BB%B6-%E7%BC%B4%E8%B4%B9.png" alt></p><p><strong>事件起点：出单员生成投保单，核保通过后，发起生成缴费通知单的操作。</strong></p><ol><li>投保微服务应用服务，调用聚合中的领域服务 createPaymentNotice 和createPaymentNoticeEvent，分别创建缴费通知单、缴费通知单事件。其中缴费通知单事件类 PaymentNoticeEvent 继承基类 DomainEvent。</li><li>利用仓储服务持久化缴费通知单相关的业务和事件数据。为了避免分布式事务，这些业务和事件数据都持久化到本地投保微服务数据库中。</li><li>通过数据库日志捕获技术或者定时程序，从数据库事件表中获取事件增量数据，发布到消息中间件。这里说明：事件发布也可以通过应用服务或者领域服务完成发布。</li><li>收款微服务在应用层从消息中间件订阅缴费通知单事件消息主题，监听并获取事件数据后，应用服务调用领域层的领域服务将事件数据持久化到本地数据库中。</li><li>收款微服务调用领域层的领域服务 PayPremium，完成缴费。</li><li>事件结束。</li></ol><p>提示：缴费完成后，后续流程的微服务还会产生很多新的领域事件，比如缴费已完成、保单已保存等等。这些后续的事件处理基本上跟 1～6 的处理机制类似。</p><h2 id="总结-v4">总结</h2><p>领域事件是 DDD 的一个重要概念，在设计时我们要重点关注领域事件，用领域事件来驱动业务的流转，尽量采用基于事件的最终一致，降低微服务之间直接访问的压力，实现微服务之间的解耦，维护领域模型的独立性和数据一致性。</p><h1>07-DDD分层架构：有效降低层与层之间的依赖</h1><h2 id="什么是-DDD-分层架构？">什么是 DDD 分层架构？</h2><p><img data-src="/images/ddd-practical-course/07-01-DDD4%E5%B1%82%E6%9E%B6%E6%9E%84.png" alt></p><ol><li>用户接口层<br>用户接口层负责向用户显示信息和解释用户指令。这里的用户可能是：用户、程序、自动化测试和批处理脚本等等。</li><li>应用层<br>应用层是很薄的一层，理论上不应该有业务规则或逻辑，主要面向用例和流程相关的操作。但应用层又位于领域层之上，因为领域层包含多个聚合，所以它可以协调多个聚合的服务和领域对象完成服务编排和组合，协作完成业务操作。</li></ol><p>此外，应用层也是微服务之间交互的通道，它可以调用其它微服务的应用服务，完成微服务之间的服务组合和编排。</p><p>这里我要提醒你一下：<strong>在设计和开发时，不要将本该放在领域层的业务逻辑放到应用层中实现。因为庞大的应用层会使领域模型失焦，时间一长你的微服务就会演化为传统的三层架构，业务逻辑会变得混乱。</strong></p><p>另外，应用服务是在应用层的，它负责服务的组合、编排和转发，负责处理业务用例的执行顺序以及结果的拼装，以粗粒度的服务通过 API 网关向前端发布。还有，应用服务还可以进行安全认证、权限校验、事务控制、发送或订阅领域事件等。</p><ol start="3"><li>领域层<br>领域层的作用是实现企业核心业务逻辑，通过各种校验手段保证业务的正确性。领域层主要体现领域模型的业务能力，它用来表达业务概念、业务状态和业务规则。</li></ol><p>领域层包含聚合根、实体、值对象、领域服务等领域模型中的领域对象。</p><p>这里我要特别解释一下其中几个领域对象的关系，以便你在设计领域层的时候能更加清楚。首先，领域模型的业务逻辑主要是由实体和领域服务来实现的，其中实体会采用充血模型来实现所有与之相关的业务功能。其次，你要知道，实体和领域服务在实现业务逻辑上不是同级的，当领域中的某些功能，单一实体（或者值对象）不能实现时，领域服务就会出马，它可以组合聚合内的多个实体（或者值对象），实现复杂的业务逻辑。</p><ol start="4"><li>基础层<br>基础层是贯穿所有层的，它的作用就是为其它各层提供通用的技术和基础服务，包括第三方工具、驱动、消息中间件、网关、文件、缓存以及数据库等。比较常见的功能还是提供数据库持久化。</li></ol><p>基础层包含基础服务，它采用依赖倒置设计，封装基础资源服务，实现应用层、领域层与基础层的解耦，降低外部资源变化对应用的影响。</p><p>比如说，在传统架构设计中，由于上层应用对数据库的强耦合，很多公司在架构演进中最担忧的可能就是换数据库了，因为一旦更换数据库，就可能需要重写大部分的代码，这对应用来说是致命的。那采用依赖倒置的设计以后，应用层就可以通过解耦来保持独立的核心业务逻辑。当数据库变更时，我们只需要更换数据库基础服务就可以了，这样就将资源变更对应用的影响降到了最低。</p><h2 id="DDD-分层架构最重要的原则是什么？">DDD 分层架构最重要的原则是什么？</h2><p>DDD 分层架构有一个重要的原则：<strong>每层只能与位于其下方的层发生耦合。</strong></p><p>而架构根据耦合的紧密程度又可以分为两种：严格分层架构和松散分层架构。优化后的 DDD分层架构模型就属于严格分层架构，任何层只能对位于其直接下方的层产生依赖。而传统的DDD 分层架构则属于松散分层架构，它允许某层与其任意下方的层发生依赖。</p><p>那我们怎么选呢？综合我的经验，为了服务的可管理，我建议你采用严格分层架构。</p><p>在严格分层架构中，领域服务只能被应用服务调用，而应用服务只能被用户接口层调用，服务是逐层对外封装或组合的，依赖关系清晰。而在松散分层架构中，领域服务可以同时被应用层或用户接口层调用，服务的依赖关系比较复杂且难管理，甚至容易使核心业务逻辑外泄。</p><p>试想下，如果领域层中的某个服务发生了重大变更，那该如何通知所有调用方同步调整和升级呢？但在严格分层架构中，你只需要逐层通知上层服务就可以了。</p><h2 id="DDD-分层架构如何推动架构演进？">DDD 分层架构如何推动架构演进？</h2><p>1.微服务架构的演进<br>通过基础篇的讲解，我们知道：领域模型中对象的层次从内到外依次是：值对象、实体、聚合和限界上下文。</p><p>实体或值对象的简单变更，一般不会让领域模型和微服务发生大的变化。但聚合的重组或拆分却可以。这是因为聚合内业务功能内聚，能独立完成特定的业务逻辑。那聚合的重组或拆分，势必就会引起业务模块和系统功能的变化了。</p><p>这里我们可以以聚合为基础单元，完成领域模型和微服务架构的演进。聚合可以作为一个整体，在不同的领域模型之间重组或者拆分，或者直接将一个聚合独立为微服务。</p><p><img data-src="/images/ddd-practical-course/07-02-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B.png" alt></p><p>我们结合上图，以微服务 1 为例，讲解下微服务架构的演进过程：</p><ul><li>当你发现微服务 1 中聚合 a 的功能经常被高频访问，以致拖累整个微服务 1 的性能时，我们可以把聚合 a 的代码，从微服务 1 中剥离出来，独立为微服务 2。这样微服务 2 就可轻松应对高性能场景。</li><li>在业务发展到一定程度以后，你会发现微服务 3 的领域模型有了变化，聚合 d 会更适合放到微服务 1 的领域模型中。这时你就可以将聚合 d 的代码整体搬迁到微服务 1 中。如果你在设计时已经定义好了聚合之间的代码边界，这个过程不会太复杂，也不会花太多时间。</li><li>最后我们发现，在经历模型和架构演进后，微服务 1 已经从最初包含聚合 a、b、c，演进为包含聚合 b、c、d 的新领域模型和微服务了。</li></ul><p>你看，好的聚合和代码模型的边界设计，可以让你快速应对业务变化，轻松实现领域模型和微服务架构的演进。</p><h2 id="2-微服务内服务的演进">2.微服务内服务的演进</h2><p>在微服务内部，实体的方法被领域服务组合和封装，领域服务又被应用服务组合和封装。在服务逐层组合和封装的过程中，你会发现这样一个有趣的现象。</p><p><img data-src="/images/ddd-practical-course/07-03%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%86%85%E6%9C%8D%E5%8A%A1%E6%BC%94%E8%BF%9B.png" alt></p><p>我们看下上面这张图。在服务设计时，你并不一定能完整预测有哪些下层服务会被多少个上层服务组装，因此领域层通常只提供一些原子服务，比如领域服务 a、b、c。但随着系统功能增强和外部接入越来越多，应用服务会不断丰富。有一天你会发现领域服务 b 和 c 同时多次被多个应用服务调用了，执行顺序也基本一致。这时你可以考虑将 b 和 c 合并，再将应用服务中 b、c 的功能下沉到领域层，演进为新的领域服务（b+c）。这样既减少了服务的数量，也减轻了上层服务组合和编排的复杂度。</p><h2 id="三层架构如何演进到-DDD-分层架构？">三层架构如何演进到 DDD 分层架构？</h2><p><img data-src="/images/ddd-practical-course/07-03%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%86%85%E6%9C%8D%E5%8A%A1%E6%BC%94%E8%BF%9B.png" alt></p><p>我们看一下上面这张图，分析一下从三层架构向 DDD 分层架构演进的过程。</p><p>首先，你要清楚，三层架构向 DDD 分层架构演进，主要发生在业务逻辑层和数据访问层。DDD 分层架构在用户接口层引入了 DTO，给前端提供了更多的可使用数据和更高的展示灵活性。</p><p>DDD 分层架构对三层架构的业务逻辑层进行了更清晰的划分，改善了三层架构核心业务逻辑混乱，代码改动相互影响大的情况。DDD 分层架构将业务逻辑层的服务拆分到了应用层和领域层。应用层快速响应前端的变化，领域层实现领域模型的能力。</p><p>另外一个重要的变化发生在数据访问层和基础层之间。三层架构数据访问采用 DAO 方式；DDD 分层架构的数据库等基础资源访问，采用了仓储（Repository）设计模式，通过依赖倒置实现各层对基础资源的解耦。</p><p>仓储又分为两部分：仓储接口和仓储实现。仓储接口放在领域层中，仓储实现放在基础层。原来三层架构通用的第三方工具包、驱动、Common、Utility、Config 等通用的公共的资源类统一放到了基础层。</p><p>最后，我想说，传统三层架构向 DDD 分层架构的演进，体现的正是领域驱动设计思想的演进。希望你也感受到了，并尝试将其应用在自己的架构设计中。</p><h1>08-微服务架构模型：几种常见模型的对比和分析</h1><h2 id="从三种架构模型看中台和微服务设计">从三种架构模型看中台和微服务设计</h2><p><strong>中台本质上是领域的子域，它可能是核心域，也可能是通用域或支撑域。通常大家认为阿里的中台对应 DDD 的通用域，将通用的公共能力沉淀为中台，对外提供通用共享服务。</strong></p><p>中台作为子域还可以继续分解为子子域，在子域分解到合适大小，通过事件风暴划分限界上下文以后，就可以定义微服务了，微服务用来实现中台的能力。表面上看，DDD、中台、微服务这三者之间似乎没什么关联，实际上它们的关系是非常紧密的，组合在一起可以作为一个理论体系用于你的中台和微服务设计。</p><h3 id="1-中台建设要聚焦领域模型">1. 中台建设要聚焦领域模型</h3><p>中台需要站在全企业的高度考虑能力的共享和复用。</p><p>中台设计时，我们需要建立中台内所有限界上下文的领域模型，DDD 建模过程中会考虑架构演进和功能的重新组合。领域模型建立的过程会对业务和应用进行清晰的逻辑和物理边界（微服务）划分。领域模型的结果会影响到后续的系统模型、架构模型和代码模型，最终影响到微服务的拆分和项目落地。</p><p>因此，在中台设计中我们首先要聚焦领域模型，将它放在核心位置。</p><h3 id="2-微服务要有合理的架构分层">2. 微服务要有合理的架构分层</h3><p>微服务设计要有分层的设计思想，让各层各司其职，建立松耦合的层间关系。</p><p>不要把与领域无关的逻辑放在领域层实现，保证领域层的纯洁和领域逻辑的稳定，避免污染领域模型。也不要把领域模型的业务逻辑放在应用层，这样会导致应用层过于庞大，最终领域模型会失焦。如果实在无法避免，我们可以引入防腐层，进行新老系统的适配和转换，过渡期完成后，可以直接将防腐层代码抛弃。</p><p>微服务内部的分层方式我们已经清楚了，那微服务之间是否也有层次依赖关系呢？如何实现微服务之间的服务集成？</p><p>有的微服务可以与前端应用集成，一起完成特定的业务，这是项目级微服务。而有的则是某个职责单一的中台微服务，企业级的业务流程需要将多个这样的微服务组合起来才能完成，这是企业级中台微服务。两类微服务由于复杂度不一样，集成方式也会有差异。</p><h4 id="项目级微服务">项目级微服务</h4><p>项目级微服务的内部遵循分层架构模型就可以了。领域模型的核心逻辑在领域层实现，服务的组合和编排在应用层实现，通过 API 网关为前台应用提供服务，实现前后端分离。但项目级的微服务可能会调用其它微服务，你看在下面这张图中，比如某个项目级微服务 B 调用认证微服务 A，完成登录和权限认证。</p><p>通常项目级微服务之间的集成，发生在微服务的应用层，由应用服务调用其它微服务发布在API 网关上的应用服务。你看下图中微服务 B 中红色框内的应用服务 B，它除了可以组合和编排自己的领域服务外，还可以组合和编排外部微服务的应用服务。它只要将编排后的服务发布到 API 网关供前端调用，这样前端就可以直接访问自己的微服务了。</p><p><img data-src="/images/ddd-practical-course/08-02-%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BE%AE%E6%9C%8D%E5%8A%A1.png" alt></p><h4 id="企业级中台微服务">企业级中台微服务</h4><p>企业级的业务流程往往是多个中台微服务一起协作完成的，那跨中台的微服务如何实现集成呢？</p><p>企业级中台微服务的集成不能像项目级微服务一样，在某一个微服务内完成跨微服务的服务组合和编排。</p><p>我们可以在中台微服务之上增加一层，你看下面这张图，增加的这一层就位于红色框内，它的主要职能就是处理跨中台微服务的服务组合和编排，以及微服务之间的协调，它还可以完成前端不同渠道应用的适配。如果再将它的业务范围扩大一些，我可以将它做成一个面向不同行业和渠道的服务平台。</p><p>我们不妨借用 BFF（服务于前端的后端，Backend for Frontends）这个词，暂且称它为 BFF微服务。BFF 微服务与其它微服务存在较大的差异，就是它没有领域模型，因此这个微服务内也不会有领域层。BFF 微服务可以承担应用层和用户接口层的主要职能，完成各个中台微服务的服务组合和编排，可以适配不同前端和渠道的要求。</p><p><img data-src="/images/ddd-practical-course/08-03-%E4%BC%81%E4%B8%9A%E7%BA%A7%E4%B8%AD%E5%8F%B0%E5%BE%AE%E6%9C%8D%E5%8A%A1.png" alt></p><h3 id="3-应用和资源的解耦与适配">3. 应用和资源的解耦与适配</h3><p>传统以数据为中心的设计模式，应用会对数据库、缓存、文件系统等基础资源产生严重依赖。</p><p>正是由于它们之间的这种强依赖的关系，我们一旦更换基础资源就会对应用产生很大的影响，因此需要为应用和资源解耦。</p><p>在微服务架构中，应用层、领域层和基础层解耦是通过仓储模式，采用依赖倒置的设计方法来实现的。在应用设计中，我们会同步考虑和基础资源的代码适配，那么一旦基础设施资源出现变更（比如换数据库），就可以屏蔽资源变更对业务代码的影响，切断业务逻辑对基础资源的<br>依赖，最终降低资源变更对应用的影响。</p><h1>09 | 中台：数字转型后到底应该共享什么？</h1><h2 id="数字化转型中台应该共享什么？">数字化转型中台应该共享什么？</h2><p>由于传统企业的商业模式和 IT 系统建设发展的历程与互联网企业不是完全一样的，因此传统企业的中台建设策略与阿里中台战略也应该有所差异，需要共享的内容也不一样。</p><p><img data-src="/images/ddd-practical-course/09-01-%E4%B8%AD%E5%8F%B0.png" alt></p><p>由于渠道多样化，传统企业不仅要将通用能力中台化，以实现通用能力的沉淀、共享和复用，这里的通用能力对应 DDD 的通用域或支撑域；传统企业还需要将核心能力中台化，以满足不同渠道的核心业务能力共享和复用的需求，避免传统核心和互联网不同渠道应用出现“后端双核心、前端两张皮”的问题，这里的核心能力对应 DDD 的核心域。</p><h1>10-DDD、中台和微服务：它们是如何协作的？</h1><p><strong>DDD 有两把利器，那就是它的战略设计和战术设计方法。</strong><br>中台在企业架构上更多偏向业务模型，形成中台的过程实际上也是业务领域不断细分的过程。在这个过程中我们会将同类通用的业务能力进行聚合和业务重构，再根据限界上下文和业务内聚的原则建立领域模型。而 DDD 的战略设计最擅长的就是领域建模。</p><p>那在中台完成领域建模后，我们就需要通过微服务来完成系统建设。此时，DDD 的战术设计又恰好可以与微服务的设计完美结合。可以说，中台和微服务正是 DDD 实战的最佳场景。</p><h2 id="DDD-的本质">DDD 的本质</h2><p>在研究和解决业务问题时，DDD 会按照一定的规则将业务领域进行细分，领域细分到一定的程度后，DDD 会将问题范围限定在特定的边界内，并在这个边界内建立领域模型，进而用代码实现该领域模型，解决相应的业务问题。领域可分解为子域，子域可继续分为子子域，一直到你认为适合建立领域模型为止。</p><p>子域还会根据自身重要性和功能属性划分为三类子域，它们分别是核心域、支撑域和通用域。</p><h2 id="中台的本质">中台的本质</h2><p>中台的本质其实就是提炼各个业务板块的共同需求，进行业务和系统抽象，形成通用的可复用的业务模型，打造成组件化产品，供前台部门使用。<strong>前台要做什么业务，需要什么资源，可以直接找中台，不需要每次都去改动自己的底层。</strong></p><h2 id="DDD、中台和微服务的协作模式">DDD、中台和微服务的协作模式</h2><p>DDD 的子域分为核心域、通用域和支撑域。划分这几个子域的主要目的是为了确定战略资源的投入，一般来说战略投入的重点是核心域，因此后面我们就可以暂时不严格区分支撑域和通用域了。</p><p>领域、中台以及微服务虽然属于不同层面的东西，但我们还是可以将他们分解对照，整理出来它们之间的关系。你看下面这张图，我是从 DDD 领域建模和中台建设这两个不同的视角对同一个企业的业务架构进行分析。</p><p><img data-src="/images/ddd-practical-course/10-02-%E4%BC%81%E4%B8%9A%E4%B8%9A%E5%8A%A1%E6%9E%B6%E6%9E%84.png" alt></p><h2 id="中台如何建模？">中台如何建模？</h2><p>中台业务抽象的过程就是业务建模的过程，对应 DDD 的战略设计。系统抽象的过程就是微服务的建设过程，对应 DDD 的战术设计。下面我们就结合 DDD 领域建模的方法，讲一下中台业务建模的过程。</p><p>第一步：按照业务流程（通常适用于核心域）或者功能属性、集合（通常适用于通用域或支撑域），将业务域细分为多个中台，再根据功能属性或重要性归类到核心中台或通用中台。核心中台设计时要考虑核心竞争力，通用中台要站在企业高度考虑共享和复用能力。</p><p>第二步：选取中台，根据用例、业务场景或用户旅程完成事件风暴，找出实体、聚合和限界上下文。依次进行领域分解，建立领域模型。</p><p>由于不同中台独立建模，某些领域对象或功能可能会重复出现在其它领域模型中，也有可能本该是同一个聚合的领域对象或功能，却分散在其它的中台里，这样会导致领域模型不完整或者业务不内聚。这里先不要着急，这一步我们只需要初步确定主领域模型就可以了，在第三步中我们还会提炼并重组这些领域对象。</p><p>第三步：以主领域模型为基础，扫描其它中台领域模型，检查并确定是否存在重复或者需要重组的领域对象、功能，提炼并重构主领域模型，完成最终的领域模型设计。</p><p>第四步：选择其它主领域模型重复第三步，直到所有主领域模型完成比对和重构。</p><p>第五步：基于领域模型完成微服务设计，完成系统落地。</p><p><img data-src="/images/ddd-practical-course/10-03-%E4%B8%AD%E5%8F%B0%E6%8B%86%E5%88%86.png" alt></p><p>结合上面这张图，你可以大致了解到 DDD 中台设计的过程。DDD 战略设计包括上述的第一步到第四步，主要为：业务域分解为中台，对中台归类，完成领域建模，建立中台业务模型。DDD 战术设计是第五步，领域模型映射为微服务，完成中台建设。</p><p><img data-src="/images/ddd-practical-course/10-03-%E4%B8%AD%E5%8F%B0%E6%8B%86%E5%88%862.png" alt></p><h1>11 | DDD实践：如何用DDD重构中台业务模型？</h1><h2 id="如何构建中台业务模型？">如何构建中台业务模型？</h2><h3 id="1-自顶向下的策略">1. 自顶向下的策略</h3><p>第一种策略是自顶向下。这种策略是先做顶层设计，从最高领域逐级分解为中台，分别建立领域模型，根据业务属性分为通用中台或核心中台。领域建模过程主要基于业务现状，暂时不考虑系统现状。<strong>自顶向下的策略适用于全新的应用系统建设，或旧系统推倒重建的情况。</strong></p><p>由于这种策略不必受限于现有系统，你可以用 DDD 领域逐级分解的领域建模方法。从下面这张图我们可以看出它的主要步骤：第一步是将领域分解为子域，子域可以分为核心域、通用域和支撑域；第二步是对子域建模，划分领域边界，建立领域模型和限界上下文；第三步则是根据限界上下文进行微服务设计。<br><img data-src="/images/ddd-practical-course/11-01-%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B.png" alt></p><h3 id="2-自底向上的策略">2. 自底向上的策略</h3><p>第二种策略是自底向上。这种策略是基于业务和系统现状完成领域建模。首先分别完成系统所在业务域的领域建模；然后对齐业务域，找出具有同类或相似业务功能的领域模型，对比分析领域模型的差异，重组领域对象，重构领域模型。这个过程会沉淀公共和复用的业务能力，会将分散的业务模型整合。<strong>自底向上策略适用于遗留系统业务模型的演进式重构。</strong></p><p>下面我以互联网电商和传统核心应用的几个典型业务域为例，带你了解具体如何采用自底向上的策略来构建中台业务模型，主要分为这样三个步骤。</p><h4 id="第一步：锁定系统所在业务域，构建领域模型。">第一步：锁定系统所在业务域，构建领域模型。</h4><p>锁定系统所在的业务域，采用事件风暴，找出领域对象，构建聚合，划分限界上下文，建立领域模型。看一下下面这张图，我们选取了传统核心应用的用户、客户、传统收付和承保四个业务域以及互联网电商业务域，共计五个业务域来完成领域建模。</p><p><img data-src="/images/ddd-practical-course/11-02-%E9%94%81%E5%AE%9A%E4%B8%9A%E5%8A%A1%E9%A2%86%E5%9F%9F.png" alt></p><p>从上面这张图中，我们可以看到传统核心共构建了八个领域模型。其中用户域构建了用户认证和权限两个领域模型，客户域构建了个人和团体两个领域模型，传统收付构建了 POS 刷卡领域模型，承保域构建了定报价、投保和保单管理三个领域模型。</p><p>互联网电商构建了报价、投保、订单、客户、用户认证和移动收付六个领域模型。</p><p>在这些领域模型的清单里，我们可以看到二者之间有很多名称相似的领域模型。深入分析后你会发现，这些名称相似的领域模型存在业务能力重复，或者业务职能分散（比如移动支付和传统支付）的问题。那在构建中台业务模型时，你就需要重点关注它们，将这些不同领域模型中重复的业务能力沉淀到中台业务模型中，将分散的领域模型整合到统一的中台业务模型中，对外提供统一的共享的中台服务。</p><h4 id="第二步：对齐业务域，构建中台业务模型。">第二步：对齐业务域，构建中台业务模型。</h4><p>在下面这张图里，你可以看到右侧的传统核心领域模型明显多于左侧的互联网电商，那我们是不是就可以得出一个初步的结论：传统核心面向企业内大部分应用，大而全，领域模型相对完备，而互联网电商面向单一渠道，领域模型相对单一。</p><p>这个结论也给我们指明了一个方向：首先我们可以将传统核心的领域模型作为主领域模型，将互联网电商领域模型作为辅助模型来构建中台业务模型。然后再将互联网电商中重复的能力沉淀到传统核心的领域模型中，只保留自己的个性能力，比如订单。中台业务建模时，既要关注领域模型的完备性，也要关注不同渠道敏捷响应市场的要求。</p><p><img data-src="/images/ddd-practical-course/11-03-%E4%BA%92%E8%81%94%E7%BD%91%E4%B8%8E%E4%BC%A0%E7%BB%9F%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9E%8B.png" alt></p><p><strong>构建多业务域的中台业务模型的过程，就是找出同一业务域内所有同类业务的领域模型，对比分析域内领域模型和聚合的差异和共同点，打破原有的模型，完成新的中台业务模型重组或归并的过程。</strong></p><p><img data-src="/images/ddd-practical-course/11-04-%E4%B8%9A%E5%8A%A1%E5%9F%9F%E9%87%8D%E6%9E%84%E5%90%8E%E7%9A%84%E4%B8%AD%E5%8F%B0%E4%B8%9A%E5%8A%A1%E6%A8%A1%E5%9E%8B.png" alt></p><h4 id="第三步：中台归类，根据领域模型设计微服务。">第三步：中台归类，根据领域模型设计微服务。</h4><p><img data-src="/images/ddd-practical-course/11-05-%E8%AE%BE%E8%AE%A1%E5%BE%AE%E6%9C%8D%E5%8A%A1.png" alt></p><h1>12-领域建模：如何用事件风暴构建领域模型？</h1><p>我们该采用什么样的方法，才能从错综复杂的业务领域中分析并构建领域模型呢？</p><p>它就是我在前面多次提到的事件风暴。事件风暴是一项团队活动，领域专家与项目团队通过头脑风暴的形式，罗列出领域中所有的领域事件，整合之后形成最终的领域事件集合，然后对每一个事件，标注出导致该事件的命令，再为每一个事件标注出命令发起方的角色。命令可以是用户发起，也可以是第三方系统调用或者定时器触发等，最后对事件进行分类，整理出实体、聚合、聚合根以及限界上下文。而<strong>事件风暴正是 DDD 战略设计中经常使用的一种方法，它可以快速分析和分解复杂的业务领域，完成领域建模。</strong></p><p><a href="/attachments/DDD实战课/12-领域建模：如何用事件风暴构建领域模型？.pdf" target="_blank">12-领域建模：如何用事件风暴构建领域模型？</a></p><h1>13 | 代码模型（上）：如何使用DDD设计微服务代码模型？</h1><p><strong>第一点：聚合之间的代码边界一定要清晰。</strong> 聚合之间的服务调用和数据关联应该是尽可能的松耦合和低关联，聚合之间的服务调用应该通过上层的应用层组合实现调用，原则上不允许聚合之间直接调用领域服务。这种松耦合的代码关联，在以后业务发展和需求变更时，可以很方便地实现业务功能和聚合代码的重组，在微服务架构演进中将会起到非常重要的作用。</p><p><strong>第二点：你一定要有代码分层的概念。</strong> 写代码时一定要搞清楚代码的职责，将它放在职责对应的代码目录内。应用层代码主要完成服务组合和编排，以及聚合之间的协作，它是很薄的一层，不应该有核心领域逻辑代码。领域层是业务的核心，领域模型的核心逻辑代码一定要在领域层实现。如果将核心领域逻辑代码放到应用层，你的基于 DDD 分层架构模型的微服务慢慢就会演变成传统的三层架构模型了。</p><p><a href="/attachments/DDD实战课/13-代码模型（上）：如何使用DDD设计微服务代码模型.pdf" target="_blank">代码模型（上）：如何使用DDD设计微服务代码模型</a></p><h1>14-代码模型（下）：如何保证领域模型与代码模型的一致性？</h1><p><a href="/attachments/DDD实战课/14-代码模型（下）：如何保证领域模型与代码模型的一致性？.pdf" target="_blank">14-代码模型（下）：如何保证领域模型与代码模型的一致性？</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;DDD实战课&lt;br&gt;
作者：欧创新&lt;/p&gt;</summary>
    
    
    
    <category term="DDD" scheme="https://jiankunking.com/categories/ddd/"/>
    
    
    <category term="读书笔记" scheme="https://jiankunking.com/tags/读书笔记/"/>
    
    <category term="原创" scheme="https://jiankunking.com/tags/原创/"/>
    
    <category term="DDD" scheme="https://jiankunking.com/tags/ddd/"/>
    
  </entry>
  
  <entry>
    <title>2023年终总结</title>
    <link href="https://jiankunking.com/2023-year-end-summary.html"/>
    <id>https://jiankunking.com/2023-year-end-summary.html</id>
    <published>2023-12-30T08:16:02.000Z</published>
    <updated>2024-06-14T08:06:27.902Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>不&quot;平凡&quot;的一年，记录一下</p></blockquote><a id="more"></a><h1>工作</h1><p>拖期很久的<a href="https://jiankunking.com/a-way-to-implement-permissions.html">权限</a>终于年初上线了，后面又参与了新版权限的方案设计、论证。权限这件事解决了我一直以来的自我怀疑，让我整个人变得更自信。</p><h2 id="感触">感触</h2><p>如果你想让某件事变得困难，甚至完全不可能完成，那么你可以让终极目标尽可能地模糊。这是因为，你肯定无法完成一个终极目标不明确的项目，你可能会原地踏步，也可能会对它改来改去，甚至很可能会放弃它。相反，如果你想完成一个重要项目，绝对有必要让终极目标尽可能地清晰。</p><h1>生活</h1><h2 id="个人">个人</h2><p>生活中今年最大的收获就是解决了多年依赖的背难受跟驼背:<a href="https://mp.weixin.qq.com/s?__biz=MzIwOTk0Mzk0OA==&amp;mid=2247484680&amp;idx=1&amp;sn=02fb65174f5de4a81ca834b0fd1c2674&amp;chksm=976d69eba01ae0fdc6c3af0a6c60643f1e44064e21d9eab9ff97b92fb066fa94626b1adc79c9&amp;token=392973150&amp;lang=zh_CN#rd" target="_blank" rel="noopener">今年花的最值的一笔钱</a></p><h2 id="家庭">家庭</h2><p>家庭成员今年或受伤或累病，比较煎熬。</p><p>今年的假期基本都用来陪家人跑医院了，目之所及，这种情况还要持续至少几个月。</p><h2 id="财务">财务</h2><ul><li>贷款商转公成功。</li><li>提前还了一些贷款。</li></ul><h2 id="读书">读书</h2><p>今年感触比较大的几段话：</p><ul><li>只应对，不预测。</li><li><a href="https://mp.weixin.qq.com/s?__biz=MzIwOTk0Mzk0OA==&amp;mid=2247484853&amp;idx=1&amp;sn=17a559a812c2facf0c5e4b94253a3766&amp;chksm=976d6956a01ae040b93ee56e8e9e43aa38190a2a41ba28656dafcb92e1b0ed342568a4cdc181&amp;token=392973150&amp;lang=zh_CN#rd" target="_blank" rel="noopener">阅读和经验训练你的世界模型。即使你忘记了你的经历或你读到的东西，它对你的世界模型的影响仍然存在。你的大脑就像一个编译过的程序，你已经失去了它的来源。它有效，但你不知道为什么。</a></li><li>从实践到认识、从认识到实践循环往复，人的认识是螺旋式上升的，波浪式前进的。</li></ul><h1>新一年的期望</h1><ul><li><p>家人健健康康、平平安安。</p></li><li><p>自己减肥成功。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;不&amp;quot;平凡&amp;quot;的一年，记录一下&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="年终总结" scheme="https://jiankunking.com/categories/年终总结/"/>
    
    
    <category term="年终总结" scheme="https://jiankunking.com/tags/年终总结/"/>
    
    <category term="2023" scheme="https://jiankunking.com/tags/2023/"/>
    
  </entry>
  
  <entry>
    <title>【Elasticsearch源码】 分片恢复分析</title>
    <link href="https://jiankunking.com/elasticsearch-shard-recovery-source-code-analysis.html"/>
    <id>https://jiankunking.com/elasticsearch-shard-recovery-source-code-analysis.html</id>
    <published>2023-12-23T12:19:55.000Z</published>
    <updated>2024-01-08T06:49:20.473Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>带着疑问学源码，第七篇：Elasticsearch 分片恢复分析<br>代码分析基于：<a href="https://github.com/jiankunking/elasticsearch" target="_blank" rel="noopener">https://github.com/jiankunking/elasticsearch</a><br>Elasticsearch 8.0.0-SNAPSHOT</p></blockquote><a id="more"></a><h1>目的</h1><p>在看源码之前先梳理一下，自己对于分片恢复的疑问点：</p><ul><li>网上对于ElasticSearch分片恢复的逻辑说法一抓一把，网上说的对不对？新版本中有没有更新？</li><li>在分片恢复的时候，如果收到Api _forcemerge请求，这时候，会如何处理?(因为副本恢复的第一节点是复制segment文件)</li><li>分片恢复的第二阶段是同步translog,这一步会不会加锁？不加锁的话，如何确保是同步完成了？</li></ul><blockquote><p>如果说看源码有捷径的话，那么找到网上一篇写的比较权威的源码分析文章跟着看，那不失为一种好方法。<br>下面源码分析部分将参考腾讯云的:<a href="https://cloud.tencent.com/developer/article/1370385" target="_blank" rel="noopener">Elasticsearch 底层系列之分片恢复解析</a>，一边参考，一边印证。</p></blockquote><h1>源码分析</h1><h2 id="目标节点请求恢复">目标节点请求恢复</h2><p>先找到分片恢复的入口:<a href="https://github.com/jiankunking/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java#L568" target="_blank" rel="noopener">IndicesClusterStateService.createOrUpdateShards</a></p><p>在这里会判断本地节点是否在routingNodes中，如果在，说明本地节点有分片创建或更新的需求，否则跳过。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">private void createOrUpdateShards(final ClusterState state) &#123;</span><br><span class="line">        // 节点到索引分片的映射关系，主要用于分片分配、均衡决策</span><br><span class="line">        // 具体的内容可以看下:https://jiankunking.com/elasticsearch-cluster-state.html</span><br><span class="line">        RoutingNode localRoutingNode = state.getRoutingNodes().node(state.nodes().getLocalNodeId());</span><br><span class="line">        if (localRoutingNode == null) &#123;</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        DiscoveryNodes nodes = state.nodes();</span><br><span class="line">        RoutingTable routingTable = state.routingTable();</span><br><span class="line"></span><br><span class="line">        for (final ShardRouting shardRouting : localRoutingNode) &#123;</span><br><span class="line">            ShardId shardId = shardRouting.shardId();</span><br><span class="line">            // failedShardsCache:https://github.com/jiankunking/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java#L116</span><br><span class="line">            // 恢复过程中失败的碎片列表；我们跟踪这些碎片，以防止在每次集群状态更新时重复恢复这些碎片</span><br><span class="line">            if (failedShardsCache.containsKey(shardId) == false) &#123;</span><br><span class="line">                AllocatedIndex&lt;? extends Shard&gt; indexService = indicesService.indexService(shardId.getIndex());</span><br><span class="line">                assert indexService != null : &quot;index &quot; + shardId.getIndex() + &quot; should have been created by createIndices&quot;;</span><br><span class="line">                Shard shard = indexService.getShardOrNull(shardId.id());</span><br><span class="line">                if (shard == null) &#123; // shard不存在则需创建</span><br><span class="line">                    assert shardRouting.initializing() : shardRouting + &quot; should have been removed by failMissingShards&quot;;</span><br><span class="line">                    createShard(nodes, routingTable, shardRouting, state);</span><br><span class="line">                &#125; else &#123; // 存在则更新</span><br><span class="line">                    updateShard(nodes, shardRouting, shard, routingTable, state);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>副本分片恢复走的是createShard分支，在该方法中，首先获取shardRouting的类型，如果恢复类型为PEER，说明该分片需要从远端获取，则需要找到源节点，然后调用<a href="https://github.com/jiankunking/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java#L593" target="_blank" rel="noopener">IndicesService.createShard</a>：</p><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-recovery.html#index-recovery-api-response-body" target="_blank" rel="noopener">RecoverySource的Type</a>有以下几种：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">EMPTY_STORE,</span><br><span class="line">EXISTING_STORE,//主分片本地恢复</span><br><span class="line">PEER,//副分片从远处主分片恢复</span><br><span class="line">SNAPSHOT,//从快照恢复</span><br><span class="line">LOCAL_SHARDS//从本节点其它分片恢复(shrink时)</span><br></pre></td></tr></table></figure><p>createShard代码如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">private void createShard(DiscoveryNodes nodes, RoutingTable routingTable, ShardRouting shardRouting, ClusterState state) &#123;</span><br><span class="line">        assert shardRouting.initializing() : &quot;only allow shard creation for initializing shard but was &quot; + shardRouting;</span><br><span class="line"></span><br><span class="line">        DiscoveryNode sourceNode = null;</span><br><span class="line">        // 如果恢复方式是peer，则会找到shard所在的源节点进行恢复</span><br><span class="line">        if (shardRouting.recoverySource().getType() == Type.PEER)  &#123;</span><br><span class="line">            sourceNode = findSourceNodeForPeerRecovery(logger, routingTable, nodes, shardRouting);</span><br><span class="line">            if (sourceNode == null) &#123;</span><br><span class="line">                logger.trace(&quot;ignoring initializing shard &#123;&#125; - no source node can be found.&quot;, shardRouting.shardId());</span><br><span class="line">                return;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            final long primaryTerm = state.metadata().index(shardRouting.index()).primaryTerm(shardRouting.id());</span><br><span class="line">            logger.debug(&quot;&#123;&#125; creating shard with primary term [&#123;&#125;]&quot;, shardRouting.shardId(), primaryTerm);</span><br><span class="line">            indicesService.createShard(</span><br><span class="line">                    shardRouting,</span><br><span class="line">                    recoveryTargetService,</span><br><span class="line">                    new RecoveryListener(shardRouting, primaryTerm),</span><br><span class="line">                    repositoriesService,</span><br><span class="line">                    failedShardHandler,</span><br><span class="line">                    this::updateGlobalCheckpointForShard,</span><br><span class="line">                    retentionLeaseSyncer,</span><br><span class="line">                    nodes.getLocalNode(),</span><br><span class="line">                    sourceNode);</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            failAndRemoveShard(shardRouting, true, &quot;failed to create shard&quot;, e, state);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">     * Finds the routing source node for peer recovery, return null if its not found. Note, this method expects the shard</span><br><span class="line">     * routing to *require* peer recovery, use &#123;@link ShardRouting#recoverySource()&#125; to check if its needed or not.</span><br><span class="line">     */</span><br><span class="line">    private static DiscoveryNode findSourceNodeForPeerRecovery(Logger logger, RoutingTable routingTable, DiscoveryNodes nodes,</span><br><span class="line">                                                               ShardRouting shardRouting) &#123;</span><br><span class="line">        DiscoveryNode sourceNode = null;</span><br><span class="line">        if (!shardRouting.primary()) &#123;</span><br><span class="line">            ShardRouting primary = routingTable.shardRoutingTable(shardRouting.shardId()).primaryShard();</span><br><span class="line">            // only recover from started primary, if we can&apos;t find one, we will do it next round</span><br><span class="line">            if (primary.active()) &#123;</span><br><span class="line">                // 找到primary shard所在节点</span><br><span class="line">                sourceNode = nodes.get(primary.currentNodeId());</span><br><span class="line">                if (sourceNode == null) &#123;</span><br><span class="line">                    logger.trace(&quot;can&apos;t find replica source node because primary shard &#123;&#125; is assigned to an unknown node.&quot;, primary);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                logger.trace(&quot;can&apos;t find replica source node because primary shard &#123;&#125; is not active.&quot;, primary);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else if (shardRouting.relocatingNodeId() != null) &#123;</span><br><span class="line">            // 找到搬迁的源节点</span><br><span class="line">            sourceNode = nodes.get(shardRouting.relocatingNodeId());</span><br><span class="line">            if (sourceNode == null) &#123;</span><br><span class="line">                logger.trace(&quot;can&apos;t find relocation source node for shard &#123;&#125; because it is assigned to an unknown node [&#123;&#125;].&quot;,</span><br><span class="line">                    shardRouting.shardId(), shardRouting.relocatingNodeId());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            throw new IllegalStateException(&quot;trying to find source node for peer recovery when routing state means no peer recovery: &quot; +</span><br><span class="line">                shardRouting);</span><br><span class="line">        &#125;</span><br><span class="line">        return sourceNode;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>源节点的确定分两种情况，如果当前shard本身不是primary shard，则源节点为primary shard所在节点，否则，如果当前shard正在搬迁中(从其他节点搬迁到本节点)，则源节点为数据搬迁的源头节点。得到源节点后调用IndicesService.createShard，在该方法中调用方法<a href="https://github.com/jiankunking/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/index/shard/IndexShard.java#L2635" target="_blank" rel="noopener">IndexShard.startRecovery</a>开始恢复。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">public void startRecovery(RecoveryState recoveryState, PeerRecoveryTargetService recoveryTargetService,</span><br><span class="line">                              PeerRecoveryTargetService.RecoveryListener recoveryListener, RepositoriesService repositoriesService,</span><br><span class="line">                              Consumer&lt;MappingMetadata&gt; mappingUpdateConsumer,</span><br><span class="line">                              IndicesService indicesService) &#123;</span><br><span class="line">        // TODO: Create a proper object to encapsulate the recovery context</span><br><span class="line">        // all of the current methods here follow a pattern of:</span><br><span class="line">        // resolve context which isn&apos;t really dependent on the local shards and then async</span><br><span class="line">        // call some external method with this pointer.</span><br><span class="line">        // with a proper recovery context object we can simply change this to:</span><br><span class="line">        // startRecovery(RecoveryState recoveryState, ShardRecoverySource source ) &#123;</span><br><span class="line">        //     markAsRecovery(&quot;from &quot; + source.getShortDescription(), recoveryState);</span><br><span class="line">        //     threadPool.generic().execute()  &#123;</span><br><span class="line">        //           onFailure () &#123; listener.failure() &#125;;</span><br><span class="line">        //           doRun() &#123;</span><br><span class="line">        //                if (source.recover(this)) &#123;</span><br><span class="line">        //                  recoveryListener.onRecoveryDone(recoveryState);</span><br><span class="line">        //                &#125;</span><br><span class="line">        //           &#125;</span><br><span class="line">        //     &#125;&#125;</span><br><span class="line">        // &#125;</span><br><span class="line">        assert recoveryState.getRecoverySource().equals(shardRouting.recoverySource());</span><br><span class="line">        switch (recoveryState.getRecoverySource().getType()) &#123;</span><br><span class="line">            case EMPTY_STORE:</span><br><span class="line">            case EXISTING_STORE:</span><br><span class="line">                executeRecovery(&quot;from store&quot;, recoveryState, recoveryListener, this::recoverFromStore);</span><br><span class="line">                break;</span><br><span class="line">            case PEER:</span><br><span class="line">                try &#123;</span><br><span class="line">                    markAsRecovering(&quot;from &quot; + recoveryState.getSourceNode(), recoveryState);</span><br><span class="line">                    recoveryTargetService.startRecovery(this, recoveryState.getSourceNode(), recoveryListener);</span><br><span class="line">                &#125; catch (Exception e) &#123;</span><br><span class="line">                    failShard(&quot;corrupted preexisting index&quot;, e);</span><br><span class="line">                    recoveryListener.onRecoveryFailure(recoveryState,</span><br><span class="line">                        new RecoveryFailedException(recoveryState, null, e), true);</span><br><span class="line">                &#125;</span><br><span class="line">                break;</span><br><span class="line">            case SNAPSHOT:</span><br><span class="line">                final String repo = ((SnapshotRecoverySource) recoveryState.getRecoverySource()).snapshot().getRepository();</span><br><span class="line">                executeRecovery(&quot;from snapshot&quot;,</span><br><span class="line">                    recoveryState, recoveryListener, l -&gt; restoreFromRepository(repositoriesService.repository(repo), l));</span><br><span class="line">                break;</span><br><span class="line">            case LOCAL_SHARDS:</span><br><span class="line">                final IndexMetadata indexMetadata = indexSettings().getIndexMetadata();</span><br><span class="line">                final Index resizeSourceIndex = indexMetadata.getResizeSourceIndex();</span><br><span class="line">                final List&lt;IndexShard&gt; startedShards = new ArrayList&lt;&gt;();</span><br><span class="line">                final IndexService sourceIndexService = indicesService.indexService(resizeSourceIndex);</span><br><span class="line">                final Set&lt;ShardId&gt; requiredShards;</span><br><span class="line">                final int numShards;</span><br><span class="line">                if (sourceIndexService != null) &#123;</span><br><span class="line">                    requiredShards = IndexMetadata.selectRecoverFromShards(shardId().id(),</span><br><span class="line">                        sourceIndexService.getMetadata(), indexMetadata.getNumberOfShards());</span><br><span class="line">                    for (IndexShard shard : sourceIndexService) &#123;</span><br><span class="line">                        if (shard.state() == IndexShardState.STARTED &amp;&amp; requiredShards.contains(shard.shardId())) &#123;</span><br><span class="line">                            startedShards.add(shard);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    numShards = requiredShards.size();</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    numShards = -1;</span><br><span class="line">                    requiredShards = Collections.emptySet();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                if (numShards == startedShards.size()) &#123;</span><br><span class="line">                    assert requiredShards.isEmpty() == false;</span><br><span class="line">                    executeRecovery(&quot;from local shards&quot;, recoveryState, recoveryListener,</span><br><span class="line">                        l -&gt; recoverFromLocalShards(mappingUpdateConsumer,</span><br><span class="line">                            startedShards.stream().filter((s) -&gt; requiredShards.contains(s.shardId())).collect(Collectors.toList()), l));</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    final RuntimeException e;</span><br><span class="line">                    if (numShards == -1) &#123;</span><br><span class="line">                        e = new IndexNotFoundException(resizeSourceIndex);</span><br><span class="line">                    &#125; else &#123;</span><br><span class="line">                        e = new IllegalStateException(&quot;not all required shards of index &quot; + resizeSourceIndex</span><br><span class="line">                            + &quot; are started yet, expected &quot; + numShards + &quot; found &quot; + startedShards.size() + &quot; can&apos;t recover shard &quot;</span><br><span class="line">                            + shardId());</span><br><span class="line">                    &#125;</span><br><span class="line">                    throw e;</span><br><span class="line">                &#125;</span><br><span class="line">                break;</span><br><span class="line">            default:</span><br><span class="line">                throw new IllegalArgumentException(&quot;Unknown recovery source &quot; + recoveryState.getRecoverySource());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>对于恢复类型为PEER的任务，恢复动作的真正执行者为<a href="https://github.com/jiankunking/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/indices/recovery/PeerRecoveryTargetService.java#L174" target="_blank" rel="noopener">PeerRecoveryTargetService.doRecovery</a>。在该方法中，首先调用getStartRecoveryRequest获取shard的metadataSnapshot，该结构中包含shard的段信息，如syncid、checksum、doc数等，然后封装为StartRecoveryRequest，通过RPC发送到源节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">private void doRecovery(final long recoveryId, final StartRecoveryRequest preExistingRequest) &#123;</span><br><span class="line">        final String actionName;</span><br><span class="line">        final TransportRequest requestToSend;</span><br><span class="line">        final StartRecoveryRequest startRequest;</span><br><span class="line">        final RecoveryState.Timer timer;</span><br><span class="line">        try (RecoveryRef recoveryRef = onGoingRecoveries.getRecovery(recoveryId)) &#123;</span><br><span class="line">            if (recoveryRef == null) &#123;</span><br><span class="line">                logger.trace(&quot;not running recovery with id [&#123;&#125;] - can not find it (probably finished)&quot;, recoveryId);</span><br><span class="line">                return;</span><br><span class="line">            &#125;</span><br><span class="line">            final RecoveryTarget recoveryTarget = recoveryRef.target();</span><br><span class="line">            timer = recoveryTarget.state().getTimer();</span><br><span class="line">            if (preExistingRequest == null) &#123;</span><br><span class="line">                try &#123;</span><br><span class="line">                    final IndexShard indexShard = recoveryTarget.indexShard();</span><br><span class="line">                    indexShard.preRecovery();</span><br><span class="line">                    assert recoveryTarget.sourceNode() != null : &quot;can not do a recovery without a source node&quot;;</span><br><span class="line">                    logger.trace(&quot;&#123;&#125; preparing shard for peer recovery&quot;, recoveryTarget.shardId());</span><br><span class="line">                    indexShard.prepareForIndexRecovery();</span><br><span class="line">                    final long startingSeqNo = indexShard.recoverLocallyUpToGlobalCheckpoint();</span><br><span class="line">                    assert startingSeqNo == UNASSIGNED_SEQ_NO || recoveryTarget.state().getStage() == RecoveryState.Stage.TRANSLOG :</span><br><span class="line">                        &quot;unexpected recovery stage [&quot; + recoveryTarget.state().getStage() + &quot;] starting seqno [ &quot; + startingSeqNo + &quot;]&quot;;</span><br><span class="line">                    // 构造recovery request </span><br><span class="line">                    startRequest = getStartRecoveryRequest(logger, clusterService.localNode(), recoveryTarget, startingSeqNo);</span><br><span class="line">                    requestToSend = startRequest;</span><br><span class="line">                    actionName = PeerRecoverySourceService.Actions.START_RECOVERY;</span><br><span class="line">                &#125; catch (final Exception e) &#123;</span><br><span class="line">                    // this will be logged as warning later on...</span><br><span class="line">                    logger.trace(&quot;unexpected error while preparing shard for peer recovery, failing recovery&quot;, e);</span><br><span class="line">                    onGoingRecoveries.failRecovery(recoveryId,</span><br><span class="line">                        new RecoveryFailedException(recoveryTarget.state(), &quot;failed to prepare shard for recovery&quot;, e), true);</span><br><span class="line">                    return;</span><br><span class="line">                &#125;</span><br><span class="line">                logger.trace(&quot;&#123;&#125; starting recovery from &#123;&#125;&quot;, startRequest.shardId(), startRequest.sourceNode());</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                startRequest = preExistingRequest;</span><br><span class="line">                requestToSend = new ReestablishRecoveryRequest(recoveryId, startRequest.shardId(), startRequest.targetAllocationId());</span><br><span class="line">                actionName = PeerRecoverySourceService.Actions.REESTABLISH_RECOVERY;</span><br><span class="line">                logger.trace(&quot;&#123;&#125; reestablishing recovery from &#123;&#125;&quot;, startRequest.shardId(), startRequest.sourceNode());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        // 向源节点发送请求，请求恢复</span><br><span class="line">        transportService.sendRequest(startRequest.sourceNode(), actionName, requestToSend,</span><br><span class="line">                new RecoveryResponseHandler(startRequest, timer));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">     * Prepare the start recovery request.</span><br><span class="line">     *</span><br><span class="line">     * @param logger         the logger</span><br><span class="line">     * @param localNode      the local node of the recovery target</span><br><span class="line">     * @param recoveryTarget the target of the recovery</span><br><span class="line">     * @param startingSeqNo  a sequence number that an operation-based peer recovery can start with.</span><br><span class="line">     *                       This is the first operation after the local checkpoint of the safe commit if exists.</span><br><span class="line">     * @return a start recovery request</span><br><span class="line">     */</span><br><span class="line">    public static StartRecoveryRequest getStartRecoveryRequest(Logger logger, DiscoveryNode localNode,</span><br><span class="line">                                                               RecoveryTarget recoveryTarget, long startingSeqNo) &#123;</span><br><span class="line">        final StartRecoveryRequest request;</span><br><span class="line">        logger.trace(&quot;&#123;&#125; collecting local files for [&#123;&#125;]&quot;, recoveryTarget.shardId(), recoveryTarget.sourceNode());</span><br><span class="line"></span><br><span class="line">        Store.MetadataSnapshot metadataSnapshot;</span><br><span class="line">        try &#123;</span><br><span class="line">            metadataSnapshot = recoveryTarget.indexShard().snapshotStoreMetadata();</span><br><span class="line">            // Make sure that the current translog is consistent with the Lucene index; otherwise, we have to throw away the Lucene index.</span><br><span class="line">            try &#123;</span><br><span class="line">                final String expectedTranslogUUID = metadataSnapshot.getCommitUserData().get(Translog.TRANSLOG_UUID_KEY);</span><br><span class="line">                final long globalCheckpoint = Translog.readGlobalCheckpoint(recoveryTarget.translogLocation(), expectedTranslogUUID);</span><br><span class="line">                assert globalCheckpoint + 1 &gt;= startingSeqNo : &quot;invalid startingSeqNo &quot; + startingSeqNo + &quot; &gt;= &quot; + globalCheckpoint;</span><br><span class="line">            &#125; catch (IOException | TranslogCorruptedException e) &#123;</span><br><span class="line">                logger.warn(new ParameterizedMessage(&quot;error while reading global checkpoint from translog, &quot; +</span><br><span class="line">                    &quot;resetting the starting sequence number from &#123;&#125; to unassigned and recovering as if there are none&quot;, startingSeqNo), e);</span><br><span class="line">                metadataSnapshot = Store.MetadataSnapshot.EMPTY;</span><br><span class="line">                startingSeqNo = UNASSIGNED_SEQ_NO;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; catch (final org.apache.lucene.index.IndexNotFoundException e) &#123;</span><br><span class="line">            // happens on an empty folder. no need to log</span><br><span class="line">            assert startingSeqNo == UNASSIGNED_SEQ_NO : startingSeqNo;</span><br><span class="line">            logger.trace(&quot;&#123;&#125; shard folder empty, recovering all files&quot;, recoveryTarget);</span><br><span class="line">            metadataSnapshot = Store.MetadataSnapshot.EMPTY;</span><br><span class="line">        &#125; catch (final IOException e) &#123;</span><br><span class="line">            if (startingSeqNo != UNASSIGNED_SEQ_NO) &#123;</span><br><span class="line">                logger.warn(new ParameterizedMessage(&quot;error while listing local files, resetting the starting sequence number from &#123;&#125; &quot; +</span><br><span class="line">                    &quot;to unassigned and recovering as if there are none&quot;, startingSeqNo), e);</span><br><span class="line">                startingSeqNo = UNASSIGNED_SEQ_NO;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                logger.warn(&quot;error while listing local files, recovering as if there are none&quot;, e);</span><br><span class="line">            &#125;</span><br><span class="line">            metadataSnapshot = Store.MetadataSnapshot.EMPTY;</span><br><span class="line">        &#125;</span><br><span class="line">        logger.trace(&quot;&#123;&#125; local file count [&#123;&#125;]&quot;, recoveryTarget.shardId(), metadataSnapshot.size());</span><br><span class="line">        request = new StartRecoveryRequest(</span><br><span class="line">            recoveryTarget.shardId(),</span><br><span class="line">            recoveryTarget.indexShard().routingEntry().allocationId().getId(),</span><br><span class="line">            recoveryTarget.sourceNode(),</span><br><span class="line">            localNode,</span><br><span class="line">            metadataSnapshot,</span><br><span class="line">            recoveryTarget.state().getPrimary(),</span><br><span class="line">            recoveryTarget.recoveryId(),</span><br><span class="line">            startingSeqNo);</span><br><span class="line">        return request;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>注意，请求的发送是异步的。</p><h2 id="源节点处理恢复请求">源节点处理恢复请求</h2><p>源节点接收到请求后会调用恢复的入口函数<a href="https://github.com/jiankunking/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/indices/recovery/PeerRecoverySourceService.java#L165" target="_blank" rel="noopener">PeerRecoverySourceService.messageReceived</a>#recover:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class StartRecoveryTransportRequestHandler implements TransportRequestHandler&lt;StartRecoveryRequest&gt; &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public void messageReceived(final StartRecoveryRequest request, final TransportChannel channel, Task task) throws Exception &#123;</span><br><span class="line">            recover(request, new ChannelActionListener&lt;&gt;(channel, Actions.START_RECOVERY, request));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>recover方法根据request得到shard并构造RecoverySourceHandler对象，然后调用handler.recoverToTarget进入恢复的执行体：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br></pre></td><td class="code"><pre><span class="line">private void recover(StartRecoveryRequest request, ActionListener&lt;RecoveryResponse&gt; listener) &#123;</span><br><span class="line">        final IndexService indexService = indicesService.indexServiceSafe(request.shardId().getIndex());</span><br><span class="line">        final IndexShard shard = indexService.getShard(request.shardId().id());</span><br><span class="line"></span><br><span class="line">        final ShardRouting routingEntry = shard.routingEntry();</span><br><span class="line"></span><br><span class="line">        if (routingEntry.primary() == false || routingEntry.active() == false) &#123;</span><br><span class="line">            throw new DelayRecoveryException(&quot;source shard [&quot; + routingEntry + &quot;] is not an active primary&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (request.isPrimaryRelocation() &amp;&amp; (routingEntry.relocating() == false ||</span><br><span class="line">            routingEntry.relocatingNodeId().equals(request.targetNode().getId()) == false)) &#123;</span><br><span class="line">            logger.debug(&quot;delaying recovery of &#123;&#125; as source shard is not marked yet as relocating to &#123;&#125;&quot;,</span><br><span class="line">                request.shardId(), request.targetNode());</span><br><span class="line">            throw new DelayRecoveryException(&quot;source shard is not marked yet as relocating to [&quot; + request.targetNode() + &quot;]&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        RecoverySourceHandler handler = ongoingRecoveries.addNewRecovery(request, shard);</span><br><span class="line">        logger.trace(&quot;[&#123;&#125;][&#123;&#125;] starting recovery to &#123;&#125;&quot;, request.shardId().getIndex().getName(), request.shardId().id(),</span><br><span class="line">            request.targetNode());</span><br><span class="line">        handler.recoverToTarget(ActionListener.runAfter(listener, () -&gt; ongoingRecoveries.remove(shard, handler)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">     * performs the recovery from the local engine to the target</span><br><span class="line">     */</span><br><span class="line">    public void recoverToTarget(ActionListener&lt;RecoveryResponse&gt; listener) &#123;</span><br><span class="line">        addListener(listener);</span><br><span class="line">        final Closeable releaseResources = () -&gt; IOUtils.close(resources);</span><br><span class="line">        try &#123;</span><br><span class="line">            cancellableThreads.setOnCancel((reason, beforeCancelEx) -&gt; &#123;</span><br><span class="line">                final RuntimeException e;</span><br><span class="line">                if (shard.state() == IndexShardState.CLOSED) &#123; // check if the shard got closed on us</span><br><span class="line">                    e = new IndexShardClosedException(shard.shardId(), &quot;shard is closed and recovery was canceled reason [&quot; + reason + &quot;]&quot;);</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    e = new CancellableThreads.ExecutionCancelledException(&quot;recovery was canceled reason [&quot; + reason + &quot;]&quot;);</span><br><span class="line">                &#125;</span><br><span class="line">                if (beforeCancelEx != null) &#123;</span><br><span class="line">                    e.addSuppressed(beforeCancelEx);</span><br><span class="line">                &#125;</span><br><span class="line">                IOUtils.closeWhileHandlingException(releaseResources, () -&gt; future.onFailure(e));</span><br><span class="line">                throw e;</span><br><span class="line">            &#125;);</span><br><span class="line">            final Consumer&lt;Exception&gt; onFailure = e -&gt; &#123;</span><br><span class="line">                assert Transports.assertNotTransportThread(RecoverySourceHandler.this + &quot;[onFailure]&quot;);</span><br><span class="line">                IOUtils.closeWhileHandlingException(releaseResources, () -&gt; future.onFailure(e));</span><br><span class="line">            &#125;;</span><br><span class="line"></span><br><span class="line">            final SetOnce&lt;RetentionLease&gt; retentionLeaseRef = new SetOnce&lt;&gt;();</span><br><span class="line"></span><br><span class="line">            runUnderPrimaryPermit(() -&gt; &#123;</span><br><span class="line">                final IndexShardRoutingTable routingTable = shard.getReplicationGroup().getRoutingTable();</span><br><span class="line">                ShardRouting targetShardRouting = routingTable.getByAllocationId(request.targetAllocationId());</span><br><span class="line">                if (targetShardRouting == null) &#123;</span><br><span class="line">                    logger.debug(&quot;delaying recovery of &#123;&#125; as it is not listed as assigned to target node &#123;&#125;&quot;, request.shardId(),</span><br><span class="line">                        request.targetNode());</span><br><span class="line">                    throw new DelayRecoveryException(&quot;source node does not have the shard listed in its state as allocated on the node&quot;);</span><br><span class="line">                &#125;</span><br><span class="line">                assert targetShardRouting.initializing() : &quot;expected recovery target to be initializing but was &quot; + targetShardRouting;</span><br><span class="line">                retentionLeaseRef.set(</span><br><span class="line">                    shard.getRetentionLeases().get(ReplicationTracker.getPeerRecoveryRetentionLeaseId(targetShardRouting)));</span><br><span class="line">            &#125;, shardId + &quot; validating recovery target [&quot;+ request.targetAllocationId() + &quot;] registered &quot;,</span><br><span class="line">                shard, cancellableThreads, logger);</span><br><span class="line">            // 获取一个保留锁，使得translog不被清理</span><br><span class="line">            final Closeable retentionLock = shard.acquireHistoryRetentionLock();</span><br><span class="line">            resources.add(retentionLock);</span><br><span class="line">            final long startingSeqNo;</span><br><span class="line">            // 判断是否可以从SequenceNumber恢复</span><br><span class="line">            // 除了异常检测和版本号检测，主要在shard.hasCompleteHistoryOperations()方法中判断请求的序列号是否小于主分片节点的localCheckpoint，</span><br><span class="line">            // 以及translog中的数据是否足以恢复(有可能因为translog数据太大或者过期删除而无法恢复)</span><br><span class="line">            final boolean isSequenceNumberBasedRecovery</span><br><span class="line">                = request.startingSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO</span><br><span class="line">                &amp;&amp; isTargetSameHistory()</span><br><span class="line">                &amp;&amp; shard.hasCompleteHistoryOperations(&quot;peer-recovery&quot;, request.startingSeqNo())</span><br><span class="line">                &amp;&amp; ((retentionLeaseRef.get() == null &amp;&amp; shard.useRetentionLeasesInPeerRecovery() == false) ||</span><br><span class="line">                   (retentionLeaseRef.get() != null &amp;&amp; retentionLeaseRef.get().retainingSequenceNumber() &lt;= request.startingSeqNo()));</span><br><span class="line">            // NB check hasCompleteHistoryOperations when computing isSequenceNumberBasedRecovery, even if there is a retention lease,</span><br><span class="line">            // because when doing a rolling upgrade from earlier than 7.4 we may create some leases that are initially unsatisfied. It&apos;s</span><br><span class="line">            // possible there are other cases where we cannot satisfy all leases, because that&apos;s not a property we currently expect to hold.</span><br><span class="line">            // Also it&apos;s pretty cheap when soft deletes are enabled, and it&apos;d be a disaster if we tried a sequence-number-based recovery</span><br><span class="line">            // without having a complete history.</span><br><span class="line"></span><br><span class="line">            if (isSequenceNumberBasedRecovery &amp;&amp; retentionLeaseRef.get() != null) &#123;</span><br><span class="line">                // all the history we need is retained by an existing retention lease, so we do not need a separate retention lock</span><br><span class="line">                retentionLock.close();</span><br><span class="line">                logger.trace(&quot;history is retained by &#123;&#125;&quot;, retentionLeaseRef.get());</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                // all the history we need is retained by the retention lock, obtained before calling shard.hasCompleteHistoryOperations()</span><br><span class="line">                // and before acquiring the safe commit we&apos;ll be using, so we can be certain that all operations after the safe commit&apos;s</span><br><span class="line">                // local checkpoint will be retained for the duration of this recovery.</span><br><span class="line">                logger.trace(&quot;history is retained by retention lock&quot;);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            final StepListener&lt;SendFileResult&gt; sendFileStep = new StepListener&lt;&gt;();</span><br><span class="line">            final StepListener&lt;TimeValue&gt; prepareEngineStep = new StepListener&lt;&gt;();</span><br><span class="line">            final StepListener&lt;SendSnapshotResult&gt; sendSnapshotStep = new StepListener&lt;&gt;();</span><br><span class="line">            final StepListener&lt;Void&gt; finalizeStep = new StepListener&lt;&gt;();</span><br><span class="line">            // 若可以基于序列号进行恢复，则获取开始的序列号</span><br><span class="line">            if (isSequenceNumberBasedRecovery) &#123;</span><br><span class="line">                // 如果基于SequenceNumber恢复，则startingSeqNo取值为恢复请求中的序列号，</span><br><span class="line">                // 从请求的序列号开始快照translog。否则取值为0，快照完整的translog。</span><br><span class="line">                logger.trace(&quot;performing sequence numbers based recovery. starting at [&#123;&#125;]&quot;, request.startingSeqNo());</span><br><span class="line">                // 获取开始序列号</span><br><span class="line">                startingSeqNo = request.startingSeqNo();</span><br><span class="line">                if (retentionLeaseRef.get() == null) &#123;</span><br><span class="line">                    createRetentionLease(startingSeqNo, sendFileStep.map(ignored -&gt; SendFileResult.EMPTY));</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    // 发送的文件设置为空</span><br><span class="line">                    sendFileStep.onResponse(SendFileResult.EMPTY);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                final Engine.IndexCommitRef safeCommitRef;</span><br><span class="line">                try &#123;</span><br><span class="line">                    // Releasing a safe commit can access some commit files.</span><br><span class="line">                    safeCommitRef = acquireSafeCommit(shard);</span><br><span class="line">                    resources.add(safeCommitRef);</span><br><span class="line">                &#125; catch (final Exception e) &#123;</span><br><span class="line">                    throw new RecoveryEngineException(shard.shardId(), 1, &quot;snapshot failed&quot;, e);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                // Try and copy enough operations to the recovering peer so that if it is promoted to primary then it has a chance of being</span><br><span class="line">                // able to recover other replicas using operations-based recoveries. If we are not using retention leases then we</span><br><span class="line">                // conservatively copy all available operations. If we are using retention leases then &quot;enough operations&quot; is just the</span><br><span class="line">                // operations from the local checkpoint of the safe commit onwards, because when using soft deletes the safe commit retains</span><br><span class="line">                // at least as much history as anything else. The safe commit will often contain all the history retained by the current set</span><br><span class="line">                // of retention leases, but this is not guaranteed: an earlier peer recovery from a different primary might have created a</span><br><span class="line">                // retention lease for some history that this primary already discarded, since we discard history when the global checkpoint</span><br><span class="line">                // advances and not when creating a new safe commit. In any case this is a best-effort thing since future recoveries can</span><br><span class="line">                // always fall back to file-based ones, and only really presents a problem if this primary fails before things have settled</span><br><span class="line">                // down.</span><br><span class="line">                startingSeqNo = Long.parseLong(safeCommitRef.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY)) + 1L;</span><br><span class="line">                logger.trace(&quot;performing file-based recovery followed by history replay starting at [&#123;&#125;]&quot;, startingSeqNo);</span><br><span class="line"></span><br><span class="line">                try &#123;</span><br><span class="line">                    final int estimateNumOps = estimateNumberOfHistoryOperations(startingSeqNo);</span><br><span class="line">                    final Releasable releaseStore = acquireStore(shard.store());</span><br><span class="line">                    resources.add(releaseStore);</span><br><span class="line">                    sendFileStep.whenComplete(r -&gt; IOUtils.close(safeCommitRef, releaseStore), e -&gt; &#123;</span><br><span class="line">                        try &#123;</span><br><span class="line">                            IOUtils.close(safeCommitRef, releaseStore);</span><br><span class="line">                        &#125; catch (final IOException ex) &#123;</span><br><span class="line">                            logger.warn(&quot;releasing snapshot caused exception&quot;, ex);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line"></span><br><span class="line">                    final StepListener&lt;ReplicationResponse&gt; deleteRetentionLeaseStep = new StepListener&lt;&gt;();</span><br><span class="line">                    runUnderPrimaryPermit(() -&gt; &#123;</span><br><span class="line">                            try &#123;</span><br><span class="line">                                // If the target previously had a copy of this shard then a file-based recovery might move its global</span><br><span class="line">                                // checkpoint backwards. We must therefore remove any existing retention lease so that we can create a</span><br><span class="line">                                // new one later on in the recovery.</span><br><span class="line">                                shard.removePeerRecoveryRetentionLease(request.targetNode().getId(),</span><br><span class="line">                                    new ThreadedActionListener&lt;&gt;(logger, shard.getThreadPool(), ThreadPool.Names.GENERIC,</span><br><span class="line">                                        deleteRetentionLeaseStep, false));</span><br><span class="line">                            &#125; catch (RetentionLeaseNotFoundException e) &#123;</span><br><span class="line">                                logger.debug(&quot;no peer-recovery retention lease for &quot; + request.targetAllocationId());</span><br><span class="line">                                deleteRetentionLeaseStep.onResponse(null);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;, shardId + &quot; removing retention lease for [&quot; + request.targetAllocationId() + &quot;]&quot;,</span><br><span class="line">                        shard, cancellableThreads, logger);</span><br><span class="line">                    // 第一阶段</span><br><span class="line">                    deleteRetentionLeaseStep.whenComplete(ignored -&gt; &#123;</span><br><span class="line">                        assert Transports.assertNotTransportThread(RecoverySourceHandler.this + &quot;[phase1]&quot;);</span><br><span class="line">                        phase1(safeCommitRef.getIndexCommit(), startingSeqNo, () -&gt; estimateNumOps, sendFileStep);</span><br><span class="line">                    &#125;, onFailure);</span><br><span class="line"></span><br><span class="line">                &#125; catch (final Exception e) &#123;</span><br><span class="line">                    throw new RecoveryEngineException(shard.shardId(), 1, &quot;sendFileStep failed&quot;, e);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            assert startingSeqNo &gt;= 0 : &quot;startingSeqNo must be non negative. got: &quot; + startingSeqNo;</span><br><span class="line"></span><br><span class="line">            sendFileStep.whenComplete(r -&gt; &#123;</span><br><span class="line">                assert Transports.assertNotTransportThread(RecoverySourceHandler.this + &quot;[prepareTargetForTranslog]&quot;);</span><br><span class="line">                // 等待phase1执行完毕，主分片节点通知副分片节点启动此分片的Engine:prepareTargetForTranslog          </span><br><span class="line">                // 该方法会阻塞处理，直到分片 Engine 启动完毕。</span><br><span class="line">                // 待副分片启动Engine 完毕，就可以正常接收写请求了。</span><br><span class="line">                // 注意，此时phase2尚未开始，此分片的恢复流程尚未结束。</span><br><span class="line">                // 等待当前操作处理完成后，以startingSeqNo为起始点，对translog做快照，开始执行phase2：</span><br><span class="line">                // For a sequence based recovery, the target can keep its local translog</span><br><span class="line">                prepareTargetForTranslog(estimateNumberOfHistoryOperations(startingSeqNo), prepareEngineStep);</span><br><span class="line">            &#125;, onFailure);</span><br><span class="line"></span><br><span class="line">            prepareEngineStep.whenComplete(prepareEngineTime -&gt; &#123;</span><br><span class="line">                assert Transports.assertNotTransportThread(RecoverySourceHandler.this + &quot;[phase2]&quot;);</span><br><span class="line">                /*</span><br><span class="line">                 * add shard to replication group (shard will receive replication requests from this point on) now that engine is open.</span><br><span class="line">                 * This means that any document indexed into the primary after this will be replicated to this replica as well</span><br><span class="line">                 * make sure to do this before sampling the max sequence number in the next step, to ensure that we send</span><br><span class="line">                 * all documents up to maxSeqNo in phase2.</span><br><span class="line">                 */</span><br><span class="line">                runUnderPrimaryPermit(() -&gt; shard.initiateTracking(request.targetAllocationId()),</span><br><span class="line">                    shardId + &quot; initiating tracking of &quot; + request.targetAllocationId(), shard, cancellableThreads, logger);</span><br><span class="line"></span><br><span class="line">                final long endingSeqNo = shard.seqNoStats().getMaxSeqNo();</span><br><span class="line">                logger.trace(&quot;snapshot for recovery; current size is [&#123;&#125;]&quot;, estimateNumberOfHistoryOperations(startingSeqNo));</span><br><span class="line">                final Translog.Snapshot phase2Snapshot = shard.newChangesSnapshot(&quot;peer-recovery&quot;, startingSeqNo, Long.MAX_VALUE, false);</span><br><span class="line">                resources.add(phase2Snapshot);</span><br><span class="line">                retentionLock.close();</span><br><span class="line"></span><br><span class="line">                // we have to capture the max_seen_auto_id_timestamp and the max_seq_no_of_updates to make sure that these values</span><br><span class="line">                // are at least as high as the corresponding values on the primary when any of these operations were executed on it.</span><br><span class="line">                final long maxSeenAutoIdTimestamp = shard.getMaxSeenAutoIdTimestamp();</span><br><span class="line">                final long maxSeqNoOfUpdatesOrDeletes = shard.getMaxSeqNoOfUpdatesOrDeletes();</span><br><span class="line">                final RetentionLeases retentionLeases = shard.getRetentionLeases();</span><br><span class="line">                final long mappingVersionOnPrimary = shard.indexSettings().getIndexMetadata().getMappingVersion();</span><br><span class="line">                // 第二阶段，发送translog</span><br><span class="line">                phase2(startingSeqNo, endingSeqNo, phase2Snapshot, maxSeenAutoIdTimestamp, maxSeqNoOfUpdatesOrDeletes,</span><br><span class="line">                    retentionLeases, mappingVersionOnPrimary, sendSnapshotStep);</span><br><span class="line"></span><br><span class="line">            &#125;, onFailure);</span><br><span class="line"></span><br><span class="line">            // Recovery target can trim all operations &gt;= startingSeqNo as we have sent all these operations in the phase 2</span><br><span class="line">            final long trimAboveSeqNo = startingSeqNo - 1;</span><br><span class="line">            sendSnapshotStep.whenComplete(r -&gt; finalizeRecovery(r.targetLocalCheckpoint, trimAboveSeqNo, finalizeStep), onFailure);</span><br><span class="line"></span><br><span class="line">            finalizeStep.whenComplete(r -&gt; &#123;</span><br><span class="line">                final long phase1ThrottlingWaitTime = 0L; // TODO: return the actual throttle time</span><br><span class="line">                final SendSnapshotResult sendSnapshotResult = sendSnapshotStep.result();</span><br><span class="line">                final SendFileResult sendFileResult = sendFileStep.result();</span><br><span class="line">                final RecoveryResponse response = new RecoveryResponse(sendFileResult.phase1FileNames, sendFileResult.phase1FileSizes,</span><br><span class="line">                    sendFileResult.phase1ExistingFileNames, sendFileResult.phase1ExistingFileSizes, sendFileResult.totalSize,</span><br><span class="line">                    sendFileResult.existingTotalSize, sendFileResult.took.millis(), phase1ThrottlingWaitTime,</span><br><span class="line">                    prepareEngineStep.result().millis(), sendSnapshotResult.sentOperations, sendSnapshotResult.tookTime.millis());</span><br><span class="line">                try &#123;</span><br><span class="line">                    future.onResponse(response);</span><br><span class="line">                &#125; finally &#123;</span><br><span class="line">                    IOUtils.close(resources);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;, onFailure);</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            IOUtils.closeWhileHandlingException(releaseResources, () -&gt; future.onFailure(e));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * Checks if we have a completed history of operations since the given starting seqno (inclusive).</span><br><span class="line">     * This method should be called after acquiring the retention lock; See &#123;@link #acquireHistoryRetentionLock()&#125;</span><br><span class="line">     */</span><br><span class="line">    public boolean hasCompleteHistoryOperations(String reason, long startingSeqNo)  &#123;</span><br><span class="line">        return getEngine().hasCompleteOperationHistory(reason, startingSeqNo);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>从上面的代码可以看出，恢复主要分两个阶段，第一阶段恢复segment文件，第二阶段发送translog。这里有个关键的地方，在恢复前，首先需要获取translogView及segment snapshot，translogView的作用是保证当前时间点到恢复结束时间段的translog不被删除，segment snapshot的作用是保证当前时间点之前的segment文件不被删除。接下来看看两阶段恢复的具体执行逻辑。phase1:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">     * Perform phase1 of the recovery operations. Once this &#123;@link IndexCommit&#125;</span><br><span class="line">     * snapshot has been performed no commit operations (files being fsync&apos;d)</span><br><span class="line">     * are effectively allowed on this index until all recovery phases are done</span><br><span class="line">     * &lt;p&gt;</span><br><span class="line">     * Phase1 examines the segment files on the target node and copies over the</span><br><span class="line">     * segments that are missing. Only segments that have the same size and</span><br><span class="line">     * checksum can be reused</span><br><span class="line">     */</span><br><span class="line">    void phase1(IndexCommit snapshot, long startingSeqNo, IntSupplier translogOps, ActionListener&lt;SendFileResult&gt; listener) &#123;</span><br><span class="line">        cancellableThreads.checkForCancel();</span><br><span class="line">        //拿到shard的存储信息</span><br><span class="line">        final Store store = shard.store();</span><br><span class="line">        try &#123;</span><br><span class="line">            StopWatch stopWatch = new StopWatch().start();</span><br><span class="line">            final Store.MetadataSnapshot recoverySourceMetadata;</span><br><span class="line">            try &#123;</span><br><span class="line">                // 拿到snapshot的metadata</span><br><span class="line">                recoverySourceMetadata = store.getMetadata(snapshot);</span><br><span class="line">            &#125; catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) &#123;</span><br><span class="line">                shard.failShard(&quot;recovery&quot;, ex);</span><br><span class="line">                throw ex;</span><br><span class="line">            &#125;</span><br><span class="line">            for (String name : snapshot.getFileNames()) &#123;</span><br><span class="line">                // MetadataSnapshot:表示从最新的Lucene提交生成的当前目录的快照。</span><br><span class="line">                // https://github.com/jiankunking/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/index/store/Store.java#L732</span><br><span class="line">                final StoreFileMetadata md = recoverySourceMetadata.get(name);</span><br><span class="line">                if (md == null) &#123;</span><br><span class="line">                    logger.info(&quot;Snapshot differs from actual index for file: &#123;&#125; meta: &#123;&#125;&quot;, name, recoverySourceMetadata.asMap());</span><br><span class="line">                    throw new CorruptIndexException(&quot;Snapshot differs from actual index - maybe index was removed metadata has &quot; +</span><br><span class="line">                            recoverySourceMetadata.asMap().size() + &quot; files&quot;, name);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            // 如果syncid相等，再继续比较下文档数，如果都相同则不用恢复</span><br><span class="line">            if (canSkipPhase1(recoverySourceMetadata, request.metadataSnapshot()) == false) &#123;</span><br><span class="line">                final List&lt;String&gt; phase1FileNames = new ArrayList&lt;&gt;();</span><br><span class="line">                final List&lt;Long&gt; phase1FileSizes = new ArrayList&lt;&gt;();</span><br><span class="line">                final List&lt;String&gt; phase1ExistingFileNames = new ArrayList&lt;&gt;();</span><br><span class="line">                final List&lt;Long&gt; phase1ExistingFileSizes = new ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">                // Total size of segment files that are recovered</span><br><span class="line">                long totalSizeInBytes = 0;</span><br><span class="line">                // Total size of segment files that were able to be re-used</span><br><span class="line">                long existingTotalSizeInBytes = 0;</span><br><span class="line"></span><br><span class="line">                // Generate a &quot;diff&quot; of all the identical, different, and missing</span><br><span class="line">                // segment files on the target node, using the existing files on</span><br><span class="line">                // the source node</span><br><span class="line">                // 找出target和source有差别的segment</span><br><span class="line">                // https://github.com/jiankunking/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/index/store/Store.java#L971</span><br><span class="line">                final Store.RecoveryDiff diff = recoverySourceMetadata.recoveryDiff(request.metadataSnapshot());</span><br><span class="line">                for (StoreFileMetadata md : diff.identical) &#123;</span><br><span class="line">                    phase1ExistingFileNames.add(md.name());</span><br><span class="line">                    phase1ExistingFileSizes.add(md.length());</span><br><span class="line">                    existingTotalSizeInBytes += md.length();</span><br><span class="line">                    if (logger.isTraceEnabled()) &#123;</span><br><span class="line">                        logger.trace(&quot;recovery [phase1]: not recovering [&#123;&#125;], exist in local store and has checksum [&#123;&#125;],&quot; +</span><br><span class="line">                                        &quot; size [&#123;&#125;]&quot;, md.name(), md.checksum(), md.length());</span><br><span class="line">                    &#125;</span><br><span class="line">                    totalSizeInBytes += md.length();</span><br><span class="line">                &#125;</span><br><span class="line">                List&lt;StoreFileMetadata&gt; phase1Files = new ArrayList&lt;&gt;(diff.different.size() + diff.missing.size());</span><br><span class="line">                phase1Files.addAll(diff.different);</span><br><span class="line">                phase1Files.addAll(diff.missing);</span><br><span class="line">                for (StoreFileMetadata md : phase1Files) &#123;</span><br><span class="line">                    if (request.metadataSnapshot().asMap().containsKey(md.name())) &#123;</span><br><span class="line">                        logger.trace(&quot;recovery [phase1]: recovering [&#123;&#125;], exists in local store, but is different: remote [&#123;&#125;], local [&#123;&#125;]&quot;,</span><br><span class="line">                            md.name(), request.metadataSnapshot().asMap().get(md.name()), md);</span><br><span class="line">                    &#125; else &#123;</span><br><span class="line">                        logger.trace(&quot;recovery [phase1]: recovering [&#123;&#125;], does not exist in remote&quot;, md.name());</span><br><span class="line">                    &#125;</span><br><span class="line">                    phase1FileNames.add(md.name());</span><br><span class="line">                    phase1FileSizes.add(md.length());</span><br><span class="line">                    totalSizeInBytes += md.length();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                logger.trace(&quot;recovery [phase1]: recovering_files [&#123;&#125;] with total_size [&#123;&#125;], reusing_files [&#123;&#125;] with total_size [&#123;&#125;]&quot;,</span><br><span class="line">                    phase1FileNames.size(), new ByteSizeValue(totalSizeInBytes),</span><br><span class="line">                    phase1ExistingFileNames.size(), new ByteSizeValue(existingTotalSizeInBytes));</span><br><span class="line">                final StepListener&lt;Void&gt; sendFileInfoStep = new StepListener&lt;&gt;();</span><br><span class="line">                final StepListener&lt;Void&gt; sendFilesStep = new StepListener&lt;&gt;();</span><br><span class="line">                final StepListener&lt;RetentionLease&gt; createRetentionLeaseStep = new StepListener&lt;&gt;();</span><br><span class="line">                final StepListener&lt;Void&gt; cleanFilesStep = new StepListener&lt;&gt;();</span><br><span class="line">                cancellableThreads.checkForCancel();</span><br><span class="line">                recoveryTarget.receiveFileInfo(phase1FileNames, phase1FileSizes, phase1ExistingFileNames,</span><br><span class="line">                        phase1ExistingFileSizes, translogOps.getAsInt(), sendFileInfoStep);</span><br><span class="line"></span><br><span class="line">                // 将需要恢复的文件发送到target node</span><br><span class="line">                sendFileInfoStep.whenComplete(r -&gt;</span><br><span class="line">                    sendFiles(store, phase1Files.toArray(new StoreFileMetadata[0]), translogOps, sendFilesStep), listener::onFailure);</span><br><span class="line"></span><br><span class="line">                sendFilesStep.whenComplete(r -&gt; createRetentionLease(startingSeqNo, createRetentionLeaseStep), listener::onFailure);</span><br><span class="line"></span><br><span class="line">                createRetentionLeaseStep.whenComplete(retentionLease -&gt;</span><br><span class="line">                    &#123;</span><br><span class="line">                        final long lastKnownGlobalCheckpoint = shard.getLastKnownGlobalCheckpoint();</span><br><span class="line">                        assert retentionLease == null || retentionLease.retainingSequenceNumber() - 1 &lt;= lastKnownGlobalCheckpoint</span><br><span class="line">                            : retentionLease + &quot; vs &quot; + lastKnownGlobalCheckpoint;</span><br><span class="line">                        // Establishes new empty translog on the replica with global checkpoint set to lastKnownGlobalCheckpoint. We want</span><br><span class="line">                        // the commit we just copied to be a safe commit on the replica, so why not set the global checkpoint on the replica</span><br><span class="line">                        // to the max seqno of this commit? Because (in rare corner cases) this commit might not be a safe commit here on</span><br><span class="line">                        // the primary, and in these cases the max seqno would be too high to be valid as a global checkpoint.</span><br><span class="line">                        cleanFiles(store, recoverySourceMetadata, translogOps, lastKnownGlobalCheckpoint, cleanFilesStep);</span><br><span class="line">                    &#125;,</span><br><span class="line">                    listener::onFailure);</span><br><span class="line"></span><br><span class="line">                final long totalSize = totalSizeInBytes;</span><br><span class="line">                final long existingTotalSize = existingTotalSizeInBytes;</span><br><span class="line">                cleanFilesStep.whenComplete(r -&gt; &#123;</span><br><span class="line">                    final TimeValue took = stopWatch.totalTime();</span><br><span class="line">                    logger.trace(&quot;recovery [phase1]: took [&#123;&#125;]&quot;, took);</span><br><span class="line">                    listener.onResponse(new SendFileResult(phase1FileNames, phase1FileSizes, totalSize, phase1ExistingFileNames,</span><br><span class="line">                        phase1ExistingFileSizes, existingTotalSize, took));</span><br><span class="line">                &#125;, listener::onFailure);</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                logger.trace(&quot;skipping [phase1] since source and target have identical sync id [&#123;&#125;]&quot;, recoverySourceMetadata.getSyncId());</span><br><span class="line"></span><br><span class="line">                // but we must still create a retention lease</span><br><span class="line">                final StepListener&lt;RetentionLease&gt; createRetentionLeaseStep = new StepListener&lt;&gt;();</span><br><span class="line">                createRetentionLease(startingSeqNo, createRetentionLeaseStep);</span><br><span class="line">                createRetentionLeaseStep.whenComplete(retentionLease -&gt; &#123;</span><br><span class="line">                    final TimeValue took = stopWatch.totalTime();</span><br><span class="line">                    logger.trace(&quot;recovery [phase1]: took [&#123;&#125;]&quot;, took);</span><br><span class="line">                    listener.onResponse(new SendFileResult(Collections.emptyList(), Collections.emptyList(), 0L, Collections.emptyList(),</span><br><span class="line">                        Collections.emptyList(), 0L, took));</span><br><span class="line">                &#125;, listener::onFailure);</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            throw new RecoverFilesRecoveryException(request.shardId(), 0, new ByteSizeValue(0L), e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>从上面代码可以看出，phase1的具体逻辑是，首先拿到待恢复shard的metadataSnapshot从而得到recoverySourceSyncId，根据request拿到recoveryTargetSyncId，比较两边的syncid，如果相同再比较源和目标的文档数，如果也相同，说明在当前提交点之前源和目标的shard对应的segments都相同，因此不用恢复segment文件(<a href="https://github.com/jiankunking/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java#L616" target="_blank" rel="noopener">canSkipPhase1</a>方法中比对的)。如果两边的syncid不同，说明segment文件有差异，则需要找出所有有差异的文件进行恢复。通过比较recoverySourceMetadata和recoveryTargetSnapshot的差异性，可以找出所有有差别的segment文件。这块逻辑如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">         * Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the</span><br><span class="line">         * recovery target and this snapshot as the source. The returned diff will hold a list of files that are:</span><br><span class="line">         * &lt;ul&gt;</span><br><span class="line">         * &lt;li&gt;identical: they exist in both snapshots and they can be considered the same ie. they don&apos;t need to be recovered&lt;/li&gt;</span><br><span class="line">         * &lt;li&gt;different: they exist in both snapshots but their they are not identical&lt;/li&gt;</span><br><span class="line">         * &lt;li&gt;missing: files that exist in the source but not in the target&lt;/li&gt;</span><br><span class="line">         * &lt;/ul&gt;</span><br><span class="line">         * This method groups file into per-segment files and per-commit files. A file is treated as</span><br><span class="line">         * identical if and on if all files in it&apos;s group are identical. On a per-segment level files for a segment are treated</span><br><span class="line">         * as identical iff:</span><br><span class="line">         * &lt;ul&gt;</span><br><span class="line">         * &lt;li&gt;all files in this segment have the same checksum&lt;/li&gt;</span><br><span class="line">         * &lt;li&gt;all files in this segment have the same length&lt;/li&gt;</span><br><span class="line">         * &lt;li&gt;the segments &#123;@code .si&#125; files hashes are byte-identical Note: This is a using a perfect hash function,</span><br><span class="line">         * The metadata transfers the &#123;@code .si&#125; file content as it&apos;s hash&lt;/li&gt;</span><br><span class="line">         * &lt;/ul&gt;</span><br><span class="line">         * &lt;p&gt;</span><br><span class="line">         * The &#123;@code .si&#125; file contains a lot of diagnostics including a timestamp etc. in the future there might be</span><br><span class="line">         * unique segment identifiers in there hardening this method further.</span><br><span class="line">         * &lt;p&gt;</span><br><span class="line">         * The per-commit files handles very similar. A commit is composed of the &#123;@code segments_N&#125; files as well as generational files</span><br><span class="line">         * like deletes (&#123;@code _x_y.del&#125;) or field-info (&#123;@code _x_y.fnm&#125;) files. On a per-commit level files for a commit are treated</span><br><span class="line">         * as identical iff:</span><br><span class="line">         * &lt;ul&gt;</span><br><span class="line">         * &lt;li&gt;all files belonging to this commit have the same checksum&lt;/li&gt;</span><br><span class="line">         * &lt;li&gt;all files belonging to this commit have the same length&lt;/li&gt;</span><br><span class="line">         * &lt;li&gt;the segments file &#123;@code segments_N&#125; files hashes are byte-identical Note: This is a using a perfect hash function,</span><br><span class="line">         * The metadata transfers the &#123;@code segments_N&#125; file content as it&apos;s hash&lt;/li&gt;</span><br><span class="line">         * &lt;/ul&gt;</span><br><span class="line">         * &lt;p&gt;</span><br><span class="line">         * NOTE: this diff will not contain the &#123;@code segments.gen&#125; file. This file is omitted on recovery.</span><br><span class="line">         */</span><br><span class="line">        public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) &#123;</span><br><span class="line">            final List&lt;StoreFileMetadata&gt; identical = new ArrayList&lt;&gt;();// 相同的file </span><br><span class="line">            final List&lt;StoreFileMetadata&gt; different = new ArrayList&lt;&gt;();// 不同的file</span><br><span class="line">            final List&lt;StoreFileMetadata&gt; missing = new ArrayList&lt;&gt;();// 缺失的file</span><br><span class="line">            final Map&lt;String, List&lt;StoreFileMetadata&gt;&gt; perSegment = new HashMap&lt;&gt;();</span><br><span class="line">            final List&lt;StoreFileMetadata&gt; perCommitStoreFiles = new ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">            for (StoreFileMetadata meta : this) &#123;</span><br><span class="line">                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) &#123; // legacy</span><br><span class="line">                    continue; // we don&apos;t need that file at all</span><br><span class="line">                &#125;</span><br><span class="line">                final String segmentId = IndexFileNames.parseSegmentName(meta.name());</span><br><span class="line">                final String extension = IndexFileNames.getExtension(meta.name());</span><br><span class="line">                if (IndexFileNames.SEGMENTS.equals(segmentId) ||</span><br><span class="line">                        DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) &#123;</span><br><span class="line">                    // only treat del files as per-commit files fnm files are generational but only for upgradable DV</span><br><span class="line">                    perCommitStoreFiles.add(meta);</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    perSegment.computeIfAbsent(segmentId, k -&gt; new ArrayList&lt;&gt;()).add(meta);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            final ArrayList&lt;StoreFileMetadata&gt; identicalFiles = new ArrayList&lt;&gt;();</span><br><span class="line">            for (List&lt;StoreFileMetadata&gt; segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) &#123;</span><br><span class="line">                identicalFiles.clear();</span><br><span class="line">                boolean consistent = true;</span><br><span class="line">                for (StoreFileMetadata meta : segmentFiles) &#123;</span><br><span class="line">                    StoreFileMetadata storeFileMetadata = recoveryTargetSnapshot.get(meta.name());</span><br><span class="line">                    if (storeFileMetadata == null) &#123;</span><br><span class="line">                        consistent = false;</span><br><span class="line">                        missing.add(meta);// 该segment在target node中不存在，则加入到missing</span><br><span class="line">                    &#125; else if (storeFileMetadata.isSame(meta) == false) &#123;</span><br><span class="line">                        consistent = false;</span><br><span class="line">                        different.add(meta);// 存在但不相同，则加入到different</span><br><span class="line">                    &#125; else &#123;</span><br><span class="line">                        identicalFiles.add(meta);// 存在且相同</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                if (consistent) &#123;</span><br><span class="line">                    identical.addAll(identicalFiles);</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    // make sure all files are added - this can happen if only the deletes are different</span><br><span class="line">                    different.addAll(identicalFiles);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical),</span><br><span class="line">                Collections.unmodifiableList(different), Collections.unmodifiableList(missing));</span><br><span class="line">            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)</span><br><span class="line">                    : &quot;some files are missing recoveryDiff size: [&quot; + recoveryDiff.size() + &quot;] metadata size: [&quot; +</span><br><span class="line">                      this.metadata.size() + &quot;] contains  segments.gen: [&quot; + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + &quot;]&quot;;</span><br><span class="line">            return recoveryDiff;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>这里将所有的segment file分为三类：identical(相同)、different(不同)、missing(target缺失)。然后将different和missing的segment files作为第一阶段需要恢复的文件发送到target node。发送完segment files后，源节点还会向目标节点发送消息以通知目标节点清理临时文件，然后也会发送消息通知目标节点打开引擎准备接收translog。</p><p><a href="https://github.com/jiankunking/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java#L902" target="_blank" rel="noopener">sendFiles</a>逻辑如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">void sendFiles(Store store, StoreFileMetadata[] files, IntSupplier translogOps, ActionListener&lt;Void&gt; listener) &#123;</span><br><span class="line">        ArrayUtil.timSort(files, Comparator.comparingLong(StoreFileMetadata::length)); // send smallest first</span><br><span class="line"></span><br><span class="line">        final MultiChunkTransfer&lt;StoreFileMetadata, FileChunk&gt; multiFileSender =</span><br><span class="line">            new MultiChunkTransfer&lt;&gt;(logger, threadPool.getThreadContext(), listener, maxConcurrentFileChunks, Arrays.asList(files)) &#123;</span><br><span class="line"></span><br><span class="line">                final Deque&lt;byte[]&gt; buffers = new ConcurrentLinkedDeque&lt;&gt;();</span><br><span class="line">                InputStreamIndexInput currentInput = null;</span><br><span class="line">                long offset = 0;</span><br><span class="line"></span><br><span class="line">                @Override</span><br><span class="line">                protected void onNewResource(StoreFileMetadata md) throws IOException &#123;</span><br><span class="line">                    offset = 0;</span><br><span class="line">                    IOUtils.close(currentInput, () -&gt; currentInput = null);</span><br><span class="line">                    final IndexInput indexInput = store.directory().openInput(md.name(), IOContext.READONCE);</span><br><span class="line">                    currentInput = new InputStreamIndexInput(indexInput, md.length()) &#123;</span><br><span class="line">                        @Override</span><br><span class="line">                        public void close() throws IOException &#123;</span><br><span class="line">                            IOUtils.close(indexInput, super::close); // InputStreamIndexInput&apos;s close is a noop</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                @Override</span><br><span class="line">                protected FileChunk nextChunkRequest(StoreFileMetadata md) throws IOException &#123;</span><br><span class="line">                    assert Transports.assertNotTransportThread(&quot;read file chunk&quot;);</span><br><span class="line">                    cancellableThreads.checkForCancel();</span><br><span class="line">                    final byte[] buffer = Objects.requireNonNullElseGet(buffers.pollFirst(), () -&gt; new byte[chunkSizeInBytes]);</span><br><span class="line">                    final int bytesRead = currentInput.read(buffer);</span><br><span class="line">                    if (bytesRead == -1) &#123;</span><br><span class="line">                        throw new CorruptIndexException(&quot;file truncated; length=&quot; + md.length() + &quot; offset=&quot; + offset, md.name());</span><br><span class="line">                    &#125;</span><br><span class="line">                    // 注意这里 这里会判断是不是最后一批</span><br><span class="line">                    final boolean lastChunk = offset + bytesRead == md.length();</span><br><span class="line">                    final FileChunk chunk = new FileChunk(md, new BytesArray(buffer, 0, bytesRead), offset, lastChunk,</span><br><span class="line">                        () -&gt; buffers.addFirst(buffer));</span><br><span class="line">                    offset += bytesRead;</span><br><span class="line">                    return chunk;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                @Override</span><br><span class="line">                protected void executeChunkRequest(FileChunk request, ActionListener&lt;Void&gt; listener) &#123;</span><br><span class="line">                    cancellableThreads.checkForCancel();</span><br><span class="line">                    recoveryTarget.writeFileChunk(</span><br><span class="line">                        request.md, request.position, ReleasableBytesReference.wrap(request.content), request.lastChunk,</span><br><span class="line">                            translogOps.getAsInt(), ActionListener.runBefore(listener, request::close));</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                @Override</span><br><span class="line">                protected void handleError(StoreFileMetadata md, Exception e) throws Exception &#123;</span><br><span class="line">                    handleErrorOnSendFiles(store, e, new StoreFileMetadata[]&#123;md&#125;);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                @Override</span><br><span class="line">                public void close() throws IOException &#123;</span><br><span class="line">                    IOUtils.close(currentInput, () -&gt; currentInput = null);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">        resources.add(multiFileSender);</span><br><span class="line">        multiFileSender.start();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>第二阶段的逻辑比较简单，只需将translog view到当前时间之间的所有translog发送给源节点即可。</p><blockquote><p>第二阶段使用当前translog的快照，而不获取写锁(但是，translog快照是translog的时间点视图)。然后，它将每个translog操作发送到目标节点，以便将其重播到新的shard中。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Perform phase two of the recovery process.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * Phase two uses a snapshot of the current translog *without* acquiring the write lock (however, the translog snapshot is</span><br><span class="line"> * point-in-time view of the translog). It then sends each translog operation to the target node so it can be replayed into the new</span><br><span class="line"> * shard.</span><br><span class="line"> *</span><br><span class="line"> * @param startingSeqNo              the sequence number to start recovery from, or &#123;@link SequenceNumbers#UNASSIGNED_SEQ_NO&#125; if all</span><br><span class="line"> *                                   ops should be sent</span><br><span class="line"> * @param endingSeqNo                the highest sequence number that should be sent</span><br><span class="line"> * @param snapshot                   a snapshot of the translog</span><br><span class="line"> * @param maxSeenAutoIdTimestamp     the max auto_id_timestamp of append-only requests on the primary</span><br><span class="line"> * @param maxSeqNoOfUpdatesOrDeletes the max seq_no of updates or deletes on the primary after these operations were executed on it.</span><br><span class="line"> * @param listener                   a listener which will be notified with the local checkpoint on the target.</span><br><span class="line"> */</span><br><span class="line">void phase2(</span><br><span class="line">        final long startingSeqNo,</span><br><span class="line">        final long endingSeqNo,</span><br><span class="line">        final Translog.Snapshot snapshot,</span><br><span class="line">        final long maxSeenAutoIdTimestamp,</span><br><span class="line">        final long maxSeqNoOfUpdatesOrDeletes,</span><br><span class="line">        final RetentionLeases retentionLeases,</span><br><span class="line">        final long mappingVersion,</span><br><span class="line">        final ActionListener&lt;SendSnapshotResult&gt; listener) throws IOException &#123;</span><br><span class="line">    if (shard.state() == IndexShardState.CLOSED) &#123;</span><br><span class="line">        throw new IndexShardClosedException(request.shardId());</span><br><span class="line">    &#125;</span><br><span class="line">    logger.trace(&quot;recovery [phase2]: sending transaction log operations (from [&quot; + startingSeqNo + &quot;] to [&quot; + endingSeqNo + &quot;]&quot;);</span><br><span class="line">    final StopWatch stopWatch = new StopWatch().start();</span><br><span class="line">    final StepListener&lt;Void&gt; sendListener = new StepListener&lt;&gt;();</span><br><span class="line">    final OperationBatchSender sender = new OperationBatchSender(startingSeqNo, endingSeqNo, snapshot, maxSeenAutoIdTimestamp,</span><br><span class="line">        maxSeqNoOfUpdatesOrDeletes, retentionLeases, mappingVersion, sendListener);</span><br><span class="line">    sendListener.whenComplete(</span><br><span class="line">        ignored -&gt; &#123;</span><br><span class="line">            final long skippedOps = sender.skippedOps.get();</span><br><span class="line">            final int totalSentOps = sender.sentOps.get();</span><br><span class="line">            final long targetLocalCheckpoint = sender.targetLocalCheckpoint.get();</span><br><span class="line">            assert snapshot.totalOperations() == snapshot.skippedOperations() + skippedOps + totalSentOps</span><br><span class="line">                : String.format(Locale.ROOT, &quot;expected total [%d], overridden [%d], skipped [%d], total sent [%d]&quot;,</span><br><span class="line">                snapshot.totalOperations(), snapshot.skippedOperations(), skippedOps, totalSentOps);</span><br><span class="line">            stopWatch.stop();</span><br><span class="line">            final TimeValue tookTime = stopWatch.totalTime();</span><br><span class="line">            logger.trace(&quot;recovery [phase2]: took [&#123;&#125;]&quot;, tookTime);</span><br><span class="line">            listener.onResponse(new SendSnapshotResult(targetLocalCheckpoint, totalSentOps, tookTime));</span><br><span class="line">        &#125;, listener::onFailure);</span><br><span class="line">    sender.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="目标节点开始恢复">目标节点开始恢复</h2><h3 id="接收segment">接收segment</h3><p>对应上一小节源节点恢复的第一阶段，源节点将所有有差异的segment发送给目标节点，目标节点接收到后会将segment文件落盘。segment files的写入函数为<a href="https://github.com/jiankunking/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java#L498" target="_blank" rel="noopener">RecoveryTarget.writeFileChunk</a>:</p><blockquote><p>真正执行的位置：<a href="https://github.com/jiankunking/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/indices/recovery/MultiFileWriter.java#L120" target="_blank" rel="noopener">MultiFileWriter.innerWriteFileChunk</a></p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public void writeFileChunk(StoreFileMetaData fileMetaData, long position, BytesReference content, boolean lastChunk, int totalTranslogOps) throws IOException &#123;</span><br><span class="line">    final Store store = store();</span><br><span class="line">    final String name = fileMetaData.name();</span><br><span class="line">    ... ...</span><br><span class="line">    if (position == 0) &#123;</span><br><span class="line">        indexOutput = openAndPutIndexOutput(name, fileMetaData, store);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        indexOutput = getOpenIndexOutput(name); // 加一层前缀，组成临时文件</span><br><span class="line">    &#125;</span><br><span class="line">    ... ...</span><br><span class="line">    while((scratch = iterator.next()) != null) &#123; </span><br><span class="line">        indexOutput.writeBytes(scratch.bytes, scratch.offset, scratch.length); // 写临时文件</span><br><span class="line">    &#125;</span><br><span class="line">    ... ...</span><br><span class="line">    store.directory().sync(Collections.singleton(temporaryFileName));  // 这里会调用fsync落盘</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="打开引擎">打开引擎</h3><p>经过上面的过程，目标节点完成了追数据的第一步。接收完segment后，目标节点打开shard对应的引擎准备接收translog，<font color="DeepPink"><strong>注意，这里打开引擎后，正在恢复的shard便可进行写入、删除(操作包括primary shard同步的请求和translog中的操作命令)</strong></font>。打开引擎的逻辑如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"> /**</span><br><span class="line">     * Opens the engine on top of the existing lucene engine and translog.</span><br><span class="line">     * The translog is kept but its operations won&apos;t be replayed.</span><br><span class="line">     */</span><br><span class="line">    public void openEngineAndSkipTranslogRecovery() throws IOException &#123;</span><br><span class="line">        assert routingEntry().recoverySource().getType() == RecoverySource.Type.PEER : &quot;not a peer recovery [&quot; + routingEntry() + &quot;]&quot;;</span><br><span class="line">        recoveryState.validateCurrentStage(RecoveryState.Stage.TRANSLOG);</span><br><span class="line">        loadGlobalCheckpointToReplicationTracker();</span><br><span class="line">        innerOpenEngineAndTranslog(replicationTracker);</span><br><span class="line">        getEngine().skipTranslogRecovery();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">private void innerOpenEngineAndTranslog(LongSupplier globalCheckpointSupplier) throws IOException &#123;</span><br><span class="line">        assert Thread.holdsLock(mutex) == false : &quot;opening engine under mutex&quot;;</span><br><span class="line">        if (state != IndexShardState.RECOVERING) &#123;</span><br><span class="line">            throw new IndexShardNotRecoveringException(shardId, state);</span><br><span class="line">        &#125;</span><br><span class="line">        final EngineConfig config = newEngineConfig(globalCheckpointSupplier);</span><br><span class="line"></span><br><span class="line">        // we disable deletes since we allow for operations to be executed against the shard while recovering</span><br><span class="line">        // but we need to make sure we don&apos;t loose deletes until we are done recovering</span><br><span class="line">        config.setEnableGcDeletes(false);// 恢复过程中不删除translog</span><br><span class="line">        updateRetentionLeasesOnReplica(loadRetentionLeases());</span><br><span class="line">        assert recoveryState.getRecoverySource().expectEmptyRetentionLeases() == false || getRetentionLeases().leases().isEmpty()</span><br><span class="line">            : &quot;expected empty set of retention leases with recovery source [&quot; + recoveryState.getRecoverySource()</span><br><span class="line">            + &quot;] but got &quot; + getRetentionLeases();</span><br><span class="line">        synchronized (engineMutex) &#123;</span><br><span class="line">            assert currentEngineReference.get() == null : &quot;engine is running&quot;;</span><br><span class="line">            verifyNotClosed();</span><br><span class="line">            // we must create a new engine under mutex (see IndexShard#snapshotStoreMetadata).</span><br><span class="line">            final Engine newEngine = engineFactory.newReadWriteEngine(config);// 创建engine</span><br><span class="line">            onNewEngine(newEngine);</span><br><span class="line">            currentEngineReference.set(newEngine);</span><br><span class="line">            // We set active because we are now writing operations to the engine; this way,</span><br><span class="line">            // we can flush if we go idle after some time and become inactive.</span><br><span class="line">            active.set(true);</span><br><span class="line">        &#125;</span><br><span class="line">        // time elapses after the engine is created above (pulling the config settings) until we set the engine reference, during</span><br><span class="line">        // which settings changes could possibly have happened, so here we forcefully push any config changes to the new engine.</span><br><span class="line">        onSettingsChanged();</span><br><span class="line">        assert assertSequenceNumbersInCommit();</span><br><span class="line">        recoveryState.validateCurrentStage(RecoveryState.Stage.TRANSLOG);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="接收并重放translog">接收并重放translog</h3><p>打开引擎后，便可以根据translog中的命令进行相应的回放动作，回放的逻辑和正常的写入、删除类似，这里需要根据translog还原出操作类型和操作数据，并根据操作数据构建相应的数据对象，然后再调用上一步打开的engine执行相应的操作，这块逻辑如下：</p><blockquote><p>IndexShard#runTranslogRecovery<br>=&gt;<br>IndexShard#applyTranslogOperation</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">// 重放translog快照中的translog操作到当前引擎。</span><br><span class="line">// 在成功回放每个translog操作后，会通知回调onOperationRecovered。</span><br><span class="line"> /**</span><br><span class="line">     * Replays translog operations from the provided translog &#123;@code snapshot&#125; to the current engine using the given &#123;@code origin&#125;.</span><br><span class="line">     * The callback &#123;@code onOperationRecovered&#125; is notified after each translog operation is replayed successfully.</span><br><span class="line">     */</span><br><span class="line">    int runTranslogRecovery(Engine engine, Translog.Snapshot snapshot, Engine.Operation.Origin origin,</span><br><span class="line">                            Runnable onOperationRecovered) throws IOException &#123;</span><br><span class="line">        int opsRecovered = 0;</span><br><span class="line">        Translog.Operation operation;</span><br><span class="line">        while ((operation = snapshot.next()) != null) &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                logger.trace(&quot;[translog] recover op &#123;&#125;&quot;, operation);</span><br><span class="line">                Engine.Result result = applyTranslogOperation(engine, operation, origin);</span><br><span class="line">                switch (result.getResultType()) &#123;</span><br><span class="line">                    case FAILURE:</span><br><span class="line">                        throw result.getFailure();</span><br><span class="line">                    case MAPPING_UPDATE_REQUIRED:</span><br><span class="line">                        throw new IllegalArgumentException(&quot;unexpected mapping update: &quot; + result.getRequiredMappingUpdate());</span><br><span class="line">                    case SUCCESS:</span><br><span class="line">                        break;</span><br><span class="line">                    default:</span><br><span class="line">                        throw new AssertionError(&quot;Unknown result type [&quot; + result.getResultType() + &quot;]&quot;);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                opsRecovered++;</span><br><span class="line">                onOperationRecovered.run();</span><br><span class="line">            &#125; catch (Exception e) &#123;</span><br><span class="line">                // TODO: Don&apos;t enable this leniency unless users explicitly opt-in</span><br><span class="line">                if (origin == Engine.Operation.Origin.LOCAL_TRANSLOG_RECOVERY &amp;&amp; ExceptionsHelper.status(e) == RestStatus.BAD_REQUEST) &#123;</span><br><span class="line">                    // mainly for MapperParsingException and Failure to detect xcontent</span><br><span class="line">                    logger.info(&quot;ignoring recovery of a corrupt translog entry&quot;, e);</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    throw ExceptionsHelper.convertToRuntime(e);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return opsRecovered;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">private Engine.Result applyTranslogOperation(Engine engine, Translog.Operation operation,</span><br><span class="line">                                                 Engine.Operation.Origin origin) throws IOException &#123;</span><br><span class="line">        // If a translog op is replayed on the primary (eg. ccr), we need to use external instead of null for its version type.</span><br><span class="line">        final VersionType versionType = (origin == Engine.Operation.Origin.PRIMARY) ? VersionType.EXTERNAL : null;</span><br><span class="line">        final Engine.Result result;</span><br><span class="line">        switch (operation.opType()) &#123;// 还原出操作类型及操作数据并调用engine执行相应的动作</span><br><span class="line">            case INDEX:</span><br><span class="line">                final Translog.Index index = (Translog.Index) operation;</span><br><span class="line">                // we set canHaveDuplicates to true all the time such that we de-optimze the translog case and ensure that all</span><br><span class="line">                // autoGeneratedID docs that are coming from the primary are updated correctly.</span><br><span class="line">                result = applyIndexOperation(engine, index.seqNo(), index.primaryTerm(), index.version(),</span><br><span class="line">                    versionType, UNASSIGNED_SEQ_NO, 0, index.getAutoGeneratedIdTimestamp(), true, origin,</span><br><span class="line">                    new SourceToParse(shardId.getIndexName(), index.id(), index.source(),</span><br><span class="line">                        XContentHelper.xContentType(index.source()), index.routing()));</span><br><span class="line">                break;</span><br><span class="line">            case DELETE:</span><br><span class="line">                final Translog.Delete delete = (Translog.Delete) operation;</span><br><span class="line">                result = applyDeleteOperation(engine, delete.seqNo(), delete.primaryTerm(), delete.version(), delete.id(),</span><br><span class="line">                    versionType, UNASSIGNED_SEQ_NO, 0, origin);</span><br><span class="line">                break;</span><br><span class="line">            case NO_OP:</span><br><span class="line">                final Translog.NoOp noOp = (Translog.NoOp) operation;</span><br><span class="line">                result = markSeqNoAsNoop(engine, noOp.seqNo(), noOp.primaryTerm(), noOp.reason(), origin);</span><br><span class="line">                break;</span><br><span class="line">            default:</span><br><span class="line">                throw new IllegalStateException(&quot;No operation defined for [&quot; + operation + &quot;]&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        return result;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>通过上面的步骤，translog的重放完毕，此后需要做一些收尾的工作，包括，refresh让回放后的最新数据可见，打开translog gc：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">    * perform the last stages of recovery once all translog operations are done.</span><br><span class="line">    * note that you should still call &#123;@link #postRecovery(String)&#125;.</span><br><span class="line">    */</span><br><span class="line">   public void finalizeRecovery() &#123;</span><br><span class="line">       recoveryState().setStage(RecoveryState.Stage.FINALIZE);</span><br><span class="line">       Engine engine = getEngine();</span><br><span class="line">       engine.refresh(&quot;recovery_finalization&quot;);</span><br><span class="line">       engine.config().setEnableGcDeletes(true);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>到这里，replica shard恢复的两个阶段便完成了，由于此时shard还处于INITIALIZING状态，还需通知master节点启动已恢复的shard：</p><blockquote><p>IndicesClusterStateService#RecoveryListener</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void onRecoveryDone(final RecoveryState state, ShardLongFieldRange timestampMillisFieldRange) &#123;</span><br><span class="line">    shardStateAction.shardStarted(</span><br><span class="line">            shardRouting,</span><br><span class="line">            primaryTerm,</span><br><span class="line">            &quot;after &quot; + state.getRecoverySource(),</span><br><span class="line">            timestampMillisFieldRange,</span><br><span class="line">            SHARD_STATE_ACTION_LISTENER);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至此，shard recovery的所有流程都已完成。</p><h1>小结</h1><h2 id="疑问点1：网上说的对不对？新版本中有没有更新？">疑问点1：网上说的对不对？新版本中有没有更新？</h2><p>到这里完整跟着腾讯云的文档走了一遍，主体的流程在Elasticsearch 8.0.0-SNAPSHOT中基本一样，只是部分方法有些调整。</p><p>所以下面这个流程还是正确的：</p><p>ES副本分片恢复主要涉及恢复的目标节点和源节点，目标节点即故障恢复的节点，源节点为提供恢复的节点。目标节点向源节点发送分片恢复请求，源节点接收到请求后主要分两阶段来处理。第一阶段，对需要恢复的shard创建snapshot，然后根据请求中的metadata对比如果 syncid 相同且 doc 数量相同则跳过，否则对比shard的segment文件差异，将有差异的segment文件发送给target node。第二阶段，为了保证target node数据的完整性，需要将本地的translog发送给target node，且对接收到的translog进行回放。整体流程如下图所示：</p><p><img data-src="/images/elasticsearch-shard-recovery-source-code-analysis/%E5%88%86%E7%89%87%E6%81%A2%E5%A4%8D%E6%B5%81%E7%A8%8B.png" alt></p><h2 id="疑问点2：在分片恢复的时候，如果收到Api-forcemerge请求，这时候，会如何处理">疑问点2：在分片恢复的时候，如果收到Api _forcemerge请求，这时候，会如何处理?</h2><p>这部分等看/_forcemerge api的时候，再解答一下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">RestForceMergeAction</span><br><span class="line">=&gt;</span><br><span class="line">TransportForceMergeAction#shardOperation(ForceMergeRequest request, ShardRouting shardRouting)</span><br><span class="line">=&gt;</span><br><span class="line">IndexShard#forceMerge(ForceMergeRequest forceMerge)</span><br><span class="line">=&gt;</span><br><span class="line">InternalEngine#forceMerge</span><br><span class="line"></span><br><span class="line">// https://github.com/jiankunking/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java#L1900</span><br><span class="line">1. 只合并删除文档</span><br><span class="line">2. 没有限制最大segment数的合并</span><br><span class="line">3. 限制了最大segment数的合并</span><br></pre></td></tr></table></figure><p>其实问题不在这里，recovery依赖segment file，当recovery启动后，程序会获取到文件的reader，这时候即使recovery，也不会删除segment file。</p><h2 id="疑问点3：片恢复的第二阶段是同步translog-这一步会不会加锁？不加锁的话，如何确保是同步完成了？">疑问点3：片恢复的第二阶段是同步translog,这一步会不会加锁？不加锁的话，如何确保是同步完成了？</h2><h3 id="完整性">完整性</h3><p>首先，phase1阶段，保证了存量的历史数据可以恢复到从分片。phase1阶段完成后，从分片引擎打开，可以正常处理index、delete请求，而translog覆盖完了整个phase1阶段，因此在phase1阶段中的index/delete操作都将被记录下来，在phase2阶段进行translog回放时，副本分片正常的index和delete操作和translog是并行执行的，这就保证了恢复开始之前的数据、恢复中的数据都会完整的写入到副本分片，保证了数据的完整性。如下图所示：</p><p><img data-src="/images/elasticsearch-shard-recovery-source-code-analysis/%E4%BB%8E%E5%88%86%E7%89%87%E6%81%A2%E5%A4%8D%E6%97%B6%E5%BA%8F%E5%9B%BE.png" alt></p><h3 id="一致性">一致性</h3><p>由于phase1阶段完成后，从分片便可正常处理写入操作，而此时从分片的写入和phase2阶段的translog回放时并行执行的，如果translog的回放慢于正常的写入操作，那么可能会导致老的数据后写入，造成数据不一致。ES为了保证数据的一致性在进行写入操作时，会比较当前写入的版本和lucene文档版本号，如果当前版本更小，说明是旧数据则不会将文档写入lucene。相关代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// https://github.com/jiankunking/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java#L993</span><br><span class="line">final OpVsLuceneDocStatus opVsLucene = compareOpToLuceneDocBasedOnSeqNo(index);</span><br><span class="line">if (opVsLucene == OpVsLuceneDocStatus.OP_STALE_OR_EQUAL) &#123;</span><br><span class="line">    plan = IndexingStrategy.processAsStaleOp(index.version(), 0);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>拓展:如何保证副分片和主分片一致</h1><blockquote><p>整理自:《Elasticsearch源码解析与优化实战》</p></blockquote><p>索引恢复过程的一个难点在于如何维护主副分片的一致性。假设副分片恢复期间一直有写操作，如何实现一致呢？<br>我们先看看早期的做法：在2.0版本之前，副分片恢复要经历三个阶段。</p><ul><li>phase1：将主分片的 Lucene 做快照，发送到 target。期间不阻塞索引操作，新增数据写到主分片的translog。</li><li>phase2：将主分片translog做快照，发送到target重放，期间不阻塞索引操作。</li><li>phase3：为主分片加写锁，将剩余的translog发送到target。此时数据量很小，写入过程的阻塞很短。</li></ul><p>从理论上来说，只要流程上允许将写操作阻塞一段时间，实现主副一致是比较容易的。</p><p>但是后来(从2.0版本开始)，也就是引入translog.view概念的同时，phase3被删除。</p><p>phase3被删除，这个阶段是重放操作(operations)，同时防止新的写入 Engine。这是不必要的，因为自恢复开始，标准的 index 操作会发送所有的操作到正在恢复中的分片。重放恢复开始时获取的view中的所有操作足够保证不丢失任何操作。</p><p>阻塞写操作的phase3被删除，恢复期间没有任何写阻塞过程。接下来需要处理的就是解决phase1和phase2之间的写操作与phase2重放操作之间的时序和冲突问题。在副分片节点，phase1结束后，假如新增索引操作和 translog 重放操作并发执行，因为时序的关系会出现新老数据交替。如何实现主副分片一致呢？</p><p>假设在第一阶段执行期间，有客户端索引操作要求将docA的内容写为1，主分片执行了这个操作，而副分片由于尚未就绪所以没有执行。第二阶段期间客户端索引操作要求写 docA 的内容为2，此时副分片已经就绪，先执行将docA写为2的新增请求，然后又收到了从主分片所在节点发送过来的translog重复写docA为1的请求该如何处理？具体流程如下图所示。</p><p><img data-src="/images/elasticsearch-shard-recovery-source-code-analysis/01.png" alt></p><p>答案是在写流程中做异常处理，通过版本号来过滤掉过期操作。写操作有三种类型：索引新文档、更新、删除。索引新文档不存在冲突问题，更新和删除操作采用相同的处理机制。每个操作都有一个版本号，这个版本号就是预期doc版本，它必须大于当前Lucene中的doc版本号，否则就放弃本次操作。对于更新操作来说，预期版本号是Lucene doc版本号+1。主分片节点写成功后新数据的版本号会放到写副本的请求中，这个请求中的版本号就是预期版本号。</p><p>这样，时序上存在错误的操作被忽略，对于特定doc，只有最新一次操作生效，保证了主副分片一致。</p><p>我们分别看一下写操作三种类型的处理机制。</p><h2 id="1．索引新文档">1．索引新文档</h2><p>不存在冲突问题，不需要处理。</p><h2 id="2．更新">2．更新</h2><p>判断本次操作的版本号是否小于Lucene中doc的版本号，如果小于，则放弃本次操作。</p><p>Index、Delete都继承自Operation，每个Operation都有一个版本号，这个版本号就是doc版本号。对于副分片的写流程来说，正常情况下是主分片写成功后，相应doc写入的版本号被放到转发写副分片的请求中。对于更新来说，就是通过主分片将原doc版本号+1后转发到副分片实现的。在对比版本号的时候：</p><p>expectedVersion = 写副分片请求中的 version = 写主分片成功后的version</p><p>通过下面的方法判断当前操作的版本号是否低于Lucene中的版本号：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// VersionType#isVersionConflictForWrites</span><br><span class="line"> EXTERNAL((byte) 1) &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) &#123;</span><br><span class="line">            if (currentVersion == Versions.NOT_FOUND) &#123;</span><br><span class="line">                return false;</span><br><span class="line">            &#125;</span><br><span class="line">            if (expectedVersion == Versions.MATCH_ANY) &#123;</span><br><span class="line">                return true;</span><br><span class="line">            &#125;</span><br><span class="line">            if (currentVersion &gt;= expectedVersion) &#123;</span><br><span class="line">                return true;</span><br><span class="line">            &#125;</span><br><span class="line">            return false;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>如果translog重放的操作在写一条&quot;老&quot;数据，则compareOpToLuceneDocBasedOnSeqNo会返回OpVsLuceneDocStatus.OP_STALE_OR_EQUAL。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">private OpVsLuceneDocStatus compareOpToLuceneDocBasedOnSeqNo(final Operation op) throws IOException &#123;</span><br><span class="line">        assert op.seqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO : &quot;resolving ops based on seq# but no seqNo is found&quot;;</span><br><span class="line">        final OpVsLuceneDocStatus status;</span><br><span class="line">        VersionValue versionValue = getVersionFromMap(op.uid().bytes());</span><br><span class="line">        assert incrementVersionLookup();</span><br><span class="line">        if (versionValue != null) &#123;</span><br><span class="line">            status = compareOpToVersionMapOnSeqNo(op.id(), op.seqNo(), op.primaryTerm(), versionValue);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            // load from index</span><br><span class="line">            assert incrementIndexVersionLookup();</span><br><span class="line">            try (Searcher searcher = acquireSearcher(&quot;load_seq_no&quot;, SearcherScope.INTERNAL)) &#123;</span><br><span class="line">                final DocIdAndSeqNo docAndSeqNo = VersionsAndSeqNoResolver.loadDocIdAndSeqNo(searcher.getIndexReader(), op.uid());</span><br><span class="line">                if (docAndSeqNo == null) &#123;</span><br><span class="line">                    status = OpVsLuceneDocStatus.LUCENE_DOC_NOT_FOUND;</span><br><span class="line">                &#125; else if (op.seqNo() &gt; docAndSeqNo.seqNo) &#123;</span><br><span class="line">                    status = OpVsLuceneDocStatus.OP_NEWER;</span><br><span class="line">                &#125; else if (op.seqNo() == docAndSeqNo.seqNo) &#123;</span><br><span class="line">                    assert localCheckpointTracker.hasProcessed(op.seqNo()) :</span><br><span class="line">                        &quot;local checkpoint tracker is not updated seq_no=&quot; + op.seqNo() + &quot; id=&quot; + op.id();</span><br><span class="line">                    status = OpVsLuceneDocStatus.OP_STALE_OR_EQUAL;</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    status = OpVsLuceneDocStatus.OP_STALE_OR_EQUAL;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return status;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>副分片在InternalEngine#index函数中通过plan判断是否写到Lucene：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// non-primary mode (i.e., replica or recovery)</span><br><span class="line">final IndexingStrategy plan = indexingStrategyForOperation(index);</span><br></pre></td></tr></table></figure><p>在 indexingStrategyForOperation函数中，plan的最终结果就是plan =IndexingStrategy.processButSkipLucene，后面会跳过写Lucene和translog的逻辑。</p><h2 id="3．-删除">3． 删除</h2><p>判断本次操作中的版本号是否小于Lucene中doc的版本号，如果小于，则放弃本次操作。</p><p>通过compareOpToLuceneDocBasedOnSeqNo方法判断本次操作是否小于Lucenne中doc的版本号，与Index操作时使用相同的比较函数。</p><p>类似的，在InternalEngine#delete函数中判断是否写到Lucene：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">final DeletionStrategy plan = deletionStrategyForOperation(delete);</span><br></pre></td></tr></table></figure><p>如果translog重放的是一个&quot;老&quot;的删除操作，则compareOpToLuceneDocBasedOnSeqNo会返回OpVsLuceneDocStatus.OP_STALE_OR_EQUAL。</p><p>plan的最终结果就是plan=DeletionStrategy.processButSkipLucene，后面会跳过Lucene删除的逻辑。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * the status of the current doc version in lucene, compared to the version in an incoming</span><br><span class="line">   * operation</span><br><span class="line">   */</span><br><span class="line">  enum OpVsLuceneDocStatus &#123;</span><br><span class="line">      /** the op is more recent than the one that last modified the doc found in lucene*/</span><br><span class="line">      OP_NEWER,</span><br><span class="line">      /** the op is older or the same as the one that last modified the doc found in lucene*/</span><br><span class="line">      OP_STALE_OR_EQUAL,</span><br><span class="line">      /** no doc was found in lucene */</span><br><span class="line">      LUCENE_DOC_NOT_FOUND</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;带着疑问学源码，第七篇：Elasticsearch 分片恢复分析&lt;br&gt;
代码分析基于：&lt;a href=&quot;https://github.com/jiankunking/elasticsearch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/jiankunking/elasticsearch&lt;/a&gt;&lt;br&gt;
Elasticsearch 8.0.0-SNAPSHOT&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="ElasticSearch" scheme="https://jiankunking.com/categories/elasticsearch/"/>
    
    
    <category term="原创" scheme="https://jiankunking.com/tags/原创/"/>
    
    <category term="ElasticSearch" scheme="https://jiankunking.com/tags/elasticsearch/"/>
    
    <category term="Shard" scheme="https://jiankunking.com/tags/shard/"/>
    
    <category term="源码" scheme="https://jiankunking.com/tags/源码/"/>
    
    <category term="Recovery" scheme="https://jiankunking.com/tags/recovery/"/>
    
  </entry>
  
  <entry>
    <title>记一次RocketMQ Client超时问题排查</title>
    <link href="https://jiankunking.com/org-apache-rocketmq-shaded-io-grpc-statusruntimeexception-deadline-exceeded-clientcall-started-after-deadline-exceeded-2-591386886s-from-now.html"/>
    <id>https://jiankunking.com/org-apache-rocketmq-shaded-io-grpc-statusruntimeexception-deadline-exceeded-clientcall-started-after-deadline-exceeded-2-591386886s-from-now.html</id>
    <published>2023-12-15T14:35:02.000Z</published>
    <updated>2024-04-18T07:10:53.385Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>问题的一种可能</p></blockquote><a id="more"></a><p>昨天同事A遇到一个诡异问题，在代码中加入消费rocketmq的代码后，在本地运行好好的程序，部署到云上测试集群A1会报出以下异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">time,cluster,namespace,service,pod,log</span><br><span class="line">2023-12-14 09:36:00,cluster-test,did-test,jiankunking-test,jiankunking-test-6b86565dbb-lzt9d,2023-12-14 09:36:00.595  INFO 1 --- [  XNIO-1 task-1] c.h.d.i.rocket.PdmRocketMqConsumer       : [TID:95e19b4533204903a985c90b068cddcc_64_17025177603380033] --- 构建mq5.0消费者：proxy:10.163.240.237:8080, topic:product_material_topic_uat_center, group:product_material_group_center</span><br><span class="line">2023-12-14 09:36:40,cluster-test,did-test,jiankunking-test,jiankunking-test-6b86565dbb-lzt9d,2023-12-14 09:36:39.995 ERROR 1 --- [  XNIO-1 task-1] c.h.d.i.rocket.PdmRocketMqConsumer       : [TID:95e19b4533204903a985c90b068cddcc_64_17025177603380033] --- 构建mq5.0消费者异常：proxy:10.163.240.237:8080, topic:product_material_topic_uat_center, group：product_material_group_center</span><br><span class="line">2023-12-14 09:36:40,cluster-test,did-test,jiankunking-test,jiankunking-test-6b86565dbb-lzt9d,</span><br><span class="line">2023-12-14 09:36:40,cluster-test,did-test,jiankunking-test,jiankunking-test-6b86565dbb-lzt9d,java.lang.IllegalStateException: Expected the service PushConsumerImpl-0 [FAILED] to be RUNNING, but the service has FAILED</span><br><span class="line">at org.apache.rocketmq.shaded.com.google.common.util.concurrent.AbstractService.checkCurrentState(AbstractService.java:381) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.com.google.common.util.concurrent.AbstractService.awaitRunning(AbstractService.java:305) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.com.google.common.util.concurrent.AbstractIdleService.awaitRunning(AbstractIdleService.java:165) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.client.java.impl.consumer.PushConsumerBuilderImpl.build(PushConsumerBuilderImpl.java:128) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at com.jiankunking.issuemanage.rocket.PdmRocketMqConsumer.initConsumer(PdmRocketMqConsumer.java:89) ~[classes!/:na]</span><br><span class="line">at com.jiankunking.issuemanage.controller.IssueTagInfoController.initRocketMq$original$l28F75DZ(IssueTagInfoController.java:103) ~[classes!/:na]</span><br><span class="line">at com.jiankunking.issuemanage.controller.IssueTagInfoController.initRocketMq$original$l28F75DZ$accessor$0D0TSRNs(IssueTagInfoController.java) ~[classes!/:na]</span><br><span class="line">at com.jiankunking.issuemanage.controller.IssueTagInfoController$auxiliary$Rnw73z4x.call(Unknown Source) ~[classes!/:na]</span><br><span class="line">at org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstMethodsInter.intercept(InstMethodsInter.java:86) ~[skywalking-agent.jar:8.12.0]</span><br><span class="line">at com.jiankunking.issuemanage.controller.IssueTagInfoController.initRocketMq(IssueTagInfoController.java) ~[classes!/:na]</span><br><span class="line">at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]</span><br><span class="line">at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]</span><br><span class="line">at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]</span><br><span class="line">at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]</span><br><span class="line">at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205) ~[spring-web-5.3.24.jar!/:5.3.24]</span><br><span class="line">at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150) ~[spring-web-5.3.24.jar!/:5.3.24]</span><br><span class="line">at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117) ~[spring-webmvc-5.3.24.jar!/:5.3.24]</span><br><span class="line">at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895) ~[spring-webmvc-5.3.24.jar!/:5.3.24]</span><br><span class="line">at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808) ~[spring-webmvc-5.3.24.jar!/:5.3.24]</span><br><span class="line">at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87) ~[spring-webmvc-5.3.24.jar!/:5.3.24]</span><br><span class="line">at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1071) ~[spring-webmvc-5.3.24.jar!/:5.3.24]</span><br><span class="line">at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:964) ~[spring-webmvc-5.3.24.jar!/:5.3.24]</span><br><span class="line">at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006) ~[spring-webmvc-5.3.24.jar!/:5.3.24]</span><br><span class="line">at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:898) ~[spring-webmvc-5.3.24.jar!/:5.3.24]</span><br><span class="line">at javax.servlet.http.HttpServlet.service(HttpServlet.java:645) ~[javax.servlet-api-4.0.1.jar!/:4.0.1]</span><br><span class="line">at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883) ~[spring-webmvc-5.3.24.jar!/:5.3.24]</span><br><span class="line">at javax.servlet.http.HttpServlet.service(HttpServlet.java:750) ~[javax.servlet-api-4.0.1.jar!/:4.0.1]</span><br><span class="line">at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:74) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:129) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100) ~[spring-web-5.3.24.jar!/:5.3.24]</span><br><span class="line">at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117) ~[spring-web-5.3.24.jar!/:5.3.24]</span><br><span class="line">at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93) ~[spring-web-5.3.24.jar!/:5.3.24]</span><br><span class="line">at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117) ~[spring-web-5.3.24.jar!/:5.3.24]</span><br><span class="line">at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201) ~[spring-web-5.3.24.jar!/:5.3.24]</span><br><span class="line">at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117) ~[spring-web-5.3.24.jar!/:5.3.24]</span><br><span class="line">at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.FilterHandler.handleRequest(FilterHandler.java:84) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.ServletChain$1.handleRequest(ServletChain.java:68) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.RedirectDirHandler.handleRequest(RedirectDirHandler.java:68) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:117) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) ~[undertow-core-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46) ~[undertow-core-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60) ~[undertow-core-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43) ~[undertow-core-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) ~[undertow-core-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.SendErrorPageHandler.handleRequest(SendErrorPageHandler.java:52) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) ~[undertow-core-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:275) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:79) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:134) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:131) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:255) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:79) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:100) ~[undertow-servlet-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.server.Connectors.executeRootHandler(Connectors.java:387) ~[undertow-core-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:852) ~[undertow-core-2.2.20.Final.jar!/:2.2.20.Final]</span><br><span class="line">at org.apache.skywalking.apm.plugin.undertow.v2x.SWRunnable.run(SWRunnable.java:45) ~[na:na]</span><br><span class="line">at org.apache.skywalking.apm.plugin.undertow.v2x.SWRunnable.run(SWRunnable.java:45) ~[na:na]</span><br><span class="line">at org.jboss.threads.ContextClassLoaderSavingRunnable.run(ContextClassLoaderSavingRunnable.java:35) ~[jboss-threads-3.1.0.Final.jar!/:3.1.0.Final]</span><br><span class="line">at org.jboss.threads.EnhancedQueueExecutor.safeRun(EnhancedQueueExecutor.java:2019) ~[jboss-threads-3.1.0.Final.jar!/:3.1.0.Final]</span><br><span class="line">at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.doRunTask(EnhancedQueueExecutor.java:1558) ~[jboss-threads-3.1.0.Final.jar!/:3.1.0.Final]</span><br><span class="line">at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1449) ~[jboss-threads-3.1.0.Final.jar!/:3.1.0.Final]</span><br><span class="line">at org.xnio.XnioWorker$WorkerThreadFactory$1$1.run(XnioWorker.java:1282) ~[xnio-api-3.8.7.Final.jar!/:3.8.7.Final]</span><br><span class="line">at java.base/java.lang.Thread.run(Thread.java:834) ~[na:na]</span><br><span class="line">Caused by: java.util.concurrent.ExecutionException: org.apache.rocketmq.shaded.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -2.591386886s from now</span><br><span class="line">at org.apache.rocketmq.shaded.com.google.common.util.concurrent.AbstractFuture.getDoneValue(AbstractFuture.java:588) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:547) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:91) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.client.java.impl.ClientImpl.startUp(ClientImpl.java:188) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.client.java.impl.consumer.PushConsumerImpl.startUp(PushConsumerImpl.java:161) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.com.google.common.util.concurrent.AbstractIdleService$DelegateService$1.run(AbstractIdleService.java:62) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.com.google.common.util.concurrent.Callables.lambda$threadRenaming$3(Callables.java:103) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">... 1 common frames omitted</span><br><span class="line">Caused by: org.apache.rocketmq.shaded.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -2.591386886s from now</span><br><span class="line">at org.apache.rocketmq.shaded.io.grpc.Status.asRuntimeException(Status.java:539) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:544) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:563) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:744) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:723) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at org.apache.rocketmq.shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133) ~[rocketmq-client-java-5.0.5.jar!/:na]</span><br><span class="line">at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[na:na]</span><br><span class="line">at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[na:na]</span><br><span class="line">... 1 common frames omitted</span><br></pre></td></tr></table></figure><p>由于本地代码跟服务端代码完全一样，在加上错误提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Expected the service PushConsumerImpl-0 [FAILED] to be RUNNING, but the service has FAILED</span><br><span class="line"></span><br><span class="line">org.apache.rocketmq.shaded.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -2.591386886s from now</span><br></pre></td></tr></table></figure><p>同事第一怀疑的点是集群网络环境，这里我的第一反应不会是网络问题，为啥这么想呢？</p><p>因为如果是网络问题，那么集群内肯定会有大量的报错，而不仅仅是这一个服务。</p><p>那么问题出现在哪呢？难道是代码的问题？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">@Slf4j</span><br><span class="line">@Component</span><br><span class="line">@Configuration</span><br><span class="line">@EnableConfigurationProperties(RocketMqProperties.class)</span><br><span class="line">public class DemoConsumer &#123;</span><br><span class="line"></span><br><span class="line">    private final ClientServiceProvider provider = ClientServiceProvider.loadService();</span><br><span class="line">    private final List&lt;PushConsumer&gt; pushConsumers = Collections.synchronizedList(new LinkedList&lt;&gt;());</span><br><span class="line"></span><br><span class="line">    private final RestTemplate restTemplate;</span><br><span class="line">    private final String endpoints;</span><br><span class="line">    private final String accessKey;</span><br><span class="line">    private final String secretKey;</span><br><span class="line">    private final String topic;</span><br><span class="line">    private final String group;</span><br><span class="line"></span><br><span class="line">    @Autowired</span><br><span class="line">    public DemoConsumer(RestTemplate restTemplate, RocketMqProperties properties) &#123;</span><br><span class="line">        this.restTemplate = restTemplate;</span><br><span class="line">        this.accessKey = properties.getAccessKey();</span><br><span class="line">        this.secretKey = properties.getSecretKey();</span><br><span class="line">        this.endpoints = properties.getEndpoints();</span><br><span class="line">        this.topic = properties.getTopic();</span><br><span class="line">        this.group = properties.getGroup();</span><br><span class="line">        listener();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void listener() &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            log.info(&quot;开始构建mq5.0消费者&quot;);</span><br><span class="line">            SessionCredentialsProvider sessionCredentialsProvider = new StaticSessionCredentialsProvider(accessKey, secretKey);</span><br><span class="line">            ClientConfiguration clientConfiguration = ClientConfiguration.newBuilder()</span><br><span class="line">                    .setEndpoints(endpoints)</span><br><span class="line">                    .setCredentialProvider(sessionCredentialsProvider)</span><br><span class="line">                    .build();</span><br><span class="line">            PushConsumer pushConsumer = provider.newPushConsumerBuilder()</span><br><span class="line">                    .setClientConfiguration(clientConfiguration)</span><br><span class="line">                    .setConsumerGroup(group)</span><br><span class="line">                    .setSubscriptionExpressions(Collections.singletonMap(topic, new FilterExpression(&quot;*&quot;, FilterExpressionType.TAG)))</span><br><span class="line">                    .setMessageListener(messageView -&gt; &#123;</span><br><span class="line">                        String tagInfo = messageView.getTag().orElse(&quot;&quot;);</span><br><span class="line">                        String code = StandardCharsets.UTF_8.decode(messageView.getBody()).toString();</span><br><span class="line">                        log.info(&quot;tagInfo:&#123;&#125; 消息内容:&#123;&#125;&quot;, tagInfo, StandardCharsets.UTF_8.decode(messageView.getBody()));</span><br><span class="line">                        if (StringUtils.isBlank(tagInfo) || StringUtils.isBlank(code)) &#123;</span><br><span class="line">                            return ConsumeResult.SUCCESS;</span><br><span class="line">                        &#125;</span><br><span class="line">                        // 拆分tagInfo 获取素材类型</span><br><span class="line">                        return ConsumeResult.SUCCESS;</span><br><span class="line">                    &#125;).build();</span><br><span class="line">            log.info(&quot;构建mq5.0消费者成功：proxy:&#123;&#125;, topic:&#123;&#125;, group：&#123;&#125;&quot;, endpoints, topic, group);</span><br><span class="line">            pushConsumers.add(pushConsumer);</span><br><span class="line">        &#125; catch (ClientException e) &#123;</span><br><span class="line">            log.info(&quot;构建mq5.0消费者失败：proxy:&#123;&#125;, topic:&#123;&#125;, group：&#123;&#125;, error:&#123;&#125;&quot;, endpoints, topic, group, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @PreDestroy</span><br><span class="line">    public void destroy() &#123;</span><br><span class="line">        pushConsumers.forEach(pushConsumer -&gt; &#123;</span><br><span class="line">            try (pushConsumer) &#123;</span><br><span class="line">                log.info(&quot;关闭mq5.0消费者：proxy:&#123;&#125;, topic:&#123;&#125;, group：&#123;&#125;&quot;, endpoints, topic, group);</span><br><span class="line">            &#125; catch (Exception e) &#123;</span><br><span class="line">                log.error(&quot;关闭mq5.0消费者失败：proxy:&#123;&#125;, topic:&#123;&#125;, group：&#123;&#125;, error:&#123;&#125;&quot;, endpoints, topic, group, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我比对了下官方demo，发现这里跟官方demo一模一样。</p><blockquote><p><a href="https://github.com/apache/rocketmq-clients/blob/master/java/client/src/main/java/org/apache/rocketmq/client/java/example/PushConsumerExample.java" target="_blank" rel="noopener">https://github.com/apache/rocketmq-clients/blob/master/java/client/src/main/java/org/apache/rocketmq/client/java/example/PushConsumerExample.java</a></p></blockquote><p>而且另一个同事B一模一样的代码在另一个集群B1能运行起来。</p><p>难道真的是网络问题？</p><p>上述异常是在服务启动的时候，抛出的，导致服务一直没有起来，这时候容器还有创建成功，也就没有对应的进程，所以无法通过抓包的方式排除网络问题。</p><p>我让同事将rocketmq初始化的逻辑调整为调用api触发，这样可以保证服务能够正常启动,然后我也可以通过网络抓包的方式来获取网络流程。</p><p>k8s集群中抓包的方式参见:<a href="https://jiankunking.blog.csdn.net/article/details/125189956" target="_blank" rel="noopener">https://jiankunking.blog.csdn.net/article/details/125189956</a></p><p>抓包截图如下：<br><img data-src="/images/deadline-exceeded-clientcall/01.png" alt></p><p>从图中可以看出网络连接正常建立，并在2.7秒左右的时间服务主动RST连接，这个时间也就是服务抛出异常的时间。</p><p>从网络抓包，也验证了咱们的想法，网络至少在TCP层面是没有问题的。</p><p>虽然抓包已经验证了，但我还是将有问题的镜像部署到部署到集群B1发现还是有问题。</p><p>那么问题会出在哪里？排查一度陷入困境。</p><p>这期间也尝试过</p><ul><li>将rocketmq的地址改成https://…</li><li>关闭、开启enableSsl</li><li>等 问题依然没有解决</li></ul><p>该服务在测试环境的配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">内存上限：800m 请求600m</span><br><span class="line">CPU上限0.3核  请求0.1核</span><br></pre></td></tr></table></figure><p>在测试环境中该服务的启动时间长达4分多钟，会不会cpu资源太少，服务启动的时候JIT占用太多资源，导致服务请求rocketmq的网络连接一直得不到调度，从而出现超时呢？</p><p>增加CPU上限到1核，服务不到一分钟就启动了，但异常依旧。</p><p>排查又卡主了，这时候想起了一个点</p><blockquote><p>1、容器云中的应用我们会默认注入skywalking、prometheus等agent<br>2、skywalking在我之前不严谨的测试中发现会占用200~300m的内存<br>3、java 命令会被复写，默认堆内存是内存上限的65%</p></blockquote><p>800*0.65=520m,再减掉200~300m,发现留给服务的内存大约220~320m之间，那么会不会是内存太少导致的问题呢？</p><p>逐步提升内存请求的上限到1200m，发现错误消失。</p><p>再回头看异常日志，发现在出现rocketmq异常的时候，发现HikariPool创建数据库连接也出现了问题</p><blockquote><p>HikariPool初始化时机是在rocketmq init之后</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2023-12-14 09:43:24,cluster-test,did-test,jiankunking,jiankunking-6b86565dbb-lzt9d,2023-12-14 09:43:24.098  WARN 1 --- [  XNIO-1 task-1] com.zaxxer.hikari.pool.PoolBase          : [TID:95e19b4533204903a985c90b068cddcc_65_17025182033670077] --- HikariPool-1 - Failed to validate connection com.MySQL.cj.jdbc.ConnectionImpl@5c4719b1 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.</span><br><span class="line">2023-12-14 09:43:24,cluster-test,did-test,jiankunking,jiankunking-6b86565dbb-lzt9d,2023-12-14 09:43:24.101  WARN 1 --- [  XNIO-1 task-1] com.zaxxer.hikari.pool.PoolBase          : [TID:95e19b4533204903a985c90b068cddcc_65_17025182033670077] --- HikariPool-1 - Failed to validate connection com.MySQL.cj.jdbc.ConnectionImpl@33b9b407 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.</span><br><span class="line">2023-12-14 09:43:24,cluster-test,did-test,jiankunking,jiankunking-6b86565dbb-lzt9d,2023-12-14 09:43:24.103  WARN 1 --- [  XNIO-1 task-1] com.zaxxer.hikari.pool.PoolBase          : [TID:95e19b4533204903a985c90b068cddcc_65_17025182033670077] --- HikariPool-1 - Failed to validate connection com.MySQL.cj.jdbc.ConnectionImpl@59c237c9 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.</span><br><span class="line">2023-12-14 09:43:24,cluster-test,did-test,jiankunking,jiankunking-6b86565dbb-lzt9d,2023-12-14 09:43:24.196  WARN 1 --- [  XNIO-1 task-1] com.zaxxer.hikari.pool.PoolBase          : [TID:95e19b4533204903a985c90b068cddcc_65_17025182033670077] --- HikariPool-1 - Failed to validate connection com.MySQL.cj.jdbc.ConnectionImpl@4c517517 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.</span><br><span class="line">2023-12-14 09:43:24,cluster-test,did-test,jiankunking,jiankunking-6b86565dbb-lzt9d,2023-12-14 09:43:24.198  WARN 1 --- [  XNIO-1 task-1] com.zaxxer.hikari.pool.PoolBase          : [TID:95e19b4533204903a985c90b068cddcc_65_17025182033670077] --- HikariPool-1 - Failed to validate connection com.MySQL.cj.jdbc.ConnectionImpl@28bf77bd (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.</span><br><span class="line">2023-12-14 09:43:24,cluster-test,did-test,jiankunking,jiankunking-6b86565dbb-lzt9d,2023-12-14 09:43:24.203  WARN 1 --- [  XNIO-1 task-1] com.zaxxer.hikari.pool.PoolBase          : [TID:95e19b4533204903a985c90b068cddcc_65_17025182033670077] --- HikariPool-1 - Failed to validate connection com.MySQL.cj.jdbc.ConnectionImpl@59d2c94b (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.</span><br><span class="line">2023-12-14 09:43:24,cluster-test,did-test,jiankunking,jiankunking-6b86565dbb-lzt9d,2023-12-14 09:43:24.296  WARN 1 --- [  XNIO-1 task-1] com.zaxxer.hikari.pool.PoolBase          : [TID:95e19b4533204903a985c90b068cddcc_65_17025182033670077] --- HikariPool-1 - Failed to validate connection com.MySQL.cj.jdbc.ConnectionImpl@a835f53 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.</span><br></pre></td></tr></table></figure><p>然后，告诉同事问题解决方案，以为这就结束了…</p><p>然而下午同事发现将内存的上限改回800m服务也可以正常启动了，难道上午哪里出了问题？</p><p>先用之前问题镜像，降低内存上限，服务真的正常启动了…</p><p><img data-src="/images/deadline-exceeded-clientcall/%E6%88%91%E7%9A%84%E5%A4%A9%E5%91%90.png" alt></p><p>既然服务现在能用之前的配置起来了，那么在没有默认注入agent的情况下，服务到底多大内存能正常启动呢？</p><p>先替换启动命令，移除默认注入的skywalking、prometheus等agent</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">java2</span><br><span class="line">-server</span><br><span class="line">-Xms300m</span><br><span class="line">-Xmx300m</span><br><span class="line">-jar</span><br><span class="line">application.jar</span><br><span class="line">--spring.profiles.active=test</span><br></pre></td></tr></table></figure><p>发现服务300m就能正常启动。</p><p>然后换回带有agent的启动命令，发现这时：</p><ul><li>内存上限=请求=500m的时候服务可以正常启动</li><li>内存上限=请求=450m的时候会出现上述的异常</li><li>内存上限=请求=400m的时候会出现容器OOMKilled的异常</li></ul><p>到这里可以明确出两个结论</p><ul><li>异常是由于内存不够产生的</li><li>产生异常的临界值没有明确<ul><li>怀疑两点：<ul><li>skywalking等注入agent内存占用受别的因素影响会发生较大波动</li><li>服务自身环境上午数据与下午数据占用内存不一致</li></ul></li></ul></li></ul><p>这里也怀疑过是集群超卖了内存请求，经排查</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">  ratio:</span><br><span class="line">    limits.cpu: 500</span><br><span class="line">    limits.memory: 500</span><br><span class="line">    requests.cpu: 100</span><br><span class="line">    requests.memory: 100</span><br></pre></td></tr></table></figure><p>请求没有超卖，超卖的是上限。</p><p>由于时间所限，暂未对服务内存占用分析及skywalking等agent内存占用分析，等有时间再续。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;问题的一种可能&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="Java" scheme="https://jiankunking.com/categories/java/"/>
    
    
    <category term="Java" scheme="https://jiankunking.com/tags/java/"/>
    
    <category term="Client" scheme="https://jiankunking.com/tags/client/"/>
    
    <category term="RocketMQ" scheme="https://jiankunking.com/tags/rocketmq/"/>
    
  </entry>
  
</feed>
